{
    "data": [
      {
        "paper_name": "A Multilingual Multi_Target Dataset for Stance Detection",
        "section": 0,
        "paragraph_id": 0,
        "full_text": "Abstract: We extract a large-scale stance detection dataset from comments written by candidates of elections in Switzerland.The dataset consists of German, French and Italian text, allowing for a cross-lingual evaluation of stance detection.It contains 67 000 comments on more than 150 political issues (targets).Unlike stance detection models that have specific target issues, we use the dataset to train a single model on all the issues.To make learning across targets possible, we prepend to each instance a natural question that represents the target (e.g.\"Do you support X?\").Baseline results from multilingual BERT show that zero-shot crosslingual and cross-target transfer of stance detection is moderately successful with this approach.",
        "sentences": [
          {
            "text": "Abstract: We extract a large-scale stance detection dataset from comments written by candidates of elections in Switzerland.",
            "label": 1
          },
          {
            "text": "The dataset consists of German, French and Italian text, allowing for a cross-lingual evaluation of stance detection.",
            "label": 1
          },
          {
            "text": "It contains 67 000 comments on more than 150 political issues (targets).",
            "label": 1
          },
          {
            "text": "Unlike stance detection models that have specific target issues, we use the dataset to train a single model on all the issues.",
            "label": 0
          },
          {
            "text": "To make learning across targets possible, we prepend to each instance a natural question that represents the target (e.g.\"Do you support X?\").",
            "label": 1
          },
          {
            "text": "Baseline results from multilingual BERT show that zero-shot crosslingual and cross-target transfer of stance detection is moderately successful with this approach.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "A Multilingual Multi_Target Dataset for Stance Detection",
        "section": 1,
        "paragraph_id": 1,
        "full_text": "In recent years many datasets have been created for the task of automated stance detection, advancing natural language understanding systems for political science, opinion research and other application areas.Typically, such benchmarks(Mohammad et al., 2016a)are composed of short pieces of text commenting on politicians or public issues and are manually annotated with their stance towards a target entity (e.g.Climate Change, or Trump).However, they are limited in scope on multiple levels(Küçük and Can, 2020).",
        "sentences": [
          {
            "text": "In recent years many datasets have been created for the task of automated stance detection, advancing natural language understanding systems for political science, opinion research and other application areas.",
            "label": 0
          },
          {
            "text": "Typically, such benchmarks(Mohammad et al., 2016a)are composed of short pieces of text commenting on politicians or public issues and are manually annotated with their stance towards a target entity (e.g.Climate Change, or Trump).",
            "label": 0
          },
          {
            "text": "However, they are limited in scope on multiple levels(Küçük and Can, 2020).",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "A Multilingual Multi_Target Dataset for Stance Detection",
        "section": 2,
        "paragraph_id": 2,
        "full_text": "First of all, it is questionable how well current stance detection methods perform in a crosslingual setting, as the multilingual datasets avail-able today are relatively small, and specific to a single target(Taulé et al., 2017(Taulé et al., , 2018)).Furthermore, specific models tend to be developed for each single target or pair of targets(Sobhani et al., 2017).Concerns have been raised that cross-target performance is often considerably lower than fully supervised performance(Küçük and Can, 2020).",
        "sentences": [
          {
            "text": "First of all, it is questionable how well current stance detection methods perform in a crosslingual setting, as the multilingual datasets avail-able today are relatively small, and specific to a single target(Taulé et al., 2017(Taulé et al., , 2018)).",
            "label": 0
          },
          {
            "text": "Furthermore, specific models tend to be developed for each single target or pair of targets(Sobhani et al., 2017).",
            "label": 0
          },
          {
            "text": "Concerns have been raised that cross-target performance is often considerably lower than fully supervised performance(Küçük and Can, 2020).",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "A Multilingual Multi_Target Dataset for Stance Detection",
        "section": 3,
        "paragraph_id": 3,
        "full_text": "In this paper we propose a much larger dataset that combines multilinguality and a multitude of topics and targets.X-stance comprises more than 150 questions about Swiss politics and more than 67k answers given by candidates running for political office in Switzerland.Questions are available in four languages: English, Swiss Standard German, French, and Italian.The language of a comment depends on the candidate's region of originIn this paper we propose a much larger dataset that combines multilinguality and a multitude of topics and targets.X-stance comprises more than 150 questions about Swiss politics and more than 67k answers given by candidates running for political office in Switzerland.Questions are available in four languages: English, Swiss Standard German, French, and Italian.The language of a comment depends on the candidate's region of origin.",
        "sentences": [
          {
            "text": "In this paper we propose a much larger dataset that combines multilinguality and a multitude of topics and targets.",
            "label": 1
          },
          {
            "text": "X-stance comprises more than 150 questions about Swiss politics and more than 67k answers given by candidates running for political office in Switzerland.",
            "label": 1
          },
          {
            "text": "Questions are available in four languages: English, Swiss Standard German, French, and Italian.",
            "label": 1
          },
          {
            "text": "The language of a comment depends on the candidate's region of origin.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "A Multilingual Multi_Target Dataset for Stance Detection",
        "section": 4,
        "paragraph_id": 4,
        "full_text": "We have extracted the data from the voting advice application Smartvote.Candidates respond to questions mainly in categorical form (yes / rather yes / rather no / no).They can also submit a freetext comment to justify or explain their categorical answer.An example is given in Figure1.",
        "sentences": [
          {
            "text": "We have extracted the data from the voting advice application Smartvote.",
            "label": 1
          },
          {
            "text": "Candidates respond to questions mainly in categorical form (yes / rather yes / rather no / no).",
            "label": 1
          },
          {
            "text": "They can also submit a freetext comment to justify or explain their categorical answer.",
            "label": 1
          },
          {
            "text": "An example is given in Figure1.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "A Multilingual Multi_Target Dataset for Stance Detection",
        "section": 5,
        "paragraph_id": 5,
        "full_text": "We transform the dataset into a stance detection task by interpreting the question as a naturallanguage representation of the target, and the commentary as the input to be classified.",
        "sentences": [
          {
            "text": "We transform the dataset into a stance detection task by interpreting the question as a naturallanguage representation of the target, and the commentary as the input to be classified.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "A Multilingual Multi_Target Dataset for Stance Detection",
        "section": 6,
        "paragraph_id": 6,
        "full_text": "The dataset is split into a multilingual training set and into several test sets to evaluate zeroshot cross-lingual and cross-target transfer.To provide a baseline, we fine-tune a multilingual BERT model(Devlin et al., 2019)on X-stance.We show that the baseline accuracy is comparable to previous stance detection benchmarks while leaving ample room for improvement.In addition, the model can generalize to a degree both crosslingually and in a cross-target setting.",
        "sentences": [
          {
            "text": "The dataset is split into a multilingual training set and into several test sets to evaluate zeroshot cross-lingual and cross-target transfer.",
            "label": 1
          },
          {
            "text": "To provide a baseline, we fine-tune a multilingual BERT model(Devlin et al., 2019)on X-stance.",
            "label": 0
          },
          {
            "text": "We show that the baseline accuracy is comparable to previous stance detection benchmarks while leaving ample room for improvement.",
            "label": 0
          },
          {
            "text": "In addition, the model can generalize to a degree both crosslingually and in a cross-target setting.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "A Multilingual Multi_Target Dataset for Stance Detection",
        "section": 7,
        "paragraph_id": 7,
        "full_text": "We have made the dataset and the code for reproducing the baseline models publicly available.Figure1: Example of a question and two answers in the X-stance dataset.The answers were submitted by electoral candidates on a voting advice website.The author of the German comment was in favor of the issue; the author of the French comment against.Both authors use comments to explain their respective stance.",
        "sentences": [
          {
            "text": "We have made the dataset and the code for reproducing the baseline models publicly available.",
            "label": 1
          },
          {
            "text": "Figure1: Example of a question and two answers in the X-stance dataset.",
            "label": 0
          },
          {
            "text": "The answers were submitted by electoral candidates on a voting advice website.",
            "label": 1
          },
          {
            "text": "The author of the German comment was in favor of the issue; the author of the French comment against.",
            "label": 1
          },
          {
            "text": "Both authors use comments to explain their respective stance.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "A Multilingual Multi_Target Dataset for Stance Detection",
        "section": 8,
        "paragraph_id": 8,
        "full_text": "Provenance We downloaded the questions and answers via the Smartvote API 2 .The downloaded data cover 175 communal, cantonal and national elections between 2011 and 2020.",
        "sentences": [
          {
            "text": "Provenance We downloaded the questions and answers via the Smartvote API 2 .",
            "label": 1
          },
          {
            "text": "The downloaded data cover 175 communal, cantonal and national elections between 2011 and 2020.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "A Multilingual Multi_Target Dataset for Stance Detection",
        "section": 9,
        "paragraph_id": 9,
        "full_text": "All candidates in an election who participate in Smartvote are asked the same set of questions, but 2 https://smartvote.chdepending on the locale they see translated versions of the questions.They can answer each question with either 'yes', 'rather yes', 'rather no', or 'no'.They can supplement each answer with a comment of at most 500 characters.",
        "sentences": [
          {
            "text": "All candidates in an election who participate in Smartvote are asked the same set of questions, but 2 https://smartvote.chdepending on the locale they see translated versions of the questions.",
            "label": 1
          },
          {
            "text": "They can answer each question with either 'yes', 'rather yes', 'rather no', or 'no'.",
            "label": 1
          },
          {
            "text": "They can supplement each answer with a comment of at most 500 characters.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "A Multilingual Multi_Target Dataset for Stance Detection",
        "section": 10,
        "paragraph_id": 10,
        "full_text": "The questions asked on Smartvote have been edited by a team of political scientists.They are intended to cover a broad range of political issues relevant at the time of the election.A detailed documentation of the design of Smartvote and the editing process of the questions is provided byThurman and Gasser (2009).",
        "sentences": [
          {
            "text": "The questions asked on Smartvote have been edited by a team of political scientists.",
            "label": 1
          },
          {
            "text": "They are intended to cover a broad range of political issues relevant at the time of the election.",
            "label": 1
          },
          {
            "text": "A detailed documentation of the design of Smartvote and the editing process of the questions is provided byThurman and Gasser (2009).",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "A Multilingual Multi_Target Dataset for Stance Detection",
        "section": 11,
        "paragraph_id": 11,
        "full_text": "Preprocessing We merged the two labels on each pole into a single label: 'yes' and 'rather yes' were combined into 'favor'; 'rather no', or 'no' into 'against'.This improves the consistency of the data and the comparability to previous stance detection datasets.We did not further preprocess the text of the comments.",
        "sentences": [
          {
            "text": "Preprocessing We merged the two labels on each pole into a single label: 'yes' and 'rather yes' were combined into 'favor'; 'rather no', or 'no' into 'against'.",
            "label": 1
          },
          {
            "text": "This improves the consistency of the data and the comparability to previous stance detection datasets.",
            "label": 1
          },
          {
            "text": "We did not further preprocess the text of the comments.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "A Multilingual Multi_Target Dataset for Stance Detection",
        "section": 12,
        "paragraph_id": 12,
        "full_text": "Language Identification As the API does not provide the language of comments, we employed a language identifier to automatically annotate this information.We used the langdetect library(Shuyo, 2010).For each responder we classified all the comments jointly, assuming that responders did not switch code during the answering of the questionnaire.",
        "sentences": [
          {
            "text": "Language Identification As the API does not provide the language of comments, we employed a language identifier to automatically annotate this information.",
            "label": 1
          },
          {
            "text": "We used the langdetect library(Shuyo, 2010).",
            "label": 1
          },
          {
            "text": "For each responder we classified all the comments jointly, assuming that responders did not switch code during the answering of the questionnaire.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "A Multilingual Multi_Target Dataset for Stance Detection",
        "section": 13,
        "paragraph_id": 13,
        "full_text": "We applied the identifier in a two-step approach.In the first run we allowed the identifier to output all 55 languages that it supports out of the box, plus Romansh, the fourth official language in Switzerland3.We found that no Romansh comments were detected and that all unexpected outputs were misclassifications of German, French or Italian comments.We further concluded that little or no Swiss German comments are in the dataset; otherwise, some of them would have manifested themselves via misclassifications (e.g. as Dutch).",
        "sentences": [
          {
            "text": "We applied the identifier in a two-step approach.",
            "label": 1
          },
          {
            "text": "In the first run we allowed the identifier to output all 55 languages that it supports out of the box, plus Romansh, the fourth official language in Switzerland3.",
            "label": 1
          },
          {
            "text": "We found that no Romansh comments were detected and that all unexpected outputs were misclassifications of German, French or Italian comments.",
            "label": 1
          },
          {
            "text": "We further concluded that little or no Swiss German comments are in the dataset; otherwise, some of them would have manifested themselves via misclassifications (e.g. as Dutch).",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "A Multilingual Multi_Target Dataset for Stance Detection",
        "section": 14,
        "paragraph_id": 14,
        "full_text": "In the second run, drawing from these conclusions, we restricted the identifier's set of choices to English, French, German and Italian.",
        "sentences": [
          {
            "text": "In the second run, drawing from these conclusions, we restricted the identifier's set of choices to English, French, German and Italian.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "A Multilingual Multi_Target Dataset for Stance Detection",
        "section": 15,
        "paragraph_id": 15,
        "full_text": "Filtering We pre-filtered the questions and answers to improve the quality of the dataset.In the right column the model encounters unseen answers to unseen questions within an unseen topic.",
        "sentences": [
          {
            "text": "Filtering We pre-filtered the questions and answers to improve the quality of the dataset.",
            "label": 1
          },
          {
            "text": "In the right column the model encounters unseen answers to unseen questions within an unseen topic.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "A Multilingual Multi_Target Dataset for Stance Detection",
        "section": 16,
        "paragraph_id": 16,
        "full_text": "The two test sets in parentheses are too small for a significant evaluation. questions and corresponding answers pertaining to national elections were included.",
        "sentences": [
          {
            "text": "The two test sets in parentheses are too small for a significant evaluation. questions and corresponding answers pertaining to national elections were included.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "A Multilingual Multi_Target Dataset for Stance Detection",
        "section": 17,
        "paragraph_id": 17,
        "full_text": "In the context of communal and cantonal elections, candidates have answered both local questions and a subset of the national questions.Of those elections, we only considered answers to the questions that also had been asked in a national election.They were only used to augment the training set while the validation and test sets were restricted to answers from national elections.",
        "sentences": [
          {
            "text": "In the context of communal and cantonal elections, candidates have answered both local questions and a subset of the national questions.",
            "label": 1
          },
          {
            "text": "Of those elections, we only considered answers to the questions that also had been asked in a national election.",
            "label": 1
          },
          {
            "text": "They were only used to augment the training set while the validation and test sets were restricted to answers from national elections.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "A Multilingual Multi_Target Dataset for Stance Detection",
        "section": 18,
        "paragraph_id": 18,
        "full_text": "We discarded the fewer than 20 comments classified as English.Furthermore, we discarded instances that met any of the following conditions: • Question is not a closed question or does not address a clearly defined political issue. • No comment was submitted by the candidate or the comment is shorter than 50 characters. • Comment starts with \"but\" or a similar indicator that the comment is not self-contained. • Comment contains a URL. In total, a fifth of the comments were filtered out.",
        "sentences": [
          {
            "text": "We discarded the fewer than 20 comments classified as English.",
            "label": 1
          },
          {
            "text": "Furthermore, we discarded instances that met any of the following conditions: • Question is not a closed question or does not address a clearly defined political issue.",
            "label": 1
          },
          {
            "text": "• No comment was submitted by the candidate or the comment is shorter than 50 characters.",
            "label": 1
          },
          {
            "text": "• Comment starts with \"but\" or a similar indicator that the comment is not self-contained.",
            "label": 1
          },
          {
            "text": "• Comment contains a URL.",
            "label": 1
          },
          {
            "text": "In total, a fifth of the comments were filtered out.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "A Multilingual Multi_Target Dataset for Stance Detection",
        "section": 19,
        "paragraph_id": 19,
        "full_text": "Topics The questions have been organized by the Smartvote editors into categories (such as \"Economy\").We further consolidated the predefined categories into 12 broad topics (Table1).",
        "sentences": [
          {
            "text": "Topics The questions have been organized by the Smartvote editors into categories (such as \"Economy\").",
            "label": 1
          },
          {
            "text": "We further consolidated the predefined categories into 12 broad topics (Table1).",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "A Multilingual Multi_Target Dataset for Stance Detection",
        "section": 20,
        "paragraph_id": 20,
        "full_text": "Compliance The dataset is shared under a CC BY-NC 4.0 license.Copyright remains with www.smartvote.ch.",
        "sentences": [
          {
            "text": "Compliance The dataset is shared under a CC BY-NC 4.0 license.",
            "label": 1
          },
          {
            "text": "Copyright remains with www.smartvote.ch.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "A Multilingual Multi_Target Dataset for Stance Detection",
        "section": 21,
        "paragraph_id": 21,
        "full_text": "Given the sensitive nature of the data, we increase the anonymity of the data by hashing the respondents' IDs.No personal attributes of the respondents are included in the dataset.We provide a data statement(Bender and Friedman, 2018)in Appendix B.",
        "sentences": [
          {
            "text": "Given the sensitive nature of the data, we increase the anonymity of the data by hashing the respondents' IDs.",
            "label": 1
          },
          {
            "text": "No personal attributes of the respondents are included in the dataset.",
            "label": 1
          },
          {
            "text": "We provide a data statement(Bender and Friedman, 2018)in Appendix B.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "A Multilingual Multi_Target Dataset for Stance Detection",
        "section": 22,
        "paragraph_id": 22,
        "full_text": "We held out the topics \"Healthcare\" and \"Political System\" from the training data and created a separate cross-topic test set that contains the questions and answers related to those topics.",
        "sentences": [
          {
            "text": "We held out the topics \"Healthcare\" and \"Political System\" from the training data and created a separate cross-topic test set that contains the questions and answers related to those topics.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "A Multilingual Multi_Target Dataset for Stance Detection",
        "section": 23,
        "paragraph_id": 23,
        "full_text": "Furthermore, in order to test cross-question generalization performance within previously seen topics, we manually selected 16 held-out questions that are distributed over the remaining 10 topics.We selected the held-out questions manually because we wanted to make sure that they are truly unseen and that no paraphrases of the questions are found in the training set.",
        "sentences": [
          {
            "text": "Furthermore, in order to test cross-question generalization performance within previously seen topics, we manually selected 16 held-out questions that are distributed over the remaining 10 topics.",
            "label": 0
          },
          {
            "text": "We selected the held-out questions manually because we wanted to make sure that they are truly unseen and that no paraphrases of the questions are found in the training set.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "A Multilingual Multi_Target Dataset for Stance Detection",
        "section": 24,
        "paragraph_id": 24,
        "full_text": "We designated Italian as a test-only language, since relatively few comments have been written in Italian.From the remaining German and French data we randomly selected a percentage of respondents as validation or as test respondents.",
        "sentences": [
          {
            "text": "We designated Italian as a test-only language, since relatively few comments have been written in Italian.",
            "label": 0
          },
          {
            "text": "From the remaining German and French data we randomly selected a percentage of respondents as validation or as test respondents.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "A Multilingual Multi_Target Dataset for Stance Detection",
        "section": 25,
        "paragraph_id": 25,
        "full_text": "As a result we received one training set, one validation set and four test sets.The sizes of the sets are listed in Table 2.We did not consider test sets that are cross-lingual and cross-target at the same time, as they would have been too small to yield significant results.",
        "sentences": [
          {
            "text": "As a result we received one training set, one validation set and four test sets.",
            "label": 0
          },
          {
            "text": "The sizes of the sets are listed in Table 2.",
            "label": 0
          },
          {
            "text": "We did not consider test sets that are cross-lingual and cross-target at the same time, as they would have been too small to yield significant results.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "A Multilingual Multi_Target Dataset for Stance Detection",
        "section": 26,
        "paragraph_id": 26,
        "full_text": "We evaluate four baselines to obtain an impression of the difficulty of the task.",
        "sentences": [
          {
            "text": "We evaluate four baselines to obtain an impression of the difficulty of the task.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "A Multilingual Multi_Target Dataset for Stance Detection",
        "section": 27,
        "paragraph_id": 27,
        "full_text": "The first pair of baselines uses the most frequent class in the training set for prediction.Specifically, the global majority class baseline predicts the most frequent class across all training targets while the target-wise majority class baseline predicts the class that is most frequent for a given target question.The latter can only be applied to the intra-target test sets.",
        "sentences": [
          {
            "text": "The first pair of baselines uses the most frequent class in the training set for prediction.",
            "label": 0
          },
          {
            "text": "Specifically, the global majority class baseline predicts the most frequent class across all training targets while the target-wise majority class baseline predicts the class that is most frequent for a given target question.",
            "label": 0
          },
          {
            "text": "The latter can only be applied to the intra-target test sets.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "A Multilingual Multi_Target Dataset for Stance Detection",
        "section": 28,
        "paragraph_id": 28,
        "full_text": "As a second baseline, we train a fastText bag-ofwords linear classifier(Joulin et al., 2017).For each comment, we select the translation of the question that matches its language, and concatenate it to the comment.We tokenize the text using the Europarl preprocessing tools(Koehn, 2005).",
        "sentences": [
          {
            "text": "As a second baseline, we train a fastText bag-ofwords linear classifier(Joulin et al., 2017).",
            "label": 0
          },
          {
            "text": "For each comment, we select the translation of the question that matches its language, and concatenate it to the comment.",
            "label": 0
          },
          {
            "text": "We tokenize the text using the Europarl preprocessing tools(Koehn, 2005).",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "A Multilingual Multi_Target Dataset for Stance Detection",
        "section": 29,
        "paragraph_id": 29,
        "full_text": "The 'against' class was slightly upsampled in the training data so that the classes are balanced when summing over all questions and topics.We use the standard settings provided by the fastText library. 4 Optimal hyperparameters from the following range were determined based on the validation accuracy: • Learning rate: 0.1, 0.2, 1 • Number of epochs: 5, 50",
        "sentences": [
          {
            "text": "The 'against' class was slightly upsampled in the training data so that the classes are balanced when summing over all questions and topics.",
            "label": 0
          },
          {
            "text": "We use the standard settings provided by the fastText library.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "A Multilingual Multi_Target Dataset for Stance Detection",
        "section": 30,
        "paragraph_id": 30,
        "full_text": "The word vectors were set to a size of 300.We do not initialize them with pre-trained multilingual embeddings since preliminary experiments did not show a beneficial effect.",
        "sentences": [
          {
            "text": "The word vectors were set to a size of 300.",
            "label": 0
          },
          {
            "text": "We do not initialize them with pre-trained multilingual embeddings since preliminary experiments did not show a beneficial effect.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "A Multilingual Multi_Target Dataset for Stance Detection",
        "section": 31,
        "paragraph_id": 31,
        "full_text": "As our main baseline model we fine-tune multilingual BERT (M-BERT) on the task(Devlin et al., 2019)which has been pre-trained jointly in 104 languages 5 and has established itself as a state of the art for various multilingual tasks(Wu and Dredze, 2019;Pires et al., 2019).Within the field of stance detection, BERT can outperform both feature-based and other neural approaches in a monolingual English setting(Ghosh et al., 2019).",
        "sentences": [
          {
            "text": "As our main baseline model we fine-tune multilingual BERT (M-BERT) on the task(Devlin et al., 2019)which has been pre-trained jointly in 104 languages 5 and has established itself as a state of the art for various multilingual tasks(Wu and Dredze, 2019;Pires et al., 2019).",
            "label": 0
          },
          {
            "text": "Within the field of stance detection, BERT can outperform both feature-based and other neural approaches in a monolingual English setting(Ghosh et al., 2019).",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "A Multilingual Multi_Target Dataset for Stance Detection",
        "section": 32,
        "paragraph_id": 32,
        "full_text": "Architecture In the context of BERT we interpret the X-stance task as sequence pair classification inspired by natural language inference tasks(Bowman et al., 2015).We follow the procedure outlined byDevlin et al. (2019)for such tasks.We designate the question as segment A and the comment as segment B. The two segments are separated with the special token [SEP], and the special token [CLS] is prepended to the sequence.The final hidden state corresponding to [CLS] is then classified by a linear layer.",
        "sentences": [
          {
            "text": "Architecture In the context of BERT we interpret the X-stance task as sequence pair classification inspired by natural language inference tasks(Bowman et al., 2015).",
            "label": 0
          },
          {
            "text": "We follow the procedure outlined byDevlin et al. (2019)for such tasks.",
            "label": 0
          },
          {
            "text": "We designate the question as segment A and the comment as segment B.",
            "label": 0
          },
          {
            "text": "The two segments are separated with the special token [SEP], and the special token [CLS] is prepended to the sequence.",
            "label": 0
          },
          {
            "text": "The final hidden state corresponding to [CLS] is then classified by a linear layer.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "A Multilingual Multi_Target Dataset for Stance Detection",
        "section": 33,
        "paragraph_id": 33,
        "full_text": "We fine-tune the full model with a cross-entropy loss, using the AllenNLP library(Gardner et al., 2018)as a basis for our implementation.",
        "sentences": [
          {
            "text": "We fine-tune the full model with a cross-entropy loss, using the AllenNLP library(Gardner et al., 2018)as a basis for our implementation.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "A Multilingual Multi_Target Dataset for Stance Detection",
        "section": 34,
        "paragraph_id": 34,
        "full_text": "Training As above, we balanced out the number of classes in the training set.We use a batch size of 16 and a maximum sequence length of 512 subwords, and performed a grid search over the following hyperparameters based on the validation accuracy: • Learning rate: 5e-5, 3e-5, 2e-5 No Italian samples were seen during training, making this a case of zero-shot cross-lingual transfer.The scores are reported as the macro-average of the F1scores for 'favor' and for 'against'.",
        "sentences": [
          {
            "text": "Training As above, we balanced out the number of classes in the training set.",
            "label": 0
          },
          {
            "text": "We use a batch size of 16 and a maximum sequence length of 512 subwords, and performed a grid search over the following hyperparameters based on the validation accuracy: • Learning rate: 5e-5, 3e-5, 2e-5 No Italian samples were seen during training, making this a case of zero-shot cross-lingual transfer.",
            "label": 0
          },
          {
            "text": "The scores are reported as the macro-average of the F1scores for 'favor' and for 'against'.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "A Multilingual Multi_Target Dataset for Stance Detection",
        "section": 35,
        "paragraph_id": 35,
        "full_text": "The grid search was repeated independently for every variant that we test in the following subsections.Furthermore, the standard recommendations for fine-tuning BERT were used: Adam with β 1 = 0.9 and β 2 = 0.999; an L2 weight decay of 0.01; a learning rate warmup over the first 10% of the steps; and a linear decay of the learning rate.A dropout probability of 0.1 was set on all layers.",
        "sentences": [
          {
            "text": "The grid search was repeated independently for every variant that we test in the following subsections.",
            "label": 0
          },
          {
            "text": "Furthermore, the standard recommendations for fine-tuning BERT were used: Adam with β 1 = 0.9 and β 2 = 0.999; an L2 weight decay of 0.01; a learning rate warmup over the first 10% of the steps; and a linear decay of the learning rate.",
            "label": 0
          },
          {
            "text": "A dropout probability of 0.1 was set on all layers.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "A Multilingual Multi_Target Dataset for Stance Detection",
        "section": 36,
        "paragraph_id": 36,
        "full_text": "Results Table3shows the results for the crosslingual setting.M-BERT performs consistently better than the previous baselines.Even the zeroshot performance in Italian, while significantly lower than the supervised scores, is much better than the target-wise majority class baseline.",
        "sentences": [
          {
            "text": "Results Table3shows the results for the crosslingual setting.",
            "label": 0
          },
          {
            "text": "M-BERT performs consistently better than the previous baselines.",
            "label": 0
          },
          {
            "text": "Even the zeroshot performance in Italian, while significantly lower than the supervised scores, is much better than the target-wise majority class baseline.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "A Multilingual Multi_Target Dataset for Stance Detection",
        "section": 37,
        "paragraph_id": 37,
        "full_text": "Results for the cross-target setting are given in Table4. Similar to the cross-lingual setting, model performance drops in the cross-target setting, but M-BERT remains the strongest baseline and easily surpasses the majority class baselines.Furthermore, the cross-question score of M-BERT is slightly lower than the cross-topic score.",
        "sentences": [
          {
            "text": "Results for the cross-target setting are given in Table4.",
            "label": 0
          },
          {
            "text": "Similar to the cross-lingual setting, model performance drops in the cross-target setting, but M-BERT remains the strongest baseline and easily surpasses the majority class baselines.",
            "label": 0
          },
          {
            "text": "Furthermore, the cross-question score of M-BERT is slightly lower than the cross-topic score.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "A Multilingual Multi_Target Dataset for Stance Detection",
        "section": 38,
        "paragraph_id": 38,
        "full_text": "The default setup preserves horizontal language consistency in that the language of the questions always corresponds to the language of the comments.For example, the Italian test instances are combined with the Italian version of the questions, even though during training the model has only ever seen the German and French version of them.",
        "sentences": [
          {
            "text": "The default setup preserves horizontal language consistency in that the language of the questions always corresponds to the language of the comments.",
            "label": 0
          },
          {
            "text": "For example, the Italian test instances are combined with the Italian version of the questions, even though during training the model has only ever seen the German and French version of them.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "A Multilingual Multi_Target Dataset for Stance Detection",
        "section": 39,
        "paragraph_id": 39,
        "full_text": "An alternative concept is vertical language consistency, whereby the questions are consistently presented in one language, regardless of the comment.To test whether horizontal or vertical consistency is more helpful, we train and evaluate M-BERT on a dataset variant where all questions are in their English version.We chose English as a lingua franca because it had the largest share of data during the pre-training of M-BERT.",
        "sentences": [
          {
            "text": "An alternative concept is vertical language consistency, whereby the questions are consistently presented in one language, regardless of the comment.",
            "label": 0
          },
          {
            "text": "To test whether horizontal or vertical consistency is more helpful, we train and evaluate M-BERT on a dataset variant where all questions are in their English version.",
            "label": 0
          },
          {
            "text": "We chose English as a lingua franca because it had the largest share of data during the pre-training of M-BERT.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "A Multilingual Multi_Target Dataset for Stance Detection",
        "section": 40,
        "paragraph_id": 40,
        "full_text": "Cross-topic Results are shown in Table5.While the effect is negligible in most settings, cross-lingual performance increases when all questions are in English.",
        "sentences": [
          {
            "text": "Cross-topic Results are shown in Table5.",
            "label": 0
          },
          {
            "text": "While the effect is negligible in most settings, cross-lingual performance increases when all questions are in English.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "A Multilingual Multi_Target Dataset for Stance Detection",
        "section": 41,
        "paragraph_id": 41,
        "full_text": "In order to rule out that only the questions or only the comments are necessary to optimally solve the task, we conduct some additional experiments: • Only use a single segment containing the comment, removing the questions from the training and test data (missing questions). • Only use the question and remove the comment (missing comments).",
        "sentences": [
          {
            "text": "In order to rule out that only the questions or only the comments are necessary to optimally solve the task, we conduct some additional experiments: • Only use a single segment containing the comment, removing the questions from the training and test data (missing questions).",
            "label": 0
          },
          {
            "text": "• Only use the question and remove the comment (missing comments).",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "A Multilingual Multi_Target Dataset for Stance Detection",
        "section": 42,
        "paragraph_id": 42,
        "full_text": "In both cases the performance decreases across all evaluation settings (Table5).The loss in performance is much higher when comments are missing, indicating that the comments contain the most important information about stance.As can be expected, the score achieved without comments is only slightly different from the target-wise majority class baseline.",
        "sentences": [
          {
            "text": "In both cases the performance decreases across all evaluation settings (Table5).",
            "label": 0
          },
          {
            "text": "The loss in performance is much higher when comments are missing, indicating that the comments contain the most important information about stance.",
            "label": 0
          },
          {
            "text": "As can be expected, the score achieved without comments is only slightly different from the target-wise majority class baseline.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "A Multilingual Multi_Target Dataset for Stance Detection",
        "section": 43,
        "paragraph_id": 43,
        "full_text": "But there is also a loss in performance when the questions are missing, which underlines the importance of pairing both pieces of text.The effect of missing questions is especially strong in the supervised and cross-lingual settings.To illustrate this, we provide in TableA8some examples of comments that occur with multiple different targets in the training set.Those examples can explain why the target can be essential for disambiguating a stance detection problem.On the other hand, the effect of omitting the questions is less pronounced in the cross-target settings.",
        "sentences": [
          {
            "text": "But there is also a loss in performance when the questions are missing, which underlines the importance of pairing both pieces of text.",
            "label": 0
          },
          {
            "text": "The effect of missing questions is especially strong in the supervised and cross-lingual settings.",
            "label": 0
          },
          {
            "text": "To illustrate this, we provide in TableA8some examples of comments that occur with multiple different targets in the training set.",
            "label": 0
          },
          {
            "text": "Those examples can explain why the target can be essential for disambiguating a stance detection problem.",
            "label": 0
          },
          {
            "text": "On the other hand, the effect of omitting the questions is less pronounced in the cross-target settings.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "A Multilingual Multi_Target Dataset for Stance Detection",
        "section": 44,
        "paragraph_id": 44,
        "full_text": "The above single-segment experiments tell us that both the comment and the question provide crucial information.But it is possible that the M-BERT model, even though trained on both segments, mainly looks at a single segment at test time.To rule this out, we probe the model with randomized data at test time: • Test the model on versions of the test sets where the comments remain in place but the questions are shuffled randomly (random questions).We make sure that the random questions come from the same test set and language as the original questions. • Keep the questions in place and randomize the comments (random comments).Again we shuffle the comments only within test set boundaries.",
        "sentences": [
          {
            "text": "The above single-segment experiments tell us that both the comment and the question provide crucial information.",
            "label": 0
          },
          {
            "text": "But it is possible that the M-BERT model, even though trained on both segments, mainly looks at a single segment at test time.",
            "label": 0
          },
          {
            "text": "To rule this out, we probe the model with randomized data at test time: • Test the model on versions of the test sets where the comments remain in place but the questions are shuffled randomly (random questions).",
            "label": 0
          },
          {
            "text": "We make sure that the random questions come from the same test set and language as the original questions.",
            "label": 0
          },
          {
            "text": "• Keep the questions in place and randomize the comments (random comments).",
            "label": 0
          },
          {
            "text": "Again we shuffle the comments only within test set boundaries.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "A Multilingual Multi_Target Dataset for Stance Detection",
        "section": 45,
        "paragraph_id": 45,
        "full_text": "The results in Table5show that the performance of the model decreases in both cases, confirming that it learns to take into account both segments.",
        "sentences": [
          {
            "text": "The results in Table5show that the performance of the model decreases in both cases, confirming that it learns to take into account both segments.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "A Multilingual Multi_Target Dataset for Stance Detection",
        "section": 46,
        "paragraph_id": 46,
        "full_text": "4.6 How Important are Spelled-Out Targets? Finally we test whether the target really needs to be represented by natural language (e.g.\"Do you support X?\").An alternative is to represent the target with a trainable embedding instead.",
        "sentences": [
          {
            "text": "4.6 How Important are Spelled-Out Targets?",
            "label": 0
          },
          {
            "text": "Finally we test whether the target really needs to be represented by natural language (e.g.\"Do you support X?\").",
            "label": 0
          },
          {
            "text": "An alternative is to represent the target with a trainable embedding instead.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "A Multilingual Multi_Target Dataset for Stance Detection",
        "section": 47,
        "paragraph_id": 47,
        "full_text": "In order to fit target embeddings smoothly into our architecture, we represent each target type with a different reserved symbol from the M-BERT vocabulary.Segment A is then set to this symbol instead of a natural language question.",
        "sentences": [
          {
            "text": "In order to fit target embeddings smoothly into our architecture, we represent each target type with a different reserved symbol from the M-BERT vocabulary.",
            "label": 0
          },
          {
            "text": "Segment A is then set to this symbol instead of a natural language question.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "A Multilingual Multi_Target Dataset for Stance Detection",
        "section": 48,
        "paragraph_id": 48,
        "full_text": "The results for this experiment are listed in the bottom row of Table5.An M-BERT model that learns target embeddings instead of encoding a question performs clearly worse in the supervised and cross-lingual settings.From this we conclude that spelled-out natural language questions provide important linguistic detail that can help in stance detection.",
        "sentences": [
          {
            "text": "The results for this experiment are listed in the bottom row of Table5.",
            "label": 0
          },
          {
            "text": "An M-BERT model that learns target embeddings instead of encoding a question performs clearly worse in the supervised and cross-lingual settings.",
            "label": 0
          },
          {
            "text": "From this we conclude that spelled-out natural language questions provide important linguistic detail that can help in stance detection.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation",
        "section": 0,
        "paragraph_id": 0,
        "full_text": "Abstract: Digital humans have witnessed extensive applications in various domains, necessitating related quality assessment studies.However, there is a lack of comprehensive digital human quality assessment (DHQA) databases.To address this gap, we propose SJTU-H3D, a subjective quality assessment database specifically designed for full-body digital humans.It comprises 40 high-quality reference digital humans and 1,120 labeled distorted counterparts generated with seven types of distortions.The SJTU-H3D database can serve as a benchmark for DHQA research, allowing evaluation and refinement of processing algorithms.Further, we propose a zero-shot DHQA approach that focuses on no-reference (NR) scenarios to ensure generalization capabilities while mitigating database bias.Our method leverages semantic and distortion features extracted from projections, as well as geometry features derived from the mesh structure of digital humans.Specifically, we employ the Contrastive Language-Image Pre-training (CLIP) model to measure semantic affinity and incorporate the Naturalness Image Quality Evaluator (NIQE) model to capture low-level distortion information.Additionally, we utilize dihedral angles as geometry descriptors to extract mesh features.By aggregating these measures, we introduce the Digital Human Quality Index (DHQI), which demonstrates significant improvements in zeroshot performance.The DHQI can also serve as a robust baseline for DHQA tasks, facilitating advancements in the field.The database and the code are available at https://github.com/zzc-1998/SJTU-H3D.",
        "sentences": [
          {
            "text": "Abstract: Digital humans have witnessed extensive applications in various domains, necessitating related quality assessment studies.",
            "label": 0
          },
          {
            "text": "However, there is a lack of comprehensive digital human quality assessment (DHQA) databases.",
            "label": 0
          },
          {
            "text": "To address this gap, we propose SJTU-H3D, a subjective quality assessment database specifically designed for full-body digital humans.",
            "label": 1
          },
          {
            "text": "It comprises 40 high-quality reference digital humans and 1,120 labeled distorted counterparts generated with seven types of distortions.",
            "label": 1
          },
          {
            "text": "The SJTU-H3D database can serve as a benchmark for DHQA research, allowing evaluation and refinement of processing algorithms.",
            "label": 1
          },
          {
            "text": "Further, we propose a zero-shot DHQA approach that focuses on no-reference (NR) scenarios to ensure generalization capabilities while mitigating database bias.",
            "label": 0
          },
          {
            "text": "Our method leverages semantic and distortion features extracted from projections, as well as geometry features derived from the mesh structure of digital humans.",
            "label": 0
          },
          {
            "text": "Specifically, we employ the Contrastive Language-Image Pre-training (CLIP) model to measure semantic affinity and incorporate the Naturalness Image Quality Evaluator (NIQE) model to capture low-level distortion information.",
            "label": 0
          },
          {
            "text": "Additionally, we utilize dihedral angles as geometry descriptors to extract mesh features.",
            "label": 0
          },
          {
            "text": "By aggregating these measures, we introduce the Digital Human Quality Index (DHQI), which demonstrates significant improvements in zeroshot performance.",
            "label": 1
          },
          {
            "text": "The DHQI can also serve as a robust baseline for DHQA tasks, facilitating advancements in the field.",
            "label": 0
          },
          {
            "text": "The database and the code are available at https://github.com/zzc-1998/SJTU-H3D.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation",
        "section": 1,
        "paragraph_id": 1,
        "full_text": "Fig.1.Motovation of our works.Unlike 2D images/videos, collection of 3D digital humans is more difficult and expensive.Therefore there is a lack of subjective databases for 3D digital humans currently.To tackle this issue, we propose the first perceptual quality assessment database for full-body digital humans (SJTU-H3D).Furthermore, in contrast to the numerous large-scale image/video quality assessment (I/VQA) databases that facilitate data-driven methodologies, supervised methods can be easily bothered by the bias of the limited number of DHQA databases, affecting generalization ability.Thus we propose a zero-shot no-reference quality assessment method to address this concern. humans.Regrettably, acquisition of digital human models is a laborious and costly process compared with 2D media such as images and videos, requiring specialized three-dimensional (3D) scanning devices and professional post-production, which makes it quite difficult to carry out digital human quality assessment (DHQA) databases.Therefore, few works about subjective DHQA have been carried out in the literature.Then the absence of large-scale subjective experiments for assessing the visual quality of digital humans further hinders progress in this domain.",
        "sentences": [
          {
            "text": "Fig.1.Motovation of our works.",
            "label": 0
          },
          {
            "text": "Unlike 2D images/videos, collection of 3D digital humans is more difficult and expensive.",
            "label": 0
          },
          {
            "text": "Therefore there is a lack of subjective databases for 3D digital humans currently.",
            "label": 0
          },
          {
            "text": "To tackle this issue, we propose the first perceptual quality assessment database for full-body digital humans (SJTU-H3D).",
            "label": 1
          },
          {
            "text": "Furthermore, in contrast to the numerous large-scale image/video quality assessment (I/VQA) databases that facilitate data-driven methodologies, supervised methods can be easily bothered by the bias of the limited number of DHQA databases, affecting generalization ability.",
            "label": 0
          },
          {
            "text": "Thus we propose a zero-shot no-reference quality assessment method to address this concern. humans.",
            "label": 0
          },
          {
            "text": "Regrettably, acquisition of digital human models is a laborious and costly process compared with 2D media such as images and videos, requiring specialized three-dimensional (3D) scanning devices and professional post-production, which makes it quite difficult to carry out digital human quality assessment (DHQA) databases.",
            "label": 0
          },
          {
            "text": "Therefore, few works about subjective DHQA have been carried out in the literature.",
            "label": 0
          },
          {
            "text": "Then the absence of large-scale subjective experiments for assessing the visual quality of digital humans further hinders progress in this domain.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation",
        "section": 2,
        "paragraph_id": 2,
        "full_text": "Therefore, in this paper, we propose a comprehensive subjective quality assessment database (SJTU-H3D) targeted at digital humans, aiming to address this research gap and contribute to the advancement of DHQA.The SJTU-H3D database introduced in this study comprises 40 high-quality reference digital humans, represented by textured meshes in full-body format, and the database includes 1,120 distorted digital humans that have been generated using seven different types of distortions.The perceptual mean opinion scores (MOSs) of these distorted digital humans are collected through a meticulously controlled subjective experiment.Notably, the SJTU-H3D database is the first large-scale database specifically designed for digital human quality assessment (DHQA) that focuses on full-body representations.The primary objective of this database is to advance the research and development of DHQA within the scientific community.Furthermore, it serves as an ideal platform for evaluating and refining various processing algorithms, including but not being limited to denoising and compression techniques.By providing a comprehensive database consisting of high-quality reference models and distorted counterparts, the proposed SJTU-H3D database offers researchers and practitioners an opportunity to explore and enhance their DHQA methodologies.The availability of such a resource is expected to significantly contribute to the growth and advancement of the DHQA research community.",
        "sentences": [
          {
            "text": "Therefore, in this paper, we propose a comprehensive subjective quality assessment database (SJTU-H3D) targeted at digital humans, aiming to address this research gap and contribute to the advancement of DHQA.",
            "label": 1
          },
          {
            "text": "The SJTU-H3D database introduced in this study comprises 40 high-quality reference digital humans, represented by textured meshes in full-body format, and the database includes 1,120 distorted digital humans that have been generated using seven different types of distortions.",
            "label": 1
          },
          {
            "text": "The perceptual mean opinion scores (MOSs) of these distorted digital humans are collected through a meticulously controlled subjective experiment.",
            "label": 1
          },
          {
            "text": "Notably, the SJTU-H3D database is the first large-scale database specifically designed for digital human quality assessment (DHQA) that focuses on full-body representations.",
            "label": 1
          },
          {
            "text": "The primary objective of this database is to advance the research and development of DHQA within the scientific community.",
            "label": 1
          },
          {
            "text": "Furthermore, it serves as an ideal platform for evaluating and refining various processing algorithms, including but not being limited to denoising and compression techniques.",
            "label": 1
          },
          {
            "text": "By providing a comprehensive database consisting of high-quality reference models and distorted counterparts, the proposed SJTU-H3D database offers researchers and practitioners an opportunity to explore and enhance their DHQA methodologies.",
            "label": 1
          },
          {
            "text": "The availability of such a resource is expected to significantly contribute to the growth and advancement of the DHQA research community.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation",
        "section": 3,
        "paragraph_id": 3,
        "full_text": "During recent years, data-driven image and video quality assessment (I/VQA) approaches[2],[3],[4],[5]have garnered significant attention and have demonstrated remarkable performance in various application domains.The success of these approaches can be partly attributed to the availability of largescale I/VQA databases such as the SPAQ database (containing 11,125 labeled images)[6]and the LSVQ database (comprising up to 38,811 annotated videos)[7].These databases have also contributed to ensuring the generalization capability and robustness of data-driven methods.However, in the realm of DHQA research, the availability of suitable perceptual quality assessment databases is limited.With the exception of the proposed SJTU-H3D database, only one perceptual quality assessment database, DHHQA[8], focusing solely on digital human heads rather than full-body representations, exists.This scarcity of databases makes it challenging to develop datadriven DHQA methods and ensure their generalization ability in practical scenarios.",
        "sentences": [
          {
            "text": "During recent years, data-driven image and video quality assessment (I/VQA) approaches[2],[3],[4],[5]have garnered significant attention and have demonstrated remarkable performance in various application domains.",
            "label": 0
          },
          {
            "text": "The success of these approaches can be partly attributed to the availability of largescale I/VQA databases such as the SPAQ database (containing 11,125 labeled images)[6]and the LSVQ database (comprising up to 38,811 annotated videos)[7].",
            "label": 0
          },
          {
            "text": "These databases have also contributed to ensuring the generalization capability and robustness of data-driven methods.",
            "label": 0
          },
          {
            "text": "However, in the realm of DHQA research, the availability of suitable perceptual quality assessment databases is limited.",
            "label": 0
          },
          {
            "text": "With the exception of the proposed SJTU-H3D database, only one perceptual quality assessment database, DHHQA[8], focusing solely on digital human heads rather than full-body representations, exists.",
            "label": 1
          },
          {
            "text": "This scarcity of databases makes it challenging to develop datadriven DHQA methods and ensure their generalization ability in practical scenarios.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation",
        "section": 4,
        "paragraph_id": 4,
        "full_text": "Hence, this challenge serves as a motivation for us to devise a zero-shot DHQA method that does not necessitate training on labeled DHQA databases.To cater to most practical applications where pristine references may not be readily available, our focus is only on no-reference (NR) methods.To extract both semantic and distortion features for evaluating the visual quality of digital humans, we employ projection rendering techniques.From a semantic perspective, we utilize the Contrastive Language-Image Pre-training (CLIP) model[9]to measure the correlation between the input projections and quality-related texts.Our hypothesis is that high-quality digital human projections should exhibit a strong correlation with positive quality-related texts and a weak correlation with negative ones.To determine the quality levels of the input projections, we design several positive-negative text pairs.The semantic affinity quality measure is then derived by computing the difference in affinity between positive and negative texts.However, CLIP operates on low-resolution images, which limits its ability to capture low-level distortion information.To address this limitation, we incorporate the completely blind Naturalness Image Quality Evaluator (NIQE)[10]to extract low-level quality representations from the raw resolution.To further enhance the accuracy of quality prediction, we also extract features from the mesh modality.For robustness and effectiveness, we choose the dihedral angle as the geometry descriptor, as it has been widely recognized for effectively capturing geometric features relevant to visual quality[11],[12],[13],[14]and its values are confined within the range of [0, π].By analyzing the changing tendency of dihedral angles corresponding to geometry compression and simplification levels, we average-pool the dihedral angles to derive the geometry loss quality measure.Finally, all three quality measures (semantic affinity quality measure, spatial naturalness quality measure, and geometry loss quality measure) are aggregated using a sum function to form the proposed Digital Human Quality Index (DHQI).Experimental results demonstrate that DHQI significantly improves zero-shot performance and even achieves competitiveness with supervised methods.In summary, our contributions are as follows: • We propose the first large-scale full-body DHQA database, SJTU-H3D, which consists of 40 high-quality digital humans represented by textured meshes and 1,120 distorted digital humans generated by 7 types of distortions.We carry out a well-controlled subjective experiment.40 human subjects are invited and a total of 44,800 ratings are collected to gather the mean opinion scores (MOSs) for 1,120 distorted digital humans.",
        "sentences": [
          {
            "text": "Hence, this challenge serves as a motivation for us to devise a zero-shot DHQA method that does not necessitate training on labeled DHQA databases.",
            "label": 0
          },
          {
            "text": "To cater to most practical applications where pristine references may not be readily available, our focus is only on no-reference (NR) methods.",
            "label": 0
          },
          {
            "text": "To extract both semantic and distortion features for evaluating the visual quality of digital humans, we employ projection rendering techniques.",
            "label": 0
          },
          {
            "text": "From a semantic perspective, we utilize the Contrastive Language-Image Pre-training (CLIP) model[9]to measure the correlation between the input projections and quality-related texts.",
            "label": 0
          },
          {
            "text": "Our hypothesis is that high-quality digital human projections should exhibit a strong correlation with positive quality-related texts and a weak correlation with negative ones.",
            "label": 0
          },
          {
            "text": "To determine the quality levels of the input projections, we design several positive-negative text pairs.",
            "label": 0
          },
          {
            "text": "The semantic affinity quality measure is then derived by computing the difference in affinity between positive and negative texts.",
            "label": 0
          },
          {
            "text": "However, CLIP operates on low-resolution images, which limits its ability to capture low-level distortion information.",
            "label": 0
          },
          {
            "text": "To address this limitation, we incorporate the completely blind Naturalness Image Quality Evaluator (NIQE)[10]to extract low-level quality representations from the raw resolution.",
            "label": 0
          },
          {
            "text": "To further enhance the accuracy of quality prediction, we also extract features from the mesh modality.",
            "label": 0
          },
          {
            "text": "For robustness and effectiveness, we choose the dihedral angle as the geometry descriptor, as it has been widely recognized for effectively capturing geometric features relevant to visual quality[11],[12],[13],[14]and its values are confined within the range of [0, π].",
            "label": 0
          },
          {
            "text": "By analyzing the changing tendency of dihedral angles corresponding to geometry compression and simplification levels, we average-pool the dihedral angles to derive the geometry loss quality measure.",
            "label": 0
          },
          {
            "text": "Finally, all three quality measures (semantic affinity quality measure, spatial naturalness quality measure, and geometry loss quality measure) are aggregated using a sum function to form the proposed Digital Human Quality Index (DHQI).",
            "label": 0
          },
          {
            "text": "Experimental results demonstrate that DHQI significantly improves zero-shot performance and even achieves competitiveness with supervised methods.",
            "label": 0
          },
          {
            "text": "In summary, our contributions are as follows: • We propose the first large-scale full-body DHQA database, SJTU-H3D, which consists of 40 high-quality digital humans represented by textured meshes and 1,120 distorted digital humans generated by 7 types of distortions.",
            "label": 1
          },
          {
            "text": "40 human subjects are invited and a total of 44,800 ratings are collected to gather the mean opinion scores (MOSs) for 1,120 distorted digital humans.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation",
        "section": 5,
        "paragraph_id": 5,
        "full_text": "In this section, we give a brief introduction to the development of 3D model quality assessment (3DQA) and noreference image quality assessment (NR-IQA) methods.",
        "sentences": [
          {
            "text": "In this section, we give a brief introduction to the development of 3D model quality assessment (3DQA) and noreference image quality assessment (NR-IQA) methods.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation",
        "section": 6,
        "paragraph_id": 6,
        "full_text": "A. 3DQA Development 1) 3DQA Databases: Early subjective 3D quality assessment (3DQA) databases primarily employ colorless point clouds and are relatively small in scale[20],[21],[22].However, recent efforts have been directed towards addressing the challenge of assessing visual quality in colored 3D models, resulting in the development of substantial 3DQA databases[20],[21],[22],[15],[16],[17],[18],[19].A detailed comparison between these databases and the proposed database is presented in TableI.From the table, it is evident that the recent 3DQA databases, with the exception of DHHQA, encompass general 3D objects and do not specifically focus on 3D digital humans.Although the DHHQA database comprises real human heads, it neglects the consideration of the body part.This highlights the significance of the proposed SJTU-H3D database.",
        "sentences": [
          {
            "text": "A. 3DQA Development 1) 3DQA Databases: Early subjective 3D quality assessment (3DQA) databases primarily employ colorless point clouds and are relatively small in scale[20],[21],[22].",
            "label": 0
          },
          {
            "text": "However, recent efforts have been directed towards addressing the challenge of assessing visual quality in colored 3D models, resulting in the development of substantial 3DQA databases[20],[21],[22],[15],[16],[17],[18],[19].",
            "label": 0
          },
          {
            "text": "A detailed comparison between these databases and the proposed database is presented in TableI.",
            "label": 1
          },
          {
            "text": "From the table, it is evident that the recent 3DQA databases, with the exception of DHHQA, encompass general 3D objects and do not specifically focus on 3D digital humans.",
            "label": 0
          },
          {
            "text": "Although the DHHQA database comprises real human heads, it neglects the consideration of the body part.",
            "label": 0
          },
          {
            "text": "This highlights the significance of the proposed SJTU-H3D database.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation",
        "section": 7,
        "paragraph_id": 7,
        "full_text": "2) 3DQA Methods: In the field of 3D quality assessment (3DQA), metrics can be broadly categorized into model-based and projection-based methods.Model-based methods[23],[24],[11],[25],[26],[27],[28],[29],[30],[31]involve extracting features directly from the 3D model, which offers the advantage of being viewpoint-invariant and relatively straightforward.However, due to the inherent complexity of 3D models, these methods can be computationally expensive and time-consuming.On the other hand, projection-based methods[15],[32],[14],[33],[34]infer the visual quality of a 3D model based on its corresponding projections.These methods leverage mature and successful 2D media analysis tools, which often lead to excellent performance.However, projection-based methods are highly dependent on the selection of viewpoints and can be susceptible to instability when subjected to various rendering setups.More recently,[36], which utilizes natural scene statistics (NSS) in the spatial domain to analyze image quality.CPBD[37]estimates blur levels by computing the cumulative probability of blur detection.BMPRI[38]predicts image quality by generating multiple pseudo-reference images obtained through further degradation of the distorted image and comparing their similarities.NFERM[39]investigates image quality using the free energy principle.Deep learning-based IQA methods have gained momentum with the advancement of deep neural networks.DBCNN[40]consists of two streams of deep neural networks to address both synthetic and authentic distortions.HyperIQA[41]employs a self-adaptive hyper network to handle challenges arising from distortion diversity and content variation in IQA tasks.MUSIQ[42]utilizes a multi-scale image quality transformer to represent image quality at different levels of granularity.StairIQA[43]hierarchically integrates features extracted from intermediate layers to leverage low-level and high-level visual information.",
        "sentences": [
          {
            "text": "2) 3DQA Methods: In the field of 3D quality assessment (3DQA), metrics can be broadly categorized into model-based and projection-based methods.",
            "label": 1
          },
          {
            "text": "Model-based methods[23],[24],[11],[25],[26],[27],[28],[29],[30],[31]involve extracting features directly from the 3D model, which offers the advantage of being viewpoint-invariant and relatively straightforward.",
            "label": 0
          },
          {
            "text": "However, due to the inherent complexity of 3D models, these methods can be computationally expensive and time-consuming.",
            "label": 0
          },
          {
            "text": "On the other hand, projection-based methods[15],[32],[14],[33],[34]infer the visual quality of a 3D model based on its corresponding projections.",
            "label": 0
          },
          {
            "text": "These methods leverage mature and successful 2D media analysis tools, which often lead to excellent performance.",
            "label": 0
          },
          {
            "text": "However, projection-based methods are highly dependent on the selection of viewpoints and can be susceptible to instability when subjected to various rendering setups.",
            "label": 0
          },
          {
            "text": "More recently,[36], which utilizes natural scene statistics (NSS) in the spatial domain to analyze image quality.",
            "label": 0
          },
          {
            "text": "CPBD[37]estimates blur levels by computing the cumulative probability of blur detection.",
            "label": 0
          },
          {
            "text": "BMPRI[38]predicts image quality by generating multiple pseudo-reference images obtained through further degradation of the distorted image and comparing their similarities.",
            "label": 0
          },
          {
            "text": "NFERM[39]investigates image quality using the free energy principle.",
            "label": 0
          },
          {
            "text": "Deep learning-based IQA methods have gained momentum with the advancement of deep neural networks.",
            "label": 0
          },
          {
            "text": "DBCNN[40]consists of two streams of deep neural networks to address both synthetic and authentic distortions.",
            "label": 0
          },
          {
            "text": "HyperIQA[41]employs a self-adaptive hyper network to handle challenges arising from distortion diversity and content variation in IQA tasks.",
            "label": 0
          },
          {
            "text": "MUSIQ[42]utilizes a multi-scale image quality transformer to represent image quality at different levels of granularity.",
            "label": 0
          },
          {
            "text": "StairIQA[43]hierarchically integrates features extracted from intermediate layers to leverage low-level and high-level visual information.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation",
        "section": 8,
        "paragraph_id": 8,
        "full_text": "2) Zero-shot NR-IQA: Zero-shot IQA methods, also known as opinion-unaware methods, have emerged, which do not rely on training on subjective-rated quality assessment databases and can operate on unseen images.The earliest zero-shot NR-IQA methods are NIQE[10]and IL-NIQE[44].NIQE extracts handcrafted natural scene statistics (NSS) features from raw-resolution images and quantifies naturalness quality by computing the Multivariate Gaussian (MVG) distance to high-quality images.IL-NIQE enhances the feature set by incorporating additional quality-aware features, including gradient features, log Gabor filter responses, and color statistics.",
        "sentences": [
          {
            "text": "2) Zero-shot NR-IQA: Zero-shot IQA methods, also known as opinion-unaware methods, have emerged, which do not rely on training on subjective-rated quality assessment databases and can operate on unseen images.",
            "label": 0
          },
          {
            "text": "The earliest zero-shot NR-IQA methods are NIQE[10]and IL-NIQE[44].",
            "label": 0
          },
          {
            "text": "NIQE extracts handcrafted natural scene statistics (NSS) features from raw-resolution images and quantifies naturalness quality by computing the Multivariate Gaussian (MVG) distance to high-quality images.",
            "label": 0
          },
          {
            "text": "IL-NIQE enhances the feature set by incorporating additional quality-aware features, including gradient features, log Gabor filter responses, and color statistics.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation",
        "section": 9,
        "paragraph_id": 9,
        "full_text": "In this section, we mainly present the construction details of the proposed SJTU-H3D database, which includes reference collection, reference characterization, distortion generation, and subjective experiment.",
        "sentences": [
          {
            "text": "In this section, we mainly present the construction details of the proposed SJTU-H3D database, which includes reference collection, reference characterization, distortion generation, and subjective experiment.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation",
        "section": 10,
        "paragraph_id": 10,
        "full_text": "In order to ensure the visual quality and content diversity of the reference 3D digital humans, a manual selection process is conducted to choose all reference digital humans from the HumanAlloy1, a wonderful platform that provides high-quality 3D humans.A total of 40 digital humans are purchased and collected for this study.These digital humans are represented as textured meshes, with texture resolutions of 2048×2048.Fig.2illustrates the rendered projections of the selected digital humans, and Table II provides detailed information regarding the number of vertices and faces for each model.",
        "sentences": [
          {
            "text": "In order to ensure the visual quality and content diversity of the reference 3D digital humans, a manual selection process is conducted to choose all reference digital humans from the HumanAlloy1, a wonderful platform that provides high-quality 3D humans.",
            "label": 1
          },
          {
            "text": "A total of 40 digital humans are purchased and collected for this study.",
            "label": 1
          },
          {
            "text": "These digital humans are represented as textured meshes, with texture resolutions of 2048×2048.",
            "label": 1
          },
          {
            "text": "Fig.2illustrates the rendered projections of the selected digital humans, and Table II provides detailed information regarding the number of vertices and faces for each model.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation",
        "section": 11,
        "paragraph_id": 11,
        "full_text": "The primary objective of our study is to curate a database that exhibits high diversity and generality while minimizing biases associated with the selection of source models.Therefore, we propose an approach to quantitatively characterize the geometry and color complexity since these two aspects are crucial for the visual quality of 3D digital humans.1) Geometry Information: In the domain of image quality assessment (IQA), the analysis of spatial information often involves computing the standard deviation of the Sobel-filtered image.Motivated by this concept, we propose a novel approach to quantify the geometry information by utilizing the standard deviation of the dihedral angles in a mesh.The dihedral angle is a fundamental metric employed in computer graphics and geometric modeling to characterize the shape and curvature of meshes[11],[12], thus drawing a parallel to the Sobel-filtering process in image analysis.It denotes the angle between two neighboring faces that share an edge within the mesh, providing valuable insights into the smoothness or sharpness of the surface.Specifically, the geometry information can be obtained as: where GI represents the geometry information and std(•) stands for the standard deviation function.By leveraging the standard deviation of dihedral angles, we aim to capture and assess the geometric characteristics of the mesh, enabling a more comprehensive evaluation of its structure and shape.",
        "sentences": [
          {
            "text": "The primary objective of our study is to curate a database that exhibits high diversity and generality while minimizing biases associated with the selection of source models.",
            "label": 1
          },
          {
            "text": "1) Geometry Information: In the domain of image quality assessment (IQA), the analysis of spatial information often involves computing the standard deviation of the Sobel-filtered image.",
            "label": 1
          },
          {
            "text": "Motivated by this concept, we propose a novel approach to quantify the geometry information by utilizing the standard deviation of the dihedral angles in a mesh.",
            "label": 0
          },
          {
            "text": "The dihedral angle is a fundamental metric employed in computer graphics and geometric modeling to characterize the shape and curvature of meshes[11],[12], thus drawing a parallel to the Sobel-filtering process in image analysis.",
            "label": 0
          },
          {
            "text": "It denotes the angle between two neighboring faces that share an edge within the mesh, providing valuable insights into the smoothness or sharpness of the surface.",
            "label": 0
          },
          {
            "text": "Specifically, the geometry information can be obtained as: where GI represents the geometry information and std(•) stands for the standard deviation function.",
            "label": 0
          },
          {
            "text": "By leveraging the standard deviation of dihedral angles, we aim to capture and assess the geometric characteristics of the mesh, enabling a more comprehensive evaluation of its structure and shape.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation",
        "section": 12,
        "paragraph_id": 12,
        "full_text": "2) Colorfulness: To evaluate the color characteristics, we focus solely on the texture map.Following the common color calculation process[45],[46], we first convert the texture from RGB channels to LAB channels and combine the standard deviation of A and B channels, which can be mathematically expressed as: where CF represents the colorfulness measure, A and B denote the corresponding color channels of the texture.Similar colorfulness measures are also employed in many IQA works[47],[48],[49],[50]as one of the metrics or features for assessing the quality of images.3) Characterization Visualization: We apply the extracted geometry information and colorfulness measure to the collection of 40 reference digital humans.The results are visualized in Fig.3.The analysis demonstrates that the selected reference 3D digital humans exhibit a wide spectrum of geometry information and colorfulness.Notably, model #24 positioned in the top-right corner showcases intricate geometry details and vibrant colorfulness.In contrast, model #15 portrays simpler geometry information and relatively subdued colorfulness.The proposed measures thoroughly capture the distinctiveness of 3D digital humans concerning their geometry and color characteristics.It is important to emphasize that these measures are directly computed from the underlying model files, thereby ensuring their stability and viewpoint invariance.",
        "sentences": [
          {
            "text": "2) Colorfulness: To evaluate the color characteristics, we focus solely on the texture map.",
            "label": 0
          },
          {
            "text": "Following the common color calculation process[45],[46], we first convert the texture from RGB channels to LAB channels and combine the standard deviation of A and B channels, which can be mathematically expressed as: where CF represents the colorfulness measure, A and B denote the corresponding color channels of the texture.",
            "label": 0
          },
          {
            "text": "3) Characterization Visualization: We apply the extracted geometry information and colorfulness measure to the collection of 40 reference digital humans.",
            "label": 0
          },
          {
            "text": "The results are visualized in Fig.3.",
            "label": 0
          },
          {
            "text": "The analysis demonstrates that the selected reference 3D digital humans exhibit a wide spectrum of geometry information and colorfulness.",
            "label": 0
          },
          {
            "text": "Notably, model #24 positioned in the top-right corner showcases intricate geometry details and vibrant colorfulness.",
            "label": 0
          },
          {
            "text": "In contrast, model #15 portrays simpler geometry information and relatively subdued colorfulness.",
            "label": 0
          },
          {
            "text": "The proposed measures thoroughly capture the distinctiveness of 3D digital humans concerning their geometry and color characteristics.",
            "label": 0
          },
          {
            "text": "It is important to emphasize that these measures are directly computed from the underlying model files, thereby ensuring their stability and viewpoint invariance.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation",
        "section": 13,
        "paragraph_id": 13,
        "full_text": "To account for the common sources of distortion, we incorporate distortions arising from both the generation process and the transmission process.During the generation process, we consider geometry noise resulting from erroneous scanning procedures, as well as color noise introduced by cameras.Furthermore, compression and simplification techniques are widely employed during the transmission process.Hence, these factors are also taken into consideration in our assessment.By considering the full range of distortion sources, we aim to provide a comprehensive evaluation of the quality of 3D digital humans.",
        "sentences": [
          {
            "text": "To account for the common sources of distortion, we incorporate distortions arising from both the generation process and the transmission process.",
            "label": 0
          },
          {
            "text": "During the generation process, we consider geometry noise resulting from erroneous scanning procedures, as well as color noise introduced by cameras.",
            "label": 1
          },
          {
            "text": "Furthermore, compression and simplification techniques are widely employed during the transmission process.",
            "label": 0
          },
          {
            "text": "Hence, these factors are also taken into consideration in our assessment.",
            "label": 0
          },
          {
            "text": "By considering the full range of distortion sources, we aim to provide a comprehensive evaluation of the quality of 3D digital humans.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation",
        "section": 14,
        "paragraph_id": 14,
        "full_text": "Therefore, to degrade the quality of the reference 3D digital humans, we apply seven types of distortions and the specific settings for each distortion type are listed in TableIII.We manually select the distortion parameters to cover most visual quality range and the details are illustrated as follows: • Geometry Noise (GN): Gaussian noise with standard deviations σ g of 0.05, 0.1, 0.15, and 0.2 is added to the vertices' geometry coordinates of the digital humans.",
        "sentences": [
          {
            "text": "Therefore, to degrade the quality of the reference 3D digital humans, we apply seven types of distortions and the specific settings for each distortion type are listed in TableIII.",
            "label": 1
          },
          {
            "text": "We manually select the distortion parameters to cover most visual quality range and the details are illustrated as follows: • Geometry Noise (GN): Gaussian noise with standard deviations σ g of 0.05, 0.1, 0.15, and 0.2 is added to the vertices' geometry coordinates of the digital humans.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation",
        "section": 15,
        "paragraph_id": 15,
        "full_text": "In accordance with the recommended procedure outlined in[15],[16], passive watching is chosen over interactive watching for the subjective experiment to mitigate potential viewing bias.The 3D digital humans are rendered into video sequences for exhibition purposes.The open3d library is utilized to generate the projections[52].The rendering window is configured with a resolution of 1080 × 1920.To capture the video frames, a horizontal and a vertical circle are employed as the predefined camera paths.Each 3D digital human is captured at one frame every 3 degree, resulting in a total of 240 frames (360 × 2 ÷ 3).These frames are then compiled into an 8-second video with a framerate of 30 frames per second.This approach ensures that the viewers can effectively perceive the significant quality information.The rendering process is depicted in Fig.5.",
        "sentences": [
          {
            "text": "In accordance with the recommended procedure outlined in[15],[16], passive watching is chosen over interactive watching for the subjective experiment to mitigate potential viewing bias.",
            "label": 0
          },
          {
            "text": "The 3D digital humans are rendered into video sequences for exhibition purposes.",
            "label": 0
          },
          {
            "text": "The open3d library is utilized to generate the projections[52].",
            "label": 0
          },
          {
            "text": "The rendering window is configured with a resolution of 1080 × 1920.",
            "label": 0
          },
          {
            "text": "To capture the video frames, a horizontal and a vertical circle are employed as the predefined camera paths.",
            "label": 0
          },
          {
            "text": "Each 3D digital human is captured at one frame every 3 degree, resulting in a total of 240 frames (360 × 2 ÷ 3).",
            "label": 0
          },
          {
            "text": "These frames are then compiled into an 8-second video with a framerate of 30 frames per second.",
            "label": 0
          },
          {
            "text": "This approach ensures that the viewers can effectively perceive the significant quality information.",
            "label": 0
          },
          {
            "text": "The rendering process is depicted in Fig.5.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation",
        "section": 16,
        "paragraph_id": 16,
        "full_text": "2) Experiment Process: A total of 40 human subjects, comprising 20 males and 20 females, are recruited to participate in the subjective experiment.Prior to the experiment, a training session is conducted, wherein additional videos generated using the same aforementioned process are presented to familiarize the subjects with the tasks.The rating process takes place within a well-controlled laboratory environment, maintaining a normal level of illumination.The viewers are seated at a distance of twice the screen height.The videos are displayed on an iMac monitor capable of supporting resolutions up to 4096×2304.The order of video presentations is randomized.To facilitate the evaluation process, a double stimuli strategy is employed, where the reference and distorted videos are simultaneously displayed on the screen.The rating interface is excited in Fig.5and the quality score ranges from 0 to 5. In order to mitigate viewer fatigue, the entire experiment is divided into 20 sessions, with each session featuring 56 digital humans.Ultimately, a total of 44,800 subjective ratings (1, 120 × 40) are collected.",
        "sentences": [
          {
            "text": "2) Experiment Process: A total of 40 human subjects, comprising 20 males and 20 females, are recruited to participate in the subjective experiment.",
            "label": 0
          },
          {
            "text": "Prior to the experiment, a training session is conducted, wherein additional videos generated using the same aforementioned process are presented to familiarize the subjects with the tasks.",
            "label": 0
          },
          {
            "text": "The rating process takes place within a well-controlled laboratory environment, maintaining a normal level of illumination.",
            "label": 0
          },
          {
            "text": "The viewers are seated at a distance of twice the screen height.",
            "label": 0
          },
          {
            "text": "The videos are displayed on an iMac monitor capable of supporting resolutions up to 4096×2304.",
            "label": 0
          },
          {
            "text": "The order of video presentations is randomized.",
            "label": 0
          },
          {
            "text": "To facilitate the evaluation process, a double stimuli strategy is employed, where the reference and distorted videos are simultaneously displayed on the screen.",
            "label": 0
          },
          {
            "text": "The rating interface is excited in Fig.5and the quality score ranges from 0 to 5.",
            "label": 0
          },
          {
            "text": "In order to mitigate viewer fatigue, the entire experiment is divided into 20 sessions, with each session featuring 56 digital humans.",
            "label": 0
          },
          {
            "text": "Ultimately, a total of 44,800 subjective ratings (1, 120 × 40) are collected.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation",
        "section": 17,
        "paragraph_id": 17,
        "full_text": "3) Subjective Data Analysis: After the subjective experiment, we calculate the z-scores from the raw ratings as follows: where and N i is the number of digital humans judged by subject i.In accordance with the ITU-R BT.500-13[53]standard, ratings from unreliable subjects are excluded from the analysis.The corresponding z-scores are linearly rescaled to the range of [0, 5].Finally, the mean opinion scores (MOSs) are computed by averaging the rescaled z-scores.",
        "sentences": [
          {
            "text": "3) Subjective Data Analysis: After the subjective experiment, we calculate the z-scores from the raw ratings as follows: where and N i is the number of digital humans judged by subject i.",
            "label": 0
          },
          {
            "text": "500-13[53]standard, ratings from unreliable subjects are excluded from the analysis.",
            "label": 0
          },
          {
            "text": "The corresponding z-scores are linearly rescaled to the range of [0, 5].",
            "label": 0
          },
          {
            "text": "Finally, the mean opinion scores (MOSs) are computed by averaging the rescaled z-scores.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation",
        "section": 18,
        "paragraph_id": 18,
        "full_text": "Fig.6illustrates the distribution of MOSs and the corresponding probability distributions for different distortion types.Interestingly, the probability distributions reveal that visual quality is less sensitive to varying levels of FS distortions compared to other distortion types.Even when reducing the face numbers to a ratio of 0.05 (only about 2k faces are preserved), the visual quality score remains higher than other distortions with similar levels.This observation indicates that visual quality is relatively resilient to FS distortions, implying that the reduction in face complexity may not significantly impact the perceived quality.",
        "sentences": [
          {
            "text": "Fig.6illustrates the distribution of MOSs and the corresponding probability distributions for different distortion types.",
            "label": 0
          },
          {
            "text": "Interestingly, the probability distributions reveal that visual quality is less sensitive to varying levels of FS distortions compared to other distortion types.",
            "label": 0
          },
          {
            "text": "Even when reducing the face numbers to a ratio of 0.05 (only about 2k faces are preserved), the visual quality score remains higher than other distortions with similar levels.",
            "label": 0
          },
          {
            "text": "This observation indicates that visual quality is relatively resilient to FS distortions, implying that the reduction in face complexity may not significantly impact the perceived quality.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation",
        "section": 19,
        "paragraph_id": 19,
        "full_text": "In this section, we introduce the three indexes that make up the whole proposed digital human quality index (DHQI), which includes the text-prompted semantic affinity quality measure, spatial naturalness quality measure, and geometry loss quality measure.These three indexes are then aligned and aggregated into the proposed DHQI quality index.The framework is exhibited in Fig.7.",
        "sentences": [
          {
            "text": "In this section, we introduce the three indexes that make up the whole proposed digital human quality index (DHQI), which includes the text-prompted semantic affinity quality measure, spatial naturalness quality measure, and geometry loss quality measure.",
            "label": 0
          },
          {
            "text": "These three indexes are then aligned and aggregated into the proposed DHQI quality index.",
            "label": 0
          },
          {
            "text": "The framework is exhibited in Fig.7.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation",
        "section": 20,
        "paragraph_id": 20,
        "full_text": "We acquire the cube-like projection set of the given digital human as follows: P = ψ(DH), where P represents the set of the 6 rendered projections and ψ(•) stands for the rendering process.Such rendering process has been employed in the popular point cloud compression standard MPEG VPCC[54]and many other 3DQA works[15],[34].The projections are utilized as the input information for the text-prompted semantic affinity and spatial naturalness measure.",
        "sentences": [
          {
            "text": "We acquire the cube-like projection set of the given digital human as follows: P = ψ(DH), where P represents the set of the 6 rendered projections and ψ(•) stands for the rendering process.",
            "label": 0
          },
          {
            "text": "Such rendering process has been employed in the popular point cloud compression standard MPEG VPCC[54]and many other 3DQA works[15],[34].",
            "label": 0
          },
          {
            "text": "The projections are utilized as the input information for the text-prompted semantic affinity and spatial naturalness measure.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation",
        "section": 21,
        "paragraph_id": 21,
        "full_text": "To assess the perception of quality related to semantic content, specifically evaluating the quality of contents and the ability to discern semantic distortions, we design the text-prompted semantic affinity quality measure.Inspired by CLIP[9]-based quality assessment tasks[55],[56], we hold the hypothesis that the projections of the high-quality digital humans should have higher affinity with positive qualityrelated descriptions (e.g.good, perfect) and lower affinity with negative quality-related descriptions (e.g.bad, distorted).",
        "sentences": [
          {
            "text": "To assess the perception of quality related to semantic content, specifically evaluating the quality of contents and the ability to discern semantic distortions, we design the text-prompted semantic affinity quality measure.",
            "label": 0
          },
          {
            "text": "Inspired by CLIP[9]-based quality assessment tasks[55],[56], we hold the hypothesis that the projections of the high-quality digital humans should have higher affinity with positive qualityrelated descriptions (e.g.good, perfect) and lower affinity with negative quality-related descriptions (e.g.bad, distorted).",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation",
        "section": 22,
        "paragraph_id": 22,
        "full_text": "1) Text Prompt Format: In accordance with the official recommendation provided by CLIP[9]and drawing from established practices, our text prompts are designed as a concatenation of three components: a prefix, a description, and a suffix.To be more precise, the text prompt T corresponding to the raw description D is defined as: T = \"a\" + D + \"projection of 3d human model\",(5)where the suffix \"projection of 3d human model\" is specifically designed to fit the task of DHQA.This carefully chosen suffix can encourage the CLIP model to prioritize and focus its attention on the detection and evaluation of content-aware distortions that may arise in the context of 3D digital humans.",
        "sentences": [
          {
            "text": "1) Text Prompt Format: In accordance with the official recommendation provided by CLIP[9]and drawing from established practices, our text prompts are designed as a concatenation of three components: a prefix, a description, and a suffix.",
            "label": 0
          },
          {
            "text": "To be more precise, the text prompt T corresponding to the raw description D is defined as: T = \"a\" + D + \"projection of 3d human model\",(5)where the suffix \"projection of 3d human model\" is specifically designed to fit the task of DHQA.",
            "label": 0
          },
          {
            "text": "This carefully chosen suffix can encourage the CLIP model to prioritize and focus its attention on the detection and evaluation of content-aware distortions that may arise in the context of 3D digital humans.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation",
        "section": 23,
        "paragraph_id": 23,
        "full_text": "2) Description Selection: We have identified descriptions pertaining to quality assessment that encompass broad evaluation aspects to ensure robustness.In this study, the general quality-related descriptions employed comprise the contrasting pairs of high quality ↔ low quality, good ↔ bad, and perfect ↔ distorted.The utilization of the high quality ↔ low quality as well as the good ↔ bad text pair assists in directing the attention of the CLIP model towards general subjective impressions.Conversely, the perfect ↔ distorted pair compels the CLIP model to prioritize the existence of distortions.",
        "sentences": [
          {
            "text": "2) Description Selection: We have identified descriptions pertaining to quality assessment that encompass broad evaluation aspects to ensure robustness.",
            "label": 0
          },
          {
            "text": "In this study, the general quality-related descriptions employed comprise the contrasting pairs of high quality ↔ low quality, good ↔ bad, and perfect ↔ distorted.",
            "label": 0
          },
          {
            "text": "The utilization of the high quality ↔ low quality as well as the good ↔ bad text pair assists in directing the attention of the CLIP model towards general subjective impressions.",
            "label": 0
          },
          {
            "text": "Conversely, the perfect ↔ distorted pair compels the CLIP model to prioritize the existence of distortions.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation",
        "section": 24,
        "paragraph_id": 24,
        "full_text": "3) Affinity Difference Computation: Given the input image I and text T , the senmantic affinity can be calculated with the assistance of CLIP as: where E I and E T stand for the image and text encoders of CLIP, F I and F T represent the CLIP-encoded features, and A(I, T ) indicates the affinity between the input image and text.Afterward, the computation of zero-shot quality affinity can be derived from the aforementioned selected descriptions by calculating the disparity between the probabilities assigned to positive and negative textual inputs:",
        "sentences": [
          {
            "text": "3) Affinity Difference Computation: Given the input image I and text T , the senmantic affinity can be calculated with the assistance of CLIP as: where E I and E T stand for the image and text encoders of CLIP, F I and F T represent the CLIP-encoded features, and A(I, T ) indicates the affinity between the input image and text.",
            "label": 0
          },
          {
            "text":"Afterward, the computation of zero-shot quality affinity can be derived from the aforementioned selected descriptions by calculating the disparity between the probabilities assigned to positive and negative textual inputs:",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation",
        "section": 25,
        "paragraph_id": 25,
        "full_text": "A(P k , T ), where the averaged affinity to the given text T , denoted by A(P, T ), is calculated by CLIP across the six projections P. In this context, T i + and T i -refer to the positive and negative text descriptions, respectively, from the i-th text pair.The variable N T represents the total number of text pairs.Furthermore, A dif f signifies the cumulative difference between the averaged positive and negative affinity.",
        "sentences": [
          {
            "text": "A(P k , T ), where the averaged affinity to the given text T , denoted by A(P, T ), is calculated by CLIP across the six projections P.",
            "label": 0
          },
          {
            "text": "In this context, T i + and T i -refer to the positive and negative text descriptions, respectively, from the i-th text pair.",
            "label": 0
          },
          {
            "text": "The variable N T represents the total number of text pairs.",
            "label": 0
          },
          {
            "text": "Furthermore, A dif f signifies the cumulative difference between the averaged positive and negative affinity.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation",
        "section": 26,
        "paragraph_id": 26,
        "full_text": "The sigmoid remapping technique is then used to map the raw difference scores A dif f obtained from perceptual quality evaluation into a range of [0, 1].This remapping is done based on the guidance provided by the Video Quality Experts Group (VQEG)[57].The purpose of sigmoid remapping is to transform the raw difference scores into a perceptually meaningful range that is easier to interpret, and the final textprompted semantic affinity quality score can be derived as:",
        "sentences": [
          {
            "text": "The sigmoid remapping technique is then used to map the raw difference scores A dif f obtained from perceptual quality evaluation into a range of [0, 1].",
            "label": 0
          },
          {
            "text": "This remapping is done based on the guidance provided by the Video Quality Experts Group (VQEG)[57].",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation",
        "section": 27,
        "paragraph_id": 27,
        "full_text": "Apart from evaluating semantic affinity, we incorporate the use of NIQE (Naturalness Image Quality Evaluator[10]) as a blind quality evaluator to assess the spatial naturalness of the digital humans.The purpose of employing NIQE is to identify and quantify common low-level distortions encountered in practical digital humans, including Gaussian noise, blur, and JPEG compression artifacts.By incorporating NIQE alongside semantic affinity evaluation, we aim to complement the assessment of high-level information with an evaluation of low-level technical quality.",
        "sentences": [
          {
            "text": "Apart from evaluating semantic affinity, we incorporate the use of NIQE (Naturalness Image Quality Evaluator[10]) as a blind quality evaluator to assess the spatial naturalness of the digital humans.",
            "label": 0
          },
          {
            "text": "The purpose of employing NIQE is to identify and quantify common low-level distortions encountered in practical digital humans, including Gaussian noise, blur, and JPEG compression artifacts.",
            "label": 0
          },
          {
            "text": "By incorporating NIQE alongside semantic affinity evaluation, we aim to complement the assessment of high-level information with an evaluation of low-level technical quality.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation",
        "section": 28,
        "paragraph_id": 28,
        "full_text": "The NIQE index operates by quantifying the disparity between the characteristics of the input image features and the anticipated distribution of features observed in \"high-quality\" images, which are derived from a diverse set of pristine natural images.Since the raw NIQE scores and the raw affinity difference scores are on different scales, it is necessary to normalize the NIQE scores to facilitate meaningful comparison.To achieve this, we divide the NIQE scores by a constant value, denoted as c 1 , which effectively restricts the majority of NIQE scores to the range of [0,1].Consequently, the spatial naturalness quality measure can be computed as follows: where N (P k ) denotes the NIQE value for the k-th projection, N (P) represents the average NIQE value across the 6 projections, and Q N stands for the spatial naturalness quality measure.It's worth noting that the NIQE scores are inversely correlated with quality and the negative sign is incorporated into the sigmoid function, allowing for a consistent interpretation and alignment of the NIQE scores with the quality evaluation framework.",
        "sentences": [
          {
            "text": "The NIQE index operates by quantifying the disparity between the characteristics of the input image features and the anticipated distribution of features observed in \"high-quality\" images, which are derived from a diverse set of pristine natural images.",
            "label": 0
          },
          {
            "text": "Since the raw NIQE scores and the raw affinity difference scores are on different scales, it is necessary to normalize the NIQE scores to facilitate meaningful comparison.",
            "label": 0
          },
          {
            "text": "To achieve this, we divide the NIQE scores by a constant value, denoted as c 1 , which effectively restricts the majority of NIQE scores to the range of [0,1].",
            "label": 0
          },
          {
            "text": "Consequently, the spatial naturalness quality measure can be computed as follows: where N (P k ) denotes the NIQE value for the k-th projection, N (P) represents the average NIQE value across the 6 projections, and Q N stands for the spatial naturalness quality measure.",
            "label": 0
          },
          {
            "text": "It's worth noting that the NIQE scores are inversely correlated with quality and the negative sign is incorporated into the sigmoid function, allowing for a consistent interpretation and alignment of the NIQE scores with the quality evaluation framework.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation",
        "section": 29,
        "paragraph_id": 29,
        "full_text": "The aforementioned measures are applied to projections, specifically the image modality.In order to enhance the model's understanding of digital humans, it is proposed to directly extract features from the mesh modality to capture the loss in geometry with respect to visual quality.",
        "sentences": [
          {
            "text": "The aforementioned measures are applied to projections, specifically the image modality.",
            "label": 0
          },
          {
            "text": "In order to enhance the model's understanding of digital humans, it is proposed to directly extract features from the mesh modality to capture the loss in geometry with respect to visual quality.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation",
        "section": 30,
        "paragraph_id": 30,
        "full_text": "1) Descriptor Selection: Various geometry attributes have been utilized to describe the quality-related geometric characteristics of meshes[14], including curvature, dihedral angle, face angle, face area, etc.For the purpose of preserving stability and improving the robustness of the proposed zeroshot method, the dihedral angle is selected as the geometry descriptor for the following reasons: a) Extensive evidence supports the effectiveness of the dihedral angle in describing geometric features relevant to visual quality[11],[12],[13],[14].b) Unlike other geometry attributes, the dihedral angle is invariant to scale.Its values are confined within the range of [0, π], thereby contributing to its robustness.The dihedral angle is the angle between two adjacent faces, which can be calculated as the dot product of corresponding normal vectors: where θj π indicates the scaled dihedral angle corresponding to the j-th edge of the mesh, Θ indicates the set of the scaled dihedral angle values, n j1 and n j2 stand for the normal vectors of the two adjacent faces whose co-edge is the j-th edge.",
        "sentences": [
          {
            "text":"1) Descriptor Selection: Various geometry attributes have been utilized to describe the quality-related geometric characteristics of meshes[14], including curvature, dihedral angle, face angle, face area, etc.For the purpose of preserving stability and improving the robustness of the proposed zeroshot method, the dihedral angle is selected as the geometry descriptor for the following reasons: a) Extensive evidence supports the effectiveness of the dihedral angle in describing geometric features relevant to visual quality[11],[12],[13],[14].b).",
            "label" :0
          },
          {
            "text": "b) Unlike other geometry attributes, the dihedral angle is invariant to scale.",
            "label": 0
          },
          {
            "text": "Its values are confined within the range of [0, π], thereby contributing to its robustness.",
            "label": 0
          },
          {
            "text": "The dihedral angle is the angle between two adjacent faces, which can be calculated as the dot product of corresponding normal vectors: where θj π indicates the scaled dihedral angle corresponding to the j-th edge of the mesh, Θ indicates the set of the scaled dihedral angle values, n j1 and n j2 stand for the normal vectors of the two adjacent faces whose co-edge is the j-th edge.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation",
        "section": 31,
        "paragraph_id": 31,
        "full_text": "2) Quality Correlation with Dihedral Angle: Lossy mesh compression and simplification techniques can potentially diminish a mesh's structural details, resulting in a smoother and simpler surface representation.In such cases, the faces comprising the smoother and simpler surface tend to exhibit dihedral angles that approach π, leading to an inherent inclination for larger dihedral angles.To substantiate this observation, we present the tendencies of the mean values of the dihedral angles in Fig.8, from which we can find a consistent upward trend in dihedral angle means as compression/simplification levels increase.Therefore, the mean values of the dihedral angle can be generally taken as an indicator of geometry detail loss caused by compression/simplification.Then geometry loss quality measure can be calculated as: where Q G represents the geometry loss quality measure, Θ indicates the mean value of the dihedral angles, and the negative sign is added to the sigmoid function due to the positive correlation between the dihedral angles' mean values and compression/simplification levels.",
        "sentences": [
          {
            "text": "2) Quality Correlation with Dihedral Angle: Lossy mesh compression and simplification techniques can potentially diminish a mesh's structural details, resulting in a smoother and simpler surface representation.",
            "label": 0
          },
          {
            "text": "In such cases, the faces comprising the smoother and simpler surface tend to exhibit dihedral angles that approach π, leading to an inherent inclination for larger dihedral angles.",
            "label": 0
          },
          {
            "text": "To substantiate this observation, we present the tendencies of the mean values of the dihedral angles in Fig.8, from which we can find a consistent upward trend in dihedral angle means as compression/simplification levels increase.",
            "label": 1
          },
          {
            "text": "Therefore, the mean values of the dihedral angle can be generally taken as an indicator of geometry detail loss caused by compression/simplification.",
            "label": 1
          },
          {
            "text": "Then geometry loss quality measure can be calculated as: where Q G represents the geometry loss quality measure, Θ indicates the mean value of the dihedral angles, and the negative sign is added to the sigmoid function due to the positive correlation between the dihedral angles' mean values and compression/simplification levels.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation",
        "section": 32,
        "paragraph_id": 32,
        "full_text": "In order to develop a reliable zero-shot perceptual quality index, we adopt a direct aggregation approach wherein we sum up the scale-aligned scores of various indices without performing any fine-tuning processes.Considering that the Q A , Q N , and Q G have undergone sigmoid rescaling, all three measures are bounded within the range of [0, 1].Consequently, we define the comprehensive unified DHQI (digital human quality index) as follows: where Q DHQI indicates the final quality values for the digital humans.",
        "sentences": [
          {
            "text": "In order to develop a reliable zero-shot perceptual quality index, we adopt a direct aggregation approach wherein we sum up the scale-aligned scores of various indices without performing any fine-tuning processes.",
            "label": 0
          },
          {
            "text": "Considering that the Q A , Q N , and Q G have undergone sigmoid rescaling, all three measures are bounded within the range of [0, 1].",
            "label": 0
          },
          {
            "text": "Consequently, we define the comprehensive unified DHQI (digital human quality index) as follows: where Q DHQI indicates the final quality values for the digital humans.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation",
        "section": 33,
        "paragraph_id": 33,
        "full_text": "V. EXPERIMENT A. Validation Setup 1) Benchmark Databases: In addition to the proposed SJTU-H3D database, we have incorporated the digital human quality assessment (DHHQA) database[8]as an additional resource for benchmark validation.The DHHQA database comprises a total of 55 scanned digital human heads that serve as reference samples, along with 1,540 labeled distorted digital human heads.These distorted samples have been intentionally degraded through the introduction of noise and compression/simplification.",
        "sentences": [
          {
            "text": "V. EXPERIMENT A.",
            "label": 0
          },
          {
            "text": "Validation Setup 1) Benchmark Databases: In addition to the proposed SJTU-H3D database, we have incorporated the digital human quality assessment (DHHQA) database[8]as an additional resource for benchmark validation.",
            "label": 0
          },
          {
            "text": "The DHHQA database comprises a total of 55 scanned digital human heads that serve as reference samples, along with 1,540 labeled distorted digital human heads.",
            "label": 1
          },
          {
            "text": "These distorted samples have been intentionally degraded through the introduction of noise and compression/simplification.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation",
        "section": 34,
        "paragraph_id": 34,
        "full_text": "2) k-fold Cross-Validation: To ensure robust evaluation, we adopt a k-fold cross-validation strategy.This approach involves dividing the database into k equally sized folds.The model is then trained on k-1 of these folds and subsequently tested on the remaining fold.This process is repeated k times, with each fold being used as the test set once.By averaging the performance across these k iterations, we obtain a more reliable estimate of the model's effectiveness, minimizing the impact of random variations.For both the SJTU-H3D and DHHQA databases, we have selected a value of k = 5 to conduct the k-fold cross-validation, ensuring a balanced evaluation across multiple subsets.It's worth mentioning that there is no content overlap between the training and testing folds.",
        "sentences": [
          {
            "text": "2) k-fold Cross-Validation: To ensure robust evaluation, we adopt a k-fold cross-validation strategy.",
            "label": 0
          },
          {
            "text": "This approach involves dividing the database into k equally sized folds.",
            "label": 0
          },
          {
            "text": "The model is then trained on k-1 of these folds and subsequently tested on the remaining fold.",
            "label": 0
          },
          {
            "text": "This process is repeated k times, with each fold being used as the test set once.",
            "label": 0
          },
          {
            "text": "By averaging the performance across these k iterations, we obtain a more reliable estimate of the model's effectiveness, minimizing the impact of random variations.",
            "label": 0
          },
          {
            "text": "For both the SJTU-H3D and DHHQA databases, we have selected a value of k = 5 to conduct the k-fold cross-validation, ensuring a balanced evaluation across multiple subsets.",
            "label": 0
          },
          {
            "text": "It's worth mentioning that there is no content overlap between the training and testing folds.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation",
        "section": 35,
        "paragraph_id": 35,
        "full_text": "To facilitate a direct and fair comparison between zeroshot and supervised methods, we validate their performance in the following way.Zero-shot methods are directly applied to the testing folds, as they do not require any training.The performance is then averaged across the testing folds and reported as the final performance.On the other hand, supervised methods undergo training on the training folds and are subsequently tested on the testing folds.Similar to zero-shot methods, the average performance is calculated and reported as the final performance.Adopting this methodology enables a direct and unbiased comparison of the performance between zero-shot and supervised methods, insights into their respective strengths and limitations.",
        "sentences": [
          {
            "text": "To facilitate a direct and fair comparison between zeroshot and supervised methods, we validate their performance in the following way.",
            "label": 0
          },
          {
            "text": "Zero-shot methods are directly applied to the testing folds, as they do not require any training.",
            "label": 0
          },
          {
            "text": "The performance is then averaged across the testing folds and reported as the final performance.",
            "label": 0
          },
          {
            "text": "On the other hand, supervised methods undergo training on the training folds and are subsequently tested on the testing folds.",
            "label": 0
          },
          {
            "text": "Similar to zero-shot methods, the average performance is calculated and reported as the final performance.",
            "label": 0
          },
          {
            "text": "Adopting this methodology enables a direct and unbiased comparison of the performance between zero-shot and supervised methods, insights into their respective strengths and limitations.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation",
        "section": 36,
        "paragraph_id": 36,
        "full_text": "3) Implemetation Details: The cube-like projection process described in Section IV-A is conducted with the assistance of open3d[52]library with a resolution of 1080P.The white background is cropped out.The projections are downsampled to 224×224 as the input of the CLIP[9]image encoder.The ViT-B-32[64]backbone with LAION-2B[65]pretrained weights is utilized as the CLIP model.To fit the DHHQA database, we replace the suffix \"projection of 3d human model\" as described in Equation5with \"projection of 3d human face\".The scale parameter c 1 constant described in Section IV-C is set as 100.The supervised training of the proposed DHQA method is conducted with the Support Vector Regression (SVR) model with RBF kernel.",
        "sentences": [
          {
            "text": "3) Implemetation Details: The cube-like projection process described in Section IV-A is conducted with the assistance of open3d[52]library with a resolution of 1080P.",
            "label": 0
          },
          {
            "text": "The white background is cropped out.",
            "label": 0
          },
          {
            "text": "The projections are downsampled to 224×224 as the input of the CLIP[9]image encoder.",
            "label": 0
          },
          {
            "text": "The ViT-B-32[64]backbone with LAION-2B[65]pretrained weights is utilized as the CLIP model.",
            "label": 0
          },
          {
            "text": "To fit the DHHQA database, we replace the suffix \"projection of 3d human model\" as described in Equation5with \"projection of 3d human face\".",
            "label": 0
          },
          {
            "text": "The scale parameter c 1 constant described in Section IV-C is set as 100.",
            "label": 0
          },
          {
            "text": "The supervised training of the proposed DHQA method is conducted with the Support Vector Regression (SVR) model with RBF kernel.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation",
        "section": 37,
        "paragraph_id": 37,
        "full_text": "The official source code is used for the competitors and default parameters are maintained.The default 5-fold crossvalidation is strictly followed for the competitors to make the comparison fair.In addition, the predicted scores of all the methods are followed by a five-parameter logistic regression to map the scores to the MOS scale.",
        "sentences": [
          {
            "text": "The official source code is used for the competitors and default parameters are maintained.",
            "label": 0
          },
          {
            "text": "The default 5-fold crossvalidation is strictly followed for the competitors to make the comparison fair.",
            "label": 0
          },
          {
            "text": "In addition, the predicted scores of all the methods are followed by a five-parameter logistic regression to map the scores to the MOS scale.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation",
        "section": 38,
        "paragraph_id": 38,
        "full_text": "The competitors' selection is conducted to ensure high diversity, which includes the zero-shot FR methods, zero-shot NR methods, and the supervised NR methods.",
        "sentences": [
          {
            "text": "The competitors' selection is conducted to ensure high diversity, which includes the zero-shot FR methods, zero-shot NR methods, and the supervised NR methods.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation",
        "section": 39,
        "paragraph_id": 39,
        "full_text": "1) Zero-shot FR Methods: We consider several classical projection-based FR methods: PSNR, SSIM[58], MS-SSIM[59], and GMSD[60].These methods are applied to the six perpendicular projections, and the resulting scores are averaged and recorded.Additionally, we incorporate three popular point-based FR metrics proposed by MPEG: PSNR p2po[61], PSNR p2pl[62], and PSNR yuv[63].For the purpose of validation, we convert the digital human models into point clouds.Furthermore, we utilize G-LPIPS*[19], which is a projection-based FR metric modified from LPIPS[66]and is designed for textured meshes.The official pretrained weights are employed for this metric.",
        "sentences": [
          {
            "text": "1) Zero-shot FR Methods: We consider several classical projection-based FR methods: PSNR, SSIM[58], MS-SSIM[59], and GMSD[60].",
            "label": 0
          },
          {
            "text": "These methods are applied to the six perpendicular projections, and the resulting scores are averaged and recorded.",
            "label": 0
          },
          {
            "text": "Additionally, we incorporate three popular point-based FR metrics proposed by MPEG: PSNR p2po[61], PSNR p2pl[62], and PSNR yuv[63].",
            "label": 0
          },
          {
            "text": "For the purpose of validation, we convert the digital human models into point clouds.",
            "label": 0
          },
          {
            "text": "Furthermore, we utilize G-LPIPS*[19], which is a projection-based FR metric modified from LPIPS[66]and is designed for textured meshes.",
            "label": 0
          },
          {
            "text": "The official pretrained weights are employed for this metric.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation",
        "section": 40,
        "paragraph_id": 40,
        "full_text": "2) Zero-shot NR Methods: These methods comprise CPBD[37], pretrained BRISQUE*[36], NIQE[10], and IL-NIQE[44].",
        "sentences": [
          {
            "text": "2) Zero-shot NR Methods: These methods comprise CPBD[37], pretrained BRISQUE*[36], NIQE[10], and IL-NIQE[44].",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation",
        "section": 41,
        "paragraph_id": 41,
        "full_text": "3) Supervised NR Methods: These methods encompass handcrafted approaches such as BRISQUE[36], NFERM[39], and BMPRI[38], which are supervised using the Support Vector Regression (SVR) model.Additionally, we include deep learning-based methods, namely DBCNN[40], Hyper-IQA[41], MUSIQ[42], and StaiIQA[43], which have been retrained for our evaluation.",
        "sentences": [
          {
            "text": "3) Supervised NR Methods: These methods encompass handcrafted approaches such as BRISQUE[36], NFERM[39], and BMPRI[38], which are supervised using the Support Vector Regression (SVR) model.",
            "label": 0
          },
          {
            "text": "Additionally, we include deep learning-based methods, namely DBCNN[40], Hyper-IQA[41], MUSIQ[42], and StaiIQA[43], which have been retrained for our evaluation.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation",
        "section": 42,
        "paragraph_id": 42,
        "full_text": "The overall performance on the SJTU-H3D and DHHQA databases are exhibited in TableIV, from which we can draw several conclusions.",
        "sentences": [
          {
            "text": "The overall performance on the SJTU-H3D and DHHQA databases are exhibited in TableIV, from which we can draw several conclusions.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation",
        "section": 43,
        "paragraph_id": 43,
        "full_text": "1) Zero-shot Performance: a) Among all the zero-shot methods compared on the SJTU-H3D database, the DHQI method demonstrates superior performance and outperforms them all.Additionally, it proves to be competitive even when compared to FR metrics on the DHHQA database.b) Nevertheless, the FR metrics that exhibit the highest performance on the DHHQA database, namely MS-SSIM & GMSD, suffer significant performance degradation when applied to the SJTU-H3D database.This decline suggests that these metrics lack robustness in handling diverse digital human content.c) In contrast, all the competing zero-shot NR methods consistently exhibit lower performance compared to the proposed DHQI method.The reason for this disparity lies in the focus of these methods on addressing low-level distortions, which restricts their ability to effectively capture and model high-level semantic quality representations.By leveraging the semantic affinity quality measure, the DHQI method can enhance the performance of zero-shot NR approaches even further.",
        "sentences": [
          {
            "text": "1) Zero-shot Performance: a) Among all the zero-shot methods compared on the SJTU-H3D database, the DHQI method demonstrates superior performance and outperforms them all.",
            "label": 0
          },
          {
            "text": "b) Nevertheless, the FR metrics that exhibit the highest performance on the DHHQA database, namely MS-SSIM & GMSD, suffer significant performance degradation when applied to the SJTU-H3D database.",
            "label": 0
          },
          {
            "text": "c) In contrast, all the competing zero-shot NR methods consistently exhibit lower performance compared to the proposed DHQI method.",
            "label": 0
          },
          {
            "text": "The reason for this disparity lies in the focus of these methods on addressing low-level distortions, which restricts their ability to effectively capture and model high-level semantic quality representations.",
            "label": 0
          },
          {
            "text": "By leveraging the semantic affinity quality measure, the DHQI method can enhance the performance of zero-shot NR approaches even further.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation",
        "section": 44,
        "paragraph_id": 44,
        "full_text": "2) Supervised Performance: Due to the significant advancements achieved by deep neural networks, deep learning-based methods such as HyperIQA and MUSIQ have demonstrated superior performance compared to traditional handcrafted methods.Despite this, the proposed DHQI method, which is solely supervised by Support Vector Regression (SVR) model, achieves the top-ranking performance on the SJTU-H3D database.One notable advantage of the proposed supervised DHQI index is its cost-effectiveness in terms of time and computational resources.The calibration process of an SVR model requires considerably less time and computational",
        "sentences": [
          {
            "text": "2) Supervised Performance: Due to the significant advancements achieved by deep neural networks, deep learning-based methods such as HyperIQA and MUSIQ have demonstrated superior performance compared to traditional handcrafted methods.",
            "label": 0
          },
          {
            "text": "Despite this, the proposed DHQI method, which is solely supervised by Support Vector Regression (SVR) model, achieves the top-ranking performance on the SJTU-H3D database.",
            "label": 0
          },
          {
            "text": "One notable advantage of the proposed supervised DHQI index is its cost-effectiveness in terms of time and computational resources.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation",
        "section": 45,
        "paragraph_id": 45,
        "full_text": "To investigate the specific effects of zero-shot methods, we present the distortion-specific performance in TableV, from which we can several observations: a) The proposed DHQI method achieves first place in three types of distortions: PC, UMC, and TD, which demonstrates its effectiveness in handling these distortions.b) The point-based methods proposed by MPEG exhibit high sensitivity to noise-related distortions.This can be attributed to the direct impact of geometry and color noise on the point-level quality characteristics.Additionally, the PSNR yuv metric demonstrates a strong discriminative ability in distinguishing quality differences within CN, PC, UMC, and TD distortions.However, it is less effective in handling cross-distortion content from a general perspective (its overall SRCC performance is just 0.5247).c) The zeroshot NR methods NIQE and IL-NIQE show competitive performance for UMC, TD, and TC distortions.This can be attributed to the fact that UMC and TD distortions introduce blurring effects to digital human projections, which aligns with the strengths of these methods.TC distortion, on the other hand, introduces typical JPEG artifacts to digital humans, which can be easily quantified by these methods as well.d) FS distortion proves to be the most challenging distortion to evaluate.This is due to the fact that the MOS distribution for FS distortion tends to be more centered, as shown in Fig.6, indicating a more fine-grained quality level that is less distinctive.FS distortion primarily causes digital humans to exhibit more geometric characteristics, which may lead to small differences in NSS reflected by the projections and result in the poor performance of NIQE and IL-NIQE.Despite the less competitive performance of the proposed method in handling FS distortion, it significantly advances the performance of NR methods in general.",
        "sentences": [
          {
            "text": "b) The point-based methods proposed by MPEG exhibit high sensitivity to noise-related distortions.",
            "label": 0
          },
          {
            "text": "This can be attributed to the direct impact of geometry and color noise on the point-level quality characteristics.",
            "label": 0
          },
          {
            "text": "Additionally, the PSNR yuv metric demonstrates a strong discriminative ability in distinguishing quality differences within CN, PC, UMC, and TD distortions.",
            "label": 0
          },
          {
            "text": "c) The zeroshot NR methods NIQE and IL-NIQE show competitive performance for UMC, TD, and TC distortions.",
            "label": 0
          },
          {
            "text": "This can be attributed to the fact that UMC and TD distortions introduce blurring effects to digital human projections, which aligns with the strengths of these methods.",
            "label": 0
          },
          {
            "text": "d) FS distortion proves to be the most challenging distortion to evaluate.",
            "label": 0
          },
          {
            "text": "This is due to the fact that the MOS distribution for FS distortion tends to be more centered, as shown in Fig.6, indicating a more fine-grained quality level that is less distinctive.",
            "label": 0
          },
          {
            "text": "FS distortion primarily causes digital humans to exhibit more geometric characteristics, which may lead to small differences in NSS reflected by the projections and result in the poor performance of NIQE and IL-NIQE.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation",
        "section": 46,
        "paragraph_id": 46,
        "full_text": "In this section, we present an analysis of the effects of different quality measures: Q A , Q N , and Q G , on the experimental performance.The combinations of these quality measures are tested, and the results are summarized in TableVI.Throughout the experiments, we maintain the default experimental setup.Table VI clearly demonstrates that among the single quality measures, Q A achieves the highest performance.This finding indicates a strong correlation between quality-aware semantic affinity and the visual quality of digital humans.It suggests that considering the quality of semantic representations is crucial for accurately assessing the visual fidelity of digital human models.Furthermore, excluding any of the three quality measures leads to a drop in performance compared to utilizing all quality measures together.This observation implies that each quality measure contributes significantly to the final results.The effectiveness of the proposed framework is thereby validated by the consistent performance improvements achieved when all quality measures are incorporated.",
        "sentences": [
          {
            "text": "In this section, we present an analysis of the effects of different quality measures: Q A , Q N , and Q G , on the experimental performance.",
            "label": 0
          },
          {
            "text": "The combinations of these quality measures are tested, and the results are summarized in TableVI.",
            "label": 0
          },
          {
            "text": "Throughout the experiments, we maintain the default experimental setup.",
            "label": 0
          },
          {
            "text": "Table VI clearly demonstrates that among the single quality measures, Q A achieves the highest performance.",
            "label": 0
          },
          {
            "text": "This finding indicates a strong correlation between quality-aware semantic affinity and the visual quality of digital humans.",
            "label": 0
          },
          {
            "text": "It suggests that considering the quality of semantic representations is crucial for accurately assessing the visual fidelity of digital human models.",
            "label": 0
          },
          {
            "text": "Furthermore, excluding any of the three quality measures leads to a drop in performance compared to utilizing all quality measures together.",
            "label": 0
          },
          {
            "text": "This observation implies that each quality measure contributes significantly to the final results.",
            "label": 0
          },
          {
            "text": "The effectiveness of the proposed framework is thereby validated by the consistent performance improvements achieved when all quality measures are incorporated.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation",
        "section": 47,
        "paragraph_id": 47,
        "full_text": "To further analyze the performance of the proposed method, we conduct the statistical test in this section.We follow the same experiment setup as in[67]and compare the difference between the predicted quality scores with the subjective ratings.All possible pairs of models are tested and the results are listed in Fig.9. Our method demonstrates remarkable superiority over 12 zero-shot methods and 5 supervised methods when compared on the SJTU-H3D database.On the DHHQA database, our method exhibits substantial outperformance compared to 9 zero-shot methods and 3 supervised methods.",
        "sentences": [
          {
            "text": "To further analyze the performance of the proposed method, we conduct the statistical test in this section.",
            "label": 0
          },
          {
            "text": "We follow the same experiment setup as in[67]and compare the difference between the predicted quality scores with the subjective ratings.",
            "label": 0
          },
          {
            "text": "All possible pairs of models are tested and the results are listed in Fig.9.",
            "label": 0
          },
          {
            "text": "Our method demonstrates remarkable superiority over 12 zero-shot methods and 5 supervised methods when compared on the SJTU-H3D database.",
            "label": 0
          },
          {
            "text": "On the DHHQA database, our method exhibits substantial outperformance compared to 9 zero-shot methods and 3 supervised methods.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation",
        "section": 48,
        "paragraph_id": 48,
        "full_text": "The increasing applications of digital humans across various domains have highlighted the need for comprehensive A black/white block means the row method is statistically worse/better than the column one.A gray block means the row method and the column method are statistically indistinguishable.The methods are denoted by the same index as in TableIV. quality assessment studies.However, the limited availability of comprehensive digital human quality assessment (DHQA) databases has posed challenges in this area.To address this gap, we have introduced the SJTU-H3D subjective quality assessment database, specifically designed for full-body digital humans.This database consists of 40 high-quality reference digital humans and 1,120 labeled distorted counterparts created with seven types of distortions.Nonetheless, the scarcity of suitable DHQA databases remains a hindrance to the development of data-driven methods.To overcome this limitation and enhance generalization capabilities, we propose a zero-shot DHQA approach that focuses on no-reference (NR) scenarios.",
        "sentences": [
          {
            "text": "The increasing applications of digital humans across various domains have highlighted the need for comprehensive A black/white block means the row method is statistically worse/better than the column one.",
            "label": 0
          },
          {
            "text": "A gray block means the row method and the column method are statistically indistinguishable.",
            "label": 0
          },
          {
            "text": "The methods are denoted by the same index as in TableIV. quality assessment studies.",
            "label": 0
          },
          {
            "text": "However, the limited availability of comprehensive digital human quality assessment (DHQA) databases has posed challenges in this area.",
            "label": 0
          },
          {
            "text": "To address this gap, we have introduced the SJTU-H3D subjective quality assessment database, specifically designed for full-body digital humans.",
            "label": 1
          },
          {
            "text": "This database consists of 40 high-quality reference digital humans and 1,120 labeled distorted counterparts created with seven types of distortions.",
            "label": 1
          },
          {
            "text": "Nonetheless, the scarcity of suitable DHQA databases remains a hindrance to the development of data-driven methods.",
            "label": 0
          },
          {
            "text": "To overcome this limitation and enhance generalization capabilities, we propose a zero-shot DHQA approach that focuses on no-reference (NR) scenarios.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation",
        "section": 49,
        "paragraph_id": 49,
        "full_text": "Our approach leverages semantic and distortion features obtained from projections, as well as geometry features derived from the mesh structure of digital humans.The proposed DHQI not only serves as a robust baseline for DHQA tasks but also facilitates advancements in the field.We hope our work can contribute to the establishment of effective evaluation frameworks and methodologies for digital humans, enabling their widespread application in diverse domains.",
        "sentences": [
          {
            "text": "Our approach leverages semantic and distortion features obtained from projections, as well as geometry features derived from the mesh structure of digital humans.",
            "label": 0
          },
          {
            "text": "The proposed DHQI not only serves as a robust baseline for DHQA tasks but also facilitates advancements in the field.",
            "label": 0
          },
          {
            "text": "We hope our work can contribute to the establishment of effective evaluation frameworks and methodologies for digital humans, enabling their widespread application in diverse domains.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Amazon_M2_ A Multilingual Multi_locale Shopping Session Dataset for Recommendation and Text Generation",
        "section": 0,
        "paragraph_id": 0,
        "full_text": "Abstract: Modeling customer shopping intentions is a crucial task for e-commerce, as it directly impacts user experience and engagement.Thus, accurately understanding customer preferences is essential for providing personalized recommendations.Session-based recommendation, which utilizes customer session data to predict their next interaction, has become increasingly popular.However, existing session datasets have limitations in terms of item attributes, user diversity, and dataset scale.As a result, they cannot comprehensively capture the spectrum of user behaviors and preferences.To bridge this gap, we present the Amazon Multilingual Multilocale Shopping Session Dataset, namely Amazon-M2.It is the first multilingual dataset consisting of millions of user sessions from six different locales, where the major languages of products are English, German, Japanese, French, Italian, and Spanish.Remarkably, the dataset can help us enhance personalization and understanding of user preferences, which can benefit various existing tasks as well as enable new tasks.To test the potential of the dataset, we introduce three tasks in this work: (1) next-product recommendation, (2) next-product recommendation with domain shifts, and (3) next-product title generation.With the above tasks, we benchmark a range of algorithms on our proposed dataset, drawing new insights for further research and practice.In addition, based on the proposed dataset and tasks, we hosted a competition in the KDD CUP 2023 2 and have attracted thousands of users and submissions.The winning solutions and the associated workshop can be accessed at our website https://kddcup23.github.io/.* Equal contribution. 2https://www.aicrowd.com/challenges/amazon-kdd-cup-23-multilingual-recommendationchallenge37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks.",
        "sentences": [
          {
            "text": "Abstract: Modeling customer shopping intentions is a crucial task for e-commerce, as it directly impacts user experience and engagement.",
            "label": 0
          },
          {
            "text": "Thus, accurately understanding customer preferences is essential for providing personalized recommendations.",
            "label": 0
          },
          {
            "text": "Session-based recommendation, which utilizes customer session data to predict their next interaction, has become increasingly popular.",
            "label": 0
          },
          {
            "text": "However, existing session datasets have limitations in terms of item attributes, user diversity, and dataset scale.",
            "label": 0
          },
          {
            "text": "As a result, they cannot comprehensively capture the spectrum of user behaviors and preferences.",
            "label": 0
          },
          {
            "text": "To bridge this gap, we present the Amazon Multilingual Multilocale Shopping Session Dataset, namely Amazon-M2.",
            "label": 1
          },
          {
            "text": "It is the first multilingual dataset consisting of millions of user sessions from six different locales, where the major languages of products are English, German, Japanese, French, Italian, and Spanish.",
            "label": 1
          },
          {
            "text": "Remarkably, the dataset can help us enhance personalization and understanding of user preferences, which can benefit various existing tasks as well as enable new tasks.",
            "label": 0
          },
          {
            "text": "To test the potential of the dataset, we introduce three tasks in this work: (1) next-product recommendation, (2) next-product recommendation with domain shifts, and (3) next-product title generation.",
            "label": 0
          },
          {
            "text": "With the above tasks, we benchmark a range of algorithms on our proposed dataset, drawing new insights for further research and practice.",
            "label": 0
          },
          {
            "text": "In addition, based on the proposed dataset and tasks, we hosted a competition in the KDD CUP 2023 2 and have attracted thousands of users and submissions.",
            "label": 0
          },
          {
            "text": "The winning solutions and the associated workshop can be accessed at our website https://kddcup23.github.io/.* Equal contribution. 2https://www.aicrowd.com/challenges/amazon-kdd-cup-23-multilingual-recommendationchallenge37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Amazon_M2_ A Multilingual Multi_locale Shopping Session Dataset for Recommendation and Text Generation",
        "section": 1,
        "paragraph_id": 1,
        "full_text": "In the era of information explosion, recommender systems have become a prevalent tool for understanding user preferences and reducing information overload[1,2,3,4,5,6].Traditionally, the majority of recommendation algorithms focus on understanding long-term user interests through utilizing user-profiles and behavioral records.However, they tend to overlook the user's current purpose which often has a dominant impact on user's next behavior.Besides, many recommendation algorithms require access to user profiles[7,8,9], which can be incomplete or even missing in real-world situations especially when users are browsing in an incognito mode.In these cases, only the most recent user interactions in the current session can be utilized for understanding their preferences.Consequently, the session-based recommendation has emerged as an effective solution for modeling user's short-term interest, focusing on a user's most recent interactions within the current session to predict the next product.Over the past few years, the session-based recommendation has gained significant attention and has prompted the development of numerous models[10,11,12,13,14,15,16,17].",
        "sentences": [
          {
            "text": "In the era of information explosion, recommender systems have become a prevalent tool for understanding user preferences and reducing information overload[1,2,3,4,5,6].",
            "label": 0
          },
          {
            "text": "Traditionally, the majority of recommendation algorithms focus on understanding long-term user interests through utilizing user-profiles and behavioral records.",
            "label": 0
          },
          {
            "text": "However, they tend to overlook the user's current purpose which often has a dominant impact on user's next behavior.",
            "label": 0
          },
          {
            "text": "Besides, many recommendation algorithms require access to user profiles[7,8,9], which can be incomplete or even missing in real-world situations especially when users are browsing in an incognito mode.",
            "label": 0
          },
          {
            "text": "In these cases, only the most recent user interactions in the current session can be utilized for understanding their preferences.",
            "label": 0
          },
          {
            "text": "Consequently, the session-based recommendation has emerged as an effective solution for modeling user's short-term interest, focusing on a user's most recent interactions within the current session to predict the next product.",
            "label": 0
          },
          {
            "text": "Over the past few years, the session-based recommendation has gained significant attention and has prompted the development of numerous models[10,11,12,13,14,15,16,17].",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Amazon_M2_ A Multilingual Multi_locale Shopping Session Dataset for Recommendation and Text Generation",
        "section": 2,
        "paragraph_id": 2,
        "full_text": "A critical ingredient for evaluating the efficacy of these methods is the session dataset.While numerous session datasets[18,19,20,11,21]have been carefully curated to meet the requirements of modeling user intent and are extensively employed for evaluating session-based recommender systems, they have several drawbacks.First, existing datasets only provide limited product attributes, resulting in incomplete product information and obscuring studies that leverage attribute information to advance the recommendation.Second, the user diversity within these datasets is limited and may not adequately represent the diversity of user-profiles and behaviors.Consequently, it can result in biased or less accurate recommendations, as the models may not capture the full range of customer preferences.Third, the dataset scale, particularly in terms of the product set, is limited, which falls short of reflecting real-world recommendation scenarios with vast product and user bases.",
        "sentences": [
          {
            "text": "A critical ingredient for evaluating the efficacy of these methods is the session dataset.",
            "label": 0
          },
          {
            "text": "While numerous session datasets[18,19,20,11,21]have been carefully curated to meet the requirements of modeling user intent and are extensively employed for evaluating session-based recommender systems, they have several drawbacks.",
            "label": 0
          },
          {
            "text": "First, existing datasets only provide limited product attributes, resulting in incomplete product information and obscuring studies that leverage attribute information to advance the recommendation.",
            "label": 0
          },
          {
            "text": "Second, the user diversity within these datasets is limited and may not adequately represent the diversity of user-profiles and behaviors.",
            "label": 0
          },
          {
            "text": "Consequently, it can result in biased or less accurate recommendations, as the models may not capture the full range of customer preferences.",
            "label": 0
          },
          {
            "text": "Third, the dataset scale, particularly in terms of the product set, is limited, which falls short of reflecting real-world recommendation scenarios with vast product and user bases.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Amazon_M2_ A Multilingual Multi_locale Shopping Session Dataset for Recommendation and Text Generation",
        "section": 3,
        "paragraph_id": 3,
        "full_text": "To break the aforementioned limitations, we introduce the Amazon Multilingual Multi-Locale Shopping Session Dataset, namely Amazon-M2, a large dataset of anonymized user sessions with their interacted products collected from multiple language sources at Amazon.Specifically, the dataset contains samples constructed from real user session data, where each sample contains a list of user-engaged products in chronological order.In addition, we provide a table of product attributes, which contains all the interacted products with their associated attributes such as title, brand, color, etc. Modeling such session data can help us better understand customers' shopping intentions, which is also the main focus of e-commerce.Particularly, the proposed dataset exhibits the following characteristics that make it unique from existing session datasets.",
        "sentences": [
          {
            "text": "To break the aforementioned limitations, we introduce the Amazon Multilingual Multi-Locale Shopping Session Dataset, namely Amazon-M2, a large dataset of anonymized user sessions with their interacted products collected from multiple language sources at Amazon.",
            "label": 1
          },
          {
            "text": "Specifically, the dataset contains samples constructed from real user session data, where each sample contains a list of user-engaged products in chronological order.",
            "label": 1
          },
          {
            "text": "In addition, we provide a table of product attributes, which contains all the interacted products with their associated attributes such as title, brand, color, etc. Modeling such session data can help us better understand customers' shopping intentions, which is also the main focus of e-commerce.",
            "label": 1
          },
          {
            "text": "Particularly, the proposed dataset exhibits the following characteristics that make it unique from existing session datasets.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Amazon_M2_ A Multilingual Multi_locale Shopping Session Dataset for Recommendation and Text Generation",
        "section": 4,
        "paragraph_id": 4,
        "full_text": "(a) Rich semantic attributes: Amazon-M2 includes rich product attributes (categorical, textual, and numerical attributes) as product features including title, price, brand, description, etc.These attributes provide a great opportunity to accurately comprehend the user's interests.To our best knowledge, it is the first session dataset to provide textual features.(b) Large scale: Amazon-M2 is a large-scale dataset with millions of user sessions and products, while existing datasets only contain tens of thousands of products.(c) Multiple locales: Amazon-M2 collected data from diverse sources, i.e., six different locales including the United Kingdom, Japan, Italian, Spanish, French, and Germany.Thus, it provides a diverse range of user behaviors and preferences, which can facilitate the design of less biased and more accurate recommendation algorithms.(d) Multiple languages: Given the included locales, Amazon-M2 is special for its multilingual property.Particularly, six different languages (English, Japanese, Italian, Spanish, French, and German) are provided.It enables us to leverage recent advances such as language models[22,23,24]to model different languages in user sessions.",
        "sentences": [
          {
            "text": "(a) Rich semantic attributes: Amazon-M2 includes rich product attributes (categorical, textual, and numerical attributes) as product features including title, price, brand, description, etc.These attributes provide a great opportunity to accurately comprehend the user's interests.",
            "label": 1
          },
          {
            "text": "To our best knowledge, it is the first session dataset to provide textual features.",
            "label": 1
          },
          {
            "text": "(b) Large scale: Amazon-M2 is a large-scale dataset with millions of user sessions and products, while existing datasets only contain tens of thousands of products.",
            "label": 1
          },
          {
            "text": "(c) Multiple locales: Amazon-M2 collected data from diverse sources, i.e., six different locales including the United Kingdom, Japan, Italian, Spanish, French, and Germany.",
            "label": 1
          },
          {
            "text": "(d) Multiple languages: Given the included locales, Amazon-M2 is special for its multilingual property.",
            "label": 1
          },
          {
            "text": "Particularly, six different languages (English, Japanese, Italian, Spanish, French, and German) are provided.",
            "label": 1
          },
          {
            "text": "It enables us to leverage recent advances such as language models[22,23,24]to model different languages in user sessions.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Amazon_M2_ A Multilingual Multi_locale Shopping Session Dataset for Recommendation and Text Generation",
        "section": 5,
        "paragraph_id": 5,
        "full_text": "By utilizing this dataset, we can perform diverse downstream tasks for evaluating relevant algorithms in recommendation and text generation.Here, we focus on three different tasks, consisting of (1) next-product recommendation, (2) next-product recommendation with domain shifts, and (3) nextproduct title generation.The first task is the classic session-based recommendation which requires models to predict the ID of the next product, where the training dataset and test dataset are from the same domain.The second task is similar to the first task but requires the models to pre-train on the large dataset from large locales and transfer the knowledge to make predictions on downstream datasets from different domains (i.e., underrepresented locales).The third task is a novel task proposed by us, which asks models to predict the title of the next product which has never been shown in the training set.Based on these tasks, we benchmark representative baselines along with simple heuristic methods.Our empirical observations suggest that the representative baselines fail to outperform simple heuristic methods in certain evaluation metrics in these new settings.Therefore, we believe that Amazon-M2 can inspire novel solutions for session-based recommendation and enable new opportunities for tasks that revolve around large language models and recommender systems. denote a dictionary of unique products that appeared in the sessions, and each product is associated with some attributes.",
        "sentences": [
          {
            "text": "By utilizing this dataset, we can perform diverse downstream tasks for evaluating relevant algorithms in recommendation and text generation.",
            "label": 0
          },
          {
            "text": "Here, we focus on three different tasks, consisting of (1) next-product recommendation, (2) next-product recommendation with domain shifts, and (3) nextproduct title generation.",
            "label": 0
          },
          {
            "text": "The first task is the classic session-based recommendation which requires models to predict the ID of the next product, where the training dataset and test dataset are from the same domain.",
            "label": 0
          },
          {
            "text": "The second task is similar to the first task but requires the models to pre-train on the large dataset from large locales and transfer the knowledge to make predictions on downstream datasets from different domains (i.e., underrepresented locales).",
            "label": 0
          },
          {
            "text": "The third task is a novel task proposed by us, which asks models to predict the title of the next product which has never been shown in the training set.",
            "label": 0
          },
          {
            "text": "Based on these tasks, we benchmark representative baselines along with simple heuristic methods.",
            "label": 0
          },
          {
            "text": "Our empirical observations suggest that the representative baselines fail to outperform simple heuristic methods in certain evaluation metrics in these new settings.",
            "label": 0
          },
          {
            "text": "Therefore, we believe that Amazon-M2 can inspire novel solutions for session-based recommendation and enable new opportunities for tasks that revolve around large language models and recommender systems.",
            "label": 0
          },
          {
            "text": "denote a dictionary of unique products that appeared in the sessions, and each product is associated with some attributes.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Amazon_M2_ A Multilingual Multi_locale Shopping Session Dataset for Recommendation and Text Generation",
        "section": 6,
        "paragraph_id": 6,
        "full_text": "Designed for session-based recommendation, Amazon-M2 is a large-scale dataset composed of customer shopping sessions with interacted products.Specifically, the dataset consists of two components: (1) user sessions where each session is a list of product IDs interacted by the current user (Figure1a), and (2) a table of products with each row representing the attributes of one product (Figure1b).Particularly, the user sessions come from six different locales, i.e., the United Kingdom (UK), Japan (JP), German (DE), Spain (ES), Italian (IT), and France (FR).Given its multi-locale nature, the dataset is also multilingual: the textual attributes (e.g., title and description) of the products in the user sessions are in multiple languages, namely, English, Italian, French, Germany, and Spanish.",
        "sentences": [
          {
            "text": "Designed for session-based recommendation, Amazon-M2 is a large-scale dataset composed of customer shopping sessions with interacted products.",
            "label": 1
          },
          {
            "text": "Specifically, the dataset consists of two components: (1) user sessions where each session is a list of product IDs interacted by the current user (Figure1a), and (2) a table of products with each row representing the attributes of one product (Figure1b).",
            "label": 1
          },
          {
            "text": "Particularly, the user sessions come from six different locales, i.e., the United Kingdom (UK), Japan (JP), German (DE), Spain (ES), Italian (IT), and France (FR).",
            "label": 1
          },
          {
            "text": "Given its multi-locale nature, the dataset is also multilingual: the textual attributes (e.g., title and description) of the products in the user sessions are in multiple languages, namely, English, Italian, French, Germany, and Spanish.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Amazon_M2_ A Multilingual Multi_locale Shopping Session Dataset for Recommendation and Text Generation",
        "section": 7,
        "paragraph_id": 7,
        "full_text": "Based on this dataset, we construct the training/test dataset for each task.A summary of our session dataset is given in Table2.It includes the number of sessions, the number of interactions, the number of products, and the average session length for six different locales.We can find that UK, DE, and JP have approximately 10 times the number of sessions/products compared to ES, FR, and IT.More details about the collection process of the dataset can be found in Appendix B.",
        "sentences": [
          {
            "text": "Based on this dataset, we construct the training/test dataset for each task.",
            "label": 1
          },
          {
            "text": "A summary of our session dataset is given in Table2.",
            "label": 1
          },
          {
            "text": "It includes the number of sessions, the number of interactions, the number of products, and the average session length for six different locales.",
            "label": 1
          },
          {
            "text": "We can find that UK, DE, and JP have approximately 10 times the number of sessions/products compared to ES, FR, and IT.",
            "label": 1
          },
          {
            "text": "More details about the collection process of the dataset can be found in Appendix B.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Amazon_M2_ A Multilingual Multi_locale Shopping Session Dataset for Recommendation and Text Generation",
        "section": 8,
        "paragraph_id": 8,
        "full_text": "Comparison with Existing Datasets.We summarize the differences between existing session datasets (especially from session-based recommendation) and Amazon-M2 in Table1.First of all, Amazon-M2 is the first dataset to provide textural information while other datasets majorly focus on the product ID information or categorical attributes.Without comprehensive product attributes, the recommendation models may struggle to capture the nuanced preferences of customers.Second, existing datasets only provide sessions from a single locale (or country) which limits their user diversity.Consequently, it may lead to biased or less accurate recommendations, as the models may not capture the full range of customer preferences.By contrast, our proposed Amazon-M2 is collected from multiple locales and is multilingual in nature.Third, our proposed Amazon-M2 provides a large number of user sessions and is on a much larger product scale, which can better reflect real-world recommendation scenarios with huge product bases.",
        "sentences": [
          {
            "text": "Comparison with Existing Datasets.",
            "label": 0
          },
          {
            "text": "We summarize the differences between existing session datasets (especially from session-based recommendation) and Amazon-M2 in Table1.",
            "label": 0
          },
          {
            "text": "First of all, Amazon-M2 is the first dataset to provide textural information while other datasets majorly focus on the product ID information or categorical attributes.",
            "label": 1
          },
          {
            "text": "Without comprehensive product attributes, the recommendation models may struggle to capture the nuanced preferences of customers.",
            "label": 0
          },
          {
            "text": "Second, existing datasets only provide sessions from a single locale (or country) which limits their user diversity.",
            "label": 0
          },
          {
            "text": "Consequently, it may lead to biased or less accurate recommendations, as the models may not capture the full range of customer preferences.",
            "label": 0
          },
          {
            "text": "By contrast, our proposed Amazon-M2 is collected from multiple locales and is multilingual in nature.",
            "label": 1
          },
          {
            "text": "Third, our proposed Amazon-M2 provides a large number of user sessions and is on a much larger product scale, which can better reflect real-world recommendation scenarios with huge product bases.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Amazon_M2_ A Multilingual Multi_locale Shopping Session Dataset for Recommendation and Text Generation",
        "section": 9,
        "paragraph_id": 9,
        "full_text": "In this section, we offer a comprehensive analysis of the Amazon-M2 dataset to uncover valuable insights.Our analysis covers several essential perspectives: long-tail phenomenon, product overlap between locales, session lengths, repeat pattern, and collaborative filtering pattern.Corresponding codes can be found here.",
        "sentences": [
          {
            "text": "In this section, we offer a comprehensive analysis of the Amazon-M2 dataset to uncover valuable insights.",
            "label": 0
          },
          {
            "text": "Our analysis covers several essential perspectives: long-tail phenomenon, product overlap between locales, session lengths, repeat pattern, and collaborative filtering pattern.",
            "label": 0
          },
          {
            "text": "Corresponding codes can be found here.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Amazon_M2_ A Multilingual Multi_locale Shopping Session Dataset for Recommendation and Text Generation",
        "section": 10,
        "paragraph_id": 10,
        "full_text": "Long-tail phenomenon[26,27]is a significant challenge in the session recommendation domain.It refers to the situation where only a handful of products enjoy high popularity, while the majority of products receive only a limited number of interactions.To investigate the presence of the long-tail phenomenon in Amazon-M2 dataset, we analyze the distribution of product frequencies, as depicted  in Figure2a.The results clearly demonstrate the existence of a long-tail distribution, where the head of the distribution represents popular items and the tail consists of less popular ones.Furthermore, we observe that the long-tail phenomenon is also evident within each individual locale.For detailed experimental results regarding this phenomenon in each locale, please refer to Appendix B.2.The long-tail distribution makes it difficult to effectively recommend less popular products, as a small number of popular items dominate the recommendations.",
        "sentences": [
          {
            "text": "Long-tail phenomenon[26,27]is a significant challenge in the session recommendation domain.",
            "label": 0
          },
          {
            "text": "It refers to the situation where only a handful of products enjoy high popularity, while the majority of products receive only a limited number of interactions.",
            "label": 0
          },
          {
            "text": "To investigate the presence of the long-tail phenomenon in Amazon-M2 dataset, we analyze the distribution of product frequencies, as depicted  in Figure2a.",
            "label": 1
          },
          {
            "text": "The results clearly demonstrate the existence of a long-tail distribution, where the head of the distribution represents popular items and the tail consists of less popular ones.",
            "label": 1
          },
          {
            "text": "Furthermore, we observe that the long-tail phenomenon is also evident within each individual locale.",
            "label": 1
          },
          {
            "text": "For detailed experimental results regarding this phenomenon in each locale, please refer to Appendix B.2.",
            "label": 0
          },
          {
            "text": "The long-tail distribution makes it difficult to effectively recommend less popular products, as a small number of popular items dominate the recommendations.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Amazon_M2_ A Multilingual Multi_locale Shopping Session Dataset for Recommendation and Text Generation",
        "section": 11,
        "paragraph_id": 11,
        "full_text": "Product overlap ratio between locales is the proportion of the same products shared by different locales.A large number of overlapping products indicates a better transferability potential when transferring the knowledge from one locale to the other.For example, cross-domain recommendation algorithms like[28]can then be successfully applied, which directly transfers the learned embedding of the overlapping products from popular locales to the underrepresented locales.We then examine product overlap between locales in Amazon-M2 with the product overlap ratio.It is calculated as",
        "sentences": [
          {
            "text": "Product overlap ratio between locales is the proportion of the same products shared by different locales.",
            "label": 0
          },
          {
            "text": "A large number of overlapping products indicates a better transferability potential when transferring the knowledge from one locale to the other.",
            "label": 1
          },
          {
            "text": "For example, cross-domain recommendation algorithms like[28]can then be successfully applied, which directly transfers the learned embedding of the overlapping products from popular locales to the underrepresented locales.",
            "label": 1
          },
          {
            "text": "We then examine product overlap between locales in Amazon-M2 with the product overlap ratio.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Amazon_M2_ A Multilingual Multi_locale Shopping Session Dataset for Recommendation and Text Generation",
        "section": 12,
        "paragraph_id": 12,
        "full_text": "|Na| , where N a and N b correspond to the products set of locale a and b, respectively.In Figure2bwe use a heatmap to show the overlap ratio, where x and y axes stand for locale a and b, respectively.From the figure, we make several observations: (1) For the products in the three large locales, i.e., UK, DE, and JP, there are not many overlapping products, except the one between UK and DE locales;",
        "sentences": [
          {
            "text": "|Na| , where N a and N b correspond to the products set of locale a and b, respectively.",
            "label": 1
          },
          {
            "text": "In Figure2bwe use a heatmap to show the overlap ratio, where x and y axes stand for locale a and b, respectively.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Amazon_M2_ A Multilingual Multi_locale Shopping Session Dataset for Recommendation and Text Generation",
        "section": 13,
        "paragraph_id": 13,
        "full_text": "(2) Considering the product overlap ratio between large locales and underrepresented locales, i.e., ES, FR, and IT, we can see a large product overlapping, indicating products in the underrepresented domain also appear in the large locales.Particularly, the overlap ratio between small locales and DE can reach around 0.4.Thus, it has the potential to facilitate knowledge transfer from large locales and areas to underrepresented regions.",
        "sentences": [
          {
            "text": "(2) Considering the product overlap ratio between large locales and underrepresented locales, i.e., ES, FR, and IT, we can see a large product overlapping, indicating products in the underrepresented domain also appear in the large locales.",
            "label": 1
          },
          {
            "text": "Particularly, the overlap ratio between small locales and DE can reach around 0.4.",
            "label": 1
          },
          {
            "text": "Thus, it has the potential to facilitate knowledge transfer from large locales and areas to underrepresented regions.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Amazon_M2_ A Multilingual Multi_locale Shopping Session Dataset for Recommendation and Text Generation",
        "section": 14,
        "paragraph_id": 14,
        "full_text": "Notably, despite the existence of overlapping products between different locales, there still remains a large proportion of distinguished products in each locale, indicating the difficulty of transferability with distribution shift.Moreover, the multilingual property, where the product textual description from different locales is in different languages, also induces to distribution shift issue.Such a multilingual issue is a long-established topic in the NLP domain.For instance,[29,30,31]point out morphology disparity, tokenization differences, and negative transfer in the multilingual scenario, leading to distribution shift.",
        "sentences": [
          {
            "text": "Notably, despite the existence of overlapping products between different locales, there still remains a large proportion of distinguished products in each locale, indicating the difficulty of transferability with distribution shift.",
            "label": 1
          },
          {
            "text": "Moreover, the multilingual property, where the product textual description from different locales is in different languages, also induces to distribution shift issue.",
            "label": 1
          },
          {
            "text": "Such a multilingual issue is a long-established topic in the NLP domain.",
            "label": 0
          },
          {
            "text": "For instance,[29,30,31]point out morphology disparity, tokenization differences, and negative transfer in the multilingual scenario, leading to distribution shift.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Amazon_M2_ A Multilingual Multi_locale Shopping Session Dataset for Recommendation and Text Generation",
        "section": 15,
        "paragraph_id": 15,
        "full_text": "Session length is an important factor in the session recommendation domain.Typically, a longer session length may lead to the interest shift challenge problem[15]with difficulties in capturing multiple user interests in one single session.Most existing algorithms[13,16]show a better performance on the shorter sessions while failing down on the longer ones.As shown in Figure2c.We can observe that the session length also exhibits a long-tail distribution: most sessions are short while only few sessions are with a length larger than 100.",
        "sentences": [
          {
            "text": "Session length is an important factor in the session recommendation domain.",
            "label": 0
          },
          {
            "text": "Typically, a longer session length may lead to the interest shift challenge problem[15]with difficulties in capturing multiple user interests in one single session.",
            "label": 0
          },
          {
            "text": "Most existing algorithms[13,16]show a better performance on the shorter sessions while failing down on the longer ones.",
            "label": 0
          },
          {
            "text": "As shown in Figure2c.",
            "label": 0
          },
          {
            "text": "We can observe that the session length also exhibits a long-tail distribution: most sessions are short while only few sessions are with a length larger than 100.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Amazon_M2_ A Multilingual Multi_locale Shopping Session Dataset for Recommendation and Text Generation",
        "section": 16,
        "paragraph_id": 16,
        "full_text": "Repeat pattern[32,33,34,35]is also an important user pattern, which refers to the phenomenon that a user repeatedly engages the same products multiple times in a single session.The presence of repeat patterns in recommender systems can potentially result in the system offering more familiar products that match users' existing preferences, which may lead to a less diverse and potentially less satisfying user experience.On the other hand, the repeat pattern is also an important property utilized in the graph-based session recommendation algorithms[13,17,36,37].Typically, those graph-based algorithms construct a session graph where each node represents a product and each edge indicates two products interacted by the user consecutively.Complicated session graphs with different structure patterns can be built when sessions exhibit evident repeat patterns.In Figure2d, we report the proportion of sessions with repeat patterns for the six locales and we can observe that there are around 35% sessions with repeat patterns across different locales.Furthermore, we examine the number of repeat products in those sessions with repeat patterns and report results on the distribution of repeated products in Figure2e.We make two observations: (1) the number of repeated products varies on different sessions; and (2) the number of repeated products in a session also follows the long-tail distribution where most sessions only appear with a few repeated products.",
        "sentences": [
          {
            "text": "Repeat pattern[32,33,34,35]is also an important user pattern, which refers to the phenomenon that a user repeatedly engages the same products multiple times in a single session.",
            "label": 1
          },
          {
            "text": "The presence of repeat patterns in recommender systems can potentially result in the system offering more familiar products that match users' existing preferences, which may lead to a less diverse and potentially less satisfying user experience.",
            "label": 0
          },
          {
            "text": "On the other hand, the repeat pattern is also an important property utilized in the graph-based session recommendation algorithms[13,17,36,37].",
            "label": 0
          },
          {
            "text": "Typically, those graph-based algorithms construct a session graph where each node represents a product and each edge indicates two products interacted by the user consecutively.",
            "label": 0
          },
          {
            "text": "Complicated session graphs with different structure patterns can be built when sessions exhibit evident repeat patterns.",
            "label": 0
          },
          {
            "text": "In Figure2d, we report the proportion of sessions with repeat patterns for the six locales and we can observe that there are around 35% sessions with repeat patterns across different locales.",
            "label": 1
          },
          {
            "text": "Furthermore, we examine the number of repeat products in those sessions with repeat patterns and report results on the distribution of repeated products in Figure2e.",
            "label": 1
          },
          {
            "text": "We make two observations: (1) the number of repeated products varies on different sessions; and (2) the number of repeated products in a session also follows the long-tail distribution where most sessions only appear with a few repeated products.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Amazon_M2_ A Multilingual Multi_locale Shopping Session Dataset for Recommendation and Text Generation",
        "section": 17,
        "paragraph_id": 17,
        "full_text": "Collaborative filtering pattern.Collaborative filtering is a widely used technique that generates recommended products based on the behavior of other similar users.It is generally utilized as an important data argumentation technique to alleviate the data sparsity issue, especially for short sessions[38,39].Since Amazon-M2 encompasses a much larger product set than existing datasets, we investigate whether collaborative filtering techniques can potentially operate in this challenging data environment.Specifically, we utilize the session collaborative filtering algorithm, Session-KNN (SKNN)[11], to identify sessions that are similar to the target user's current session.The similarity score of SKNN can be calculated in the following steps.First, for a particular session s, we first determines a set of its most similar sessions N (s) ⊆ S with the cosine similarity sim(s, s j ) = |s ∩ s j |/ |s||s j |.Second, the score of a candidate product e in similar sessions N (s) is then calculated by: score(e, s) = n∈N (s) sim(s, n)I n (e), where the indicator function I n (e) is • GRU4REC++[40]is an improved model based on GRU4Rec which adopts two techniques to improve the performance of GRU4Rec, including a data augmentation process and a method to account for shifts in the 1input data distribution • NARM[15]employs RNNs with attention mechanism to capture the user's main purpose and sequential behavior.• STAMP[16]captures user's current interest and general interests based on the last-click product and whole session, respectively.• SRGNN[13]is the first to employ GNN layer to capture user interest in the current session.",
        "sentences": [
          {
            "text": "Collaborative filtering pattern.",
            "label": 1
          },
          {
            "text": "Collaborative filtering is a widely used technique that generates recommended products based on the behavior of other similar users.",
            "label": 0
          },
          {
            "text": "It is generally utilized as an important data argumentation technique to alleviate the data sparsity issue, especially for short sessions[38,39].",
            "label": 0
          },
          {
            "text": "Since Amazon-M2 encompasses a much larger product set than existing datasets, we investigate whether collaborative filtering techniques can potentially operate in this challenging data environment.",
            "label": 1
          },
          {
            "text": "Specifically, we utilize the session collaborative filtering algorithm, Session-KNN (SKNN)[11], to identify sessions that are similar to the target user's current session.",
            "label": 0
          },
          {
            "text": "The similarity score of SKNN can be calculated in the following steps.",
            "label": 0
          },
          {
            "text": "First, for a particular session s, we first determines a set of its most similar sessions N (s) ⊆ S with the cosine similarity sim(s, s j ) = |s ∩ s j |/ |s||s j |.",
            "label": 0
          },
          {
            "text": "• SRGNN[13]is the first to employ GNN layer to capture user interest in the current session.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Amazon_M2_ A Multilingual Multi_locale Shopping Session Dataset for Recommendation and Text Generation",
        "section": 18,
        "paragraph_id": 18,
        "full_text": "• CORE[41]ensures that sessions and items are in the same representation space via encoding the session embedding as a linear combination of item embeddings.• MGS[42]incorporates product attribute information to construct a mirror graph, aiming to learn better preference understanding via combining session graph and mirror graph.Notably, MGS can only adapt categorical attributes.Therefore, we discretize the price attribute as the input feature.",
        "sentences": [
          {
            "text": "• CORE[41]ensures that sessions and items are in the same representation space via encoding the session embedding as a linear combination of item embeddings.",
            "label": 0
          },
          {
            "text": "• MGS[42]incorporates product attribute information to construct a mirror graph, aiming to learn better preference understanding via combining session graph and mirror graph.",
            "label": 0
          },
          {
            "text": "Notably, MGS can only adapt categorical attributes.",
            "label": 0
          },
          {
            "text": "Therefore, we discretize the price attribute as the input feature.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Amazon_M2_ A Multilingual Multi_locale Shopping Session Dataset for Recommendation and Text Generation",
        "section": 19,
        "paragraph_id": 19,
        "full_text": "In addition, we include a simple yet effective method, Popularity, by simply recommending all users the most popular products.We utilize Mean Reciprocal Rank@K (MRR@100) and Recall@100 to evaluate various recommendation algorithms.More results on NDCG@100 metric can be found in Appendix C. Corresponding codes can be found here. Results & Observations.The experiment results across different locales can be found in Table3.",
        "sentences": [
          {
            "text": "In addition, we include a simple yet effective method, Popularity, by simply recommending all users the most popular products.",
            "label": 0
          },
          {
            "text": "We utilize Mean Reciprocal Rank@K (MRR@100) and Recall@100 to evaluate various recommendation algorithms.",
            "label": 0
          },
          {
            "text": "More results on NDCG@100 metric can be found in Appendix C.",
            "label": 0
          },
          {
            "text": "Corresponding codes can be found here.",
            "label": 0
          },
          {
            "text": "Results & Observations.",
            "label": 0
          },
          {
            "text": "The experiment results across different locales can be found in Table3.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Amazon_M2_ A Multilingual Multi_locale Shopping Session Dataset for Recommendation and Text Generation",
        "section": 20,
        "paragraph_id": 20,
        "full_text": "We can observe that the popularity heuristic generally outperforms all the deep models with respect to both MRR and Recall.The only exception is that CORE achieves better performance on Recall.On one hand, the success of the popularity heuristic indicates that the product popularity is a strong bias for this dataset.On the other hand, it indicates that the large product set in Amazon-M2 poses great challenges in developing effective recommendation algorithms.Thus, more advanced recommendation strategies are needed for handling the challenging Amazon-M2 dataset.",
        "sentences": [
          {
            "text": "We can observe that the popularity heuristic generally outperforms all the deep models with respect to both MRR and Recall.",
            "label": 0
          },
          {
            "text": "The only exception is that CORE achieves better performance on Recall.",
            "label": 0
          },
          {
            "text": "On one hand, the success of the popularity heuristic indicates that the product popularity is a strong bias for this dataset.",
            "label": 1
          },
          {
            "text": "On the other hand, it indicates that the large product set in Amazon-M2 poses great challenges in developing effective recommendation algorithms.",
            "label": 1
          },
          {
            "text": "Thus, more advanced recommendation strategies are needed for handling the challenging Amazon-M2 dataset.",
            "label": 1
          }
        ]
      },   
      {
        "paper_name": "An Annotated Dataset for Explainable Interpersonal Risk Factors of Mental Disturbance in Social Media Posts",
        "section": 0,
        "paragraph_id": 0,
        "full_text": "Abstract: With a surge in identifying suicidal risk and its severity in social media posts, we argue that a more consequential and explainable research is required for optimal impact on clinical psychology practice and personalized mental healthcare.The success of computational intelligence techniques for inferring mental illness from social media resources, points to natural language processing as a lens for determining Interpersonal Risk Factors (IRF) in human writings.Motivated with limited availability of datasets for social NLP research community, we construct and release a new annotated dataset with human-labelled explanations and classification of IRF affecting mental disturbance on social media: (i) Thwarted Belongingness (TBE), and (ii) Perceived Burdensomeness (PBU).We establish baseline models on our dataset facilitating future research directions to develop realtime personalized AI models by detecting patterns of TBE and PBU in emotional spectrum of user's historical social media profile.",
        "sentences": [
          {
            "text": "Abstract: With a surge in identifying suicidal risk and its severity in social media posts, we argue that a more consequential and explainable research is required for optimal impact on clinical psychology practice and personalized mental healthcare.",
            "label": 0
          },
          {
            "text": "The success of computational intelligence techniques for inferring mental illness from social media resources, points to natural language processing as a lens for determining Interpersonal Risk Factors (IRF) in human writings.",
            "label": 0
          },
          {
            "text": "Motivated with limited availability of datasets for social NLP research community, we construct and release a new annotated dataset with human-labelled explanations and classification of IRF affecting mental disturbance on social media: (i) Thwarted Belongingness (TBE), and (ii) Perceived Burdensomeness (PBU).",
            "label": 1
          },
          {
            "text": "We establish baseline models on our dataset facilitating future research directions to develop realtime personalized AI models by detecting patterns of TBE and PBU in emotional spectrum of user's historical social media profile.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "An Annotated Dataset for Explainable Interpersonal Risk Factors of Mental Disturbance in Social Media Posts",
        "section": 1,
        "paragraph_id": 1,
        "full_text": "The World Health Organization (WHO) emphasizes the importance of significantly accelerating suicide prevention efforts to fulfill the United Nations' Sustainable Development Goal (SDG) objective by 2030(Saxena and Kline, 2021).Reports released in August 20211indicate that 1.6 million people in England were on waiting lists for mental health care.An estimated 8 million people were unable to obtain assistance from a specialist, as they were not considered sick enough to qualify.As suicide remains one of the leading causes of the death worldwide2, this situation underscores the need of mental health interpretations from social media data where people express.",
        "sentences": [
          {
            "text": "The World Health Organization (WHO) emphasizes the importance of significantly accelerating suicide prevention efforts to fulfill the United Nations' Sustainable Development Goal (SDG) objective by 2030(Saxena and Kline, 2021).",
            "label": 0
          },
          {
            "text": "Reports released in August 20211indicate that 1.6 million people in England were on waiting lists for mental health care.",
            "label": 0
          },
          {
            "text": "An estimated 8 million people were unable to obtain assistance from a specialist, as they were not considered sick enough to qualify.",
            "label": 0
          },
          {
            "text": "As suicide remains one of the leading causes of the death worldwide2, this situation underscores the need of mental health interpretations from social media data where people express.",
            "label": 0
          }        
        ]
      },
      {
        "paper_name": "An Annotated Dataset for Explainable Interpersonal Risk Factors of Mental Disturbance in Social Media Posts",
        "section": 2,
        "paragraph_id": 2,
        "full_text": "Thinking beyond Low-level Analysis: Classification and Prediction themselves and their thoughts, beliefs/emotions with ease(Wongkoblap et al., 2022).The individuals dying by suicide hinder the psychological assessments where a self-reported text or personal writings might be a valuable asset in attempting to assess an individual's specific personality status and mind rationale(Garg, 2023).With strong motivation of thinking beyond low-level analysis, Figure1suggests personalization through higherlevel analysis of human writings.As, the social media platforms are frequently relied upon as open fora for honest disclosure(Resnik et al., 2021), we examine mental disturbance in Reddit posts aiming to discover Interpersonal Risk Factors (IRF) in text.",
        "sentences": [
          {
            "text": "Thinking beyond Low-level Analysis: Classification and Prediction themselves and their thoughts, beliefs/emotions with ease(Wongkoblap et al., 2022).",
            "label": 0
          },
          {
            "text": "The individuals dying by suicide hinder the psychological assessments where a self-reported text or personal writings might be a valuable asset in attempting to assess an individual's specific personality status and mind rationale(Garg, 2023).",
            "label": 0
          },
          {
            "text": "With strong motivation of thinking beyond low-level analysis, Figure1suggests personalization through higherlevel analysis of human writings.",
            "label": 0
          },
          {
            "text": "As, the social media platforms are frequently relied upon as open fora for honest disclosure(Resnik et al., 2021), we examine mental disturbance in Reddit posts aiming to discover Interpersonal Risk Factors (IRF) in text.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "An Annotated Dataset for Explainable Interpersonal Risk Factors of Mental Disturbance in Social Media Posts",
        "section": 3,
        "paragraph_id": 3,
        "full_text": "Interpersonal relationships are the strong connections that a person with their closest social circle (peers, intimate-partners and family members) which can shape an individual's behavior and range of experience(Puzia et al., 2014).Affecting such interpersonal relationships influences the associated risk factors resulting in mental disturbance.According to interpersonal-psychological theory of suicidal behavior(Joiner et al., 2005)et al., 2022;Tsakalidis et al., 2022;Gaur et al., 2018)as intrinsic classification task.",
        "sentences": [
          {
            "text": "Interpersonal relationships are the strong connections that a person with their closest social circle (peers, intimate-partners and family members) which can shape an individual's behavior and range of experience(Puzia et al., 2014).",
            "label": 0
          },
          {
            "text": "Affecting such interpersonal relationships influences the associated risk factors resulting in mental disturbance.",
            "label": 0
          },
          {
            "text": "According to interpersonal-psychological theory of suicidal behavior(Joiner et al., 2005)et al., 2022;Tsakalidis et al., 2022;Gaur et al., 2018)as intrinsic classification task.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "An Annotated Dataset for Explainable Interpersonal Risk Factors of Mental Disturbance in Social Media Posts",
        "section": 4,
        "paragraph_id": 4,
        "full_text": "Computational approaches may better understand the technological advancements in psychology research, aiding in the early detection, prediction and evaluation, management and follow-up of those experiencing suicidal thoughts and behaviors.Most automated systems require available datasets for computational advancements.Past studies show that the availability of relevant datasets in mental healthcare domain is scarce for IRF due to sensitive nature of data as shown in Table 1(Su et al., 2020;Garg, 2023).To this end, we introduce an annotated Reddit dataset for classifying TBE and PBU.The explanatory power of this dataset lies in supporting the motivational interviewing and mental health triaging where early detection of potential risk may trigger an alarm for the need of a mental health practitioner.We adhere to ethical considerations for constructing and releasing our dataset publicly on Github5.",
        "sentences": [
          {
            "text": "Computational approaches may better understand the technological advancements in psychology research, aiding in the early detection, prediction and evaluation, management and follow-up of those experiencing suicidal thoughts and behaviors.",
            "label": 0
          },
          {
            "text": "Most automated systems require available datasets for computational advancements.",
            "label": 0
          },
          {
            "text": "Past studies show that the availability of relevant datasets in mental healthcare domain is scarce for IRF due to sensitive nature of data as shown in Table 1(Su et al., 2020;Garg, 2023).",
            "label": 0
          },
          {
            "text": "To this end, we introduce an annotated Reddit dataset for classifying TBE and PBU.",
            "label": 1
          },
          {
            "text": "The explanatory power of this dataset lies in supporting the motivational interviewing and mental health triaging where early detection of potential risk may trigger an alarm for the need of a mental health practitioner.",
            "label": 1
          },
          {
            "text": "We adhere to ethical considerations for constructing and releasing our dataset publicly on Github5.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "An Annotated Dataset for Explainable Interpersonal Risk Factors of Mental Disturbance in Social Media Posts",
        "section": 5,
        "paragraph_id": 5,
        "full_text": "2.1 Corpus ConstructionHaque et al. (2021)used two subreddits r/depression and r/suicidewatch to scrape the SDCNL data and to validate a label correction methodology through manual annotation of this dataset for depression versus suicide.They ad-dressed the then existing ethical issues impacting dataset availability with public release of their dataset.In addition to 1896 posts of SDCNL dataset, we collected 3362 additional instances from Reddit on r/depression and r/SuicideW atch through PRAW API6from 02 December 2021 to 04 January 2022 with about 100 data points per day (to maintain variation in the dataset).On initial screening, we found (i) posts with no self-advocacy, (ii) empty/irrelevant posts.We manually filter them to deduce self-advocacy in texts leveraging 3155 additional samples, which results in a total of 5051 data points(Garg et al., 2022).We removed 694 of the data points depicting no assessment of mental disturbance.Moreover, people write prolonged texts when they indicate IRF which is inline with the conventional arguments where prolonged remarks get better responses from others in comparison of the transient remarks(Park et al., 2015).The length of real-time Reddit posts varies from a few characters to thousands of words.We limit the maximum length of every post to 300 words resulting in 3522 posts as a final corpus.",
        "sentences": [
          {
            "text": "2.1 Corpus ConstructionHaque et al. (2021)used two subreddits r/depression and r/suicidewatch to scrape the SDCNL data and to validate a label correction methodology through manual annotation of this dataset for depression versus suicide.",
            "label": 0
          },
          {
            "text": "They ad-dressed the then existing ethical issues impacting dataset availability with public release of their dataset.",
            "label": 0
          },
          {
            "text": "In addition to 1896 posts of SDCNL dataset, we collected 3362 additional instances from Reddit on r/depression and r/SuicideW atch through PRAW API6from 02 December 2021 to 04 January 2022 with about 100 data points per day (to maintain variation in the dataset).",
            "label": 1
          },
          {
            "text": "On initial screening, we found (i) posts with no self-advocacy, (ii) empty/irrelevant posts.",
            "label": 1
          },
          {
            "text": "We manually filter them to deduce self-advocacy in texts leveraging 3155 additional samples, which results in a total of 5051 data points(Garg et al., 2022).",
            "label": 1
          },
          {
            "text": "We removed 694 of the data points depicting no assessment of mental disturbance.",
            "label": 1
          },
          {
            "text": "Moreover, people write prolonged texts when they indicate IRF which is inline with the conventional arguments where prolonged remarks get better responses from others in comparison of the transient remarks(Park et al., 2015).",
            "label": 0
          },
          {
            "text": "The length of real-time Reddit posts varies from a few characters to thousands of words.",
            "label": 0
          },
          {
            "text": "We limit the maximum length of every post to 300 words resulting in 3522 posts as a final corpus.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "An Annotated Dataset for Explainable Interpersonal Risk Factors of Mental Disturbance in Social Media Posts",
        "section": 6,
        "paragraph_id": 6,
        "full_text": "Classification of IRF, being a complex and highly subjective task, may induce errors with naive judgment.To mitigate this problem, we build a team of three experts: (i) a clinical psychologist for training annotators and validating annotations with psychological viewpoint, (ii) a rehabilitation counselor for comprehending human mind to understand users' IRF, and (iii) a social NLP expert suggesting text based markings in Reddit posts.To negotiate and mitigate the trade-off between three different perspectives, our experts build annotation guidelines 7 to mark (i) TBE, and (ii) PBU.The experts annotated 40 samples of the corpus in isolation using these annotation guidelines to avoid biases and discover possible dilemmas due to the subjective nature of tasks.Therefore, we accommodate perplexity guidelines to simplify the task and facilitate unbiased future annotations.",
        "sentences": [
          {
            "text": "Classification of IRF, being a complex and highly subjective task, may induce errors with naive judgment.",
            "label": 0
          },
          {
            "text": "To mitigate this problem, we build a team of three experts: (i) a clinical psychologist for training annotators and validating annotations with psychological viewpoint, (ii) a rehabilitation counselor for comprehending human mind to understand users' IRF, and (iii) a social NLP expert suggesting text based markings in Reddit posts.",
            "label": 1
          },
          {
            "text": "To negotiate and mitigate the trade-off between three different perspectives, our experts build annotation guidelines 7 to mark (i) TBE, and (ii) PBU.",
            "label": 1
          },
          {
            "text": "The experts annotated 40 samples of the corpus in isolation using these annotation guidelines to avoid biases and discover possible dilemmas due to the subjective nature of tasks.",
            "label": 1
          },
          {
            "text": "Therefore, we accommodate perplexity guidelines to simplify the task and facilitate unbiased future annotations.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "An Annotated Dataset for Explainable Interpersonal Risk Factors of Mental Disturbance in Social Media Posts",
        "section": 7,
        "paragraph_id": 7,
        "full_text": "On observing the statistics of our dataset in Table2, we found 54.71% and 32.56% of positive data points with underlying 255489 and 156620 words for TBE and PBU, respectively.It is interesting to note that although the average number of sentences to express PBU is less than TBE, the observations are different for average number of words.We calculate the Pearson Correlation Coefficient (PCC) for our cross-sectional study on TBE and PBU as 0.0577 which shows slight correlation between the two.Our dataset paves the way for longitudinal studies which is expected to witness increased PCC due to wide spread emotional spectrum(Kolnogorova et al., 2021;Harrigian et al., 2020).On The most frequent words for identifying (i) TBE are alone, lonely, nobody to talk, someone, isolated, lost, and (ii) PBU are die, suicide, suicidal, kill, burden, cut myself. 10Our approach for identifying TBe and PBu goes beyond a simple keyword detector.Instead, we utilize a more sophisticated method that considers the context and relationships between words.For instance, consider a following sample: Massive party at a friend's house-one of 10 WordCloud is given in Appendix C. my closest friends is there, loads of my close friends are there, i wasn't invited.wasn't told.only found out on snapchat from their stories.spending new years eve on teamspeak muting my mic every time i break down :) Despite the absence of trigger words, our approach flags this post as positive for TBu based on its indicators 'friend', 'teamspeak', 'friends', 'invited', 'snapchat', to name a few.",
        "sentences": [
          {
            "text": "On observing the statistics of our dataset in Table2, we found 54.71% and 32.56% of positive data points with underlying 255489 and 156620 words for TBE and PBU, respectively.",
            "label": 1
          },
          {
            "text": "It is interesting to note that although the average number of sentences to express PBU is less than TBE, the observations are different for average number of words.",
            "label": 1
          },
          {
            "text": "We calculate the Pearson Correlation Coefficient (PCC) for our cross-sectional study on TBE and PBU as 0.0577 which shows slight correlation between the two.",
            "label": 0
          },
          {
            "text": "Our dataset paves the way for longitudinal studies which is expected to witness increased PCC due to wide spread emotional spectrum(Kolnogorova et al., 2021;Harrigian et al., 2020).",
            "label": 0
          },
          {
            "text": "On The most frequent words for identifying (i) TBE are alone, lonely, nobody to talk, someone, isolated, lost, and (ii) PBU are die, suicide, suicidal, kill, burden, cut myself.",
            "label": 1
          },
          {
            "text": "10Our approach for identifying TBe and PBu goes beyond a simple keyword detector.",
            "label": 0
          },
          {
            "text": "Instead, we utilize a more sophisticated method that considers the context and relationships between words.",
            "label": 0
          },
          {
            "text": "For instance, consider a following sample: Massive party at a friend's house-one of 10 WordCloud is given in Appendix C.",
            "label": 0
          },
          {
            "text": "spending new years eve on teamspeak muting my mic every time i break down :) Despite the absence of trigger words, our approach flags this post as positive for TBu based on its indicators 'friend', 'teamspeak', 'friends', 'invited', 'snapchat', to name a few.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "An Annotated Dataset for Explainable Interpersonal Risk Factors of Mental Disturbance in Social Media Posts",
        "section": 8,
        "paragraph_id": 8,
        "full_text": "We perform extensive analysis to build baselines with three different conventional methods.We first apply Recurrent neural networks where a given text, embedded with GloVe 840B-300 11 , is sent to a 2-layer RNN model (LSTM, GRU) with 64 hidden neurons and the output is forwarded to two separate fully connected heads: (i) TBE and (ii) PBU.Each of the fully connected blocks have one hidden layer with 16 neurons and ReLU activation function, and an output layer with sigmoid activation.The loss function is Binary_CrossEntropy and optimizer is adam with lr = 0.001.Next, we apply pretrained transformer-based models.The input is tokenized using a pre-trained transformers' tokenizer to obtain a 768-dimensional vector which is then fed to a similar fully connected network as the previous architecture with hidden layer size as 48.We experimented with roberta-base, bert-base-uncased, distilbert-base-uncased, and mental/mental-bertbase-uncased models.Finally, we use the Ope-nAI embeddings API12to convert the input text into 1536-dimensional embeddings through 'textembedding-ada-002' engine which are used to train a classifier.We test the robustness of this approach over: (i) Logistic Regression, (ii) Random Forest, (iii) Support Vector Machine (iv) Multi Layer Perceptron, and (v) XGBoost.We further use two explainable methods: (i) LIME and (ii) SHAP on one of the best performing transformer-based models, MentalBERT(Ji et al., 2022), to obtain the top keywords(Danilevsky et al., 2020;Zirikly and Dredze, 2022).We compare them with the ground truth ROUGE scores for -Precision (P), Recall (R), and F1-score (F).",
        "sentences": [
          {
            "text": "We perform extensive analysis to build baselines with three different conventional methods.",
            "label": 0
          },
          {
            "text": "We first apply Recurrent neural networks where a given text, embedded with GloVe 840B-300 11 , is sent to a 2-layer RNN model (LSTM, GRU) with 64 hidden neurons and the output is forwarded to two separate fully connected heads: (i) TBE and (ii) PBU.",
            "label": 0
          },
          {
            "text": "Each of the fully connected blocks have one hidden layer with 16 neurons and ReLU activation function, and an output layer with sigmoid activation.",
            "label": 0
          },
          {
            "text": "The loss function is Binary_CrossEntropy and optimizer is adam with lr = 0.001.",
            "label": 0
          },
          {
            "text": "Next, we apply pretrained transformer-based models.",
            "label": 0
          },
          {
            "text": "The input is tokenized using a pre-trained transformers' tokenizer to obtain a 768-dimensional vector which is then fed to a similar fully connected network as the previous architecture with hidden layer size as 48.",
            "label": 0
          },
          {
            "text": "We experimented with roberta-base, bert-base-uncased, distilbert-base-uncased, and mental/mental-bertbase-uncased models.",
            "label": 0
          },
          {
            "text": "Finally, we use the Ope-nAI embeddings API12to convert the input text into 1536-dimensional embeddings through 'textembedding-ada-002' engine which are used to train a classifier.",
            "label": 0
          },
          {
            "text": "We test the robustness of this approach over: (i) Logistic Regression, (ii) Random Forest, (iii) Support Vector Machine (iv) Multi Layer Perceptron, and (v) XGBoost.",
            "label": 0
          },
          {
            "text": "We further use two explainable methods: (i) LIME and (ii) SHAP on one of the best performing transformer-based models, MentalBERT(Ji et al., 2022), to obtain the top keywords(Danilevsky et al., 2020;Zirikly and Dredze, 2022).",
            "label": 0
          },
          {
            "text": "We compare them with the ground truth ROUGE scores for -Precision (P), Recall (R), and F1-score (F).",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "An Annotated Dataset for Explainable Interpersonal Risk Factors of Mental Disturbance in Social Media Posts",
        "section": 9,
        "paragraph_id": 9,
        "full_text": "For consistency, we used the same experimental settings for all models and split the dataset into the train, validation, and test sets.All results are reported on the test set, which makes up 30% of the whole dataset.We used the grid search optimization technique to optimize the parameters.To tune the number of layers (n), we empirically experimented with the values: learning rate (lr): lr ∈ {0.001, 0.0001, 0.00001} and optimization (O): O ∈ {'Adam', 'Adamax', 'AdamW'} with a batchsize of 16, 32 were used.We used base version pre-trained language models (LMs) using Hugging-Face13, an open-source Python library.We used optimized parameters for each baseline to find precision, recall, F1-score, and Accuracy.Varying lengths of posts are padded to 256 tokens with truncation.Each model was trained for 20 epochs, and the best-performing model based on the average accuracy score was saved.Thus, we set hyperparameter for our experiments as Optimizer = Adam, learning rate = 1e-3, batch size= 16, and epochs=20.",
        "sentences": [
          {
            "text": "For consistency, we used the same experimental settings for all models and split the dataset into the train, validation, and test sets.",
            "label": 0
          },
          {
            "text": "All results are reported on the test set, which makes up 30% of the whole dataset.",
            "label": 1
          },
          {
            "text": "We used the grid search optimization technique to optimize the parameters.",
            "label": 0
          },
          {
            "text": "To tune the number of layers (n), we empirically experimented with the values: learning rate (lr): lr ∈ {0.001, 0.0001, 0.00001} and optimization (O): O ∈ {'Adam', 'Adamax', 'AdamW'} with a batchsize of 16, 32 were used.",
            "label": 0
          },
          {
            "text": "We used base version pre-trained language models (LMs) using Hugging-Face13, an open-source Python library.",
            "label": 0
          },
          {
            "text": "We used optimized parameters for each baseline to find precision, recall, F1-score, and Accuracy.",
            "label": 0
          },
          {
            "text": "Varying lengths of posts are padded to 256 tokens with truncation.",
            "label": 0
          },
          {
            "text": "Each model was trained for 20 epochs, and the best-performing model based on the average accuracy score was saved.",
            "label": 0
          },
          {
            "text": "Thus, we set hyperparameter for our experiments as Optimizer = Adam, learning rate = 1e-3, batch size= 16, and epochs=20.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "An Annotated Dataset for Explainable Interpersonal Risk Factors of Mental Disturbance in Social Media Posts",
        "section": 10,
        "paragraph_id": 10,
        "full_text": "Table3shows the performance of state-of-the-art methods in terms of precision, recall, F1-score, and accuracy.The current models have moderately low performance in this task, possibly due to a lack of ability to capture contextual information in the text.MentalBERT, a transformer-based language model, initialized with BERT-Base and trained with mental health-related posts collected from Reddit, had the best performance among BERT-based models, with an F1-score of 76.73% and 62.77% for TBE and PBU, respectively.This is likely due to the fact that it was trained on the same context as the task, namely health-related posts on Reddit.The combination of OpenAI embeddings and a classifier outperforms RNN and transformer-based models.The highest F1-Score of 81.23% was achieved by logistic regression for TBE, while the best performing model for PBU was SVM with an F1-score of 76.90%.We also analyzed the explainability of the model using LIME and SHAP methods of explainable AI for NLP on the best performing transformer model (MentalBERT) for TBE and PBU.We obtain results for all positive data points in the testing dataset and observe high recall of text-spans with reference to the ground truth as shown in Table4.We find the scope of improvement by limiting the superfluous text-spans found in the resulting set of words.The consistency in results suggests the need of contextual/domain-specific knowledge and infusing commonsense to improve explainable classifiers for a given task.",
        "sentences": [
          {
            "text": "Table3shows the performance of state-of-the-art methods in terms of precision, recall, F1-score, and accuracy.",
            "label": 0
          },
          {
            "text": "The current models have moderately low performance in this task, possibly due to a lack of ability to capture contextual information in the text.",
            "label": 0
          },
          {
            "text": "MentalBERT, a transformer-based language model, initialized with BERT-Base and trained with mental health-related posts collected from Reddit, had the best performance among BERT-based models, with an F1-score of 76.73% and 62.77% for TBE and PBU, respectively.",
            "label": 0
          },
          {
            "text": "This is likely due to the fact that it was trained on the same context as the task, namely health-related posts on Reddit.",
            "label": 0
          },
          {
            "text": "The combination of OpenAI embeddings and a classifier outperforms RNN and transformer-based models.",
            "label": 0
          },
          {
            "text": "The highest F1-Score of 81.23% was achieved by logistic regression for TBE, while the best performing model for PBU was SVM with an F1-score of 76.90%.",
            "label": 0
          },
          {
            "text": "We also analyzed the explainability of the model using LIME and SHAP methods of explainable AI for NLP on the best performing transformer model (MentalBERT) for TBE and PBU.",
            "label": 0
          },
          {
            "text": "We obtain results for all positive data points in the testing dataset and observe high recall of text-spans with reference to the ground truth as shown in Table4.",
            "label": 0
          },
          {
            "text": "We find the scope of improvement by limiting the superfluous text-spans found in the resulting set of words.",
            "label": 0
          },
          {
            "text": "The consistency in results suggests the need of contextual/domain-specific knowledge and infusing commonsense to improve explainable classifiers for a given task.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Antisemitic Messages_ A Guide to High_Quality Annotation and a Labeled Dataset of Tweets",
        "section": 0,
        "paragraph_id": 0,
        "full_text": "Abstract: One of the major challenges in automatic hate speech detection is the lack of datasets that cover a wide range of biased and unbiased messages and that are consistently labeled.We propose a labeling procedure that addresses some of the common weaknesses of labeled datasets.We focus on antisemitic speech on Twitter and create a labeled dataset of 6,941 tweets that cover a wide range of topics common in conversations about Jews, Israel, and antisemitism between January 2019 and December 2021 by drawing from representative samples with relevant keywords.Our annotation process aims to strictly apply a commonly used definition of antisemitism by forcing annotators to specify which part of the definition applies, and by giving them the option to personally disagree with the definition on a case-by-case basis.Labeling tweets that call out antisemitism, report antisemitism, or are otherwise related to antisemitism (such as the Holocaust) but are not actually antisemitic can help reduce false positives in automated detection.The dataset includes 1,250 tweets (18%) that are antisemitic according to the International Holocaust Remembrance Alliance (IHRA) definition of antisemitism.",
        "sentences": [
          {
            "text": "Abstract: One of the major challenges in automatic hate speech detection is the lack of datasets that cover a wide range of biased and unbiased messages and that are consistently labeled.",
            "label": 0
          },
          {
            "text": "We propose a labeling procedure that addresses some of the common weaknesses of labeled datasets.",
            "label": 0
          },
          {
            "text": "We focus on antisemitic speech on Twitter and create a labeled dataset of 6,941 tweets that cover a wide range of topics common in conversations about Jews, Israel, and antisemitism between January 2019 and December 2021 by drawing from representative samples with relevant keywords.",
            "label": 1
          },
          {
            "text": "Our annotation process aims to strictly apply a commonly used definition of antisemitism by forcing annotators to specify which part of the definition applies, and by giving them the option to personally disagree with the definition on a case-by-case basis.",
            "label": 1
          },
          {
            "text": "Labeling tweets that call out antisemitism, report antisemitism, or are otherwise related to antisemitism (such as the Holocaust) but are not actually antisemitic can help reduce false positives in automated detection.",
            "label": 1
          },
          {
            "text": "The dataset includes 1,250 tweets (18%) that are antisemitic according to the International Holocaust Remembrance Alliance (IHRA) definition of antisemitism.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Antisemitic Messages_ A Guide to High_Quality Annotation and a Labeled Dataset of Tweets",
        "section": 1,
        "paragraph_id": 1,
        "full_text": "It is important to note, however, that the dataset is not comprehensive.Many topics are still not covered, and it only includes tweets collected from Twitter between January 2019 and December 2021.Additionally, the dataset only includes tweets that were written in English.Despite these limitations, we hope that this is a meaningful contribution to improving the automated detection of antisemitic speech.",
        "sentences": [
          {
            "text": "It is important to note, however, that the dataset is not comprehensive.",
            "label": 1
          },
          {
            "text": "Many topics are still not covered, and it only includes tweets collected from Twitter between January 2019 and December 2021.",
            "label": 1
          },
          {
            "text": "Additionally, the dataset only includes tweets that were written in English.",
            "label": 1
          },
          {
            "text": "Despite these limitations, we hope that this is a meaningful contribution to improving the automated detection of antisemitic speech.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Antisemitic Messages_ A Guide to High_Quality Annotation and a Labeled Dataset of Tweets",
        "section": 2,
        "paragraph_id": 2,
        "full_text": "Automated hate speech detection has advanced significantly in recent years with the development of deep learning techniques and large-scale training data.Many studies have shown that machine learning algorithms, especially deep neural networks, can achieve high accuracy in detecting hate speech within test datasets.",
        "sentences": [
          {
            "text": "Automated hate speech detection has advanced significantly in recent years with the development of deep learning techniques and large-scale training data.",
            "label": 0
          },
          {
            "text": "Many studies have shown that machine learning algorithms, especially deep neural networks, can achieve high accuracy in detecting hate speech within test datasets.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Antisemitic Messages_ A Guide to High_Quality Annotation and a Labeled Dataset of Tweets",
        "section": 3,
        "paragraph_id": 3,
        "full_text": "Hate speech detection using automated methods such as BERT, ELECTRA, or Perspective API has been developed to investigate large datasets containing toxic speech patterns and conspiracy-related content(Alkomah and Ma 2022;MacAvaney et al. 2019;Poletto et al. 2021).",
        "sentences": [
          {
            "text": "Hate speech detection using automated methods such as BERT, ELECTRA, or Perspective API has been developed to investigate large datasets containing toxic speech patterns and conspiracy-related content(Alkomah and Ma 2022;MacAvaney et al. 2019;Poletto et al. 2021).",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Antisemitic Messages_ A Guide to High_Quality Annotation and a Labeled Dataset of Tweets",
        "section": 4,
        "paragraph_id": 4,
        "full_text": "However, the task of detecting hate speech remains challenging for several reasons(Yin and Zubiaga 2021).First, the datasets on which the models are trained are relatively small.They do not include all variations of hate speech manifestations in a rapidly changing environment.Second, there is no single definition of hate speech.It can be expressed in a variety of ways, often involves a high degree of subjectivity, and depends on cultural, social, and historical factors, making it difficult to identify and classify consistently.Third, proper classification often requires more context than what is readily available, such as previous discussions in a thread or a history of ironic messages from particular users.Calling out hate speech or reporting on stereotypes often results in false positives.A quick test with ChatGPT can illustrate this.ChatGPT correctly identifies antisemitic stereotypes in the message \"Fox News trashes Georges Soros while praising Joe Rogan using some antisemitic tropes -puppet master using his money to control the world.Then Pete Hegseth goes into a rant about the nonsense conspiracy theory Cultural Marxism.This is from Fox & Friends morning show.\"However, ChatGPT classifies it as an antisemitic message itself.Despite these challenges, the research community has made significant progress in developing models that can detect hate speech.These models often rely on a combination of linguistic features, such as word ngrams, word embeddings, and contextual features, such as the identity of the author or the presence of certain keywords.",
        "sentences": [
          {
            "text": "However, the task of detecting hate speech remains challenging for several reasons(Yin and Zubiaga 2021).",
            "label": 0
          },
          {
            "text": "First, the datasets on which the models are trained are relatively small.",
            "label": 0
          },
          {
            "text": "They do not include all variations of hate speech manifestations in a rapidly changing environment.",
            "label": 0
          },
          {
            "text": "Second, there is no single definition of hate speech.",
            "label": 0
          },
          {
            "text": "It can be expressed in a variety of ways, often involves a high degree of subjectivity, and depends on cultural, social, and historical factors, making it difficult to identify and classify consistently.",
            "label": 0
          },
          {
            "text": "Third, proper classification often requires more context than what is readily available, such as previous discussions in a thread or a history of ironic messages from particular users.",
            "label": 0
          },
          {
            "text": "Calling out hate speech or reporting on stereotypes often results in false positives.",
            "label": 0
          },
          {
            "text": "A quick test with ChatGPT can illustrate this.",
            "label": 0
          },
          {
            "text": "ChatGPT correctly identifies antisemitic stereotypes in the message \"Fox News trashes Georges Soros while praising Joe Rogan using some antisemitic tropes -puppet master using his money to control the world.",
            "label": 0
          },
          {
            "text": "Then Pete Hegseth goes into a rant about the nonsense conspiracy theory Cultural Marxism.",
            "label": 0
          },
          {
            "text": "This is from Fox & Friends morning show.",
            "label": 0
          },
          {
            "text": "\"However, ChatGPT classifies it as an antisemitic message itself.",
            "label": 0
          },
          {
            "text": "Despite these challenges, the research community has made significant progress in developing models that can detect hate speech.",
            "label": 0
          },
          {
            "text": "These models often rely on a combination of linguistic features, such as word ngrams, word embeddings, and contextual features, such as the identity of the author or the presence of certain keywords.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Antisemitic Messages_ A Guide to High_Quality Annotation and a Labeled Dataset of Tweets",
        "section": 5,
        "paragraph_id": 5,
        "full_text": "It's worth noting that while the quality of these models has improved, they are not perfect and can still produce false positives or false negatives.As a result, it is important to use these models as part of a larger framework that includes human review and oversight.It is also important to ensure that these models are trained on diverse, representative data and that their results are interpretable and transparent.",
        "sentences": [
          {
            "text": "It's worth noting that while the quality of these models has improved, they are not perfect and can still produce false positives or false negatives.",
            "label": 0
          },
          {
            "text": "As a result, it is important to use these models as part of a larger framework that includes human review and oversight.",
            "label": 0
          },
          {
            "text": "It is also important to ensure that these models are trained on diverse, representative data and that their results are interpretable and transparent.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Antisemitic Messages_ A Guide to High_Quality Annotation and a Labeled Dataset of Tweets",
        "section": 6,
        "paragraph_id": 6,
        "full_text": "Narrowing hate speech to hostile attitudes toward specific communities or groups of people can help improve automated detection models, as it may be easier to consistently label datasets with a precise definition.",
        "sentences": [
          {
            "text": "Narrowing hate speech to hostile attitudes toward specific communities or groups of people can help improve automated detection models, as it may be easier to consistently label datasets with a precise definition.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Antisemitic Messages_ A Guide to High_Quality Annotation and a Labeled Dataset of Tweets",
        "section": 7,
        "paragraph_id": 7,
        "full_text": "While there are many labeled datasets on hate speech in general, few datasets focus specifically on antisemitism.(Chandra et al. 2021) built a labeled dataset on antisemitism from 3,102 posts on Twitter and 3,509 posts on Gab, focusing on messages containing both images and text, as well as words related to Jews, such as \"Jewish,\" \"Hasidic,\" \"Hebrew,\" \"Semitic,\" \"Judaistic,\" \"israeli,\" \"yahudi,\" \"yehudi,\" and also slurs.The Twitter dataset, including IDs and annotations, is publicly available.1Three annotators labeled posts as antisemitic or not and classified antisemitic posts into one of the four categories: political, economic, religious, or racial antisemitism.",
        "sentences": [
          {
            "text": "While there are many labeled datasets on hate speech in general, few datasets focus specifically on antisemitism.",
            "label": 0
          },
          {
            "text": "(Chandra et al. 2021) built a labeled dataset on antisemitism from 3,102 posts on Twitter and 3,509 posts on Gab, focusing on messages containing both images and text, as well as words related to Jews, such as \"Jewish,\" \"Hasidic,\" \"Hebrew,\" \"Semitic,\" \"Judaistic,\" \"israeli,\" \"yahudi,\" \"yehudi,\" and also slurs.",
            "label": 0
          },
          {
            "text": "The Twitter dataset, including IDs and annotations, is publicly available.1Three annotators labeled posts as antisemitic or not and classified antisemitic posts into one of the four categories: political, economic, religious, or racial antisemitism.(Steffen et al. 2022) labeled a dataset of 3,663 German-language Telegram messages about antisemitism and conspiracy theories.They retrieved the messages from Telegram channels that were used to protest government measures to contain the pandemic.The messages were posted between March 11, 2020, and December 19, 2021.Both projects make their datasets available publicly or, in the latter case, on request.Both projects use the International Holocaust Remembrance Alliance's Working Definition of Antisemitism (IHRA Definition) as a guideline for determining whether a message is antisemitic or not.2This is also the case for Schwarz-Friesel's comprehensive study of online messages in German (Schwarz-Friesel 2019).Guhl et al.'s study of the German far-right(Guhl, Ebner, and Rau 2020), and the ongoing Decoding Antisemitism project, which examines online comments on articles in mainstream media outlets in English, German, and French(Ascone et al. 2022;Becker and Allington 2021).We have shown that the definition can be successfully used to classify online messages when inferences from the definition, such as \"classical antisemitic stereotypes,\" are spelled out(Jikeli, Cavar, and Miehling 2019).",
            "label": 0
          },
          {
            "text": "(Steffen et al. 2022) labeled a dataset of 3,663 German-language Telegram messages about antisemitism and conspiracy theories.",
            "label": 0
          },
          {
            "text": "They retrieved the messages from Telegram channels that were used to protest government measures to contain the pandemic.",
            "label": 0
          },
          {
            "text": "The messages were posted between March 11, 2020, and December 19, 2021.",
            "label": 0
          },
          {
            "text": "Both projects make their datasets available publicly or, in the latter case, on request.",
            "label": 0
          },
          {
            "text": "2This is also the case for Schwarz-Friesel's comprehensive study of online messages in German (Schwarz-Friesel 2019).",
            "label": 0
          },
          {
            "text": "Guhl et al.'s study of the German far-right(Guhl, Ebner, and Rau 2020), and the ongoing Decoding Antisemitism project, which examines online comments on articles in mainstream media outlets in English, German, and French(Ascone et al. 2022;Becker and Allington 2021).",
            "label": 0
          },
          {
            "text": "We have shown that the definition can be successfully used to classify online messages when inferences from the definition, such as \"classical antisemitic stereotypes,\" are spelled out(Jikeli, Cavar, and Miehling 2019).",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Antisemitic Messages_ A Guide to High_Quality Annotation and a Labeled Dataset of Tweets",
        "section": 8,
        "paragraph_id": 8,
        "full_text": "Research on automated detection of hate speech and conspiracy theories related to the Covid-19 pandemic or QAnon has shown that Jews figure prominently in conspiracy fantasies(Vergani et al. 2022;Hoseini et al. 2021;La Morgia et al. 2021).Automated detection of antisemitic speech could also help to identify conspiracy theories.",
        "sentences": [
          {
            "text": "Research on automated detection of hate speech and conspiracy theories related to the Covid-19 pandemic or QAnon has shown that Jews figure prominently in conspiracy fantasies(Vergani et al. 2022;Hoseini et al. 2021;La Morgia et al. 2021).",
            "label": 0
          },
          {
            "text": "Automated detection of antisemitic speech could also help to identify conspiracy theories.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Antisemitic Messages_ A Guide to High_Quality Annotation and a Labeled Dataset of Tweets",
        "section": 9,
        "paragraph_id": 9,
        "full_text": "However, time-consuming manual annotation and consistent labeling are the bottleneck for most supervised machine learning projects.Our project on antisemitic tweets is in principle not different from many other projects on hate speech dataset, which include defining a classification scheme, labeling guidelines, collecting adequate data, preprocessing this data according to the task, training experts on labeling, and building a final corpus(Pustejovsky and Stubbs 2013).",
        "sentences": [
          {
            "text": "However, time-consuming manual annotation and consistent labeling are the bottleneck for most supervised machine learning projects.",
            "label": 0
          },
          {
            "text": "Our project on antisemitic tweets is in principle not different from many other projects on hate speech dataset, which include defining a classification scheme, labeling guidelines, collecting adequate data, preprocessing this data according to the task, training experts on labeling, and building a final corpus(Pustejovsky and Stubbs 2013).",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Antisemitic Messages_ A Guide to High_Quality Annotation and a Labeled Dataset of Tweets",
        "section": 10,
        "paragraph_id": 10,
        "full_text": "However, we propose a number of measures to ensure the production of high quality datasets.We are making the IDs and the text of our dataset available, along with our label of whether or not they are antisemitic.Later this year, we will add the label of calling out antisemitism.The usernames in the tweets are not anonymized, as we believe this information may be useful for further research.",
        "sentences": [
          {
            "text": "However, we propose a number of measures to ensure the production of high quality datasets.",
            "label": 1
          },
          {
            "text": "We are making the IDs and the text of our dataset available, along with our label of whether or not they are antisemitic.",
            "label": 1
          },
          {
            "text": "Later this year, we will add the label of calling out antisemitism.",
            "label": 1
          },
          {
            "text": "The usernames in the tweets are not anonymized, as we believe this information may be useful for further research.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Antisemitic Messages_ A Guide to High_Quality Annotation and a Labeled Dataset of Tweets",
        "section": 11,
        "paragraph_id": 11,
        "full_text": "Our corpus covers a wide range of antisemitic and nonantisemitic conversations about Jews on Twitter from 2019 to 2021.We used Indiana University's Observatory on Social Media (OSoMe) database to identify relevant messages.The OSoMe database contains 10 percent of all live tweets on a statistically relevant basis.We queried with two keywords that are likely to result in a wide range of conversations about Jews as a religious, ethnic, or political community: \"Jews\" and \"Israel.\"We then added samples with more targeted keywords that are likely to generate a high percentage of antisemitic tweets, namely the slurs \"K---s\" 3 and \"ZioNazi*.\"We ran 14 queries for different time periods between January 2019 and December 2021.The queries returned tweet ID numbers.We then randomly sampled 2,000 tweets from each query, pulled the text and metadata from the Twitter archive from those that were still live, filtered for English tweets using Google's language detection library, and randomly selected 500 tweets from the remaining tweets. 4We took screenshots of the sampled tweets for documentation.We repeated this process for four queries, resulting in two samples for those queries and 18 samples in total, see Table1.",
        "sentences": [
          {
            "text": "Our corpus covers a wide range of antisemitic and nonantisemitic conversations about Jews on Twitter from 2019 to 2021.",
            "label": 1
          },
          {
            "text": "We used Indiana University's Observatory on Social Media (OSoMe) database to identify relevant messages.",
            "label": 1
          },
          {
            "text": "The OSoMe database contains 10 percent of all live tweets on a statistically relevant basis.",
            "label": 1
          },
          {
            "text": "We queried with two keywords that are likely to result in a wide range of conversations about Jews as a religious, ethnic, or political community: \"Jews\" and \"Israel.\"",
            "label": 1
          },
          {
            "text": "We then added samples with more targeted keywords that are likely to generate a high percentage of antisemitic tweets, namely the slurs \"K---s\" 3 and \"ZioNazi*.\"",
            "label": 1
          },
          {
            "text": "\"We ran 14 queries for different time periods between January 2019 and December 2021.",
            "label": 1
          },
          {
            "text": "The queries returned tweet ID numbers.",
            "label": 1
          },
          {
            "text": "We then randomly sampled 2,000 tweets from each query, pulled the text and metadata from the Twitter archive from those that were still live, filtered for English tweets using Google's language detection library, and randomly selected 500 tweets from the remaining tweets.",
            "label": 1
          },
          {
            "text": "4We took screenshots of the sampled tweets for documentation.",
            "label": 1
          },
          {
            "text": "We repeated this process for four queries, resulting in two samples for those queries and 18 samples in total, see Table1.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Antisemitic Messages_ A Guide to High_Quality Annotation and a Labeled Dataset of Tweets",
        "section": 12,
        "paragraph_id": 12,
        "full_text": "We annotated the tweets, considering the text, images, videos, and links, in their \"natural\" context, including threads.We used a detailed annotation guideline(Jikeli, Cavar, and Miehling 2019), based on the IHRA Definition, which has been endorsed and recommended by more than 30 governments and international organizations 5 and is frequently used to monitor and record antisemitic incidents.We divided the definition into 12 paragraphs.Each of the paragraphs addresses different forms and tropes of antisemitism.We created an online annotation tool (https://annotationportal.com) to make labeling easier, more consistent, and less prone to errors, including in the process of recording the annotations.The portal displays the tweet and a clickable annotation form, see Figure1.It automatically saves each annotation, including the time spent labeling each tweet.",
        "sentences": [
          {
            "text": "We annotated the tweets, considering the text, images, videos, and links, in their \"natural\" context, including threads.",
            "label": 1
          },
          {
            "text": "We used a detailed annotation guideline(Jikeli, Cavar, and Miehling 2019), based on the IHRA Definition, which has been endorsed and recommended by more than 30 governments and international organizations 5 and is frequently used to monitor and record antisemitic incidents.",
            "label": 1
          },
          {
            "text": "We divided the definition into 12 paragraphs.",
            "label": 1
          },
          {
            "text": "Each of the paragraphs addresses different forms and tropes of antisemitism.",
            "label": 1
          },
          {
            "text": "We created an online annotation tool (https://annotationportal.com) to make labeling easier, more consistent, and less prone to errors, including in the process of recording the annotations.",
            "label": 1
          },
          {
            "text": "The portal displays the tweet and a clickable annotation form, see Figure1.",
            "label": 1
          },
          {
            "text": "It automatically saves each annotation, including the time spent labeling each tweet.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Antisemitic Messages_ A Guide to High_Quality Annotation and a Labeled Dataset of Tweets",
        "section": 13,
        "paragraph_id": 13,
        "full_text": "The Annotation Portal retrieves live tweets by referencing their ID number.Our annotators first look at the tweet, and if they are unsure of the meaning, they are prompted to look at the entire thread, replies, likes, links, and comments.A click on the visualized tweet opens a new tab in the browser, displaying the message on the Twitter page in its \"natural\" environment.",
        "sentences": [
          {
            "text": "The Annotation Portal retrieves live tweets by referencing their ID number.",
            "label": 1
          },
          {
            "text": "Our annotators first look at the tweet, and if they are unsure of the meaning, they are prompted to look at the entire thread, replies, likes, links, and comments.",
            "label": 1
          },
          {
            "text": "A click on the visualized tweet opens a new tab in the browser, displaying the message on the Twitter page in its \"natural\" environment.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Antisemitic Messages_ A Guide to High_Quality Annotation and a Labeled Dataset of Tweets",
        "section": 14,
        "paragraph_id": 14,
        "full_text": "The portal is designed to help annotators consistently label messages as antisemitic or not according to the IHRA definition.After verifying that the message is still live and in English, they select from a drop-down menu where they classify the message as \"confident antisemitic,\" \"probably antisemitic,\" \"probably not 3 \"K----s\" stands for the antisemitic slur \"kikes.\" 4 For the queries with the two slurs the sample size was smaller because fewer tweets remained after this process.5 https://www.holocaustremembrance.com/resources/workinantisemitic,\" \"confident not antisemitic,\" or \"don't know.\"The annotation guideline, including the definition, is linked in a PDF document.When annotators label a tweet as \"probably\" or \"confident\" antisemitic, they must also select an applicable section of the IHRA definition to move on to the next tweet.If annotators feel the tweet is antisemitic, but no section of the definition applies, they will classify the tweet as not antisemitic according to the IHRA definition, and check a box indicating that they disagree with the IHRA definition for that tweet.Annotators can also use this the other way around, that is, they can label the message as antisemitic according to the IHRA definition, but personally disagree with it in a particular case.The option to personally disagree g-definitions-charters/working-definitionantisemitism/adoption-endorsement 6 This is a screenshot of our updated form.The question about the content type was not used for the annotation of all samples in this dataset. with the definition on a case-by-case basis is intended to encourage a stricter application of the IHRA definition rather than the individual definitions of the annotators.",
        "sentences": [
          {
            "text": "The portal is designed to help annotators consistently label messages as antisemitic or not according to the IHRA definition.",
            "label": 0
          },
          {
            "text": "After verifying that the message is still live and in English, they select from a drop-down menu where they classify the message as \"confident antisemitic,\" \"probably antisemitic,\" \"probably not 3 \"K----s\" stands for the antisemitic slur \"kikes.\" ",
            "label": 1
          },
          {
            "text": "4 For the queries with the two slurs the sample size was smaller because fewer tweets remained after this process.5 https://www.holocaustremembrance.com/resources/workinantisemitic,\" \"confident not antisemitic,\" or \"don't know.\"",
            "label": 1
          },
          {
            "text": "\"The annotation guideline, including the definition, is linked in a PDF document.",
            "label": 1
          },
          {
            "text": "When annotators label a tweet as \"probably\" or \"confident\" antisemitic, they must also select an applicable section of the IHRA definition to move on to the next tweet.",
            "label": 1
          },
          {
            "text": "If annotators feel the tweet is antisemitic, but no section of the definition applies, they will classify the tweet as not antisemitic according to the IHRA definition, and check a box indicating that they disagree with the IHRA definition for that tweet.",
            "label": 1
          },
          {
            "text": "Annotators can also use this the other way around, that is, they can label the message as antisemitic according to the IHRA definition, but personally disagree with it in a particular case.",
            "label": 1
          },
          {
            "text": "The option to personally disagree g-definitions-charters/working-definitionantisemitism/adoption-endorsement 6 This is a screenshot of our updated form.",
            "label": 1
          },
          {
            "text": "The question about the content type was not used for the annotation of all samples in this dataset.",
            "label": 1
          },
          {
            "text": "with the definition on a case-by-case basis is intended to encourage a stricter application of the IHRA definition rather than the individual definitions of the annotators.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Antisemitic Messages_ A Guide to High_Quality Annotation and a Labeled Dataset of Tweets",
        "section": 15,
        "paragraph_id": 15,
        "full_text": "Asking annotators to choose between a very negative, negative, neutral, positive, or very positive sentiment for the tweet regarding Jews, Judaism, or Israel further helps annotators apply the definition by allowing them to express that the tweet has a negative sentiment even if they are unable to find a part of the definition that applies.Messages that call out or report antisemitism are also flagged, as are tweets that are sarcastic and tweets that relate to the Holocaust, including comparisons to contemporary issues.Since the Covid pandemic, we have seen an increase in Holocaust distortions, most of which do not fall under the IHRA definition.We have therefore added a Holocaust distortion label.",
        "sentences": [
          {
            "text": "Asking annotators to choose between a very negative, negative, neutral, positive, or very positive sentiment for the tweet regarding Jews, Judaism, or Israel further helps annotators apply the definition by allowing them to express that the tweet has a negative sentiment even if they are unable to find a part of the definition that applies.",
            "label": 1
          },
          {
            "text": "Messages that call out or report antisemitism are also flagged, as are tweets that are sarcastic and tweets that relate to the Holocaust, including comparisons to contemporary issues.",
            "label": 1
          },
          {
            "text": "Since the Covid pandemic, we have seen an increase in Holocaust distortions, most of which do not fall under the IHRA definition.",
            "label": 1
          },
          {
            "text": "We have therefore added a Holocaust distortion label.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Antisemitic Messages_ A Guide to High_Quality Annotation and a Labeled Dataset of Tweets",
        "section": 16,
        "paragraph_id": 16,
        "full_text": "As an element of what we consider annotation reliability, our annotators meet on a weekly basis to discuss potentially difficult tweets.A tweet can be considered difficult if its content or context is not easily understandable, or if it is unclear which paragraph of the IHRA definition applies.",
        "sentences": [
          {
            "text": "As an element of what we consider annotation reliability, our annotators meet on a weekly basis to discuss potentially difficult tweets.",
            "label": 1
          },
          {
            "text": "A tweet can be considered difficult if its content or context is not easily understandable, or if it is unclear which paragraph of the IHRA definition applies.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Antisemitic Messages_ A Guide to High_Quality Annotation and a Labeled Dataset of Tweets",
        "section": 17,
        "paragraph_id": 17,
        "full_text": "Two annotators labeled each sample.After both annotators completed their annotations, they discussed their disagreements about whether or not the tweets were antisemitic. 7Reasons for disagreement included mislabeling due to fatigue, lack of understanding of the context, and overlooking some aspects of the messages.In almost all cases, the discussion led to an agreement.The tweets that did not reach agreement, that is, where one annotator labeled the message as antisemitic and the other did not, were not included in our final labeled dataset.Table1shows the annotation results after discussion with the remaining number of tweets and the percentage of antisemitic tweets. 8Many dataset labeling projects provide kappa coefficients to measure the quality of inter-annotator agreement.This does not make sense in our case because we discuss all disagreements, and few disagreements remain.However, kappa requires independent classification.Therefore, kappa is an artificial value for our dataset.It is very close to 1.Our overall pre-discussion Cohen's kappa is 0.66, varying 7 Annotators discussed messages that one of them labeled as confident or probably antisemitic and the other one labeled as confident or probably not antisemitic or \"I don't know.\" 8Before publishing the dataset, we checked for errors, including a possible overlap between tweets labeled as from sample to sample. 9Not surprisingly, it is lower when the data has a skewed distribution, that is, when either very few tweets are antisemitic or if very few tweets in a sample are not antisemitic.Kappa does not seem appropriate for measuring annotation reliability for our dataset, and perhaps for social data annotation in general.",
        "sentences": [
          {
            "text": "Two annotators labeled each sample.",
            "label": 1
          },
          {
            "text": "After both annotators completed their annotations, they discussed their disagreements about whether or not the tweets were antisemitic.",
            "label": 1
          },
          {
            "text": "7Reasons for disagreement included mislabeling due to fatigue, lack of understanding of the context, and overlooking some aspects of the messages.",
            "label": 1
          },
          {
            "text": "In almost all cases, the discussion led to an agreement.",
            "label": 1
          },
          {
            "text": "The tweets that did not reach agreement, that is, where one annotator labeled the message as antisemitic and the other did not, were not included in our final labeled dataset.",
            "label": 1
          },
          {
            "text": "Table1shows the annotation results after discussion with the remaining number of tweets and the percentage of antisemitic tweets.",
            "label": 1
          },
          {
            "text": "8Many dataset labeling projects provide kappa coefficients to measure the quality of inter-annotator agreement.",
            "label": 1
          },
          {
            "text": "This does not make sense in our case because we discuss all disagreements, and few disagreements remain.",
            "label": 0
          },
          {
            "text": "However, kappa requires independent classification.",
            "label": 0
          },
          {
            "text": "Therefore, kappa is an artificial value for our dataset.",
            "label": 0
          },
          {
            "text": "It is very close to 1.",
            "label": 0
          },
          {
            "text": "Our overall pre-discussion Cohen's kappa is 0.66, varying 7 Annotators discussed messages that one of them labeled as confident or probably antisemitic and the other one labeled as confident or probably not antisemitic or \"I don't know.\"",
            "label": 0
          },
          {
            "text": "\" 8Before publishing the dataset, we checked for errors, including a possible overlap between tweets labeled as from sample to sample.",
            "label": 1
          },
          {
            "text": "9Not surprisingly, it is lower when the data has a skewed distribution, that is, when either very few tweets are antisemitic or if very few tweets in a sample are not antisemitic.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Antisemitic Messages_ A Guide to High_Quality Annotation and a Labeled Dataset of Tweets",
        "section": 18,
        "paragraph_id": 18,
        "full_text": "The weekly group discussions and the discussions among the annotators helped the annotators to better understand the context of online conversations about events and online celebrities in the US, UK, India, or elsewhere.The annotators became increasingly familiar with the contexts because they often revolved around similar topics.",
        "sentences": [
          {
            "text": "The weekly group discussions and the discussions among the annotators helped the annotators to better understand the context of online conversations about events and online celebrities in the US, UK, India, or elsewhere.",
            "label": 0
          },
          {
            "text": "The annotators became increasingly familiar with the contexts because they often revolved around similar topics.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Antisemitic Messages_ A Guide to High_Quality Annotation and a Labeled Dataset of Tweets",
        "section": 19,
        "paragraph_id": 19,
        "full_text": "In our published labeled dataset, we use binary categories, treating ratings of confident/probably not antisemitic, and don't know as not antisemitic and probably/confident antisemitic as antisemitic.While annotators discussed disagreements about their antisemitism ratings, they did not discuss disagreements about their \"calling out\" ratings.The label \"calling out\" was more common than the label \"antisemitism\" in many samples, but it was inconsistent, especially in the annotations at the beginning of the project.We are relabeling the dataset for \"calling out\" and will publish the results in an update of the dataset.An overview of the annotation results of the label \"antisemitism\" per sample can be found in Table1.529 tweets (8%) with the slur \"ZioNazi*\" and 283 tweets (4%) with the slur \"K---s.\"Some of the keywords may also appear in other samples, e.g., a tweet may contain both the word \"Jews\" and \"Israel.\"",
        "sentences": [
          {
            "text": "In our published labeled dataset, we use binary categories, treating ratings of confident/probably not antisemitic, and don't know as not antisemitic and probably/confident antisemitic as antisemitic.",
            "label": 1
          },
          {
            "text": "While annotators discussed disagreements about their antisemitism ratings, they did not discuss disagreements about their \"calling out\" ratings.",
            "label": 1
          },
          {
            "text": "The label \"calling out\" was more common than the label \"antisemitism\" in many samples, but it was inconsistent, especially in the annotations at the beginning of the project.",
            "label": 1
          },
          {
            "text": "We are relabeling the dataset for \"calling out\" and will publish the results in an update of the dataset.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Antisemitic Messages_ A Guide to High_Quality Annotation and a Labeled Dataset of Tweets",
        "section": 20,
        "paragraph_id": 20,
        "full_text": "Out of 4,605 tweets containing the keyword \"Jews,\" 483 tweets (11%) are considered antisemitic.Out of 1,524 tweets containing the keyword \"Israel,\" 203 tweets (13%) are antisemitic.Our dataset contains 283 tweets with the antisemitic slur \"k---s.\"It is not surprising that many messages, 34% in our samples (97 messages), with this keyword are antisemitic, but we also noticed that many tweets containing the slur \"k---s\" are calling out the use of this term.In contrast, the vast majority of tweets with the slur \"ZioNazi*\" are antisemitic: 467 out of 529, or 88%.",
        "sentences": [
          {
            "text": "Out of 4,605 tweets containing the keyword \"Jews,\" 483 tweets (11%) are considered antisemitic.",
            "label": 1
          },
          {
            "text": "Out of 1,524 tweets containing the keyword \"Israel,\" 203 tweets (13%) are antisemitic.",
            "label": 1
          },
          {
            "text": "Our dataset contains 283 tweets with the antisemitic slur \"k---s.\"",
            "label": 1
          },
          {
            "text": "\"It is not surprising that many messages, 34% in our samples (97 messages), with this keyword are antisemitic, but we also noticed that many tweets containing the slur \"k---s\" are calling out the use of this term.",
            "label": 1
          },
          {
            "text": "In contrast, the vast majority of tweets with the slur \"ZioNazi*\" are antisemitic: 467 out of 529, or 88%.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Antisemitic Messages_ A Guide to High_Quality Annotation and a Labeled Dataset of Tweets",
        "section": 21,
        "paragraph_id": 21,
        "full_text": "The labeled dataset on antisemitism is now awaiting testing, and to facilitate this we have made it available on Zenodo(Jikeli et al. 2023).",
        "sentences": [
          {
            "text": "The labeled dataset on antisemitism is now awaiting testing, and to facilitate this we have made it available on Zenodo(Jikeli et al. 2023).",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Antisemitic Messages_ A Guide to High_Quality Annotation and a Labeled Dataset of Tweets",
        "section": 22,
        "paragraph_id": 22,
        "full_text": "Our labeled dataset of 6,941 tweets is based on representative samples of tweets containing the common keywords \"Jews\" and \"Israel\" and keywords more likely to be used in antisemitic contexts, the slurs \"ZioNazi*\" and \"k---s.\"It includes 1,250 tweets (18%) that are antisemitic according to IHRA's Working Definition of Antisemitism.",
        "sentences": [
          {
            "text": "Our labeled dataset of 6,941 tweets is based on representative samples of tweets containing the common keywords \"Jews\" and \"Israel\" and keywords more likely to be used in antisemitic contexts, the slurs \"ZioNazi*\" and \"k---s.\"",
            "label": 1
          },
          {
            "text":"It includes 1,250 tweets (18%) that are antisemitic according to IHRA's Working Definition of Antisemitism.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Antisemitic Messages_ A Guide to High_Quality Annotation and a Labeled Dataset of Tweets",
        "section": 23,
        "paragraph_id": 23,
        "full_text": "The majority of the tweets (66%) come from queries with the keyword \"Jews,\" which is representative of a continuous time period from January 2019 to December 2021.It is reasonable to assume that our dataset is a good reflection of discussions on Twitter about Jews and covers the most prevalent topics, at least when the word \"Jews\" is mentioned and for the three-year period covered by the dataset.483 tweets (11%) with the keyword \"Jews\" were labeled as antisemitic.It is also reasonable to assume that they cover most of the relevant topics of antisemitic discussions about Jews on Twitter during this period.",
        "sentences": [
          {
            "text": "The majority of the tweets (66%) come from queries with the keyword \"Jews,\" which is representative of a continuous time period from January 2019 to December 2021.",
            "label": 1
          },
          {
            "text": "It is reasonable to assume that our dataset is a good reflection of discussions on Twitter about Jews and covers the most prevalent topics, at least when the word \"Jews\" is mentioned and for the three-year period covered by the dataset.483 tweets (11%) with the keyword \"Jews\" were labeled as antisemitic.",
            "label": 1
          },
          {
            "text": "It is also reasonable to assume that they cover most of the relevant topics of antisemitic discussions about Jews on Twitter during this period.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Antisemitic Messages_ A Guide to High_Quality Annotation and a Labeled Dataset of Tweets",
        "section": 24,
        "paragraph_id": 24,
        "full_text": "203 out of 1,524 (13%) tweets about Israel are antisemitic.They are likely to cover the main tropes and discussions during this period.However, this period does not include heightened tensions in the Israeli-Palestinian conflict, such as the flare-up of the conflict in May 2021.We will be updating our dataset with data from all of 2021 and 2022 in the near future.",
        "sentences": [
          {
            "text": "203 out of 1,524 (13%) tweets about Israel are antisemitic.",
            "label": 1
          },
          {
            "text": "They are likely to cover the main tropes and discussions during this period.",
            "label": 1
          },
          {
            "text": "However, this period does not include heightened tensions in the Israeli-Palestinian conflict, such as the flare-up of the conflict in May 2021.",
            "label": 1
          },
          {
            "text": "We will be updating our dataset with data from all of 2021 and 2022 in the near future.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Antisemitic Messages_ A Guide to High_Quality Annotation and a Labeled Dataset of Tweets",
        "section": 25,
        "paragraph_id": 25,
        "full_text": "The slurs \"ZioNazi*\" and \"K---s\" are not used very often by Twitter users compared to the words \"Jews\" and \"Israel.\"While there are millions of tweets containing the latter words each year, there are \"only\" a few tens of thousands of tweets containing these slurs.The majority of tweets containing the slur \"Zionazi*,\" are used in an approving way and have an antisemitic message (88%).This is not the case when the word \"k---s\" is used.Only one-third are antisemitic.",
        "sentences": [
          {
            "text": "The slurs \"ZioNazi*\" and \"K---s\" are not used very often by Twitter users compared to the words \"Jews\" and \"Israel.\"",
            "label": 1
          },
          {
            "text": "While there are millions of tweets containing the latter words each year, there are \"only\" a few tens of thousands of tweets containing these slurs.",
            "label": 1
          },
          {
            "text": "The majority of tweets containing the slur \"Zionazi*,\" are used in an approving way and have an antisemitic message (88%).",
            "label": 1
          },
          {
            "text": "This is not the case when the word \"k---s\" is used.",
            "label": 1
          },
          {
            "text": "Only one-third are antisemitic.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Antisemitic Messages_ A Guide to High_Quality Annotation and a Labeled Dataset of Tweets",
        "section": 26,
        "paragraph_id": 26,
        "full_text": "The tweets in our dataset cover a wide range of antisemitic and non-antisemitic conversations about Jews.However, the dataset needs to be enlarged and constantly updated to cover all topics comprehensively.Labeling tweets that call out antisemitism, report antisemitism, or are otherwise related to antisemitism (such as the Holocaust) but are not actually antisemitic can help reduce false positives in automated detection.",
        "sentences": [
          {
            "text": "The tweets in our dataset cover a wide range of antisemitic and non-antisemitic conversations about Jews.",
            "label": 1
          },
          {
            "text": "However, the dataset needs to be enlarged and constantly updated to cover all topics comprehensively.",
            "label": 1
          },
          {
            "text": "Labeling tweets that call out antisemitism, report antisemitism, or are otherwise related to antisemitism (such as the Holocaust) but are not actually antisemitic can help reduce false positives in automated detection.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Antisemitic Messages_ A Guide to High_Quality Annotation and a Labeled Dataset of Tweets",
        "section": 27,
        "paragraph_id": 27,
        "full_text": "It is important to annotate online messages in their \"natural\" context.Context, including pictures, memes, or previous comments within a thread, can completely change the meaning of a message.",
        "sentences": [
          {
            "text": "It is important to annotate online messages in their \"natural\" context.",
            "label": 0
          },
          {
            "text": "Context, including pictures, memes, or previous comments within a thread, can completely change the meaning of a message.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Antisemitic Messages_ A Guide to High_Quality Annotation and a Labeled Dataset of Tweets",
        "section": 28,
        "paragraph_id": 28,
        "full_text": "The annotation process encourages annotators to apply a widely used definition of antisemitism consistently, even if they disagree on certain aspects or on certain cases.Our annotation process appears to be robust although it is difficult to measure because a key element of our annotation process is the discussion among annotators of their disagreements and weekly discussions of tweets that are difficult to classify.This violates the assumption of independent classification in kappa calculations.Percentage agreement and Cohen's kappa almost reach their maximum after annotators discuss and revisit tweets on which they previously disagreed.In the pre-discussion annotation, we do not aim for 100% agreement.Rather, we want annotators from different perspectives to fully understand the messages of each tweet, which they can then explain in discussions focused on deciding whether or not it is antisemitic according to the IHRA definition.The dataset only includes tweets with 100% agreement between annotators.The pre-discussion inter-rater reliability has a kappa value of 0.66.We consider the training of qualified annotators and the discussion process to be essential for building an accurately labeled dataset.",
        "sentences": [
          {
            "text": "The annotation process encourages annotators to apply a widely used definition of antisemitism consistently, even if they disagree on certain aspects or on certain cases.",
            "label": 0
          },
          {
            "text": "Our annotation process appears to be robust although it is difficult to measure because a key element of our annotation process is the discussion among annotators of their disagreements and weekly discussions of tweets that are difficult to classify.",
            "label": 1
          },
          {
            "text": "This violates the assumption of independent classification in kappa calculations.",
            "label": 1
          },
          {
            "text": "Percentage agreement and Cohen's kappa almost reach their maximum after annotators discuss and revisit tweets on which they previously disagreed.",
            "label": 1
          },
          {
            "text": "In the pre-discussion annotation, we do not aim for 100% agreement.",
            "label": 1
          },
          {
            "text": "Rather, we want annotators from different perspectives to fully understand the messages of each tweet, which they can then explain in discussions focused on deciding whether or not it is antisemitic according to the IHRA definition.",
            "label": 1
          },
          {
            "text": "The dataset only includes tweets with 100% agreement between annotators.",
            "label": 1
          },
          {
            "text": "The pre-discussion inter-rater reliability has a kappa value of 0.66.",
            "label": 0
          },
          {
            "text": "We consider the training of qualified annotators and the discussion process to be essential for building an accurately labeled dataset.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "AntM 2 C_ A Large Scale Dataset For Multi_Scenario Multi_Modal CTR Prediction",
        "section": 0,
        "paragraph_id": 0,
        "full_text": "Abstract: Click-through rate (CTR) prediction is a crucial issue in recommendation systems, directly impacting user experience and platform revenue.In recent years, CTR has garnered attention from both industry and academia, leading to the emergence of various public CTR datasets.However, existing CTR datasets primarily suffer from the following limitations.Firstly, users generally click different types of items from multiple scenarios, and modeling the CTR from multiple scenarios can provide a more comprehensive understanding of users and share knowledge between different scenarios.Existing datasets only include CTR data for the same type of items from a single scenario.Secondly, multi-modal features are essential in multi-scenario CTR prediction as they effectively address the issue of inconsistent ID encoding between different scenarios.The existing datasets are based on ID features and lack multi-modal features.Third, a large-scale CTR dataset can provide a more reliable and comprehensive evaluation of complex models, fully reflecting the performance differences between models.While the scale of existing datasets is around 100 million, which is relatively small compared to the real-world industrial CTR prediction.To address these limitations, we propose AntM 2 C, a Multi-Scenario Multi-Modal CTR dataset based on real industrial data from the Alipay platform.Specifically, AntM 2 C possesses the following characteristics: 1) It covers CTR data of 5 different types of items from Alipay, providing insights into the preferences of users for different items, including advertisements, vouchers, mini-programs, contents, and videos.2) Apart from ID-based features, AntM 2 C also provides 2 multi-modal features, raw text and image features, which can effectively establish connections between items with different IDs. 3) AntM 2 C provides 1 billion CTR data with 200 features, including:",
        "sentences": [
          {
            "text": "Abstract: Click-through rate (CTR) prediction is a crucial issue in recommendation systems, directly impacting user experience and platform revenue.",
            "label": 0
          },
          {
            "text": "In recent years, CTR has garnered attention from both industry and academia, leading to the emergence of various public CTR datasets.",
            "label": 0
          },
          {
            "text": "However, existing CTR datasets primarily suffer from the following limitations.",
            "label": 0
          },
          {
            "text": "Firstly, users generally click different types of items from multiple scenarios, and modeling the CTR from multiple scenarios can provide a more comprehensive understanding of users and share knowledge between different scenarios.",
            "label": 0
          },
          {
            "text": "Existing datasets only include CTR data for the same type of items from a single scenario.",
            "label": 0
          },
          {
            "text": "Secondly, multi-modal features are essential in multi-scenario CTR prediction as they effectively address the issue of inconsistent ID encoding between different scenarios.",
            "label": 0
          },
          {
            "text": "The existing datasets are based on ID features and lack multi-modal features.",
            "label": 0
          },
          {
            "text": "Third, a large-scale CTR dataset can provide a more reliable and comprehensive evaluation of complex models, fully reflecting the performance differences between models.",
            "label": 0
          },
          {
            "text": "While the scale of existing datasets is around 100 million, which is relatively small compared to the real-world industrial CTR prediction.",
            "label": 0
          },
          {
            "text": "To address these limitations, we propose AntM 2 C, a Multi-Scenario Multi-Modal CTR dataset based on real industrial data from the Alipay platform.",
            "label": 1
          },
          {
            "text": "Specifically, AntM 2 C possesses the following characteristics: 1) It covers CTR data of 5 different types of items from Alipay, providing insights into the preferences of users for different items, including advertisements, vouchers, mini-programs, contents, and videos.",
            "label": 1
          },
          {
            "text": "2) Apart from ID-based features, AntM 2 C also provides 2 multi-modal features, raw text and image features, which can effectively establish connections between items with different IDs.",
            "label": 1
          },
          {
            "text":"3) AntM 2 C provides 1 billion CTR data with 200 features, including:",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "AntM 2 C_ A Large Scale Dataset For Multi_Scenario Multi_Modal CTR Prediction",
        "section": 1,
        "paragraph_id": 1,
        "full_text": "Click-through rate (CTR) prediction plays a significant role in various domains, including online advertising, search engines, and recommendation systems.CTR prediction refers to the task of estimating the probability that a user will click on a given item.It is essential for optimizing ad revenue, enhancing user experience, and improving engagement.One of the challenging issues in CTR prediction lies in the faithful evaluation of the model.Public CTR datasets provide a standardized and benchmarked environment for evaluating the performance of different CTR models.This enables researchers to compare the effectiveness of different models and identify the most suitable ones for specific applications.",
        "sentences": [
          {
            "text": "Click-through rate (CTR) prediction plays a significant role in various domains, including online advertising, search engines, and recommendation systems.",
            "label": 0
          },
          {
            "text": "CTR prediction refers to the task of estimating the probability that a user will click on a given item.",
            "label": 0
          },
          {
            "text": "It is essential for optimizing ad revenue, enhancing user experience, and improving engagement.",
            "label": 0
          },
          {
            "text": "One of the challenging issues in CTR prediction lies in the faithful evaluation of the model.",
            "label": 0
          },
          {
            "text": "Public CTR datasets provide a standardized and benchmarked environment for evaluating the performance of different CTR models.",
            "label": 0
          },
          {
            "text": "This enables researchers to compare the effectiveness of different models and identify the most suitable ones for specific applications.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "AntM 2 C_ A Large Scale Dataset For Multi_Scenario Multi_Modal CTR Prediction",
        "section": 2,
        "paragraph_id": 2,
        "full_text": "However, in order to meet the constantly growing demands of users, the current CTR scenarios and items are becoming increasingly diverse, and the amount of CTR data is also increasing.For example, in Alipay, CTR occurs in the consumer coupons at marketing campaigns, videos on the tab3 page, and mini-programs after a search.As a result, the existing CTR datasets suffer from the following limitations.Firstly, in real-world industrial CTR prediction, users generally click various types of items from different business scenarios, reflecting their preferences for different items.For example, on Alipay, a user may browse a video about coffee on the Tab3 page, then click on a coffee coupon during a marketing campaign, and finally use the Alipay search to click a coffee ordering mini-program to place an order.Jointly modeling this multi-scenario CTR data can provide a more comprehensive understanding of user preferences, and the knowledge across scenarios can be shared to improve the CTR performance in each scenario.However, existing CTR datasets have a limited range of item types and generally originate from the same business scenario, which fails to capture the multi-scenario preferences of users.For example, Criteo1and Avazu2only involve CTR data for advertisements.As e-commerce platforms, both Amazon3and AliExpress4provide CTR data for their e-commerce items.Tenrec[14]focuses more on video and article recommendations.Secondly, multi-modal features can address the issue of inconsistent IDs for similar items in different business scenarios and effectively establish a bridge between different scenarios.For example, a video about coffee and a coffee coupon have different IDs in different business scenarios.Directly using ID features cannot perceive the relationship between these two items.Multi-modal features inherently carry semantic meaning and can better compensate for the inconsistency of ID features across different domains.Additionally, with the rise of large language models (LLMs), combining LLMs with CTR prediction has become an emerging research field.Existing CTR datasets are based on ID features and lack abundant multi-modal features, resulting in the CTR model being unable to test the performance in multi-scenarios and multi-modal settings.Furthermore, large-scale datasets can reliably and comprehensively reflect the performance of CTR models, while also highlighting the differences between CTR models.The existing datasets are typically at the scale of 100 million, which is insufficient to further validate the capabilities in larger-scale industrial scenarios.",
        "sentences": [
          {
            "text": "However, in order to meet the constantly growing demands of users, the current CTR scenarios and items are becoming increasingly diverse, and the amount of CTR data is also increasing.",
            "label": 0
          },
          {
            "text": "For example, in Alipay, CTR occurs in the consumer coupons at marketing campaigns, videos on the tab3 page, and mini-programs after a search.",
            "label": 0
          },
          {
            "text": "As a result, the existing CTR datasets suffer from the following limitations.",
            "label": 0
          },
          {
            "text": "Firstly, in real-world industrial CTR prediction, users generally click various types of items from different business scenarios, reflecting their preferences for different items.",
            "label": 0
          },
          {
            "text": "For example, on Alipay, a user may browse a video about coffee on the Tab3 page, then click on a coffee coupon during a marketing campaign, and finally use the Alipay search to click a coffee ordering mini-program to place an order.",
            "label": 0
          },
          {
            "text": "Jointly modeling this multi-scenario CTR data can provide a more comprehensive understanding of user preferences, and the knowledge across scenarios can be shared to improve the CTR performance in each scenario.",
            "label": 0
          },
          {
            "text": "However, existing CTR datasets have a limited range of item types and generally originate from the same business scenario, which fails to capture the multi-scenario preferences of users.",
            "label": 0
          },
          {
            "text": "For example, Criteo1and Avazu2only involve CTR data for advertisements.",
            "label": 0
          },
          {
            "text": "As e-commerce platforms, both Amazon3and AliExpress4provide CTR data for their e-commerce items.",
            "label": 0
          },
          {
            "text": "Tenrec[14]focuses more on video and article recommendations.",
            "label": 0
          },
          {
            "text": "Secondly, multi-modal features can address the issue of inconsistent IDs for similar items in different business scenarios and effectively establish a bridge between different scenarios.",
            "label": 0
          },
          {
            "text": "For example, a video about coffee and a coffee coupon have different IDs in different business scenarios.",
            "label": 0
          },
          {
            "text": "Directly using ID features cannot perceive the relationship between these two items.",
            "label": 0
          },
          {
            "text": "Multi-modal features inherently carry semantic meaning and can better compensate for the inconsistency of ID features across different domains.",
            "label": 0
          },
          {
            "text": "Additionally, with the rise of large language models (LLMs), combining LLMs with CTR prediction has become an emerging research field.",
            "label": 0
          },
          {
            "text": "Existing CTR datasets are based on ID features and lack abundant multi-modal features, resulting in the CTR model being unable to test the performance in multi-scenarios and multi-modal settings.",
            "label": 0
          },
          {
            "text": "Furthermore, large-scale datasets can reliably and comprehensively reflect the performance of CTR models, while also highlighting the differences between CTR models.",
            "label": 0
          },
          {
            "text": "The existing datasets are typically at the scale of 100 million, which is insufficient to further validate the capabilities in larger-scale industrial scenarios.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "AntM 2 C_ A Large Scale Dataset For Multi_Scenario Multi_Modal CTR Prediction",
        "section": 3,
        "paragraph_id": 3,
        "full_text": "To address the aforementioned challenges, we propose the AntM 2 C dataset, a large-scale multi-scenario multi-modal dataset for CTR prediction.Compared with existing CTR datasets, AntM 2 C has the following advantages: • Diverse business scenarios and item types: AntM 2 C contains different types of items from five typical business scenarios on the Alipay platform, including advertisements, vouchers, mini-programs, contents, and videos.Each business scenario has a unique data distribution.The abundant intersecting users and similar items between scenarios enable a more comprehensive evaluation for multi-scenario CTR modeling.Through one evaluation, the effectiveness of the CTR model can be evaluated in multiple business scenarios.• Multi-modal feature system: AntM 2 C not only includes ID features but also provides rich multi-modal features such as text and image, which can establish connections between similar items across scenarios and provide better evaluation for multi-modal CTR models.Furthermore, the feature system in AntM 2 C includes up to 200 features5, making it more closely aligned with real-world CTR prediction in industrial scenarios.• Largest data scale: AntM 2 C comprises 200 million users and 6 million items, with a total of 1 billion samples 5 .The average number of interactions per user is above 50.To the best of our knowledge, AntM 2 C is the largest public CTR dataset in terms of scale, which can provide comprehensive and reliable CTR evaluation results.• Comprehensive benchmark: Based on AntM 2 C, three typical CTR tasks have been built, including multi-scenario modeling, cold-start modeling, and multi-modal modeling.",
        "sentences": [
          {
            "text": "To address the aforementioned challenges, we propose the AntM 2 C dataset, a large-scale multi-scenario multi-modal dataset for CTR prediction.",
            "label": 1
          },
          {
            "text": "Compared with existing CTR datasets, AntM 2 C has the following advantages: • Diverse business scenarios and item types: AntM 2 C contains different types of items from five typical business scenarios on the Alipay platform, including advertisements, vouchers, mini-programs, contents, and videos.",
            "label": 1
          },
          {
            "text": "Each business scenario has a unique data distribution.",
            "label": 1
          },
          {
            "text": "The abundant intersecting users and similar items between scenarios enable a more comprehensive evaluation for multi-scenario CTR modeling.",
            "label": 0
          },
          {
            "text": "• Multi-modal feature system: AntM 2 C not only includes ID features but also provides rich multi-modal features such as text and image, which can establish connections between similar items across scenarios and provide better evaluation for multi-modal CTR models.",
            "label": 1
          },
          {
            "text": "• Largest data scale: AntM 2 C comprises 200 million users and 6 million items, with a total of 1 billion samples 5 .",
            "label": 1
          },
          {
            "text": "The average number of interactions per user is above 50.",
            "label": 1
          },
          {
            "text": "To the best of our knowledge, AntM 2 C is the largest public CTR dataset in terms of scale, which can provide comprehensive and reliable CTR evaluation results.",
            "label": 1
          },
          {
            "text": "• Comprehensive benchmark: Based on AntM 2 C, three typical CTR tasks have been built, including multi-scenario modeling, cold-start modeling, and multi-modal modeling.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "AntM 2 C_ A Large Scale Dataset For Multi_Scenario Multi_Modal CTR Prediction",
        "section": 4,
        "paragraph_id": 4,
        "full_text": "Benchmark evaluation results based on state-of-the-art models are also provided.The rest of the paper is organized as follows.In Section 2, we briefly review some related works about public CTR datasets.In Section 3, we give a detailed introduction to the dataset collection and data analysis.In Section 4, we conduct empirical studies with baseline CTR methods on different CTR tasks.",
        "sentences": [
          {
            "text": "Benchmark evaluation results based on state-of-the-art models are also provided.",
            "label": 1
          },
          {
            "text": "The rest of the paper is organized as follows.",
            "label": 0
          },
          {
            "text": "In Section 2, we briefly review some related works about public CTR datasets.",
            "label": 0
          },
          {
            "text": "In Section 3, we give a detailed introduction to the dataset collection and data analysis.",
            "label": 0
          },
          {
            "text": "In Section 4, we conduct empirical studies with baseline CTR methods on different CTR tasks.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "AntM 2 C_ A Large Scale Dataset For Multi_Scenario Multi_Modal CTR Prediction",
        "section": 5,
        "paragraph_id": 5,
        "full_text": "The existing public CTR datasets can be roughly divided into two categories: single-scenario and multi-scenario.Both have been widely adopted by the evaluation of CTR methods.",
        "sentences": [
          {
            "text": "The existing public CTR datasets can be roughly divided into two categories: single-scenario and multi-scenario.",
            "label": 0
          },
          {
            "text": "Both have been widely adopted by the evaluation of CTR methods.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "AntM 2 C_ A Large Scale Dataset For Multi_Scenario Multi_Modal CTR Prediction",
        "section": 6,
        "paragraph_id": 6,
        "full_text": "The Criteo dataset is one of the publicly available datasets for CTR prediction.It contains over 45 million records of user interactions with advertisements, including features such as click-through rates, impression rates, and user demographics.Similar to the Criteo dataset, the Avazu dataset contains over 40 million records of user interactions with mobile advertisements.It includes features such as device information, app category, and user demographics.One of the main limitations of the Criteo and Avazu dataset is they only include CTR data for advertisements and cannot be used to evaluate CTR for other business scenarios or types of items.Additionally, the datasets do not provide text information about the advertisement or user, which can limit the scope of the multi-modal modeling.",
        "sentences": [
          {
            "text": "The Criteo dataset is one of the publicly available datasets for CTR prediction.",
            "label": 0
          },
          {
            "text": "It contains over 45 million records of user interactions with advertisements, including features such as click-through rates, impression rates, and user demographics.",
            "label": 0
          },
          {
            "text": "Similar to the Criteo dataset, the Avazu dataset contains over 40 million records of user interactions with mobile advertisements.",
            "label": 0
          },
          {
            "text": "It includes features such as device information, app category, and user demographics.",
            "label": 0
          },
          {
            "text": "One of the main limitations of the Criteo and Avazu dataset is they only include CTR data for advertisements and cannot be used to evaluate CTR for other business scenarios or types of items.",
            "label": 0
          },
          {
            "text": "Additionally, the datasets do not provide text information about the advertisement or user, which can limit the scope of the multi-modal modeling.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "AntM 2 C_ A Large Scale Dataset For Multi_Scenario Multi_Modal CTR Prediction",
        "section": 7,
        "paragraph_id": 7,
        "full_text": "The AliExpress is a dataset gathered from real-world traffic logs of the search system in AliExpress.This dataset is collected from 5 countries: Russia, Spain, French, Netherlands, and America, which can be seen as 5 scenarios.It can be used to develop and evaluate CTR prediction models for e-commerce platforms.The Tenrec dataset is a multipurpose dataset for CTR prediction where click data was collected from two scenarios: articles and videos.Although the above datasets cover different scenarios, the items within these scenarios are similar.The AliExpress dataset only consists of ecommerce items, and Tenrec involves videos and articles that only reflect the personal interests of users in the entertainment and cultural aspects.Additionally, similar to single-scenario datasets, both  of these datasets lack textual modal information and only provide features such as IDs.This limitation restricts the application of multi-modal modeling.",
        "sentences": [
          {
            "text": "The AliExpress is a dataset gathered from real-world traffic logs of the search system in AliExpress.",
            "label": 0
          },
          {
            "text": "This dataset is collected from 5 countries: Russia, Spain, French, Netherlands, and America, which can be seen as 5 scenarios.",
            "label": 0
          },
          {
            "text": "It can be used to develop and evaluate CTR prediction models for e-commerce platforms.",
            "label": 0
          },
          {
            "text": "The Tenrec dataset is a multipurpose dataset for CTR prediction where click data was collected from two scenarios: articles and videos.",
            "label": 0
          },
          {
            "text": "Although the above datasets cover different scenarios, the items within these scenarios are similar.",
            "label": 0
          },
          {
            "text": "The AliExpress dataset only consists of ecommerce items, and Tenrec involves videos and articles that only reflect the personal interests of users in the entertainment and cultural aspects.",
            "label": 0
          },
          {
            "text": "Additionally, similar to single-scenario datasets, both  of these datasets lack textual modal information and only provide features such as IDs.",
            "label": 0
          },
          {
            "text": "This limitation restricts the application of multi-modal modeling.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "AntM 2 C_ A Large Scale Dataset For Multi_Scenario Multi_Modal CTR Prediction",
        "section": 8,
        "paragraph_id": 8,
        "full_text": "AntM 2 C's data is collected from Alipay, a leading platform for payments and digital services.In order to meet the growing demands of users, Alipay recommends various types of items from different business scenarios to users.",
        "sentences": [
          {
            "text": "AntM 2 C's data is collected from Alipay, a leading platform for payments and digital services.",
            "label": 1
          },
          {
            "text": "In order to meet the growing demands of users, Alipay recommends various types of items from different business scenarios to users.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "AntM 2 C_ A Large Scale Dataset For Multi_Scenario Multi_Modal CTR Prediction",
        "section": 9,
        "paragraph_id": 9,
        "full_text": "3.1.1Scenarios.AntM 2 C collects CTR data in five scenarios on Alipay, and there are differences in the types of items in each scenario.As shown in Figure1, the CTR prediction occurs in multiple scenarios, including services and content on search, vouchers on marketing, videos on Tab3 page, and advertisements on the membership page.In the search scenario, when a user enters search words, several relevant mini-apps of services or content are displayed for the user to click on.Marketing scenarios recommend some consumer vouchers, and users click the coupons they are willing to use.On the Tab3 page, the recommended items are primarily short videos, and users will click to watch the videos they are interested in.On the membership page, users may click on some online advertisements.In conclusion, AntM 2 C includes various types of items from different business scenarios.In section 3.2.2,we will show that there are differences in the data distribution of these different scenarios.The rich and diverse items provide a more comprehensive evaluation for CTR prediction.",
        "sentences": [
          {
            "text": "3.1.1Scenarios.",
            "label": 1
          },
          {
            "text": "AntM 2 C collects CTR data in five scenarios on Alipay, and there are differences in the types of items in each scenario.",
            "label": 1
          },
          {
            "text": "As shown in Figure1, the CTR prediction occurs in multiple scenarios, including services and content on search, vouchers on marketing, videos on Tab3 page, and advertisements on the membership page.",
            "label": 1
          },
          {
            "text": "In the search scenario, when a user enters search words, several relevant mini-apps of services or content are displayed for the user to click on.",
            "label": 1
          },
          {
            "text": "Marketing scenarios recommend some consumer vouchers, and users click the coupons they are willing to use.",
            "label": 1
          },
          {
            "text": "On the Tab3 page, the recommended items are primarily short videos, and users will click to watch the videos they are interested in.",
            "label": 1
          },
          {
            "text": "On the membership page, users may click on some online advertisements.",
            "label": 1
          },
          {
            "text": "In conclusion, AntM 2 C includes various types of items from different business scenarios.",
            "label": 1
          },
          {
            "text": "2,we will show that there are differences in the data distribution of these different scenarios.",
            "label": 1
          },
          {
            "text": "The rich and diverse items provide a more comprehensive evaluation for CTR prediction.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "AntM 2 C_ A Large Scale Dataset For Multi_Scenario Multi_Modal CTR Prediction",
        "section": 10,
        "paragraph_id": 10,
        "full_text": "3.1.2Data Sampling.AntM 2 C collects 9-day (from 20230709 to 20230717) CTR samples from the above-mentioned five scenarios and then filters out 1 billion samples of relatively high-activity users who have a total click count ≥ 30 across all scenarios.In the first stage of open sourcing, we randomly sampled 10 million data from these 1 billion samples, and their statistical properties are shown in Table1.We will open all 1 billion data in the subsequent stage.For the purpose of protecting user privacy, we do not explicitly indicate Table1: Data statistics of AntM 2 C. To protect user privacy, AntM 2 C anonymizes the scenario names as A-E.The click rate is calculated by dividing the number of clicks by the number of exposures.Since negative sampling is applied to the samples, the click rate may be higher than the actual value.A-E).The horizontal axis represents the number of frequencies for users/items, while the vertical axis represents the number of users/items at that frequency.It can be observed that, in terms of item distribution, all scenarios exhibit a long-tail distribution, with 80% of the sample appearing less than 5 frequencies.This long-tail distribution is consistent with real-world situations.As for user distribution, there are differences between scenarios.In scenario B, the distribution of user frequency has two peaks, one at less than 5 times and the other around 50 times.After the frequency is greater than 50, the number of users decreases as the frequency increases.In other scenarios, the exposure frequency of users follows a long-tail distribution similar to that of items, where more exposure frequency leads to fewer users.Due to the overlapping users between scenarios, the long-tail distribution of users in multiple scenarios becomes a normal distribution in the global samples.Most users have an exposure frequency of around 50.Overall, the distribution of items and users in AntM 2 C reflects CTR prediction in practice.",
        "sentences": [
          {
            "text": "3.1.2Data Sampling.",
            "label": 1
          },
          {
            "text": "AntM 2 C collects 9-day (from 20230709 to 20230717) CTR samples from the above-mentioned five scenarios and then filters out 1 billion samples of relatively high-activity users who have a total click count ≥ 30 across all scenarios.",
            "label": 1
          },
          {
            "text": "In the first stage of open sourcing, we randomly sampled 10 million data from these 1 billion samples, and their statistical properties are shown in Table1.",
            "label": 1
          },
          {
            "text": "We will open all 1 billion data in the subsequent stage.",
            "label": 1
          },
          {
            "text": "For the purpose of protecting user privacy, we do not explicitly indicate Table1: Data statistics of AntM 2 C.",
            "label": 1
          },
          {
            "text": "To protect user privacy, AntM 2 C anonymizes the scenario names as A-E.",
            "label": 1
          },
          {
            "text": "The click rate is calculated by dividing the number of clicks by the number of exposures.",
            "label": 1
          },
          {
            "text": "Since negative sampling is applied to the samples, the click rate may be higher than the actual value.A-E).",
            "label": 1
          },
          {
            "text": "The horizontal axis represents the number of frequencies for users/items, while the vertical axis represents the number of users/items at that frequency.",
            "label": 1
          },
          {
            "text": "It can be observed that, in terms of item distribution, all scenarios exhibit a long-tail distribution, with 80% of the sample appearing less than 5 frequencies.",
            "label": 1
          },
          {
            "text": "This long-tail distribution is consistent with real-world situations.",
            "label": 1
          },
          {
            "text": "As for user distribution, there are differences between scenarios.",
            "label": 1
          },
          {
            "text": "In scenario B, the distribution of user frequency has two peaks, one at less than 5 times and the other around 50 times.",
            "label": 1
          },
          {
            "text": "After the frequency is greater than 50, the number of users decreases as the frequency increases.",
            "label": 1
          },
          {
            "text": "In other scenarios, the exposure frequency of users follows a long-tail distribution similar to that of items, where more exposure frequency leads to fewer users.",
            "label": 1
          },
          {
            "text": "Due to the overlapping users between scenarios, the long-tail distribution of users in multiple scenarios becomes a normal distribution in the global samples.",
            "label": 1
          },
          {
            "text": "Most users have an exposure frequency of around 50.",
            "label": 1
          },
          {
            "text": "Overall, the distribution of items and users in AntM 2 C reflects CTR prediction in practice.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "AntM 2 C_ A Large Scale Dataset For Multi_Scenario Multi_Modal CTR Prediction",
        "section": 11,
        "paragraph_id": 11,
        "full_text": "The feature system of AntM 2 C, as shown in Table3, includes ID features of users and items, as well as raw text features.",
        "sentences": [
          {
            "text": "The feature system of AntM 2 C, as shown in Table3, includes ID features of users and items, as well as raw text features.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "AntM 2 C_ A Large Scale Dataset For Multi_Scenario Multi_Modal CTR Prediction",
        "section": 12,
        "paragraph_id": 12,
        "full_text": "The user features consist of static profile features6and user sequence features.The static profile features include basic user attributes such as gender, age, occupation, etc.The sequence features provide the user's recent activities on Alipay, including clicked mini-apps, searched services, purchased items, etc.As mentioned in Section 3.1.3,these user features have been desensitized and encrypted for the purpose of user privacy protection and appear in the dataset in an encrypted ID format, making it impossible to reconstruct the original user features.In addition to the ID-based features, AntM 2 C also includes the raw text of user search entities to provide multi-modal evaluation.1.It should be noted that there are a large number of negative samples in the actual online logs (samples that were exposed but not clicked on).To address this issue, negative sampling was performed which resulted in a higher click-through rate in the AntM 2 C dataset compared to that in the actual online logs.",
        "sentences": [
          {
            "text": "The user features consist of static profile features6and user sequence features.",
            "label": 1
          },
          {
            "text": "The static profile features include basic user attributes such as gender, age, occupation, etc.The sequence features provide the user's recent activities on Alipay, including clicked mini-apps, searched services, purchased items, etc.",
            "label": 1
          },
          {
            "text": "As mentioned in Section 3.1.3,these user features have been desensitized and encrypted for the purpose of user privacy protection and appear in the dataset in an encrypted ID format, making it impossible to reconstruct the original user features.",
            "label": 1
          },
          {
            "text": "In addition to the ID-based features, AntM 2 C also includes the raw text of user search entities to provide multi-modal evaluation.1.",
            "label": 1
          },
          {
            "text": "It should be noted that there are a large number of negative samples in the actual online logs (samples that were exposed but not clicked on).",
            "label": 1
          },
          {
            "text": "To address this issue, negative sampling was performed which resulted in a higher click-through rate in the AntM 2 C dataset compared to that in the actual online logs.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "AntM 2 C_ A Large Scale Dataset For Multi_Scenario Multi_Modal CTR Prediction",
        "section": 13,
        "paragraph_id": 13,
        "full_text": "In this section, we describe the applications of AntM 2 C in several CTR prediction tasks.We briefly introduce each task and report the results of some baseline methods.We select the commonly used AUC (Area Under the Curve) as the metrics for all experiments.",
        "sentences": [
          {
            "text": "In this section, we describe the applications of AntM 2 C in several CTR prediction tasks.",
            "label": 0
          },
          {
            "text": "We briefly introduce each task and report the results of some baseline methods.",
            "label": 0
          },
          {
            "text": "We select the commonly used AUC (Area Under the Curve) as the metrics for all experiments.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "AntM 2 C_ A Large Scale Dataset For Multi_Scenario Multi_Modal CTR Prediction",
        "section": 14,
        "paragraph_id": 14,
        "full_text": "The baseline methods and evaluation results in the experiment provide a demo of using AntM 2 C.More baselines and evaluations will continue to be updated in future work.",
        "sentences": [
          {
            "text": "The baseline methods and evaluation results in the experiment provide a demo of using AntM 2 C.",
            "label": 0
          },
          {
            "text": "More baselines and evaluations will continue to be updated in future work.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "AntM 2 C_ A Large Scale Dataset For Multi_Scenario Multi_Modal CTR Prediction",
        "section": 15,
        "paragraph_id": 15,
        "full_text": "Multi-scenario CTR prediction is a common issue in industrial recommendation systems.It builds a unified model by leveraging CTR data from multiple scenarios.The knowledge sharing between scenarios enables the multi-scenario model to achieve better performance compared to single-scene modeling.We conduct an evaluation on multi-scenario CTR prediction using different baseline methods based on the 5 scenarios in the AntM 2 C dataset.3. The text features will be used for multi-modal evaluation (see in Section 4.3).",
        "sentences": [
          {
            "text": "Multi-scenario CTR prediction is a common issue in industrial recommendation systems.",
            "label": 0
          },
          {
            "text": "It builds a unified model by leveraging CTR data from multiple scenarios.",
            "label": 0
          },
          {
            "text": "The knowledge sharing between scenarios enables the multi-scenario model to achieve better performance compared to single-scene modeling.",
            "label": 0
          },
          {
            "text": "We conduct an evaluation on multi-scenario CTR prediction using different baseline methods based on the 5 scenarios in the AntM 2 C dataset.",
            "label": 0
          },
          {
            "text": "3. The text features will be used for multi-modal evaluation (see in Section 4.3).",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "AntM 2 C_ A Large Scale Dataset For Multi_Scenario Multi_Modal CTR Prediction",
        "section": 16,
        "paragraph_id": 16,
        "full_text": "We mainly choose the multitask methods as the baseline methods for multi-scenario CTR prediction.We treat the CTR estimation for each scenario as a task and share the knowledge among the scenarios at the bottom layer, with each scenario's CTR score output at the tower layer.The baseline methods and hyperparameter settings are as follows: • DNN: The DNN is trained on a mixture of samples from all scenarios without tasks, serving as the baseline for multiscenario CTR prediction.The DNN consists of three layers with 128, 32, and 2 units, respectively.The following multitask model has the same number of layers and unit settings as the DNN.• Shared Bottom[10]: Shared bottom is the most fundamental model in multi-task learning, where the knowledge is shared among the tasks at the bottom layer.Each task has its own independent tower layer and outputs the corresponding CTR score7. • MMoE[7]: Based on the shared bottom, MMOE introduces multiple expert networks, each specialized in predicting a specific task, sharing a common input layer.Additionally, MMOE adds a gating network that assigns different weights to each expert based on the input data to determine their influence on predicting the output for a specific task.In the experiment, we set the number of experts in MMOE to 68.• PLE[12]: Based on MMOE, PLE further designs task-specific experts for each task, while retaining the shared expert.This structure allows the model to better learn the differences and correlations among tasks.We set the number of experts in PLE to be the same as MMOE, with each of the five scenarios having its own specific expert and one globally shared expert 7 .",
        "sentences": [
          {
            "text": "We mainly choose the multitask methods as the baseline methods for multi-scenario CTR prediction.",
            "label": 0
          },
          {
            "text": "We treat the CTR estimation for each scenario as a task and share the knowledge among the scenarios at the bottom layer, with each scenario's CTR score output at the tower layer.",
            "label": 0
          },
          {
            "text": "The baseline methods and hyperparameter settings are as follows: • DNN: The DNN is trained on a mixture of samples from all scenarios without tasks, serving as the baseline for multiscenario CTR prediction.",
            "label": 0
          },
          {
            "text": "The DNN consists of three layers with 128, 32, and 2 units, respectively.",
            "label": 0
          },
          {
            "text": "• Shared Bottom[10]: Shared bottom is the most fundamental model in multi-task learning, where the knowledge is shared among the tasks at the bottom layer.",
            "label": 0
          },
          {
            "text": "Each task has its own independent tower layer and outputs the corresponding CTR score7.",
            "label": 0
          },
          {
            "text": "• MMoE[7]: Based on the shared bottom, MMOE introduces multiple expert networks, each specialized in predicting a specific task, sharing a common input layer.",
            "label": 0
          },
          {
            "text": "Additionally, MMOE adds a gating network that assigns different weights to each expert based on the input data to determine their influence on predicting the output for a specific task.",
            "label": 0
          },
          {
            "text": "• PLE[12]: Based on MMOE, PLE further designs task-specific experts for each task, while retaining the shared expert.",
            "label": 0
          },
          {
            "text": "This structure allows the model to better learn the differences and correlations among tasks.",
            "label": 0
          },
          {
            "text": "We set the number of experts in PLE to be the same as MMOE, with each of the five scenarios having its own specific expert and one globally shared expert 7 .",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "AntM 2 C_ A Large Scale Dataset For Multi_Scenario Multi_Modal CTR Prediction",
        "section": 17,
        "paragraph_id": 17,
        "full_text": "All baseline methods utilized the Adam[5]optimizer with a learning rate of 1e-3 for parameter optimization.The models were trained for 5 epochs with a batch size of 512.5shows the evaluation results of different baseline methods on multi-scenario CTR prediction, from which we can draw the following conclusions.Firstly, compared to the DNN model that trains all data together without considering scenario characteristics, all multi-task models achieve better performance.This demonstrates that in AntM 2 C, there are differences and commonalities between scenarios, and simply mixing training data will not achieve the best results.Secondly, the CTR performance varies across each scenario, indicating different levels of difficulty between scenarios.For example, in scenario B, where there is a large amount of data, the AUC is generally above 0.93, while in scenario D, the AUC is only around 0.68.The diverse business scenarios and items in AntM 2 C enable a more comprehensive and diverse evaluation of CTR.Finally, the expert-structured MMOE and PLE outperform the shared bottom model, demonstrating that refined model design can enhance the performance on AntM 2 C. AntM 2 C is capable of reflecting the differences between different models.",
        "sentences": [
          {
            "text": "All baseline methods utilized the Adam[5]optimizer with a learning rate of 1e-3 for parameter optimization.",
            "label": 0
          },
          {
            "text": "The models were trained for 5 epochs with a batch size of 512.5shows the evaluation results of different baseline methods on multi-scenario CTR prediction, from which we can draw the following conclusions.",
            "label": 0
          },
          {
            "text": "Firstly, compared to the DNN model that trains all data together without considering scenario characteristics, all multi-task models achieve better performance.",
            "label": 0
          },
          {
            "text": "This demonstrates that in AntM 2 C, there are differences and commonalities between scenarios, and simply mixing training data will not achieve the best results.",
            "label": 0
          },
          {
            "text": "Secondly, the CTR performance varies across each scenario, indicating different levels of difficulty between scenarios.",
            "label": 0
          },
          {
            "text": "For example, in scenario B, where there is a large amount of data, the AUC is generally above 0.93, while in scenario D, the AUC is only around 0.68.",
            "label": 0
          },
          {
            "text": "The diverse business scenarios and items in AntM 2 C enable a more comprehensive and diverse evaluation of CTR.",
            "label": 1
          },
          {
            "text": "Finally, the expert-structured MMOE and PLE outperform the shared bottom model, demonstrating that refined model design can enhance the performance on AntM 2 C.",
            "label": 0
          },
          {
            "text": "AntM 2 C is capable of reflecting the differences between different models.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "AntM 2 C_ A Large Scale Dataset For Multi_Scenario Multi_Modal CTR Prediction",
        "section": 18,
        "paragraph_id": 18,
        "full_text": "The cold-start problem is a challenging issue in recommendation systems.Training high-quality CTR models using sparse user-item interaction data is a challenging task.Cold-start primarily involves two aspects: users and items.As shown in Figure2, the AntM 2 C dataset exhibits a natural long-tail distribution in both users and items.Therefore, we conduct a comprehensive evaluation of coldstart baseline methods based on AntM 2 C dataset.",
        "sentences": [
          {
            "text": "The cold-start problem is a challenging issue in recommendation systems.",
            "label": 0
          },
          {
            "text": "Training high-quality CTR models using sparse user-item interaction data is a challenging task.",
            "label": 0
          },
          {
            "text": "Cold-start primarily involves two aspects: users and items.",
            "label": 0
          },
          {
            "text": "As shown in Figure2, the AntM 2 C dataset exhibits a natural long-tail distribution in both users and items.",
            "label": 1
          },
          {
            "text": "Therefore, we conduct a comprehensive evaluation of coldstart baseline methods based on AntM 2 C dataset.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "AntM 2 C_ A Large Scale Dataset For Multi_Scenario Multi_Modal CTR Prediction",
        "section": 19,
        "paragraph_id": 19,
        "full_text": "In cold-start CTR prediction, we split the dataset based on time, using data before 20230717 as the training set and data on 20230717 as the validation and test sets.Based on this data division, we simulated two common cold-start problems in practice: few-shot and zero-shot.",
        "sentences": [
          {
            "text": "In cold-start CTR prediction, we split the dataset based on time, using data before 20230717 as the training set and data on 20230717 as the validation and test sets.",
            "label": 1
          },
          {
            "text": "Based on this data division, we simulated two common cold-start problems in practice: few-shot and zero-shot.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "AntM 2 C_ A Large Scale Dataset For Multi_Scenario Multi_Modal CTR Prediction",
        "section": 20,
        "paragraph_id": 20,
        "full_text": "• Few-shot: users and items that appear in the training set with a count greater than 0 and less than9, meaning there is only a small amount of training data for these users and items.• Zero-shot: users and items that have never appeared in the training set, indicating that either the user is visiting the scenario for the first time or the item has been launched and added to the scenario on the first day.",
        "sentences": [
          {
            "text": "• Few-shot: users and items that appear in the training set with a count greater than 0 and less than9, meaning there is only a small amount of training data for these users and items.",
            "label": 1
          },
          {
            "text": "• Zero-shot: users and items that have never appeared in the training set, indicating that either the user is visiting the scenario for the first time or the item has been launched and added to the scenario on the first day.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "AntM 2 C_ A Large Scale Dataset For Multi_Scenario Multi_Modal CTR Prediction",
        "section": 21,
        "paragraph_id": 21,
        "full_text": "Table6shows the data distribution of the test set under cold-start CTR evaluation.By using this dataset division, we can comprehensively evaluate and compare the performance of CTR models on few-shot and zero-shot samples.For few-shot samples, we can observe the model's performance with only a small amount of training data and evaluate the model's generalization ability.For zero-shot samples, we can evaluate the model's recommendation ability on samples that it has never seen before.",
        "sentences": [
          {
            "text": "Table6shows the data distribution of the test set under cold-start CTR evaluation.",
            "label": 1
          },
          {
            "text": "By using this dataset division, we can comprehensively evaluate and compare the performance of CTR models on few-shot and zero-shot samples.",
            "label": 1
          },
          {
            "text": "For few-shot samples, we can observe the model's performance with only a small amount of training data and evaluate the model's generalization ability.",
            "label": 0
          },
          {
            "text": "For zero-shot samples, we can evaluate the model's recommendation ability on samples that it has never seen before.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "AntM 2 C_ A Large Scale Dataset For Multi_Scenario Multi_Modal CTR Prediction",
        "section": 22,
        "paragraph_id": 22,
        "full_text": "The key issue in cold-start modeling is how to learn user preferences and embeddings of users and items with limited data.In recent years, meta-learning-based cold-start methods have become state-of-the-art methods.We selected several representative methods with publicly available code as our baseline models.",
        "sentences": [
          {
            "text": "The key issue in cold-start modeling is how to learn user preferences and embeddings of users and items with limited data.",
            "label": 0
          },
          {
            "text": "In recent years, meta-learning-based cold-start methods have become state-of-the-art methods.",
            "label": 0
          },
          {
            "text": "We selected several representative methods with publicly available code as our baseline models.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "AntM 2 C_ A Large Scale Dataset For Multi_Scenario Multi_Modal CTR Prediction",
        "section": 23,
        "paragraph_id": 23,
        "full_text": "• DropoutNet[13]: The DropoutNet is a popular cold-start method which applies dropout to control input, and exploits the average representations of interacted items/users to enhance the embeddings of users/items.We implemented the DropoutNet algorithm based on open-source code10.• MAML[2]: The MAML algorithm is a popular meta-learning approach that aims to enable fast adaptation to new tasks with limited data.MAML learns a good initialization of model parameters that can be effectively adapted to new tasks quickly.We treat each user and item as a task in MAML, and conduct meta-training on warm items.Then we perform meta-testing on cold-start items.The subsequent metalearning-based algorithms will also follow this task setting.• MeLU[6]: The MeLU algorithm is the first to apply the MAML to address the cold-start problem in recommender systems.Building upon MAML, MeLU ensures the stability of the learning process by not updating the embeddings in the inner loop (support set)..Although MetaEmb only optimizes the embeddings of items, we have also applied the same approach to optimize the embeddings of users.",
        "sentences": [
          {
            "text": "• DropoutNet[13]: The DropoutNet is a popular cold-start method which applies dropout to control input, and exploits the average representations of interacted items/users to enhance the embeddings of users/items.",
            "label": 0
          },
          {
            "text": "• MAML[2]: The MAML algorithm is a popular meta-learning approach that aims to enable fast adaptation to new tasks with limited data.",
            "label": 0
          },
          {
            "text": "MAML learns a good initialization of model parameters that can be effectively adapted to new tasks quickly.",
            "label": 0
          },
          {
            "text": "We treat each user and item as a task in MAML, and conduct meta-training on warm items.",
            "label": 0
          },
          {
            "text": "Then we perform meta-testing on cold-start items.",
            "label": 0
          },
          {
            "text": "• MeLU[6]: The MeLU algorithm is the first to apply the MAML to address the cold-start problem in recommender systems.",
            "label": 0
          },
          {
            "text": "Although MetaEmb only optimizes the embeddings of items, we have also applied the same approach to optimize the embeddings of users.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "AntM 2 C_ A Large Scale Dataset For Multi_Scenario Multi_Modal CTR Prediction",
        "section": 24,
        "paragraph_id": 24,
        "full_text": "These base models share the common embedding and DNN structure.The dimensionality of embedding vectors of each input field is fixed to 32 for all our experiments.The Adam optimizer with a learning rate of 1e-3 is used to optimize the model parameters, and the training is performed for 3 epochs with a batch size of 512.In addition to the aforementioned cold-start algorithms, the DNN (without any cold-start optimization) is also considered as the baseline method for cold-start CTR.7shows the CTR performance for cold-start users and items.Because there is limited data for cold start users and items, we do not calculate AUC by scenarios, and evaluate the overall performance of cold start users and items.From the table, we can observe several phenomena.Firstly, compared to the results shown in Table5, the AUC for cold-start users and items are generally lower than the overall level, which demonstrates that AntM 2 C's data can effectively reflect the differences between cold and warm items and users.Secondly, different cold-start methods show distinguishable results in AntM 2 C, and all of them are significantly better than the DNN model without cold-start optimization.This indicates that AntM 2 C can effectively compare the effects of different cold-start methods and demonstrate the distinctiveness between methods.Finally, the lower performance of zero-shot compared to few-shot indicates that zero-shot CTR prediction is more challenging than few-shot.The two cold start modes provided by AntM 2 C can comprehensively evaluate cold-start CTR prediction.",
        "sentences": [
          {
            "text": "These base models share the common embedding and DNN structure.",
            "label": 0
          },
          {
            "text": "The dimensionality of embedding vectors of each input field is fixed to 32 for all our experiments.",
            "label": 0
          },
          {
            "text": "The Adam optimizer with a learning rate of 1e-3 is used to optimize the model parameters, and the training is performed for 3 epochs with a batch size of 512.",
            "label": 0
          },
          {
            "text": "7shows the CTR performance for cold-start users and items.",
            "label": 0
          },
          {
            "text": "Because there is limited data for cold start users and items, we do not calculate AUC by scenarios, and evaluate the overall performance of cold start users and items.",
            "label": 0
          },
          {
            "text": "From the table, we can observe several phenomena.",
            "label": 0
          },
          {
            "text": "Firstly, compared to the results shown in Table5, the AUC for cold-start users and items are generally lower than the overall level, which demonstrates that AntM 2 C's data can effectively reflect the differences between cold and warm items and users.",
            "label": 1
          },
          {
            "text": "Secondly, different cold-start methods show distinguishable results in AntM 2 C, and all of them are significantly better than the DNN model without cold-start optimization.",
            "label": 1
          },
          {
            "text": "This indicates that AntM 2 C can effectively compare the effects of different cold-start methods and demonstrate the distinctiveness between methods.",
            "label": 1
          },
          {
            "text": "Finally, the lower performance of zero-shot compared to few-shot indicates that zero-shot CTR prediction is more challenging than few-shot.",
            "label": 1
          },
          {
            "text": "The two cold start modes provided by AntM 2 C can comprehensively evaluate cold-start CTR prediction.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "AntM 2 C_ A Large Scale Dataset For Multi_Scenario Multi_Modal CTR Prediction",
        "section": 25,
        "paragraph_id": 25,
        "full_text": "With the rise of large language models (LLMs), it has become a hot research topic to effectively transfer the knowledge of LLM to CTR prediction.There have been many works[3,4,9,11]based on multi-modal CTR modeling using features such as item and user text.AntM 2 C contains raw text features for both users and",
        "sentences": [
          {
            "text": "With the rise of large language models (LLMs), it has become a hot research topic to effectively transfer the knowledge of LLM to CTR prediction.",
            "label": 0
          },
          {
            "text": "There have been many works[3,4,9,11]based on multi-modal CTR modeling using features such as item and user text.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "AntM 2 C_ A Large Scale Dataset For Multi_Scenario Multi_Modal CTR Prediction",
        "section": 26,
        "paragraph_id": 26,
        "full_text": "In multi-modal evaluation, we adapt the same data processing approach as in multi-scenario evaluation mentioned in Section 4.1.1,and additionally include the text features from Table3: user query entities and item entities.The text features will be used as inputs to the model together with other ID features.",
        "sentences": [
          {
            "text": "In multi-modal evaluation, we adapt the same data processing approach as in multi-scenario evaluation mentioned in Section 4.1.1,and additionally include the text features from Table3: user query entities and item entities.",
            "label": 0
          },
          {
            "text": "The text features will be used as inputs to the model together with other ID features.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "AntM 2 C_ A Large Scale Dataset For Multi_Scenario Multi_Modal CTR Prediction",
        "section": 27,
        "paragraph_id": 27,
        "full_text": "For the baseline model, we use the language model to process the text features, and then concatenate the text embedding with other ID features and input them into the multi-scenario model described in Section 4.1.2.For ease of evaluation, we choose MMoE as the backbone and pre-trained Bertbase13[1]as the text embedding extractor.The output dimension of Bert's embeddings is 768.Then, a DNN with two layers, each layer having [768, 32] units, is used to reduce the dimension of Bert's embedding to 32.This reduced embedding is concatenated with other features and input into the MMOE model.More powerful language models and the application of text features will continue to be supplemented in future works.",
        "sentences": [
          {
            "text": "For the baseline model, we use the language model to process the text features, and then concatenate the text embedding with other ID features and input them into the multi-scenario model described in Section 4.1.2.",
            "label": 0
          },
          {
            "text": "For ease of evaluation, we choose MMoE as the backbone and pre-trained Bertbase13[1]as the text embedding extractor.",
            "label": 0
          },
          {
            "text": "The output dimension of Bert's embeddings is 768.",
            "label": 0
          },
          {
            "text": "Then, a DNN with two layers, each layer having [768, 32] units, is used to reduce the dimension of Bert's embedding to 32.",
            "label": 0
          },
          {
            "text": "This reduced embedding is concatenated with other features and input into the MMOE model.",
            "label": 0
          },
          {
            "text": "More powerful language models and the application of text features will continue to be supplemented in future works.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "AntM 2 C_ A Large Scale Dataset For Multi_Scenario Multi_Modal CTR Prediction",
        "section": 28,
        "paragraph_id": 28,
        "full_text": ". Table8shows the evaluation results of the multimodal CTR.It can be observed that, after adding the text modality, the CTR performance is better in data-sparse scenarios C, D, and E compared to using only the ID modality in the MMoE.Since the current baseline for using the text modality is relatively simple, the improvement in performance is not significant.However, this shows the potential of the text modality provided in AntM 2 C to improve CTR performance.",
        "sentences": [
          {
            "text": "Table8shows the evaluation results of the multimodal CTR.",
            "label": 0
          },
          {
            "text": "It can be observed that, after adding the text modality, the CTR performance is better in data-sparse scenarios C, D, and E compared to using only the ID modality in the MMoE.",
            "label": 0
          },
          {
            "text": "Since the current baseline for using the text modality is relatively simple, the improvement in performance is not significant.",
            "label": 0
          },
          {
            "text": "However, this shows the potential of the text modality provided in AntM 2 C to improve CTR performance.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "AU_AIR_ A Multi_modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance",
        "section": 0,
        "paragraph_id": 0,
        "full_text": "Abstract: Unmanned aerial vehicles (UAVs) with mounted cameras have the advantage of capturing aerial (bird-view) images.The availability of aerial visual data and the recent advances in object detection algorithms led the computer vision community to focus on object detection tasks on aerial images.As a result of this, several aerial datasets have been introduced, including visual data with object annotations.UAVs are used solely as flying-cameras in these datasets, discarding different data types regarding the flight (e.g., time, location, internal sensors).In this work, we propose a multi-purpose aerial dataset (AU-AIR) that has multi-modal sensor data (i.e., visual, time, location, altitude, IMU, velocity) collected in real-world outdoor environments.The AU-AIR dataset includes meta-data for extracted frames (i.e., bounding box annotations for trafficrelated object category) from recorded RGB videos.Moreover, we emphasize the differences between natural and aerial images in the context of object detection task.For this end, we train and test mobile object detectors (including YOLOv3-Tiny and MobileNetv2-SSDLite) on the AU-AIR dataset, which are applicable for real-time object detection using on-board computers with UAVs.Since our dataset has diversity in recorded data types, it contributes to filling the gap between computer vision and robotics.The dataset is available at https://bozcani.github.io/auairdataset.",
        "sentences": [
          {
            "text": "Abstract: Unmanned aerial vehicles (UAVs) with mounted cameras have the advantage of capturing aerial (bird-view) images.",
            "label": 0
          },
          {
            "text": "The availability of aerial visual data and the recent advances in object detection algorithms led the computer vision community to focus on object detection tasks on aerial images.",
            "label": 0
          },
          {
            "text": "As a result of this, several aerial datasets have been introduced, including visual data with object annotations.",
            "label": 0
          },
          {
            "text": "UAVs are used solely as flying-cameras in these datasets, discarding different data types regarding the flight (e.g., time, location, internal sensors).",
            "label": 0
          },
          {
            "text": "In this work, we propose a multi-purpose aerial dataset (AU-AIR) that has multi-modal sensor data (i.e., visual, time, location, altitude, IMU, velocity) collected in real-world outdoor environments.",
            "label": 1
          },
          {
            "text": "The AU-AIR dataset includes meta-data for extracted frames (i.e., bounding box annotations for trafficrelated object category) from recorded RGB videos.",
            "label": 1
          },
          {
            "text": "Moreover, we emphasize the differences between natural and aerial images in the context of object detection task.",
            "label": 1
          },
          {
            "text": "For this end, we train and test mobile object detectors (including YOLOv3-Tiny and MobileNetv2-SSDLite) on the AU-AIR dataset, which are applicable for real-time object detection using on-board computers with UAVs.",
            "label": 1
          },
          {
            "text": "Since our dataset has diversity in recorded data types, it contributes to filling the gap between computer vision and robotics.",
            "label": 1
          },
          {
            "text": "The dataset is available at https://bozcani.github.io/auairdataset.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "AU_AIR_ A Multi_modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance",
        "section": 1,
        "paragraph_id": 1,
        "full_text": "Unmanned aerial vehicles (UAVs) are extensively used as flying platforms of sensors for different domains such as traffic surveillance[1], managing the urban environment[2], package delivery[3]or aerial cinematography[4].For these applications, UAVs are equipped with mounted cameras and mainly gather visual data of the environment.Then, computer vision algorithms are applied to aerial visual data to extract high-level information regarding the environment.",
        "sentences": [
          {
            "text": "Unmanned aerial vehicles (UAVs) are extensively used as flying platforms of sensors for different domains such as traffic surveillance[1], managing the urban environment[2], package delivery[3]or aerial cinematography[4].",
            "label": 0
          },
          {
            "text": "For these applications, UAVs are equipped with mounted cameras and mainly gather visual data of the environment.",
            "label": 0
          },
          {
            "text": "Then, computer vision algorithms are applied to aerial visual data to extract high-level information regarding the environment.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "AU_AIR_ A Multi_modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance",
        "section": 2,
        "paragraph_id": 2,
        "full_text": "Object detection is one of the most studied problems in computer vision.The recent advances in deep learning (variants of convolutional neural networks (CNNs) mainly) have led to breakthrough object detection performances with the availability of large datasets and computing power.Since these methods require a large number of training samples, several datasets (e.g., COCO[5], Pascal VOC[6]) have been introduced for benchmarking for the object detection task.The samples in these datasets consist of natural images that are mainly captured by handheld cameras.The significant differences between natural and aerial images (such as object layouts and sizes) cause these object detectors to have trouble to find objects in aerial images.Therefore, several datasets (e.g.,[7]-[13]) have been introduced in recent years as a benchmark for object detection in aerial images.Besides visual data gathered by a camera, the data from other sensors might give crucial information about the environment.The use of UAVs as only flying cameras cut off the potential advance in multi-modal object detection algorithms for aerial applications.For instance, the recent advances in perception for autonomous driving have brought new datasets such as[14]-[16]including multi-modal data (e.g., RGB images, Global Positioning System (GPS) coordinates, inertial measurement unit (IMU) data).Although the data fusion for object detection is still open research topic[17], these multi-modal datasets allow a benchmark for further research.However, to the best of our knowledge, there is no such multi-modal dataset collected in a real-world outdoor environment for UAVs.",
        "sentences": [
          {
            "text": "Object detection is one of the most studied problems in computer vision.",
            "label": 0
          },
          {
            "text": "The recent advances in deep learning (variants of convolutional neural networks (CNNs) mainly) have led to breakthrough object detection performances with the availability of large datasets and computing power.",
            "label": 0
          },
          {
            "text": "Since these methods require a large number of training samples, several datasets (e.g., COCO[5], Pascal VOC[6]) have been introduced for benchmarking for the object detection task.",
            "label": 0
          },
          {
            "text": "The samples in these datasets consist of natural images that are mainly captured by handheld cameras.",
            "label": 0
          },
          {
            "text": "The significant differences between natural and aerial images (such as object layouts and sizes) cause these object detectors to have trouble to find objects in aerial images.",
            "label": 0
          },
          {
            "text": "Therefore, several datasets (e.g.,[7]-[13]) have been introduced in recent years as a benchmark for object detection in aerial images.",
            "label": 0
          },
          {
            "text": "Besides visual data gathered by a camera, the data from other sensors might give crucial information about the environment.",
            "label": 0
          },
          {
            "text": "The use of UAVs as only flying cameras cut off the potential advance in multi-modal object detection algorithms for aerial applications.",
            "label": 0
          },
          {
            "text": "For instance, the recent advances in perception for autonomous driving have brought new datasets such as[14]-[16]including multi-modal data (e.g., RGB images, Global Positioning System (GPS) coordinates, inertial measurement unit (IMU) data).",
            "label": 0
          },
          {
            "text": "Although the data fusion for object detection is still open research topic[17], these multi-modal datasets allow a benchmark for further research.",
            "label": 0
          },
          {
            "text": "However, to the best of our knowledge, there is no such multi-modal dataset collected in a real-world outdoor environment for UAVs.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "AU_AIR_ A Multi_modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance",
        "section": 3,
        "paragraph_id": 3,
        "full_text": "In this work, we present a multi-modal UAV dataset (The AU-AIR dataset) in order to push forward the development of computer vision and robotic algorithms targeted at autonomous aerial surveillance.The AU-AIR dataset meets vision and robotics for UAVs having the multi-modal data from different on-board sensors.The dataset consists of 8 video streams (over 2 hours in total) for traffic surveillance.The videos mainly are recorded at Skejby Nordlandsvej and P.O Pedersensvej roads (Aarhus, Denmark).The dataset includes aerial videos, time, GPS coordinates and the altitude of the UAV, IMU data, and the velocity.The videos are recorded at different flight altitudes from 5 meters to 30 meters and in different camera angles from 45 degrees to 90 degrees (i.e., complete bird-view images that the camera is perpendicular to the Earth).Instances belonging to different object categories related to the traffic surveillance context are annotated with bounding boxes in video frames.Moreover, each extracted video frame is labeled with the flight data (See Fig.1).",
        "sentences": [
          {
            "text": "In this work, we present a multi-modal UAV dataset (The AU-AIR dataset) in order to push forward the development of computer vision and robotic algorithms targeted at autonomous aerial surveillance.",
            "label": 1
          },
          {
            "text": "The AU-AIR dataset meets vision and robotics for UAVs having the multi-modal data from different on-board sensors.",
            "label": 1
          },
          {
            "text": "The dataset consists of 8 video streams (over 2 hours in total) for traffic surveillance.",
            "label": 1
          },
          {
            "text": "The videos mainly are recorded at Skejby Nordlandsvej and P.O Pedersensvej roads (Aarhus, Denmark).",
            "label": 1
          },
          {
            "text": "The dataset includes aerial videos, time, GPS coordinates and the altitude of the UAV, IMU data, and the velocity.",
            "label": 1
          },
          {
            "text": "The videos are recorded at different flight altitudes from 5 meters to 30 meters and in different camera angles from 45 degrees to 90 degrees (i.e., complete bird-view images that the camera is perpendicular to the Earth).",
            "label": 1
          },
          {
            "text": "Instances belonging to different object categories related to the traffic surveillance context are annotated with bounding boxes in video frames.",
            "label": 1
          },
          {
            "text": "Moreover, each extracted video frame is labeled with the flight data (See Fig.1).",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "AU_AIR_ A Multi_modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance",
        "section": 4,
        "paragraph_id": 4,
        "full_text": "The whole dataset includes 32,823 labeled video frames with object annotations and the corresponding flight data.Eight object categories are annotated including person, car, van, truck, motorbike, bike, bus, trailer.The total number of annotated instances is 132,034.The dataset is split into 30,000 training-validation samples and 2,823 test samples.",
        "sentences": [
          {
            "text": "The whole dataset includes 32,823 labeled video frames with object annotations and the corresponding flight data.",
            "label": 1
          },
          {
            "text": "Eight object categories are annotated including person, car, van, truck, motorbike, bike, bus, trailer.",
            "label": 1
          },
          {
            "text": "The total number of annotated instances is 132,034.",
            "label": 1
          },
          {
            "text": "The dataset is split into 30,000 training-validation samples and 2,823 test samples.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "AU_AIR_ A Multi_modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance",
        "section": 5,
        "paragraph_id": 5,
        "full_text": "In this work, we emphasize differences between aerial and natural images in the context of object detection tasks.To this end, we compare image samples and object instances between the AU-AIR dataset and the COCO dataset[5].In our experiments, we train and evaluate two mobile object detectors (including YOLOv3-tiny[18]and MobileNetv2-SSD Lite[19]on the AU-AIR dataset.We form a baseline, including mobile object detectors since we focus on realtime performance and the applicability of object detection task onboard computers mounted on UAV.",
        "sentences": [
          {
            "text": "In this work, we emphasize differences between aerial and natural images in the context of object detection tasks.",
            "label": 0
          },
          {
            "text": "To this end, we compare image samples and object instances between the AU-AIR dataset and the COCO dataset[5].",
            "label": 1
          },
          {
            "text": "In our experiments, we train and evaluate two mobile object detectors (including YOLOv3-tiny[18]and MobileNetv2-SSD Lite[19]on the AU-AIR dataset.",
            "label": 0
          },
          {
            "text": "We form a baseline, including mobile object detectors since we focus on realtime performance and the applicability of object detection task onboard computers mounted on UAV.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "AU_AIR_ A Multi_modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance",
        "section": 6,
        "paragraph_id": 6,
        "full_text": "In recent years, several drone datasets have been introduced for object detection tasks ([7]-[13]).Zhu et al.[7]propose a UAV dataset (VisDrone) consisting of visual data and object annotations in images and frames.In the VisDrone dataset, object instances belonging the certain categories are annotated by bounding boxes and category labels.Besides object annotations, VisDrone includes some vision-related attributes such as the visibility of a scene, occlusion status.Du et al.[8]propose a benchmark dataset for object detection and tracking in aerial images.The dataset also includes meta information regarding the flight altitude.Hsieh et al.[9]propose a UAV-based counting dataset (CARPK) including object instances that belong to the car category.Robicquet et al.[10]introduce a UAV dataset (Stanford) that collects images and videos of six types of objects in the Stanford campus area.In this dataset, some of the object categories dominate the dataset having a high number of samples, whereas the remaining object categories have significantly less number of instances.Mueller et al.[11]propose synthetic dataset created by a simulator for target tracking with a UAV.Collins et al.[12]introduce a benchmarking website (VIVID) with an evaluation dataset collected under the DARPA VIVID program.Krajewski et al.propose an aerial dataset collected from highways, including object bounding boxes and labels of vehicles.",
        "sentences": [
          {
            "text": "In recent years, several drone datasets have been introduced for object detection tasks ([7]-[13]).",
            "label": 0
          },
          {
            "text": "Zhu et al.[7]propose a UAV dataset (VisDrone) consisting of visual data and object annotations in images and frames.",
            "label": 0
          },
          {
            "text": "In the VisDrone dataset, object instances belonging the certain categories are annotated by bounding boxes and category labels.",
            "label": 0
          },
          {
            "text": "Besides object annotations, VisDrone includes some vision-related attributes such as the visibility of a scene, occlusion status.",
            "label": 0
          },
          {
            "text": "Du et al.[8]propose a benchmark dataset for object detection and tracking in aerial images.",
            "label": 0
          },
          {
            "text": "The dataset also includes meta information regarding the flight altitude.",
            "label": 0
          },
          {
            "text": "Hsieh et al.[9]propose a UAV-based counting dataset (CARPK) including object instances that belong to the car category.",
            "label": 0
          },
          {
            "text": "Robicquet et al.[10]introduce a UAV dataset (Stanford) that collects images and videos of six types of objects in the Stanford campus area.",
            "label": 0
          },
          {
            "text": "In this dataset, some of the object categories dominate the dataset having a high number of samples, whereas the remaining object categories have significantly less number of instances.",
            "label": 0
          },
          {
            "text": "Mueller et al.[11]propose synthetic dataset created by a simulator for target tracking with a UAV.",
            "label": 0
          },
          {
            "text": "Collins et al.[12]introduce a benchmarking website (VIVID) with an evaluation dataset collected under the DARPA VIVID program.",
            "label": 0
          },
          {
            "text": "Krajewski et al.propose an aerial dataset collected from highways, including object bounding boxes and labels of vehicles.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "AU_AIR_ A Multi_modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance",
        "section": 7,
        "paragraph_id": 7,
        "full_text": "These datasets are annotated by common objects in an environment such as humans and different types of vehicles (e.g., car, bike, van).However, they only include visual data and bounding box annotations for objects and discard other sensory data.Among these studies, only UAVDT[8]includes an attribute that gives limited information about the flight altitude (i.e., labels such as \"low-level\", \"mid-level\" and \"high-level\").",
        "sentences": [
          {
            "text": "These datasets are annotated by common objects in an environment such as humans and different types of vehicles (e.g., car, bike, van).",
            "label": 0
          },
          {
            "text": "However, they only include visual data and bounding box annotations for objects and discard other sensory data.",
            "label": 0
          },
          {
            "text": "Among these studies, only UAVDT[8]includes an attribute that gives limited information about the flight altitude (i.e., labels such as \"low-level\", \"mid-level\" and \"high-level\").",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "AU_AIR_ A Multi_modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance",
        "section": 8,
        "paragraph_id": 8,
        "full_text": "Fonder et al.[20]propose a synthetic dataset (Mid-Air) for low altitude drone flights in unstructured environments (e.g., forest, country).It includes multi-modal data regarding the flight (e.g., visual, GPS, IMU data) without any annotations for visual data.",
        "sentences": [
          {
            "text": "Fonder et al.[20]propose a synthetic dataset (Mid-Air) for low altitude drone flights in unstructured environments (e.g., forest, country).",
            "label": 0
          },
          {
            "text": "It includes multi-modal data regarding the flight (e.g., visual, GPS, IMU data) without any annotations for visual data.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "AU_AIR_ A Multi_modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance",
        "section": 9,
        "paragraph_id": 9,
        "full_text": "There are also multi-modal drone datasets in the literature ([20]-[24]).However, the visual data are not collected for object detection since the main focus of these studies is the UAV navigation.Therefore, these datasets do not have object annotations.The comparison of existing datasets is given in TableI.",
        "sentences": [
          {
            "text": "There are also multi-modal drone datasets in the literature ([20]-[24]).",
            "label": 0
          },
          {
            "text": "However, the visual data are not collected for object detection since the main focus of these studies is the UAV navigation.",
            "label": 0
          },
          {
            "text": "Therefore, these datasets do not have object annotations.",
            "label": 0
          },
          {
            "text": "The comparison of existing datasets is given in TableI.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "AU_AIR_ A Multi_modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance",
        "section": 10,
        "paragraph_id": 10,
        "full_text": "Looking also at the summary of the existing studies in TableI, the followings are the main contributions of this work: • To the best of our knowledge, the AU-AIR dataset is the first multi-modal UAV dataset for object detection.",
        "sentences": [
          {
            "text": "Looking also at the summary of the existing studies in TableI, the followings are the main contributions of this work: • To the best of our knowledge, the AU-AIR dataset is the first multi-modal UAV dataset for object detection.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "AU_AIR_ A Multi_modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance",
        "section": 11,
        "paragraph_id": 11,
        "full_text": "The dataset includes flight data (i.e., time, GPS, altitude, IMU data) in addition to visual data and objects annotations.• Considering the real-time applicability, we form a baseline training and testing mobile object detectors with the AU-AIR dataset.We emphasize the differences between object detection in aerial images and natural images.",
        "sentences": [
          {
            "text":"The dataset includes flight data (i.e., time, GPS, altitude, IMU data) in addition to visual data and objects annotations.",
            "label": 0
          },
          {
            "text": "• Considering the real-time applicability, we form a baseline training and testing mobile object detectors with the AU-AIR dataset.",
            "label": 0
          },
          {
            "text": "We emphasize the differences between object detection in aerial images and natural images.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "AU_AIR_ A Multi_modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance",
        "section": 12,
        "paragraph_id": 12,
        "full_text": "The availability of large amounts of data and processing power enables deep neural networks to achieve state-of-theart results for object detection.Currently, deep learningbased object detectors are separated into two groups.The first group consists of region-based CNNs that ascend on image classifiers.Region-based CNNs propose image regions that are likely to contain an object and classify the region into a predefined object category.The second group has only one stage converting to the object detection problem into the bounding box prediction for objects, without re-purposing image classifiers.Faster-R-CNN[25]is one of the wellknown models belonging to the first group, YOLO[26]and SSD[27]are the popular object detectors that belong to the second group.",
        "sentences": [
          {
            "text": "The availability of large amounts of data and processing power enables deep neural networks to achieve state-of-theart results for object detection.",
            "label": 0
          },
          {
            "text": "Currently, deep learningbased object detectors are separated into two groups.",
            "label": 0
          },
          {
            "text": "The first group consists of region-based CNNs that ascend on image classifiers.",
            "label": 0
          },
          {
            "text": "Region-based CNNs propose image regions that are likely to contain an object and classify the region into a predefined object category.",
            "label": 0
          },
          {
            "text": "The second group has only one stage converting to the object detection problem into the bounding box prediction for objects, without re-purposing image classifiers.",
            "label": 0
          },
          {
            "text": "Faster-R-CNN[25]is one of the wellknown models belonging to the first group, YOLO[26]and SSD[27]are the popular object detectors that belong to the second group.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "AU_AIR_ A Multi_modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance",
        "section": 13,
        "paragraph_id": 13,
        "full_text": "Deep learning-based object detectors have trained and performed on large datasets such as COCO[5]and PASCAL[6].These datasets include natural images that contain a single object or multi objects in their natural environments.Most of the images in these datasets are captured by humans using a handheld camera so that the vast majority of images have side-view.There are challenges of the object detection in natural images such as occlusion, illumination changes, rotation, low resolution, crowd existence of instances.",
        "sentences": [
          {
            "text": "Deep learning-based object detectors have trained and performed on large datasets such as COCO[5]and PASCAL[6].",
            "label": 0
          },
          {
            "text": "These datasets include natural images that contain a single object or multi objects in their natural environments.",
            "label": 0
          },
          {
            "text": "Most of the images in these datasets are captured by humans using a handheld camera so that the vast majority of images have side-view.",
            "label": 0
          },
          {
            "text": "There are challenges of the object detection in natural images such as occlusion, illumination changes, rotation, low resolution, crowd existence of instances.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "AU_AIR_ A Multi_modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance",
        "section": 14,
        "paragraph_id": 14,
        "full_text": "Aerial images have different characteristics from natural images due to having a bird's-eye view.First of all, objects in natural images are much larger than their counterparts in aerial images.For example, an object category such as humans may occupy a large number of pixels in natural images.However, it may have a few numbers of pixels   in an aerial image that is quite challenging to detect for object detectors (See Fig.2).Moreover, aerial images can be fed to a network with higher dimensions that increases computational cost in order to prevent the diminishing of pixels belonging to small objects.Secondly, an occlusion is observed in different conditions for natural and aerial images.In natural images, an object instance may be occluded by another foreground object instance (e.g., a human in front of a car).However, objects in aerial images are less likely to be occluded by other foreground objects (especially, bird-view images captured by a camera that is perpendicular to the Earth).(See Fig.3.",
        "sentences": [
          {
            "text": "Aerial images have different characteristics from natural images due to having a bird's-eye view.",
            "label": 0
          },
          {
            "text": "First of all, objects in natural images are much larger than their counterparts in aerial images.",
            "label": 0
          },
          {
            "text": "For example, an object category such as humans may occupy a large number of pixels in natural images.",
            "label": 0
          },
          {
            "text": "However, it may have a few numbers of pixels   in an aerial image that is quite challenging to detect for object detectors (See Fig.2).",
            "label": 0
          },
          {
            "text": "Moreover, aerial images can be fed to a network with higher dimensions that increases computational cost in order to prevent the diminishing of pixels belonging to small objects.",
            "label": 0
          },
          {
            "text": "Secondly, an occlusion is observed in different conditions for natural and aerial images.",
            "label": 0
          },
          {
            "text": "In natural images, an object instance may be occluded by another foreground object instance (e.g., a human in front of a car).",
            "label": 0
          },
          {
            "text": "(See Fig.3.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "AU_AIR_ A Multi_modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance",
        "section": 15,
        "paragraph_id": 15,
        "full_text": "Thirdly, the perspective in aerial images makes appearances of objects short and squat.This fact diminishes the information regarding an object height (See Fig.4).Moreover, although aerial images can supply more contextual information about an environment by a broader view angle, the object instances may be amid cluttered.",
        "sentences": [
          {
            "text": "Thirdly, the perspective in aerial images makes appearances of objects short and squat.",
            "label": 0
          },
          {
            "text": "This fact diminishes the information regarding an object height (See Fig.4).",
            "label": 0
          },
          {
            "text": "Moreover, although aerial images can supply more contextual information about an environment by a broader view angle, the object instances may be amid cluttered.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "AU_AIR_ A Multi_modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance",
        "section": 16,
        "paragraph_id": 16,
        "full_text": "Lastly, having a drone to capture aerial images, the altitude changes during the flight can cause varieties in object size and appearance in aerial images.Therefore, a recording of aerial videos at different altitudes may change the levels of challenges mentioned above.",
        "sentences": [
          {
            "text": "Lastly, having a drone to capture aerial images, the altitude changes during the flight can cause varieties in object size and appearance in aerial images.",
            "label": 0
          },
          {
            "text": "Therefore, a recording of aerial videos at different altitudes may change the levels of challenges mentioned above.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "AU_AIR_ A Multi_modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance",
        "section": 17,
        "paragraph_id": 17,
        "full_text": "To address the challenges mentioned in Section II, we propose a multi-modal drone dataset (AU-AIR) including videos, object annotations in the extracted frames and sensor data for the corresponding frames.The data are captured by low-level flight (max.30 meters) and for the scenario of a traffic surveillance.The AU-AIR dataset consists of video clips, sensor data, and object bounding box annotations for video frames.",
        "sentences": [
          {
            "text": "To address the challenges mentioned in Section II, we propose a multi-modal drone dataset (AU-AIR) including videos, object annotations in the extracted frames and sensor data for the corresponding frames.",
            "label": 1
          },
          {
            "text": "The data are captured by low-level flight (max.30 meters) and for the scenario of a traffic surveillance.",
            "label": 1
          },
          {
            "text": "The AU-AIR dataset consists of video clips, sensor data, and object bounding box annotations for video frames.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "AU_AIR_ A Multi_modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance",
        "section": 18,
        "paragraph_id": 18,
        "full_text": "We have used a quadrotor (Parrot Bebop 2) to capture the videos and record the flight data.An on-board camera has recorded the videos with a resolution of 1920 × 1080 pixels at 30 frames per second (fps).The sensor data have been recorded for every 20 milliseconds.",
        "sentences": [
          {
            "text": "We have used a quadrotor (Parrot Bebop 2) to capture the videos and record the flight data.",
            "label": 1
          },
          {
            "text": "An on-board camera has recorded the videos with a resolution of 1920 × 1080 pixels at 30 frames per second (fps).",
            "label": 1
          },
          {
            "text": "The sensor data have been recorded for every 20 milliseconds.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "AU_AIR_ A Multi_modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance",
        "section": 19,
        "paragraph_id": 19,
        "full_text": "The AU-AIR dataset consists of 8 video clips (approximately in 2 hours of a total length) with 32,823 extracted frames.All videos are recorded for a scenario of aerial traffic surveillance at the intersection of Skejby Nordlandsvej and P.O Pedersensvej (Aarhus, Denmark) on windless days.Moreover, the videos cover various lighting conditions due to the time of the day and the weather conditions (e.g., sunny, partly sunny, cloudy).",
        "sentences": [
          {
            "text": "The AU-AIR dataset consists of 8 video clips (approximately in 2 hours of a total length) with 32,823 extracted frames.",
            "label": 1
          },
          {
            "text": "All videos are recorded for a scenario of aerial traffic surveillance at the intersection of Skejby Nordlandsvej and P.",
            "label": 1
          },
          {
            "text": "Moreover, the videos cover various lighting conditions due to the time of the day and the weather conditions (e.g., sunny, partly sunny, cloudy).",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "AU_AIR_ A Multi_modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance",
        "section": 20,
        "paragraph_id": 20,
        "full_text": "Capturing an aerial video with a UAV brings different challenges for visual surveillance that are significantly different from natural images.To add these challenges in our dataset, we have captured the videos in different flight altitudes and camera angles.The flight altitude changes between 10 meters to 30 meters in the videos and the camera angle is adjusted from 45 degrees to 90 degrees (perpendicular to the Earth).An increase in the camera angle makes object detection task more challenging since images get differ from natural images.",
        "sentences": [
          {
            "text": "Capturing an aerial video with a UAV brings different challenges for visual surveillance that are significantly different from natural images.",
            "label": 0
          },
          {
            "text": "To add these challenges in our dataset, we have captured the videos in different flight altitudes and camera angles.",
            "label": 1
          },
          {
            "text": "The flight altitude changes between 10 meters to 30 meters in the videos and the camera angle is adjusted from 45 degrees to 90 degrees (perpendicular to the Earth).",
            "label": 1
          },
          {
            "text": "An increase in the camera angle makes object detection task more challenging since images get differ from natural images.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "AU_AIR_ A Multi_modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance",
        "section": 21,
        "paragraph_id": 21,
        "full_text": "Although the videos have been recorded with 30 fps, we have extracted five frames for every second in order to prevent the redundant occurrence of frames.Both of raw videos and extracted frames have a resolution of 1920×1080 pixels.",
        "sentences": [
          {
            "text": "Although the videos have been recorded with 30 fps, we have extracted five frames for every second in order to prevent the redundant occurrence of frames.",
            "label": 1
          },
          {
            "text": "Both of raw videos and extracted frames have a resolution of 1920×1080 pixels.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "AU_AIR_ A Multi_modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance",
        "section": 22,
        "paragraph_id": 22,
        "full_text": "Considering a traffic surveillance scenario, we have manually annotated specific object categories in the frames.For annotation, we used a bounding box and object category index for each instance.The annotated object categories include eight types of objects which highly occur during the traffic surveillance: person, car, bus, van, truck, bike, motorbike, and trailer.",
        "sentences": [
          {
            "text": "Considering a traffic surveillance scenario, we have manually annotated specific object categories in the frames.",
            "label": 1
          },
          {
            "text": "For annotation, we used a bounding box and object category index for each instance.",
            "label": 1
          },
          {
            "text": "The annotated object categories include eight types of objects which highly occur during the traffic surveillance: person, car, bus, van, truck, bike, motorbike, and trailer.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "AU_AIR_ A Multi_modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance",
        "section": 23,
        "paragraph_id": 23,
        "full_text": "For annotation, we employed workers on Amazons Mechanical Turk (AMT)[28].In order to increase the labeling quality, three workers annotated the same frame separately.Then, we combined annotations if they have the same object labels, and whose bounding boxes overlap more than a certain threshold.We chose a threshold as a value of 0.75 experimentally.In case this condition is not satisfied, we manually fine-tuned the bounding boxes and class labels.The category distribution over the dataset can be seen in Fig.5.In the context of traffic surveillance, cars appear significantly more than other classes, and three vehicle types (car, van, truck) have a major portion of annotated bounding boxes.",
        "sentences": [
          {
            "text": "For annotation, we employed workers on Amazons Mechanical Turk (AMT)[28].",
            "label": 1
          },
          {
            "text": "In order to increase the labeling quality, three workers annotated the same frame separately.",
            "label": 1
          },
          {
            "text": "Then, we combined annotations if they have the same object labels, and whose bounding boxes overlap more than a certain threshold.",
            "label": 1
          },
          {
            "text": "We chose a threshold as a value of 0.75 experimentally.",
            "label": 1
          },
          {
            "text": "In case this condition is not satisfied, we manually fine-tuned the bounding boxes and class labels.",
            "label": 1
          },
          {
            "text": "The category distribution over the dataset can be seen in Fig.5.",
            "label": 1
          },
          {
            "text": "In the context of traffic surveillance, cars appear significantly more than other classes, and three vehicle types (car, van, truck) have a major portion of annotated bounding boxes.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "AU_AIR_ A Multi_modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance",
        "section": 24,
        "paragraph_id": 24,
        "full_text": "The AU-AIR dataset includes frames that are captured in different flight altitudes (See Fig.6).We recorded the data mainly for 10 meters, 20 meters, and 30 meters with different camera angles from 45 degrees to 90 degrees.",
        "sentences": [
          {
            "text": "The AU-AIR dataset includes frames that are captured in different flight altitudes (See Fig.6).",
            "label": 1
          },
          {
            "text": "We recorded the data mainly for 10 meters, 20 meters, and 30 meters with different camera angles from 45 degrees to 90 degrees.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "AU_AIR_ A Multi_modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance",
        "section": 25,
        "paragraph_id": 25,
        "full_text": "In addition to visual data and object annotations, the AU-AIR dataset includes sensor data that are logged during the video recording.In the dataset, we have the following attributes for each extracted frame: • la: latitude of the UAV (read from GPS sensor).",
        "sentences": [
          {
            "text": "In addition to visual data and object annotations, the AU-AIR dataset includes sensor data that are logged during the video recording.",
            "label": 1
          },
          {
            "text":"In the dataset, we have the following attributes for each extracted frame: • la: latitude of the UAV (read from GPS sensor).",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "AU_AIR_ A Multi_modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance",
        "section": 26,
        "paragraph_id": 26,
        "full_text": "• lo: longitude of the UAV (read from GPS sensor) • a: altitude of the UAV (read from altimeter) • φ: UAV roll angle (rotation around the x axis) (read from IMU sensor) • θ: UAV pitch angle (rotation around the y axis) (read from IMU sensor) • ψ: UAV yaw angle (rotation around the z axis) (read from IMU sensor) • V x : speed on the x axis • V y : speed on the y axis • V z : speed on the z axis TableIIshows unit values and ranges for each attribute except the date.The date (d) has a format of MMDDYYYY-HHMMSS where MM, DD, YYYY, HH, MM, SS indicates the month, day, year, hour, minutes, and second, respectively.The velocities (V x , V y , V z ) and rotation angles (φ, θ, ψ) are calculated according to the UAV body-frame given in Fig.7.",
        "sentences": [
          {
            "text": "• lo: longitude of the UAV (read from GPS sensor) • a: altitude of the UAV (read from altimeter) • φ: UAV roll angle (rotation around the x axis) (read from IMU sensor) • θ: UAV pitch angle (rotation around the y axis) (read from IMU sensor) • ψ: UAV yaw angle (rotation around the z axis) (read from IMU sensor) • V x : speed on the x axis • V y : speed on the y axis • V z : speed on the z axis TableIIshows unit values and ranges for each attribute except the date.",
            "label": 1
          },
          {
            "text": "The date (d) has a format of MMDDYYYY-HHMMSS where MM, DD, YYYY, HH, MM, SS indicates the month, day, year, hour, minutes, and second, respectively.",
            "label": 1
          },
          {
            "text": "The velocities (V x , V y , V z ) and rotation angles (φ, θ, ψ) are calculated according to the UAV body-frame given in Fig.7.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "AU_AIR_ A Multi_modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance",
        "section": 27,
        "paragraph_id": 27,
        "full_text": "We train and evaluate mobile object detectors with our dataset.During the evaluation, we consider real-time performance rather than achieving a state-of-the-art accuracy for the sake of the applicability.Therefore, we choose two mobile object detectors (YOLOv3-Tiny[18]and MobileNetv2-SSDLite[19]), which have a reasonable trade-off between the detection accuracy and the inference time.",
        "sentences": [
          {
            "text": "We train and evaluate mobile object detectors with our dataset.",
            "label": 0
          },
          {
            "text": "During the evaluation, we consider real-time performance rather than achieving a state-of-the-art accuracy for the sake of the applicability.",
            "label": 0
          },
          {
            "text": "Therefore, we choose two mobile object detectors (YOLOv3-Tiny[18]and MobileNetv2-SSDLite[19]), which have a reasonable trade-off between the detection accuracy and the inference time.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "AU_AIR_ A Multi_modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance",
        "section": 28,
        "paragraph_id": 28,
        "full_text": "We configure YOLOv3-Tiny[18]and MobileNetv2-SSDLite[19]for the bench-marking using the default parameters (e.g., learning rate, input size) as suggested in the original papers.We use the models that are trained on the COCO dataset as backbones.",
        "sentences": [
          {
            "text": "We configure YOLOv3-Tiny[18]and MobileNetv2-SSDLite[19]for the bench-marking using the default parameters (e.g., learning rate, input size) as suggested in the original papers.",
            "label": 0
          },
          {
            "text": "We use the models that are trained on the COCO dataset as backbones.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "AU_AIR_ A Multi_modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance",
        "section": 29,
        "paragraph_id": 29,
        "full_text": "We split the AU-AIR dataset into %60 training, %10 validation and %30 testing samples.The object detectors are adapted to the total number of classes in the AU-AIR dataset (8 classes in total) by changing their last layers.",
        "sentences": [
          {
            "text": "We split the AU-AIR dataset into %60 training, %10 validation and %30 testing samples.",
            "label": 0
          },
          {
            "text": "The object detectors are adapted to the total number of classes in the AU-AIR dataset (8 classes in total) by changing their last layers.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "AU_AIR_ A Multi_modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance",
        "section": 30,
        "paragraph_id": 30,
        "full_text": "To compare detection performances, we use mean average precision (mAP) that is a prominent metric in object detection[5],[6].It is the mean of the average precision (AP) values which compute the precision score for an object category at discretized recall values over 0 to 1[6].We consider 11 different recall values as in[6]and the intersection over union (IoU) threshold as 0.5.",
        "sentences": [
          {
            "text": "To compare detection performances, we use mean average precision (mAP) that is a prominent metric in object detection[5],[6].",
            "label": 0
          },
          {
            "text": "It is the mean of the average precision (AP) values which compute the precision score for an object category at discretized recall values over 0 to 1[6].",
            "label": 0
          },
          {
            "text": "We consider 11 different recall values as in[6]and the intersection over union (IoU) threshold as 0.5.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "AU_AIR_ A Multi_modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance",
        "section": 31,
        "paragraph_id": 31,
        "full_text": "For benchmarking, we train YOLOv3-Tiny and MobileNetv2-SSDLite with the AU-AIR Dataset.We use the batch size of 32 and Adam optimizer with the default parameters (alpha= 0.001, beta1=0.9,beta2=0.999).",
        "sentences": [
          {
            "text": "For benchmarking, we train YOLOv3-Tiny and MobileNetv2-SSDLite with the AU-AIR Dataset.",
            "label": 0
          },
          {
            "text": "We use the batch size of 32 and Adam optimizer with the default parameters (alpha= 0.001, beta1=0.9,beta2=0.999).",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "AU_AIR_ A Multi_modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance",
        "section": 32,
        "paragraph_id": 32,
        "full_text": "The training is stopped when the validation error starts to increase.Both networks are pre-trained on the COCO dataset.In order to see the effect of the training with an aerial dataset and a natural image dataset, we also use YOLOv3-Tiny and MobileNetv2-SSDLite trained on the COCO dataset without further training with the AU-AIR dataset.The results are given in TableIII.As shown in TableIII, the networks only trained on the COCO dataset have poor results.This is expected since the characteristics of natural images are significantly different from natural images.",
        "sentences": [
          {
            "text": "The training is stopped when the validation error starts to increase.",
            "label": 0
          },
          {
            "text": "Both networks are pre-trained on the COCO dataset.",
            "label": 0
          },
          {
            "text": "In order to see the effect of the training with an aerial dataset and a natural image dataset, we also use YOLOv3-Tiny and MobileNetv2-SSDLite trained on the COCO dataset without further training with the AU-AIR dataset.",
            "label": 0
          },
          {
            "text": "The results are given in TableIII.",
            "label": 0
          },
          {
            "text": "As shown in TableIII, the networks only trained on the COCO dataset have poor results.",
            "label": 0
          },
          {
            "text": "This is expected since the characteristics of natural images are significantly different from natural images.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "AU_AIR_ A Multi_modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance",
        "section": 33,
        "paragraph_id": 33,
        "full_text": "We observe that the AP values of motorbike and bicycle categories are significantly lower than the AP values of other categories.This fact might happen due to the class imbalance problem and the small object sizes of these categories.However, the bus category has the highest AP value, although there are fewer bus instances.This might result from the large size of bus instances in the frames.Furthermore, although the size of human instances is usually as small as the sizes of motorbike and bicycles, the AP values of the human category are relatively higher than these classes.This fact might be a consequence of the high number of human instances.There is no available AP values for the van and trailer categories in Table III since they do not exist in the COCO dataset.",
        "sentences": [
          {
            "text": "We observe that the AP values of motorbike and bicycle categories are significantly lower than the AP values of other categories.",
            "label": 0
          },
          {
            "text": "This fact might happen due to the class imbalance problem and the small object sizes of these categories.",
            "label": 0
          },
          {
            "text": "However, the bus category has the highest AP value, although there are fewer bus instances.",
            "label": 0
          },
          {
            "text": "This might result from the large size of bus instances in the frames.",
            "label": 0
          },
          {
            "text": "Furthermore, although the size of human instances is usually as small as the sizes of motorbike and bicycles, the AP values of the human category are relatively higher than these classes.",
            "label": 0
          },
          {
            "text": "This fact might be a consequence of the high number of human instances.",
            "label": 0
          },
          {
            "text": "There is no available AP values for the van and trailer categories in Table III since they do not exist in the COCO dataset.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "AU_AIR_ A Multi_modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance",
        "section": 34,
        "paragraph_id": 34,
        "full_text": "The baselines trained on the AU-AIR dataset are good at finding objects in aerial images that are captured at different altitudes and view angles.Qualitative results can be seen in Fig.8.",
        "sentences": [
          {
            "text": "The baselines trained on the AU-AIR dataset are good at finding objects in aerial images that are captured at different altitudes and view angles.",
            "label": 0
          },
          {
            "text": "Qualitative results can be seen in Fig.8.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "AU_AIR_ A Multi_modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance",
        "section": 35,
        "paragraph_id": 35,
        "full_text": "Among the baselines, YOLOv3-Tiny has higher AP values and mAP value compared to MobileNetv2-SSDLite.There is no significant difference between inference times (17.5 FPS and 17 FPS for YOLOv3-Tiny and MobileNetv2-SSDLite on TX2, respectively).",
        "sentences": [
          {
            "text": "Among the baselines, YOLOv3-Tiny has higher AP values and mAP value compared to MobileNetv2-SSDLite.",
            "label": 0
          },
          {
            "text": "There is no significant difference between inference times (17.5 FPS and 17 FPS for YOLOv3-Tiny and MobileNetv2-SSDLite on TX2, respectively).",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "AU_AIR_ A Multi_modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance",
        "section": 36,
        "paragraph_id": 36,
        "full_text": "Since the number of instances of each object category is imbalanced in the AU-AIR dataset (Fig.5), we consider several methods to solve the imbalanced class problem in the next version of the dataset.As a first step, we will try to collect more data to balance the number of instances.Besides, we may consider adding synthetic data (i.e., changing the brightness of images, translation, rotation) to increase the number of object categories which has a low number of samples in the current version.",
        "sentences": [
          {
            "text": "Since the number of instances of each object category is imbalanced in the AU-AIR dataset (Fig.5), we consider several methods to solve the imbalanced class problem in the next version of the dataset.",
            "label": 1
          },
          {
            "text": "As a first step, we will try to collect more data to balance the number of instances.",
            "label": 1
          },
          {
            "text": "Besides, we may consider adding synthetic data (i.e., changing the brightness of images, translation, rotation) to increase the number of object categories which has a low number of samples in the current version.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "AU_AIR_ A Multi_modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance",
        "section": 37,
        "paragraph_id": 37,
        "full_text": "We use AMT to annotate objects in images.Although three different people annotate one image and the annotations are manually checked by ourselves, there might be still overlooked samples that have weak annotations (e.g., unlabelled instances, loose bounding box drawings).Therefore, we consider using a three-step workflow proposed by Su et al.[29].In this workflow, the first worker draws a bounding box around an instance, the second worker verifies whether the bounding box is correctly drawn, and the third worker checks whether all object instances are annotated.",
        "sentences": [
          {
            "text": "We use AMT to annotate objects in images.",
            "label": 1
          },
          {
            "text": "Although three different people annotate one image and the annotations are manually checked by ourselves, there might be still overlooked samples that have weak annotations (e.g., unlabelled instances, loose bounding box drawings).",
            "label": 1
          },
          {
            "text": "Therefore, we consider using a three-step workflow proposed by Su et al.[29].",
            "label": 1
          },
          {
            "text": "In this workflow, the first worker draws a bounding box around an instance, the second worker verifies whether the bounding box is correctly drawn, and the third worker checks whether all object instances are annotated.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "AU_AIR_ A Multi_modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance",
        "section": 38,
        "paragraph_id": 38,
        "full_text": "Unlike other UAV object detection datasets, ours includes sensor data corresponding to each frame.In this work, we give a baseline only for object annotations and visual data.As future work, more baselines may be added to encourage research using sensor data (e.g., navigation and control of a UAV, object detection using multi-modal data).Also, we can add more visual sensors, such as multi-spectral cameras.",
        "sentences": [
          {
            "text": "Unlike other UAV object detection datasets, ours includes sensor data corresponding to each frame.",
            "label": 1
          },
          {
            "text": "In this work, we give a baseline only for object annotations and visual data.",
            "label": 1
          },
          {
            "text": "As future work, more baselines may be added to encourage research using sensor data (e.g., navigation and control of a UAV, object detection using multi-modal data).",
            "label": 1
          },
          {
            "text": "Also, we can add more visual sensors, such as multi-spectral cameras.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "AU_AIR_ A Multi_modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance",
        "section": 39,
        "paragraph_id": 39,
        "full_text": "We have used a ready-to-fly quadrotor (i.e., Parrot Bebop 2) to collect the whole dataset.We also consider collecting more samples from other platforms (e.g., different types of UAVs) using cameras that have different resolutions and frame rates.",
        "sentences": [
          {
            "text": "We have used a ready-to-fly quadrotor (i.e., Parrot Bebop 2) to collect the whole dataset.",
            "label": 1
          },
          {
            "text": "We also consider collecting more samples from other platforms (e.g., different types of UAVs) using cameras that have different resolutions and frame rates.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "AU_AIR_ A Multi_modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance",
        "section": 40,
        "paragraph_id": 40,
        "full_text": "In this dataset, traffic surveillance is the primary context.In future work, we consider increasing the number of environment contexts to increase diversity in the dataset.",
        "sentences": [
          {
            "text": "In this dataset, traffic surveillance is the primary context.",
            "label": 1
          },
          {
            "text": "In future work, we consider increasing the number of environment contexts to increase diversity in the dataset.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "AU_AIR_ A Multi_modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance",
        "section": 41,
        "paragraph_id": 41,
        "full_text": "In this work, we propose the AU-AIR dataset that is a multi-modal UAV dataset collected in an outdoor environment.Our aim is to fill the gap between computer vision and robotics having a diverse range of recorded data types for UAVs.Including visual data, object annotations, and flight data, it can be used for different research fields focused on data fusion.",
        "sentences": [
          {
            "text": "In this work, we propose the AU-AIR dataset that is a multi-modal UAV dataset collected in an outdoor environment.",
            "label": 1
          },
          {
            "text": "Our aim is to fill the gap between computer vision and robotics having a diverse range of recorded data types for UAVs.",
            "label": 1
          },
          {
            "text": "Including visual data, object annotations, and flight data, it can be used for different research fields focused on data fusion.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "AU_AIR_ A Multi_modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance",
        "section": 42,
        "paragraph_id": 42,
        "full_text": "We have emphasized the differences between natural images and aerial images affecting the object detection task.Moreover, since we consider real-time performance and applicability in real-world scenarios, we have created a baseline, including two mobile object detectors in the literature (i.e., YOLOv3-Tiny[18]and MobileNetv2-SSDLite[19]).",
        "sentences": [
          {
            "text": "We have emphasized the differences between natural images and aerial images affecting the object detection task.",
            "label": 1
          },
          {
            "text": "Moreover, since we consider real-time performance and applicability in real-world scenarios, we have created a baseline, including two mobile object detectors in the literature (i.e., YOLOv3-Tiny[18]and MobileNetv2-SSDLite[19]).",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "AU_AIR_ A Multi_modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance",
        "section": 43,
        "paragraph_id": 43,
        "full_text": "In our experiments, we showed that mobile networks trained on natural images have trouble in detecting objects in aerial images.",
        "sentences": [
          {
            "text": "In our experiments, we showed that mobile networks trained on natural images have trouble in detecting objects in aerial images.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "AV_NeRF_ Learning Neural Fields for Real_World Audio_Visual Scene Synthesis",
        "section": 0,
        "paragraph_id": 0,
        "full_text": "Abstract: Can machines recording an audio-visual scene produce realistic, matching audiovisual experiences at novel positions and novel view directions?We answer it by studying a new task-real-world audio-visual scene synthesis-and a first-of-itskind NeRF-based approach for multimodal learning.Concretely, given a video recording of an audio-visual scene, the task is to synthesize new videos with spatial audios along arbitrary novel camera trajectories in that scene.We propose an acoustic-aware audio generation module that integrates prior knowledge of audio propagation into NeRF, in which we implicitly associate audio generation with the 3D geometry and material properties of a visual environment.Furthermore, we present a coordinate transformation module that expresses a view direction relative to the sound source, enabling the model to learn sound source-centric acoustic fields.To facilitate the study of this new task, we collect a high-quality Real-World Audio-Visual Scene (RWAVS) dataset.We demonstrate the advantages of our method on this real-world dataset and the simulation-based SoundSpaces dataset.We recommend that readers visit our project page for convincing comparisons: https://liangsusan-git.github.io/project/avnerf/.",
        "sentences": [
          {
            "text": "Abstract: Can machines recording an audio-visual scene produce realistic, matching audiovisual experiences at novel positions and novel view directions?",
            "label": 0
          },
          {
            "text": "We answer it by studying a new task-real-world audio-visual scene synthesis-and a first-of-itskind NeRF-based approach for multimodal learning.",
            "label": 0
          },
          {
            "text": "Concretely, given a video recording of an audio-visual scene, the task is to synthesize new videos with spatial audios along arbitrary novel camera trajectories in that scene.",
            "label": 0
          },
          {
            "text": "We propose an acoustic-aware audio generation module that integrates prior knowledge of audio propagation into NeRF, in which we implicitly associate audio generation with the 3D geometry and material properties of a visual environment.",
            "label": 0
          },
          {
            "text": "Furthermore, we present a coordinate transformation module that expresses a view direction relative to the sound source, enabling the model to learn sound source-centric acoustic fields.",
            "label": 0
          },
          {
            "text": "To facilitate the study of this new task, we collect a high-quality Real-World Audio-Visual Scene (RWAVS) dataset.",
            "label": 1
          },
          {
            "text": "We demonstrate the advantages of our method on this real-world dataset and the simulation-based SoundSpaces dataset.",
            "label": 1
          },
          {
            "text": "We recommend that readers visit our project page for convincing comparisons: https://liangsusan-git.github.io/project/avnerf/.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "AV_NeRF_ Learning Neural Fields for Real_World Audio_Visual Scene Synthesis",
        "section": 1,
        "paragraph_id": 1,
        "full_text": "We study a new task, real-world audio-visual scene synthesis, to generate target videos and audios along novel camera trajectories from source audio-visual recordings of known trajectories.By learning from real-world source videos with binaural audio, we aim to generate target video frames and spatial audios that exhibit consistency with the given camera trajectory visually and acoustically.This consistency ensures perceptual realism and immersion, enriching the overall user experience.",
        "sentences": [
          {
            "text": "We study a new task, real-world audio-visual scene synthesis, to generate target videos and audios along novel camera trajectories from source audio-visual recordings of known trajectories.",
            "label": 0
          },
          {
            "text": "By learning from real-world source videos with binaural audio, we aim to generate target video frames and spatial audios that exhibit consistency with the given camera trajectory visually and acoustically.",
            "label": 0
          },
          {
            "text": "This consistency ensures perceptual realism and immersion, enriching the overall user experience.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "AV_NeRF_ Learning Neural Fields for Real_World Audio_Visual Scene Synthesis",
        "section": 2,
        "paragraph_id": 2,
        "full_text": "As far as we know, attempts in the audio-visual learning literature [1-11] have yet to succeed in solving this challenging task thus far.Although there are similar works[12][13][14][15], these methods have constraints that limit their ability to solve this new task.Luo et al. [12]  propose neural acoustic fields to model sound propagation in a room.Su et al. [13]  introduce representing audio scenes by disentangling the scene's geometry features.These methods are tailored for estimating room impulse response signals in a simulation environment that are difficult to obtain in a real-world scene.Concurrent to our work, ViGAS proposed by Chen et al. [15]  learns to synthesize new sounds by inferring the audio-visual cues.However, ViGAS is limited to a few viewpoints for audio generation.",
        "sentences": [
          {
            "text": "As far as we know, attempts in the audio-visual learning literature [1-11] have yet to succeed in solving this challenging task thus far.",
            "label": 0
          },
          {
            "text": "Although there are similar works[12][13][14][15], these methods have constraints that limit their ability to solve this new task.",
            "label": 0
          },
          {
            "text": "Luo et al. [12]  propose neural acoustic fields to model sound propagation in a room.",
            "label": 0
          },
          {
            "text": "Su et al. [13]  introduce representing audio scenes by disentangling the scene's geometry features.",
            "label": 0
          },
          {
            "text": "These methods are tailored for estimating room impulse response signals in a simulation environment that are difficult to obtain in a real-world scene.",
            "label": 0
          },
          {
            "text": "Concurrent to our work, ViGAS proposed by Chen et al. [15]  learns to synthesize new sounds by inferring the audio-visual cues.",
            "label": 0
          },
          {
            "text": "However, ViGAS is limited to a few viewpoints for audio generation.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "AV_NeRF_ Learning Neural Fields for Real_World Audio_Visual Scene Synthesis",
        "section": 3,
        "paragraph_id": 3,
        "full_text": "We introduce AV-NeRF, a novel NeRF-based method of synthesizing real-world audio-visual scenes.AV-NeRF enables the generation of videos and spatial audios, following arbitrary camera trajectories.It utilizes source videos and camera poses as references.AV-NeRF consists of two branches: A-NeRF, which learns the acoustic fields of an environment, and V-NeRF, which models color and density fields.We represent a static audio field as a continuous function using A-NeRF, which takes the listener's position and head direction as input.A-NeRF effectively models the energy decay of sound as the sound travels from the source to the listener by correlating the listener's position with the 37th Conference on Neural Information Processing Systems (NeurIPS 2023).",
        "sentences": [
          {
            "text": "We introduce AV-NeRF, a novel NeRF-based method of synthesizing real-world audio-visual scenes.",
            "label": 0
          },
          {
            "text": "AV-NeRF enables the generation of videos and spatial audios, following arbitrary camera trajectories.",
            "label": 0
          },
          {
            "text": "It utilizes source videos and camera poses as references.",
            "label": 0
          },
          {
            "text": "AV-NeRF consists of two branches: A-NeRF, which learns the acoustic fields of an environment, and V-NeRF, which models color and density fields.",
            "label": 0
          },
          {
            "text": "We represent a static audio field as a continuous function using A-NeRF, which takes the listener's position and head direction as input.",
            "label": 0
          },
          {
            "text": "A-NeRF effectively models the energy decay of sound as the sound travels from the source to the listener by correlating the listener's position with the 37th Conference on Neural Information Processing Systems (NeurIPS 2023).",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "AV_NeRF_ Learning Neural Fields for Real_World Audio_Visual Scene Synthesis",
        "section": 4,
        "paragraph_id": 4,
        "full_text": "To the best of our knowledge, our method is the first NeRF-based system capable of synthesizing realworld videos with perceptually realistic binaural audios at arbitrary poses.However, existing datasets do not meet the specific requirements of our experiments, particularly in terms of simultaneously providing camera poses, high-quality binaural audios, and images.Therefore, we curated a highquality audio-visual scene dataset (real) to address this gap and facilitate further research on this problem.Additionally, we utilize (synthetic) SoundSpaces dataset[4]to validate our method.",
        "sentences": [
          {
            "text": "To the best of our knowledge, our method is the first NeRF-based system capable of synthesizing realworld videos with perceptually realistic binaural audios at arbitrary poses.",
            "label": 0
          },
          {
            "text": "However, existing datasets do not meet the specific requirements of our experiments, particularly in terms of simultaneously providing camera poses, high-quality binaural audios, and images.",
            "label": 0
          },
          {
            "text": "Therefore, we curated a highquality audio-visual scene dataset (real) to address this gap and facilitate further research on this problem.",
            "label": 1
          },
          {
            "text": "Additionally, we utilize (synthetic) SoundSpaces dataset[4]to validate our method.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "AV_NeRF_ Learning Neural Fields for Real_World Audio_Visual Scene Synthesis",
        "section": 5,
        "paragraph_id": 5,
        "full_text": "(1) RWAVS Dataset.We collected the Real-World Audio-Visual Scene (RWAVS) dataset to benchmark our method.In order to increase the diversity of our dataset, we recorded data across different scenarios.Fig.5shows the example scenarios we used for data recording, including both indoor and outdoor environments, which we believe represent most daily settings.RWAVS dataset comprises multimodal data, including camera poses, high-quality binaural audios, and videos.Unlike Replay-NVAS dataset[15], where the environment and the recording viewpoint are constant, RWAVS dataset contains various viewpoints in diverse environments.During data recording, we randomly moved around the environment while holding the device, capturing various acoustic and visual signals. RWAVS dataset encompasses all positions and directions (360 • ) within an environment.",
        "sentences": [
          {
            "text": "(1) RWAVS Dataset.",
            "label": 1
          },
          {
            "text": "We collected the Real-World Audio-Visual Scene (RWAVS) dataset to benchmark our method.",
            "label": 1
          },
          {
            "text": "In order to increase the diversity of our dataset, we recorded data across different scenarios.",
            "label": 1
          },
          {
            "text": "Fig.5shows the example scenarios we used for data recording, including both indoor and outdoor environments, which we believe represent most daily settings.",
            "label": 1
          },
          {
            "text": "RWAVS dataset comprises multimodal data, including camera poses, high-quality binaural audios, and videos.",
            "label": 1
          },
          {
            "text": "Unlike Replay-NVAS dataset[15], where the environment and the recording viewpoint are constant, RWAVS dataset contains various viewpoints in diverse environments.",
            "label": 1
          },
          {
            "text": "During data recording, we randomly moved around the environment while holding the device, capturing various acoustic and visual signals.",
            "label": 1
          },
          {
            "text": "RWAVS dataset encompasses all positions and directions (360 • ) within an environment.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "AV_NeRF_ Learning Neural Fields for Real_World Audio_Visual Scene Synthesis",
        "section": 6,
        "paragraph_id": 6,
        "full_text": "In detail, we employed a 3Dio Free Space XLR binaural microphone for capturing high-quality stereo audio, a TASCAM DR-60DMKII for recording and storing audio, and a GoPro Max for capturing accompanying videos.Additionally, an LG XBOOM 360 omnidirectional speaker was used as the sound source.For each environment and sound source combination, we collected data ranging from 10 to 25 minutes, resulting in a total collection of 232 minutes (3.8 hours) of data from diverse environments with varying source positions.",
        "sentences": [
          {
            "text": "In detail, we employed a 3Dio Free Space XLR binaural microphone for capturing high-quality stereo audio, a TASCAM DR-60DMKII for recording and storing audio, and a GoPro Max for capturing accompanying videos.",
            "label": 1
          },
          {
            "text": "Additionally, an LG XBOOM 360 omnidirectional speaker was used as the sound source.",
            "label": 1
          },
          {
            "text": "For each environment and sound source combination, we collected data ranging from 10 to 25 minutes, resulting in a total collection of 232 minutes (3.8 hours) of data from diverse environments with varying source positions.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "AV_NeRF_ Learning Neural Fields for Real_World Audio_Visual Scene Synthesis",
        "section": 7,
        "paragraph_id": 7,
        "full_text": "We extract key frames at 1 fps from recorded videos and use COLMAP[46]to estimate the corresponding camera pose.Each key frame is accompanied by one-second binaural audio and one-second source audio, forming a complete data sample.For audio clips with noticeable background noise, we perform noise suppression using Adobe Audition[47].We split 80% data as training samples and the rest as validation samples.After pre-processing, we obtain 9850 and 2469 samples for training and validation, respectively.This dataset is challenging because of the diverse environments and various camera poses.We will release this dataset to the research community.",
        "sentences": [
          {
            "text": "We extract key frames at 1 fps from recorded videos and use COLMAP[46]to estimate the corresponding camera pose.",
            "label": 1
          },
          {
            "text": "Each key frame is accompanied by one-second binaural audio and one-second source audio, forming a complete data sample.",
            "label": 1
          },
          {
            "text": "For audio clips with noticeable background noise, we perform noise suppression using Adobe Audition[47].",
            "label": 1
          },
          {
            "text": "We split 80% data as training samples and the rest as validation samples.",
            "label": 1
          },
          {
            "text": "After pre-processing, we obtain 9850 and 2469 samples for training and validation, respectively.",
            "label": 1
          },
          {
            "text": "This dataset is challenging because of the diverse environments and various camera poses.",
            "label": 1
          },
          {
            "text": "We will release this dataset to the research community.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "AV_NeRF_ Learning Neural Fields for Real_World Audio_Visual Scene Synthesis",
        "section": 8,
        "paragraph_id": 8,
        "full_text": "(2) SoundSpaces Dataset.While RWAVS offers realistic training samples, its realism restricts its scale because it is time-consuming to record high-quality multimodal data in the real world.Therefore, we use the synthetic SoundSpaces dataset to augment our experiments.To evaluate our method on SoundSpaces dataset, we modify AV-NeRF to estimate impulse responses instead of the acoustic mask while keeping all other components intact.We follow NAF[12]selecting six representative indoor scenes, consisting of two single rooms with rectangular walls, two single rooms with non-rectangular walls, and two multi-room layouts.In each scene, SoundSpaces dataset provides an extensive collection of impulse response signals for sound source and sound receiver pairs, which are densely sampled from a 2D room grid.Each pair includes four discrete head orientations (0 • , 90 • , 180 • , and 270 • ), and each orientation is associated with two-channel binaural RIRs.We render RGB and depth images for each sound receiver pose using Habitat-Sim simulator[48,49].We maintain the same training/test split as NAF, allocating 90% data for training and 10% data for testing.",
        "sentences": [
          {
            "text": "(2) SoundSpaces Dataset.",
            "label": 1
          },
          {
            "text": "While RWAVS offers realistic training samples, its realism restricts its scale because it is time-consuming to record high-quality multimodal data in the real world.",
            "label": 1
          },
          {
            "text": "Therefore, we use the synthetic SoundSpaces dataset to augment our experiments.",
            "label": 1
          },
          {
            "text": "To evaluate our method on SoundSpaces dataset, we modify AV-NeRF to estimate impulse responses instead of the acoustic mask while keeping all other components intact.",
            "label": 1
          },
          {
            "text": "We follow NAF[12]selecting six representative indoor scenes, consisting of two single rooms with rectangular walls, two single rooms with non-rectangular walls, and two multi-room layouts.",
            "label": 1
          },
          {
            "text": "In each scene, SoundSpaces dataset provides an extensive collection of impulse response signals for sound source and sound receiver pairs, which are densely sampled from a 2D room grid.",
            "label": 1
          },
          {
            "text": "Each pair includes four discrete head orientations (0 • , 90 • , 180 • , and 270 • ), and each orientation is associated with two-channel binaural RIRs.",
            "label": 1
          },
          {
            "text": "We render RGB and depth images for each sound receiver pose using Habitat-Sim simulator[48,49].",
            "label": 1
          },
          {
            "text": "We maintain the same training/test split as NAF, allocating 90% data for training and 10% data for testing.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "AV_NeRF_ Learning Neural Fields for Real_World Audio_Visual Scene Synthesis",
        "section": 9,
        "paragraph_id": 9,
        "full_text": "Comparison with State-of-the-art.We compare AV-NeRF with the following baselines: (1) Mono-Mono duplicates the source audio a s twice to generate a fake binaural audio without modifying the source audio; (2) Mono-Energy assumes that the average energy of the target audio a t is known, scales the energy of the input audio to match the target, and duplicates the scaled audio to generate a stereo audio; (3) Stereo-Energy assumes that the energy of the two channels of the target audio a t is known, separately scales the energy of the input audio to match the target, and combines the two scaled channels to generate a stereo audio; (4) IRNAS[13]learns representing audio scenes by disentangling scene's geometry features with implicit neural fields, and we adapt INRAS to predict wave masks on RWAVS dataset; (5) NAF[12]designs local feature grids and an implicit decoder to capture the sound propagation in a physical scene, and we modify NAF to predict magnitude masks on RWAVS dataset; (6) ViGAS[15]achieves novel-view acoustic synthesis by analyzing audio-visual cues from source viewpoints.We select magnitude distance (MAG)[29], which measures the audio quality in the time-frequency domain, and envelope distance (ENV)[30], which measures the audio quality in the time domain, to evaluate various methods.Please refer to the supplementary material for implementation details.AV-NeRF outperforms all baselines across different environments, including office, house, apartment, and outdoors, by a significant margin (Table1(2) adding visual information to the input of A-NeRF is the most effective multimodal fusion method compared with concatenation and adding visual information to all layers of A-NeRF (Table2middle); (3) using embeddings represent relative angles outperforms applying positional encoding to either absolute or relative angles (Table2right).\"Absolute Direction\" represents applying positional encoding to the absolute angle, \"Relative Direction\" means transforming the relative angle with the positional encoding, and \"Relative Embedding\" is the embedding method.",
        "sentences": [
          {
            "text": "Comparison with State-of-the-art.",
            "label": 0
          },
          {
            "text": "We compare AV-NeRF with the following baselines: (1) Mono-Mono duplicates the source audio a s twice to generate a fake binaural audio without modifying the source audio; (2) Mono-Energy assumes that the average energy of the target audio a t is known, scales the energy of the input audio to match the target, and duplicates the scaled audio to generate a stereo audio; (3) Stereo-Energy assumes that the energy of the two channels of the target audio a t is known, separately scales the energy of the input audio to match the target, and combines the two scaled channels to generate a stereo audio; (4) IRNAS[13]learns representing audio scenes by disentangling scene's geometry features with implicit neural fields, and we adapt INRAS to predict wave masks on RWAVS dataset; (5) NAF[12]designs local feature grids and an implicit decoder to capture the sound propagation in a physical scene, and we modify NAF to predict magnitude masks on RWAVS dataset; (6) ViGAS[15]achieves novel-view acoustic synthesis by analyzing audio-visual cues from source viewpoints.",
            "label": 0
          },
          {
            "text": "We select magnitude distance (MAG)[29], which measures the audio quality in the time-frequency domain, and envelope distance (ENV)[30], which measures the audio quality in the time domain, to evaluate various methods.",
            "label": 0
          },
          {
            "text": "Please refer to the supplementary material for implementation details.",
            "label": 0
          },
          {
            "text": "AV-NeRF outperforms all baselines across different environments, including office, house, apartment, and outdoors, by a significant margin (Table1(2) adding visual information to the input of A-NeRF is the most effective multimodal fusion method compared with concatenation and adding visual information to all layers of A-NeRF (Table2middle); (3) using embeddings represent relative angles outperforms applying positional encoding to either absolute or relative angles (Table2right).",
            "label": 0
          },
          {
            "text": "\"Absolute Direction\" represents applying positional encoding to the absolute angle, \"Relative Direction\" means transforming the relative angle with the positional encoding, and \"Relative Embedding\" is the embedding method.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "AV_NeRF_ Learning Neural Fields for Real_World Audio_Visual Scene Synthesis",
        "section": 10,
        "paragraph_id": 10,
        "full_text": "Visualization.We visualize the synthesized audio-visual scenes in Fig.6to intuitively assess the generation quality of our model.AV-NeRF can synthesize realistic binaural audios that have the same signal envelope and channel difference as the ground-truth audios.We compare AV-NeRF with traditional audio coding methods[50,51]and advanced learningbased neural field methods[12,13]using T60, C50, and EDT metrics[13].Please refer to our supplementary material for implementation details.Table3shows that AV-NeRF outruns both traditional and advanced methods, achieving 21% relative improvement on T60 metric compared with the previous state-of-the-art method INRAS, 5% on C50, and 16% on EDT.",
        "sentences": [
          {
            "text": "Visualization.",
            "label": 0
          },
          {
            "text": "We visualize the synthesized audio-visual scenes in Fig.6to intuitively assess the generation quality of our model.",
            "label": 0
          },
          {
            "text": "AV-NeRF can synthesize realistic binaural audios that have the same signal envelope and channel difference as the ground-truth audios.",
            "label": 0
          },
          {
            "text": "We compare AV-NeRF with traditional audio coding methods[50,51]and advanced learningbased neural field methods[12,13]using T60, C50, and EDT metrics[13].",
            "label": 0
          },
          {
            "text": "Please refer to our supplementary material for implementation details.",
            "label": 0
          },
          {
            "text": "Table3shows that AV-NeRF outruns both traditional and advanced methods, achieving 21% relative improvement on T60 metric compared with the previous state-of-the-art method INRAS, 5% on C50, and 16% on EDT.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment",
        "section": 0,
        "paragraph_id": 0,
        "full_text": "Abstract: Banding, also known as staircase-like contours, frequently occurs in flat areas of images/videos processed by the compression or quantization algorithms.As undesirable artifacts, banding destroys the original image structure, thus inevitably degrading users' quality of experience (QoE).In this paper, we systematically investigate the banding image quality assessment (IQA) problem, aiming to detect the image banding artifacts and evaluate their perceptual visual quality.Considering that the existing image banding databases only contain limited content sources and banding generation methods, and lack perceptual quality labels (i.e.mean opinion scores), we first build the largest banding IQA database so far, named Banding Artifact Noticeable Database (BAND-2k), which consists of 2,000 banding images generated by 15 compression and quantization schemes.A total of 23 workers participated in the subjective IQA experiment, yielding over 214,000 patch-level banding class labels and 44,371 reliable image-level quality rating scores.Subsequently, we develop an effective no-reference (NR) banding evaluator for banding detection and quality assessment by leveraging frequency characteristics of banding artifacts.To be more specific, a dual convolutional neural network (CNN) is employed to concurrently learn the feature representation from the high-frequency and low-frequency maps, thereby enhancing the ability to discern banding artifacts.The quality score of a banding image is generated by pooling the banding detection maps masked by the spatial frequency filters.The experimental results demonstrate that our banding evaluator achieves a remarkably high accuracy in banding detection and also exhibits high SRCC and PLCC results with the perceptual quality labels, even without directly learning a regression model for banding quality evaluation.These findings unveil the strong correlations between the intensity of banding artifacts and the perceptual visual quality, thus validating the necessity of banding quality assessment.The BAND-2k database and the proposed banding evaluator will be available at https://github.com/zijianchen98/BAND-2k.",
        "sentences": [
          {
            "text": "Abstract: Banding, also known as staircase-like contours, frequently occurs in flat areas of images/videos processed by the compression or quantization algorithms.",
            "label": 0
          },
          {
            "text": "As undesirable artifacts, banding destroys the original image structure, thus inevitably degrading users' quality of experience (QoE).",
            "label": 0
          },
          {
            "text": "In this paper, we systematically investigate the banding image quality assessment (IQA) problem, aiming to detect the image banding artifacts and evaluate their perceptual visual quality.",
            "label": 0
          },
          {
            "text": "Considering that the existing image banding databases only contain limited content sources and banding generation methods, and lack perceptual quality labels (i.e.mean opinion scores), we first build the largest banding IQA database so far, named Banding Artifact Noticeable Database (BAND-2k), which consists of 2,000 banding images generated by 15 compression and quantization schemes.",
            "label": 1
          },
          {
            "text": "A total of 23 workers participated in the subjective IQA experiment, yielding over 214,000 patch-level banding class labels and 44,371 reliable image-level quality rating scores.",
            "label": 1
          },
          {
            "text": "Subsequently, we develop an effective no-reference (NR) banding evaluator for banding detection and quality assessment by leveraging frequency characteristics of banding artifacts.",
            "label": 0
          },
          {
            "text": "To be more specific, a dual convolutional neural network (CNN) is employed to concurrently learn the feature representation from the high-frequency and low-frequency maps, thereby enhancing the ability to discern banding artifacts.",
            "label": 0
          },
          {
            "text": "The quality score of a banding image is generated by pooling the banding detection maps masked by the spatial frequency filters.",
            "label": 0
          },
          {
            "text": "The experimental results demonstrate that our banding evaluator achieves a remarkably high accuracy in banding detection and also exhibits high SRCC and PLCC results with the perceptual quality labels, even without directly learning a regression model for banding quality evaluation.",
            "label": 0
          },
          {
            "text": "These findings unveil the strong correlations between the intensity of banding artifacts and the perceptual visual quality, thus validating the necessity of banding quality assessment.",
            "label": 0
          },
          {
            "text": "The BAND-2k database and the proposed banding evaluator will be available at https://github.com/zijianchen98/BAND-2k.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment",
        "section": 1,
        "paragraph_id": 1,
        "full_text": "R ECENT years have witnessed a rapid emergence of media streaming services and social platforms.YouTube, Netflix, and TikTok account for more than half of the world's video traffic.Improving the quality of images under limited encoding, Zijian Chen, Wei Sun, Jun Jia, Zicheng Zhang, Xiongkuo Min, and Guangtao Zhai are with the Institute of Image Communication and Information Processing, Shanghai Jiao Tong University, Shanghai 200240, China (e-mail: {zijian.chen,sunguwei, jiajun0302, zzc1998, minxiongkuo, zhaiguangtao}@sjtu.edu.cn).",
        "sentences": [
          {
            "text": "R ECENT years have witnessed a rapid emergence of media streaming services and social platforms.",
            "label": 0
          },
          {
            "text": "YouTube, Netflix, and TikTok account for more than half of the world's video traffic.",
            "label": 0
          },
          {
            "text": "Improving the quality of images under limited encoding, Zijian Chen, Wei Sun, Jun Jia, Zicheng Zhang, Xiongkuo Min, and Guangtao Zhai are with the Institute of Image Communication and Information Processing, Shanghai Jiao Tong University, Shanghai 200240, China (e-mail: {zijian.chen,sunguwei, jiajun0302, zzc1998, minxiongkuo, zhaiguangtao}@sjtu.edu.cn).",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment",
        "section": 2,
        "paragraph_id": 2,
        "full_text": "Fangfang Lu is with the College of Computer Science and Technology, Shanghai University of Electric Power, Shanghai 200290, China (email:lufangfang@shiep.edu.cn).",
        "sentences": [
          {
            "text": "Fangfang Lu is with the College of Computer Science and Technology, Shanghai University of Electric Power, Shanghai 200290, China (email:lufangfang@shiep.edu.cn).",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment",
        "section": 3,
        "paragraph_id": 3,
        "full_text": "Jing Liu is with the School of Electrical and Information Engineering, Tianjin University, Tianjin, China.(e-mail:jliu tju@tju.edu.cn).",
        "sentences": [
          {
            "text": "Jing Liu is with the School of Electrical and Information Engineering, Tianjin University, Tianjin, China.(e-mail:jliu tju@tju.edu.cn).",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment",
        "section": 4,
        "paragraph_id": 4,
        "full_text": "Ru Huang is with the School of Information Science & Engineering, East China University of Science and Technology, Shanghai 200237, China (email:huangrabbit@ecust.edu.cn).transmission bandwidth, and storage condition is a necessary prerequisite for meeting the quality of experience (QoE) of users.In the stages between image acquisition and display, an image may suffer from various types of degradation, while banding artifacts are a kind of false contour distortion that is quite perceptible to the human eye.Since the visual quality of image contents greatly affects the QoE of end-users, it is highly desirable to design an effective banding image quality assessment (IQA) method, which aims to automatically detect the traces of such false contours and predict the objective quality of banding images that can be used to develop pre-processing or postprocessing debanding algorithms and optimize the performance of streaming media application.",
        "sentences": [
          {
            "text": "transmission bandwidth, and storage condition is a necessary prerequisite for meeting the quality of experience (QoE) of users.",
            "label": 0
          },
          {
            "text": "In the stages between image acquisition and display, an image may suffer from various types of degradation, while banding artifacts are a kind of false contour distortion that is quite perceptible to the human eye.",
            "label": 0
          },
          {
            "text": "Since the visual quality of image contents greatly affects the QoE of end-users, it is highly desirable to design an effective banding image quality assessment (IQA) method, which aims to automatically detect the traces of such false contours and predict the objective quality of banding images that can be used to develop pre-processing or postprocessing debanding algorithms and optimize the performance of streaming media application.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment",
        "section": 5,
        "paragraph_id": 5,
        "full_text": "Normally, banding artifacts take on the appearance of annual rings, radiation circles, halos, or geographical contour lines and especially exist in the background regions (e.g., sky, water, and wall surface), where the color transition is not smooth enough.Nearly all existing image or video encoders, including H.264/AVC[28], VP9[38], and H.265/HEVC[27]can introduce such artifacts more or less.Current banding IQA research can be divided into two categories: subjective quality assessment and objective quality assessment.The existing subjective banding IQA research[39],[41]-[43]mainly investigates the limited banding scenarios with internal-used and undisclosed databases while lacking the quality label and may be insufficiently generalizable to large-scale commercial applications.Meanwhile, general IQA methods aiming at common distortions are inapplicable for banding exacerbated images due to the essential differences between them.First, banding artifacts usually hold tiny, staircaselike, and regional structures, which can be regarded as a kind of high-frequency artifact in smooth areas, while general distortions occur obviously in the whole image and are globally uniform.Second, the perceptual severity of banding is quantified based on its fraction of coverage and intensity in an image, which is widely divergent from the design philosophy of many existing IQA approaches.As a result, it is challenging to design an effective banding IQA method.",
        "sentences": [
          {
            "text": "Normally, banding artifacts take on the appearance of annual rings, radiation circles, halos, or geographical contour lines and especially exist in the background regions (e.g., sky, water, and wall surface), where the color transition is not smooth enough.",
            "label": 0
          },
          {
            "text": "Nearly all existing image or video encoders, including H.264/AVC[28], VP9[38], and H.265/HEVC[27]can introduce such artifacts more or less.",
            "label": 0
          },
          {
            "text": "Current banding IQA research can be divided into two categories: subjective quality assessment and objective quality assessment.",
            "label": 0
          },
          {
            "text": "The existing subjective banding IQA research[39],[41]-[43]mainly investigates the limited banding scenarios with internal-used and undisclosed databases while lacking the quality label and may be insufficiently generalizable to large-scale commercial applications.",
            "label": 0
          },
          {
            "text": "Meanwhile, general IQA methods aiming at common distortions are inapplicable for banding exacerbated images due to the essential differences between them.",
            "label": 0
          },
          {
            "text": "First, banding artifacts usually hold tiny, staircaselike, and regional structures, which can be regarded as a kind of high-frequency artifact in smooth areas, while general distortions occur obviously in the whole image and are globally uniform.",
            "label": 0
          },
          {
            "text": "Second, the perceptual severity of banding is quantified based on its fraction of coverage and intensity in an image, which is widely divergent from the design philosophy of many existing IQA approaches.",
            "label": 0
          },
          {
            "text": "As a result, it is challenging to design an effective banding IQA method.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment",
        "section": 6,
        "paragraph_id": 6,
        "full_text": "To address these limitations, we first conduct a comprehensive subjective study of banding exacerbated images and create the largest banding IQA database to date with reliable mean opinion scores (MOS) and patch-level banding labels.We also propose a novel no-reference banding evaluator for banding detection and quality assessment by leveraging the frequency characteristics of banding artifacts.First, due to the dissimilar peculiarities of banding and smooth regions, the same distortion in different regions, e.g., textual and pictorial regions, may lead to different visual perception of human beings.Considering that banding manifests as a high-frequency artifact that exists in the lowfrequency smooth region, we propose a dual-branch CNN, which takes the high-frequency map and low-frequency map as inputs simultaneously, to hierarchically incorporate different visual features from the first layer and the last layer of our Resnet-50 backbone, thus making the model learn more effective banding feature representation and achieving more accurate banding region discrimination.Furthermore, inspired by the previous studies[15],[16], spatial frequency extracts information consistent with the human visual system (HVS), which not only reflects the overall active level in an image but also intuitively quantifies the contrast information.In other words, the value of spatial frequency is large in smooth areas, while becoming small in areas with harsh contrast changes, i.e., banding areas.Based on this mechanism, we adopt a spatial frequency masking strategy to refine the detected banding map and then pool the masked banding detection map to obtain the image-level banding quality score.In summary, this paper makes the following contributions:I), BAND-2k is times larger than them[39],[41]in terms of the number of contents and includes more comprehensive compression means while providing patch-level banding labels for training deep learning models.• We propose a novel no-reference banding evaluator for banding detection and quality assessment based on frequency characteristics of banding artifacts, which utilizes a dual-branch CNN model to extract hierarchical banding-related feature representation from the high-frequency maps and low-frequency maps simultaneously.A spatial frequency masking strategy is introduced to refine the visibility of banding contours, and then combine with the detected banding map to generate subjectively consistent banding quality scores.• Experimental results show that the proposed banding evaluator achieves the best performance in banding detection and significantly surpasses baselines in terms of SRCC and PLCC in the banding IQA task, which demonstrates the effectiveness of the proposed model.The remainder of this paper is organized as follows.Section II provides an overview of related works, including the stateof-the-art banding databases and detection methods.Section III introduces the construction of the BAND-2k database and the subjective assessment study.Section IV proposes an effective no-reference banding evaluator for banding detection and quality assessment.Section V gives the experimental results and analysis.Section VI concludes this paper.",
        "sentences": [
          {
            "text": "To address these limitations, we first conduct a comprehensive subjective study of banding exacerbated images and create the largest banding IQA database to date with reliable mean opinion scores (MOS) and patch-level banding labels.",
            "label": 1
          },
          {
            "text": "We also propose a novel no-reference banding evaluator for banding detection and quality assessment by leveraging the frequency characteristics of banding artifacts.",
            "label": 0
          },
          {
            "text": "First, due to the dissimilar peculiarities of banding and smooth regions, the same distortion in different regions, e.g., textual and pictorial regions, may lead to different visual perception of human beings.",
            "label": 0
          },
          {
            "text": "Considering that banding manifests as a high-frequency artifact that exists in the lowfrequency smooth region, we propose a dual-branch CNN, which takes the high-frequency map and low-frequency map as inputs simultaneously, to hierarchically incorporate different visual features from the first layer and the last layer of our Resnet-50 backbone, thus making the model learn more effective banding feature representation and achieving more accurate banding region discrimination.",
            "label": 0
          },
          {
            "text": "Furthermore, inspired by the previous studies[15],[16], spatial frequency extracts information consistent with the human visual system (HVS), which not only reflects the overall active level in an image but also intuitively quantifies the contrast information.",
            "label": 0
          },
          {
            "text": "In other words, the value of spatial frequency is large in smooth areas, while becoming small in areas with harsh contrast changes, i.e., banding areas.",
            "label": 0
          },
          {
            "text": "Based on this mechanism, we adopt a spatial frequency masking strategy to refine the detected banding map and then pool the masked banding detection map to obtain the image-level banding quality score.",
            "label": 0
          },
          {
            "text": "• We propose a novel no-reference banding evaluator for banding detection and quality assessment based on frequency characteristics of banding artifacts, which utilizes a dual-branch CNN model to extract hierarchical banding-related feature representation from the high-frequency maps and low-frequency maps simultaneously.",
            "label": 0
          },
          {
            "text": "• Experimental results show that the proposed banding evaluator achieves the best performance in banding detection and significantly surpasses baselines in terms of SRCC and PLCC in the banding IQA task, which demonstrates the effectiveness of the proposed model.",
            "label": 0
          },
          {
            "text": "The remainder of this paper is organized as follows.",
            "label": 0
          },
          {
            "text": "Section II provides an overview of related works, including the stateof-the-art banding databases and detection methods.",
            "label": 0
          },
          {
            "text": "Section III introduces the construction of the BAND-2k database and the subjective assessment study.",
            "label": 0
          },
          {
            "text": "Section IV proposes an effective no-reference banding evaluator for banding detection and quality assessment.",
            "label": 1
          },
          {
            "text": "Section V gives the experimental results and analysis.",
            "label": 0
          },
          {
            "text": "Section VI concludes this paper.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment",
        "section": 7,
        "paragraph_id": 7,
        "full_text": "In this section, we first provide an overview of the state-ofthe-art banding-related databases (TableI) and then review the banding detection and evaluation methods.",
        "sentences": [
          {
            "text": "In this section, we first provide an overview of the state-ofthe-art banding-related databases (TableI) and then review the banding detection and evaluation methods.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment",
        "section": 8,
        "paragraph_id": 8,
        "full_text": "The first banding artifact-relevant VQA database was proposed by Wang et al.[39], which consists of 21 stimuli with different quantization grades generated by VP9[38]from 7 clips of 1280 × 720 30fps video.Authors in[41]investigated the effect of encoding parameters and dithering on the visibility of banding.Nine 4k-10bit source clips from the existing Netflix catalogue between 1 and 5 seconds were used to generate banding distorted videos.Each source content was downsampled to appropriate resolutions (1080p, 2k, or 4k) with certain bit-depth and further compressed by libaom (an AV1 codec library) at QPs {12, 20, 32}.More recently, Kapoor et al.[43]constructed one of the first databases for data-driven image banding assessment models.This research included about 1,440 images shot from over 600 pristine HD videos with a resolution of 1920 × 1080.Six levels of bit-depth quantization in luminance and chrominance channels are introduced to obtain different intensities of banding.Meanwhile, the banding images were semi-automatically segmented and labeled into banded and non-banded to form a patch-level banding dataset, which allows for training machine learning-based and deep learning-based banding classification methods.However, to the best of our knowledge, thus far there still lacking a benchmarking dataset in the banding detection and the corresponding banding IQA domain.Researchers either resort to image/video quality datasets that do not aim at banding distortion or build a small, attribute-restricted, in-house dataset by themselves.This motivates us to construct a large-scale subjective assessment database focus on the perceived bandingaffected image quality.",
        "sentences": [
          {
            "text": "The first banding artifact-relevant VQA database was proposed by Wang et al.[39], which consists of 21 stimuli with different quantization grades generated by VP9[38]from 7 clips of 1280 × 720 30fps video.",
            "label": 0
          },
          {
            "text": "Authors in[41]investigated the effect of encoding parameters and dithering on the visibility of banding.",
            "label": 0
          },
          {
            "text": "Nine 4k-10bit source clips from the existing Netflix catalogue between 1 and 5 seconds were used to generate banding distorted videos.",
            "label": 0
          },
          {
            "text": "Each source content was downsampled to appropriate resolutions (1080p, 2k, or 4k) with certain bit-depth and further compressed by libaom (an AV1 codec library) at QPs {12, 20, 32}.",
            "label": 0
          },
          {
            "text": "More recently, Kapoor et al.[43]constructed one of the first databases for data-driven image banding assessment models.",
            "label": 0
          },
          {
            "text": "This research included about 1,440 images shot from over 600 pristine HD videos with a resolution of 1920 × 1080.",
            "label": 0
          },
          {
            "text": "Six levels of bit-depth quantization in luminance and chrominance channels are introduced to obtain different intensities of banding.",
            "label": 0
          },
          {
            "text": "Meanwhile, the banding images were semi-automatically segmented and labeled into banded and non-banded to form a patch-level banding dataset, which allows for training machine learning-based and deep learning-based banding classification methods.",
            "label": 0
          },
          {
            "text": "However, to the best of our knowledge, thus far there still lacking a benchmarking dataset in the banding detection and the corresponding banding IQA domain.",
            "label": 0
          },
          {
            "text": "Researchers either resort to image/video quality datasets that do not aim at banding distortion or build a small, attribute-restricted, in-house dataset by themselves.",
            "label": 0
          },
          {
            "text": "This motivates us to construct a large-scale subjective assessment database focus on the perceived bandingaffected image quality.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment",
        "section": 9,
        "paragraph_id": 9,
        "full_text": "Early research on banding detection mainly focuses on false contour identification, which aims to find the wrong boundary rather than a \"true\" region edge in the image.Authors in[31]-[33]utilized monotonicity or non-monotonicity features of local support regions including the gradient, contrast, variance, and entropy information to measure the loss of low-amplitude detail caused by banding.However, these works ignored the perceptual characteristics of the human visual system (HVS) and thus did not perform a good correlation with subjective tests.Another banding detection strategy is conducted at the pixel-level estimation and segmentation.Bhagavathy et al.[30]proposed to identify banding artifacts by calculating the likelihood of pixel difference.Baugh et al.[34]measured the severity of banding based on the number of a group of connected pixels with the same color.Wang et al.[39]first detected uniform segments to find possible banding areas and further incorporated edge features (e.g.length and contrast) to capture false boundaries.Nevertheless, these kinds of methods are typically sensitive to edge noise and are computationally expensive, causing limited application in realtime scenarios.",
        "sentences": [
          {
            "text": "Early research on banding detection mainly focuses on false contour identification, which aims to find the wrong boundary rather than a \"true\" region edge in the image.",
            "label": 0
          },
          {
            "text": "Authors in[31]-[33]utilized monotonicity or non-monotonicity features of local support regions including the gradient, contrast, variance, and entropy information to measure the loss of low-amplitude detail caused by banding.",
            "label": 0
          },
          {
            "text": "However, these works ignored the perceptual characteristics of the human visual system (HVS) and thus did not perform a good correlation with subjective tests.",
            "label": 0
          },
          {
            "text": "Another banding detection strategy is conducted at the pixel-level estimation and segmentation.",
            "label": 0
          },
          {
            "text": "Bhagavathy et al.[30]proposed to identify banding artifacts by calculating the likelihood of pixel difference.",
            "label": 0
          },
          {
            "text": "Baugh et al.[34]measured the severity of banding based on the number of a group of connected pixels with the same color.",
            "label": 0
          },
          {
            "text": "Wang et al.[39]first detected uniform segments to find possible banding areas and further incorporated edge features (e.g.length and contrast) to capture false boundaries.",
            "label": 0
          },
          {
            "text": "Nevertheless, these kinds of methods are typically sensitive to edge noise and are computationally expensive, causing limited application in realtime scenarios.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment",
        "section": 10,
        "paragraph_id": 10,
        "full_text": "Towards addressing these problems, Tu et al.[42]presented a completely no-reference banding detection method, which combines various properties of HVS with a number of preprocessing steps to refine banding edge detection.Instead of regarding banding detection as a false edge detection problem, Tandon et al.[41]heuristically utilized the effect of contrast sensitivity function (CSF) on banding visibility and its dependence on spatial frequency.Based on this, Krasula et al.[40]further compared the banding annoyance with more commonly studied compression artifacts and proposed a banding-aware video quality metric.In recent years, deep learning approaches have prevailed in various VQA tasks.As the pioneering work, Kapoor et al.[43]developed an automated CNN-based banding detector for the first time, which is a simple two-stage algorithm and gives rise to devising other learning-based techniques.",
        "sentences": [
          {
            "text": "Towards addressing these problems, Tu et al.[42]presented a completely no-reference banding detection method, which combines various properties of HVS with a number of preprocessing steps to refine banding edge detection.",
            "label": 0
          },
          {
            "text": "Instead of regarding banding detection as a false edge detection problem, Tandon et al.[41]heuristically utilized the effect of contrast sensitivity function (CSF) on banding visibility and its dependence on spatial frequency.",
            "label": 0
          },
          {
            "text": "Based on this, Krasula et al.[40]further compared the banding annoyance with more commonly studied compression artifacts and proposed a banding-aware video quality metric.",
            "label": 0
          },
          {
            "text": "In recent years, deep learning approaches have prevailed in various VQA tasks.",
            "label": 0
          },
          {
            "text": "As the pioneering work, Kapoor et al.[43]developed an automated CNN-based banding detector for the first time, which is a simple two-stage algorithm and gives rise to devising other learning-based techniques.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment",
        "section": 11,
        "paragraph_id": 11,
        "full_text": "In this work, we build a large-scale banding database and propose a data-driven banding indicator that can generate pixelwise banding visibility maps with corresponding subjectively  consistent quality scores by combining human visual mechanisms and deep learning techniques.",
        "sentences": [
          {
            "text": "In this work, we build a large-scale banding database and propose a data-driven banding indicator that can generate pixelwise banding visibility maps with corresponding subjectively  consistent quality scores by combining human visual mechanisms and deep learning techniques.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment",
        "section": 12,
        "paragraph_id": 12,
        "full_text": "Subjective banding image quality assessment facilitates the development of automatic objective banding image and video quality models.We created the largest banding database in existence, denoted as the BAND-2k database, which consists of 2,000 banding distorted images and over 214,000 patch-level banding class labels.Then, a subjective experiment was conducted to obtain the mean opinion scores (MOS) of the BAND-2k database.The workflow of the banding database construction is shown in Fig.1.",
        "sentences": [
          {
            "text": "Subjective banding image quality assessment facilitates the development of automatic objective banding image and video quality models.",
            "label": 1
          },
          {
            "text": "We created the largest banding database in existence, denoted as the BAND-2k database, which consists of 2,000 banding distorted images and over 214,000 patch-level banding class labels.",
            "label": 0
          },
          {
            "text": "Then, a subjective experiment was conducted to obtain the mean opinion scores (MOS) of the BAND-2k database.",
            "label": 1
          },
          {
            "text": "The workflow of the banding database construction is shown in Fig.1.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment",
        "section": 13,
        "paragraph_id": 13,
        "full_text": "To build a content-rich and balanced database, we manually collected source videos including computer graphics (CG), usergenerated content (UGC), and professionally-generated content (PGC) from two popular media websites Bilibili.comand Youtube.com.Then, 885 clips with multiple spatial resolutions (i.e., 4096×2160, 3840×2160, 1920×1080) and frame rates (i.e., 60, 50, 30, 25) are chosen as candidate.Note that videos on the mentioned websites are firstly annotated by the community with assigned a number of favorites, views, and downloads.These statistics correlate with the content and quality of a video, which guides our choices to some extent.All videos selected on the website are released under an appropriate creative commons (CC) license that allows further editing and redistribution.After content selection, we further unified the format of all video clips, especially the spatial resolution and the pixel format, which avoid the effect of other facts on visual quality.Concretely, we first converted the frame rate of the original clips to 25fps, which is to reduce the storage pressure while ensuring the graphics quality.Considering the commonly used aspect ratio of the user interface and displays is 16:9, we cropped the partially unqualified videos rather than shrinking images unevenly.Then, we downsampled the trimmed spatial resolution 3840×2160 to a lower resolution -1920×1080 for the following subjective study.",
        "sentences": [
          {
            "text": "To build a content-rich and balanced database, we manually collected source videos including computer graphics (CG), usergenerated content (UGC), and professionally-generated content (PGC) from two popular media websites Bilibili.comand Youtube.com.",
            "label": 1
          },
          {
            "text": "Then, 885 clips with multiple spatial resolutions (i.e., 4096×2160, 3840×2160, 1920×1080) and frame rates (i.e., 60, 50, 30, 25) are chosen as candidate.",
            "label": 1
          },
          {
            "text": "Note that videos on the mentioned websites are firstly annotated by the community with assigned a number of favorites, views, and downloads.",
            "label": 1
          },
          {
            "text": "These statistics correlate with the content and quality of a video, which guides our choices to some extent.",
            "label": 1
          },
          {
            "text": "All videos selected on the website are released under an appropriate creative commons (CC) license that allows further editing and redistribution.",
            "label": 1
          },
          {
            "text": "After content selection, we further unified the format of all video clips, especially the spatial resolution and the pixel format, which avoid the effect of other facts on visual quality.",
            "label": 1
          },
          {
            "text": "Concretely, we first converted the frame rate of the original clips to 25fps, which is to reduce the storage pressure while ensuring the graphics quality.",
            "label": 1
          },
          {
            "text": "Considering the commonly used aspect ratio of the user interface and displays is 16:9, we cropped the partially unqualified videos rather than shrinking images unevenly.",
            "label": 1
          },
          {
            "text": "Then, we downsampled the trimmed spatial resolution 3840×2160 to a lower resolution -1920×1080 for the following subjective study.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment",
        "section": 14,
        "paragraph_id": 14,
        "full_text": "Before conducting the following experiments, we manually removed the videos that are either too dark or bright, overly blurry or colorful, which helps to obtain more reliable subjective assessments.To avoid redundancy and to make sure the diversity of selected contents, we also conducted attribute analysis studies on the selected images.Four metrics that correlated with human perception, i.e., contrast, brightness, sharpness, and colorfulness, are adopted as content diversity metrics.All video attributes are calculated on every 10 frames to reduce computational complexity, which are then averaged over frames of each video sequence.Fig.2shows the distribution of attributes extracted from the selected videos.",
        "sentences": [
          {
            "text": "Before conducting the following experiments, we manually removed the videos that are either too dark or bright, overly blurry or colorful, which helps to obtain more reliable subjective assessments.",
            "label": 1
          },
          {
            "text": "To avoid redundancy and to make sure the diversity of selected contents, we also conducted attribute analysis studies on the selected images.",
            "label": 1
          },
          {
            "text": "Four metrics that correlated with human perception, i.e., contrast, brightness, sharpness, and colorfulness, are adopted as content diversity metrics.",
            "label": 1
          },
          {
            "text": "All video attributes are calculated on every 10 frames to reduce computational complexity, which are then averaged over frames of each video sequence.",
            "label": 1
          },
          {
            "text": "Fig.2shows the distribution of attributes extracted from the selected videos.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment",
        "section": 15,
        "paragraph_id": 15,
        "full_text": "• Contrast: The contrast metric is simply defined as the standard deviation of pixel gray-scale intensities[25].",
        "sentences": [
          {
            "text": "• Contrast: The contrast metric is simply defined as the standard deviation of pixel gray-scale intensities[25].",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment",
        "section": 16,
        "paragraph_id": 16,
        "full_text": "• Colorfulness: The colorfulness metric is measured by the R, G, and B components[25].We first compute two matrices of differences between channels rg = R -G and yb = 1 2 (R + G) -B.Then, the colorfulness metric can be calculated as µ 2 rg + µ 2 yb + σ 2 rg + σ 2 yb , where µ and σ are the mean and standard deviation of their corresponding terms, respectively.",
        "sentences": [
          {
            "text": "• Colorfulness: The colorfulness metric is measured by the R, G, and B components[25].",
            "label": 1
          },
          {
            "text": "We first compute two matrices of differences between channels rg = R -G and yb = 1 2 (R + G) -B.",
            "label": 1
          },
          {
            "text": "Then, the colorfulness metric can be calculated as µ 2 rg + µ 2 yb + σ 2 rg + σ 2 yb , where µ and σ are the mean and standard deviation of their corresponding terms, respectively.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment",
        "section": 17,
        "paragraph_id": 17,
        "full_text": "• Sharpness: The cumulative probability of blur detection (CPBD) metric[24]is used to measure the image sharpness, which estimates the probability of detecting blur at each edge.",
        "sentences": [
          {
            "text": "• Sharpness: The cumulative probability of blur detection (CPBD) metric[24]is used to measure the image sharpness, which estimates the probability of detecting blur at each edge.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment",
        "section": 18,
        "paragraph_id": 18,
        "full_text": "• Brightness: The brightness of an image is obtained directly from the pixel gray intensities in R, G, and B channels.",
        "sentences": [
          {
            "text": "• Brightness: The brightness of an image is obtained directly from the pixel gray intensities in R, G, and B channels.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment",
        "section": 19,
        "paragraph_id": 19,
        "full_text": "Finally, the number of source videos are reduced to 873.Fig.3displays thumbnails for 30 selected representative video clips.",
        "sentences": [
          {
            "text": "Finally, the number of source videos are reduced to 873.",
            "label": 1
          },
          {
            "text": "Fig.3displays thumbnails for 30 selected representative video clips.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment",
        "section": 20,
        "paragraph_id": 20,
        "full_text": "To simulate authentic banding artifacts that exist in real viewing scenarios, we introduced four encoding/transcoding strategies including H.264/AVC[28], H.265/HEVC[27], VP9[38], and bit-depth manipulation[43]with fifteen quantization schemes in total.For H.264/AVC and H.265/HEVC, considering the range of their quantization parameter (QP in ffmpeg), we selected three typical QP values, namely, QP = {18, 33, 43}.This is because that coded video clips with a QP value smaller than 18 normally provide perceptual lossless quality, while coded video clips with a QP value larger than 43 will not be able to offer adequate quality, which may affect the subjective test of banding artifacts.Similarly, for the VP9 encoder, the QP values under our close inspection are chosen as {28, 45, 60}.Moreover, we applied the same quantization strategies as in[43], which introduces banding distortion by scaling bit-depth in luminance and chrominance channels.Here, the color coding scheme YCbCr4:2:0 is applied for maximum compatibility.To sum up, we generated fifteen levels of banding with different intensities and shapes to enhance the diversity of the database.The banding database is then built by extracting frames from the distorted video clips, resulting in 2,000 images with a resolution of 1920×1080.Fig.4shows the visualization results of banding exacerbated images.",
        "sentences": [
          {
            "text": "To simulate authentic banding artifacts that exist in real viewing scenarios, we introduced four encoding/transcoding strategies including H.264/AVC[28], H.265/HEVC[27], VP9[38], and bit-depth manipulation[43]with fifteen quantization schemes in total.For H.264/AVC and H.265/HEVC, considering the range of their quantization parameter (QP in ffmpeg), we selected three typical QP values, namely, QP = {18, 33, 43}.",
            "label": 1
          },
          {
            "text": "This is because that coded video clips with a QP value smaller than 18 normally provide perceptual lossless quality, while coded video clips with a QP value larger than 43 will not be able to offer adequate quality, which may affect the subjective test of banding artifacts.",
            "label": 1
          },
          {
            "text": "Similarly, for the VP9 encoder, the QP values under our close inspection are chosen as {28, 45, 60}.",
            "label": 1
          },
          {
            "text": "Moreover, we applied the same quantization strategies as in[43], which introduces banding distortion by scaling bit-depth in luminance and chrominance channels.",
            "label": 1
          },
          {
            "text": "Here, the color coding scheme YCbCr4:2:0 is applied for maximum compatibility.",
            "label": 1
          },
          {
            "text": "To sum up, we generated fifteen levels of banding with different intensities and shapes to enhance the diversity of the database.",
            "label": 1
          },
          {
            "text": "The banding database is then built by extracting frames from the distorted video clips, resulting in 2,000 images with a resolution of 1920×1080.",
            "label": 1
          },
          {
            "text": "Fig.4shows the visualization results of banding exacerbated images.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment",
        "section": 21,
        "paragraph_id": 21,
        "full_text": "In the process of image patch labeling, we initially intend to extract image patches of size 235×235 from banding images directly and perform annotation operations.However, considering the theoretical number of patches, it will cost a lot of manpower and time to label patch by patch.Therefore, we first segmented the banding images roughly and label them into banded and non-banded regions.Then, labelled image patches are generated from these segmented and labelled images by a sliding window.Specifically, we followed the same demarcation of the banded and non-banded images in[43]that a patch is labelled as banded if it has more than 30% overlap with banded regions in the image.",
        "sentences": [
          {
            "text": "In the process of image patch labeling, we initially intend to extract image patches of size 235×235 from banding images directly and perform annotation operations.",
            "label": 1
          },
          {
            "text": "However, considering the theoretical number of patches, it will cost a lot of manpower and time to label patch by patch.",
            "label": 0
          },
          {
            "text": "Therefore, we first segmented the banding images roughly and label them into banded and non-banded regions.",
            "label": 1
          },
          {
            "text": "Then, labelled image patches are generated from these segmented and labelled images by a sliding window.",
            "label": 1
          },
          {
            "text": "Specifically, we followed the same demarcation of the banded and non-banded images in[43]that a patch is labelled as banded if it has more than 30% overlap with banded regions in the image.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment",
        "section": 22,
        "paragraph_id": 22,
        "full_text": "Eventually, a banding dataset containing 2,000 distorted images with 1920×1080 resolution and 214,324 labelled image patches of size 235×235 is built.To the best of our knowledge, it is the largest banding dataset in existence, which enables training various machine/deep learning based banding detection models and facilitates the development of image/video debanding techniques.TableIIreports the composition of labelled image patch dataset.It can be observed that the number of banded patches is a bit smaller than non-banded patches due to the fact that banding usually appears in smooth background areas.",
        "sentences": [
          {
            "text": "Eventually, a banding dataset containing 2,000 distorted images with 1920×1080 resolution and 214,324 labelled image patches of size 235×235 is built.",
            "label": 1
          },
          {
            "text": "To the best of our knowledge, it is the largest banding dataset in existence, which enables training various machine/deep learning based banding detection models and facilitates the development of image/video debanding techniques.",
            "label": 1
          },
          {
            "text": "TableIIreports the composition of labelled image patch dataset.",
            "label": 1
          },
          {
            "text": "It can be observed that the number of banded patches is a bit smaller than non-banded patches due to the fact that banding usually appears in smooth background areas.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment",
        "section": 23,
        "paragraph_id": 23,
        "full_text": "As shown in Fig.1, the subjective quality study contains four steps.In addition to preparing the experimental environment, subjects should pass the qualification test first to participate in the study.After the subjective rating, all resulting scores need to be analyzed and examined before generating the final mean opinion score (MOS).",
        "sentences": [
          {
            "text": "As shown in Fig.1, the subjective quality study contains four steps.",
            "label": 0
          },
          {
            "text": "In addition to preparing the experimental environment, subjects should pass the qualification test first to participate in the study.",
            "label": 0
          },
          {
            "text": "After the subjective rating, all resulting scores need to be analyzed and examined before generating the final mean opinion score (MOS).",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment",
        "section": 24,
        "paragraph_id": 24,
        "full_text": "1) Experimental Environment Setting: In this study, a total of 25 inexperienced subjects are gathered in a laboratory environment, where relevant experimental configuration must satisfy the following requirements: • Considering the viewing effect, desktops and laptops are allowed as displays. • The resolution of displays must be larger than or equal to 1920×1080 to show the images without spatial downsampling. • The viewing distance and optimal horizontal viewing angle are set as 1.9 times the height of the display and respectively.Other settings such as the ambient brightness, lighting, and background are configured according to the ITU-R BT.500 recommendation[26].As a result, we used a 27-inch AOC Q27U2D monitor with a resolution of 2560×1440 for assessment with 25 subjects.Due to the large number of images to be assessed, we divided the dataset into ten sessions to avoid visual fatigue.Each session of tests took nearly 2 hours with a 30-minute break for each participant.",
        "sentences": [
          {
            "text": "1) Experimental Environment Setting: In this study, a total of 25 inexperienced subjects are gathered in a laboratory environment, where relevant experimental configuration must satisfy the following requirements: • Considering the viewing effect, desktops and laptops are allowed as displays.",
            "label": 0
          },
          {
            "text": "• The resolution of displays must be larger than or equal to 1920×1080 to show the images without spatial downsampling.",
            "label": 0
          },
          {
            "text": "• The viewing distance and optimal horizontal viewing angle are set as 1.9 times the height of the display and respectively.",
            "label": 0
          },
          {
            "text": "500 recommendation[26].",
            "label": 0
          },
          {
            "text": "As a result, we used a 27-inch AOC Q27U2D monitor with a resolution of 2560×1440 for assessment with 25 subjects.",
            "label": 0
          },
          {
            "text": "Due to the large number of images to be assessed, we divided the dataset into ten sessions to avoid visual fatigue.",
            "label": 1
          },
          {
            "text": "Each session of tests took nearly 2 hours with a 30-minute break for each participant.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment",
        "section": 25,
        "paragraph_id": 25,
        "full_text": "2) Qualification Test: Before starting the main experiment, subjects are required to pass a quiz to get the qualification of conducting follow-up experiments.Firstly, we manually selected 10 labeled banding exacerbated images beyond the database as training images to familiarize subjects with the operation interface and the goal of this subjective test.The quiz consists of two parts including banding classification and image-level quality rating.In banding classification, subjects were told to divide the test image into banded or non-banded.In image-level quality rating, subjects were instructed to focus on the coverage and intensity of banding areas, as well as the overall quality of images to get the final quality score.The rating scale is continuous from 0 to 100 while a higher value indicates more severe banding (i.e., more visible or occupying larger portion of the image).To make the quiz objective and fair, we take the banding images labeled by domain experts as the ground truth, while two existing banding metrics BBAND[42]and DBI[43]are used to determine the normal range of the rating scores.That is the quality scores rated by subjects should not exceed 20% of the above banding metrics.As a result, only 23 subjects with an accuracy above 80% in banding classification and subjective scores in the normal range were allowed to pass the quiz.Note that the steps for taking a quiz are the same as the main experiment, which aims to guarantee the consistency of results.",
        "sentences": [
          {
            "text": "2) Qualification Test: Before starting the main experiment, subjects are required to pass a quiz to get the qualification of conducting follow-up experiments.",
            "label": 0
          },
          {
            "text": "Firstly, we manually selected 10 labeled banding exacerbated images beyond the database as training images to familiarize subjects with the operation interface and the goal of this subjective test.",
            "label": 0
          },
          {
            "text": "The quiz consists of two parts including banding classification and image-level quality rating.",
            "label": 0
          },
          {
            "text": "In banding classification, subjects were told to divide the test image into banded or non-banded.",
            "label": 0
          },
          {
            "text": "In image-level quality rating, subjects were instructed to focus on the coverage and intensity of banding areas, as well as the overall quality of images to get the final quality score.",
            "label": 0
          },
          {
            "text": "The rating scale is continuous from 0 to 100 while a higher value indicates more severe banding (i.e., more visible or occupying larger portion of the image).",
            "label": 0
          },
          {
            "text": "To make the quiz objective and fair, we take the banding images labeled by domain experts as the ground truth, while two existing banding metrics BBAND[42]and DBI[43]are used to determine the normal range of the rating scores.",
            "label": 0
          },
          {
            "text": "That is the quality scores rated by subjects should not exceed 20% of the above banding metrics.",
            "label": 0
          },
          {
            "text": "As a result, only 23 subjects with an accuracy above 80% in banding classification and subjective scores in the normal range were allowed to pass the quiz.",
            "label": 0
          },
          {
            "text": "Note that the steps for taking a quiz are the same as the main experiment, which aims to guarantee the consistency of results.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment",
        "section": 26,
        "paragraph_id": 26,
        "full_text": "3) Formal Study: We adopted the single-stimulus (SS) method in this test.Ten golden images that have the acknowledged high quality or poor quality (assessed by BBAND[42]and DBI[43]) were added to each session for controlling the scoring deviations.Besides, 3 repeated images are randomly inserted into each session to ensure consistency of scores before and after subjects scoring.At last, 23 qualified subjects were asked to provide their opinions on the shuffled image groups.The resulting scores were collected and packed for further analysis.",
        "sentences": [
          {
            "text": "3) Formal Study: We adopted the single-stimulus (SS) method in this test.",
            "label": 0
          },
          {
            "text": "Ten golden images that have the acknowledged high quality or poor quality (assessed by BBAND[42]and DBI[43]) were added to each session for controlling the scoring deviations.",
            "label": 0
          },
          {
            "text": "Besides, 3 repeated images are randomly inserted into each session to ensure consistency of scores before and after subjects scoring.",
            "label": 0
          },
          {
            "text": "At last, 23 qualified subjects were asked to provide their opinions on the shuffled image groups.",
            "label": 0
          },
          {
            "text": "The resulting scores were collected and packed for further analysis.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment",
        "section": 27,
        "paragraph_id": 27,
        "full_text": "4) Result Analysis: In total, 46,000 scores were collected by 23 qualified subjects in the main study phase.However, considering the qualification quiz cannot completely disallow those unreliable workers to muddle through to the main study while reliable subjects may also occasionally score odd values, which may be caused by the inter-individual differences in perceiving the quality of the unique characteristics of different contents.Thus, we further investigated the confidence of rating scores and removed outliers following the Grubbs' test[23],[45].",
        "sentences": [
          {
            "text": "4) Result Analysis: In total, 46,000 scores were collected by 23 qualified subjects in the main study phase.",
            "label": 0
          },
          {
            "text": "However, considering the qualification quiz cannot completely disallow those unreliable workers to muddle through to the main study while reliable subjects may also occasionally score odd values, which may be caused by the inter-individual differences in perceiving the quality of the unique characteristics of different contents.",
            "label": 0
          },
          {
            "text": "Thus, we further investigated the confidence of rating scores and removed outliers following the Grubbs' test[23],[45].",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment",
        "section": 28,
        "paragraph_id": 28,
        "full_text": "Concretely, let s = (s 1 , s 2 , . . ., s N ) denote a set of raw scores collected for one distorted image.The test statistics is defined as the maximum absolute deviation of a sample standard deviation (SD) unit from the sample mean, which is mathematically expressed as where s and σ s denote the mean and standard deviation of the sample, respectively.Then, given a significant level α, a sample is detected as an outlier if where t 2 α/(2N ),N -2 represents the upper critical value of the tdistribution with N -2 degrees of freedom and a significance level of α/(2N ).Empirically, we set the significance level α at 0.05.Then, a sample is identified as an outlier if its distance to the sample mean is larger than 2.5 times SD and is removed.Following the aforementioned steps, the total number of scores Fig.6.The overall architecture of the proposed method.Given a banding distorted image, it is first divided into patches.Then, the patch-level high-frequency map (HFM) and low-frequency map (LFM) are generated by Sobel operation and piece-wise smooth algorithm[19], respectively.After that, a dual-branch CNN (CNN H and CNN L ) is deployed to extract hierarchical features with different visual information and thus classify the patches into banded or non-banded.Lastly, a spatial frequency masking strategy is introduced to refine the banding map and calculate the image-level banding quality score.Note that the dual-branch networks do not share parameters. was reduced to 44,371, and MOS was created by averaging the scores for each image.Fig.5presents the histogram of MOSs over the entire database, showing a broad MOS distribution of banding images.",
        "sentences": [
          {
            "text": "Concretely, let s = (s 1 , s 2 , . . ., s N ) denote a set of raw scores collected for one distorted image.",
            "label": 0
          },
          {
            "text": "The test statistics is defined as the maximum absolute deviation of a sample standard deviation (SD) unit from the sample mean, which is mathematically expressed as where s and σ s denote the mean and standard deviation of the sample, respectively.",
            "label": 0
          },
          {
            "text": "Then, given a significant level α, a sample is detected as an outlier if where t 2 α/(2N ),N -2 represents the upper critical value of the tdistribution with N -2 degrees of freedom and a significance level of α/(2N ).",
            "label": 0
          },
          {
            "text": "Empirically, we set the significance level α at 0.05.",
            "label": 0
          },
          {
            "text": "Then, a sample is identified as an outlier if its distance to the sample mean is larger than 2.5 times SD and is removed.",
            "label": 0
          },
          {
            "text": "Following the aforementioned steps, the total number of scores Fig.6.",
            "label": 0
          },
          {
            "text": "The overall architecture of the proposed method.",
            "label": 0
          },
          {
            "text": "Given a banding distorted image, it is first divided into patches.",
            "label": 0
          },
          {
            "text": "Then, the patch-level high-frequency map (HFM) and low-frequency map (LFM) are generated by Sobel operation and piece-wise smooth algorithm[19], respectively.",
            "label": 0
          },
          {
            "text": "After that, a dual-branch CNN (CNN H and CNN L ) is deployed to extract hierarchical features with different visual information and thus classify the patches into banded or non-banded.",
            "label": 0
          },
          {
            "text": "Lastly, a spatial frequency masking strategy is introduced to refine the banding map and calculate the image-level banding quality score.",
            "label": 0
          },
          {
            "text": "Note that the dual-branch networks do not share parameters.",
            "label": 0
          },
          {
            "text": "was reduced to 44,371, and MOS was created by averaging the scores for each image.",
            "label": 0
          },
          {
            "text": "Fig.5presents the histogram of MOSs over the entire database, showing a broad MOS distribution of banding images.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment",
        "section": 29,
        "paragraph_id": 29,
        "full_text": "In this section, we describe the architecture of the proposed banding evaluator in detail, as shown in Fig.6.",
        "sentences": [
          {
            "text": "In this section, we describe the architecture of the proposed banding evaluator in detail, as shown in Fig.6.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment",
        "section": 30,
        "paragraph_id": 30,
        "full_text": "As stated before, banding usually appear as high-frequency information in the smooth background, while humans perceive high-frequency texture regions and low-frequency plateau regions through different neural channels concurrently, and transfer the upper visual features into the cerebral cortex for final processing[17],[44].Inspired by this, we employ high-frequency maps (HFM) and low-frequency maps (LFM) as the deep learning network inputs, which represent the texture and structural information of the image respectively, to mimic the recognition mechanism of the human brain for better banding identification.High-frequency Maps.Since gradient has been widely used to represent edge information and has been confirmed beneficial to acquire high-frequency components with low computational cost[20]-[22], we apply the isotropic Sobel operator to each patch for enhancing the details of banding artifacts.Given an input patch I, the high-frequency map is calculated by where S x and S y are the horizontal and vertical isotropic Sobel operators, respectively.\" * \" denotes the convolution operation.",
        "sentences": [
          {
            "text": "As stated before, banding usually appear as high-frequency information in the smooth background, while humans perceive high-frequency texture regions and low-frequency plateau regions through different neural channels concurrently, and transfer the upper visual features into the cerebral cortex for final processing[17],[44].",
            "label": 0
          },
          {
            "text": "Inspired by this, we employ high-frequency maps (HFM) and low-frequency maps (LFM) as the deep learning network inputs, which represent the texture and structural information of the image respectively, to mimic the recognition mechanism of the human brain for better banding identification.",
            "label": 0
          },
          {
            "text": "High-frequency Maps.",
            "label": 0
          },
          {
            "text": "Since gradient has been widely used to represent edge information and has been confirmed beneficial to acquire high-frequency components with low computational cost[20]-[22], we apply the isotropic Sobel operator to each patch for enhancing the details of banding artifacts.",
            "label": 0
          },
          {
            "text": "Given an input patch I, the high-frequency map is calculated by where S x and S y are the horizontal and vertical isotropic Sobel operators, respectively.\" * \" denotes the convolution operation.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment",
        "section": 31,
        "paragraph_id": 31,
        "full_text": "Low-frequency Maps.To maintain the principal content of the image and filter out the influence of high-frequency information, we use the piece-wise smooth algorithm[19]to generate the low-frequency map by minimizing a function for image approximation recovery: where L represents the low-frequency map, Ω and E denotes the image domain and edge set, respectively.P indicates the pixel and E dσ represents the total edge length.The coefficients α and β are positive regularization constants.An example of frequency maps is shown in Fig.7.",
        "sentences": [
          {
            "text": "Low-frequency Maps.",
            "label": 0
          },
          {
            "text": "To maintain the principal content of the image and filter out the influence of high-frequency information, we use the piece-wise smooth algorithm[19]to generate the low-frequency map by minimizing a function for image approximation recovery: where L represents the low-frequency map, Ω and E denotes the image domain and edge set, respectively.",
            "label": 0
          },
          {
            "text": "P indicates the pixel and E dσ represents the total edge length.",
            "label": 0
          },
          {
            "text": "The coefficients α and β are positive regularization constants.",
            "label": 0
          },
          {
            "text": "An example of frequency maps is shown in Fig.7.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment",
        "section": 32,
        "paragraph_id": 32,
        "full_text": "To obtain the overall banding score, we first divide the banding image into 235×235 patches and apply CNN-based classification to each patch, resulting in a banding classification label for each patch, i.e., banded or non-banded.As shown in Fig.6, the proposed network consists of two parallel branches, namely CNN H and CNN L , which take the patch-level high-frequency map and low-frequency map as input, respectively.For each branch, we propose to use Resnet-50[18]as the backbone.Specifically, we incorporate the feature maps extracted from the first convolutional layer and the last layer of Resnet-50 as hierarchical visual features, which represent different visual information[46],[47]and can be used as predictive information to enhance the discrimination ability of the network for banding and non-banded regions.Afterward, the features extracted from two branches are concatenated first and reshaped into 128dimensional vectors through two fully-connected layers, which is further followed with the sigmoid activation function to output the final predicted label, namely banded or non-banded.Of note is that sharing parameters is extremely unfavorable for extracting low-and high-frequency features simultaneously, we thereby deploy two branches that work independently and do not share parameters.The loss function adopted here is binary cross entropy.",
        "sentences": [
          {
            "text": "To obtain the overall banding score, we first divide the banding image into 235×235 patches and apply CNN-based classification to each patch, resulting in a banding classification label for each patch, i.e., banded or non-banded.",
            "label": 0
          },
          {
            "text": "As shown in Fig.6, the proposed network consists of two parallel branches, namely CNN H and CNN L , which take the patch-level high-frequency map and low-frequency map as input, respectively.",
            "label": 0
          },
          {
            "text": "For each branch, we propose to use Resnet-50[18]as the backbone.",
            "label": 0
          },
          {
            "text": "Specifically, we incorporate the feature maps extracted from the first convolutional layer and the last layer of Resnet-50 as hierarchical visual features, which represent different visual information[46],[47]and can be used as predictive information to enhance the discrimination ability of the network for banding and non-banded regions.",
            "label": 0
          },
          {
            "text": "Afterward, the features extracted from two branches are concatenated first and reshaped into 128dimensional vectors through two fully-connected layers, which is further followed with the sigmoid activation function to output the final predicted label, namely banded or non-banded.",
            "label": 0
          },
          {
            "text": "Of note is that sharing parameters is extremely unfavorable for extracting low-and high-frequency features simultaneously, we thereby deploy two branches that work independently and do not share parameters.",
            "label": 0
          },
          {
            "text": "The loss function adopted here is binary cross entropy.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment",
        "section": 33,
        "paragraph_id": 33,
        "full_text": "With the trained CNN H and CNN L , each input patch is predicted to obtain a banding or non-banded label.To better guide the pre-processing and post-processing debanding algorithms, it is necessary to generate a quality score for the entire banding image.Since the visibility of edge is also affected by content, we further consider the effect of spatially varying content information on the local quality of human perception.As a consequence, we introduce the spatial frequency masking strategy to determine the weighting factor for the detected banding regions in each patch adaptively and thus obtain the image-level banding severity score while refining the visibility of banding artifacts.",
        "sentences": [
          {
            "text": "With the trained CNN H and CNN L , each input patch is predicted to obtain a banding or non-banded label.",
            "label": 0
          },
          {
            "text": "To better guide the pre-processing and post-processing debanding algorithms, it is necessary to generate a quality score for the entire banding image.",
            "label": 0
          },
          {
            "text": "Since the visibility of edge is also affected by content, we further consider the effect of spatially varying content information on the local quality of human perception.",
            "label": 0
          },
          {
            "text": "As a consequence, we introduce the spatial frequency masking strategy to determine the weighting factor for the detected banding regions in each patch adaptively and thus obtain the image-level banding severity score while refining the visibility of banding artifacts.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment",
        "section": 34,
        "paragraph_id": 34,
        "full_text": "1) Spatial Frequency Masking: The spatial frequency is defined as the activity level of an image, which establishes a filterbank based on the visual stimulus and is in accordance with HVS[16].In this paper, we propose to apply spatial frequency as an effective contrast criterion to banding measurement.Specifically, given an image of size I W × I H , divided into N × N patches, where I W and I H denote the number of columns and rows respectively.The column (CF k ) and row (RF k ) frequencies of the image patches are given by where I(x, y) is the pixel value of the image patch.Then, the resulting spatial frequency of an N × N patch is computed as where k is the number of patches (1 ≤ k ≤ I W I H N 2 ).Since most banding regions are likely to have large contrast including edges and textures, which should be assigned greater weights than the smooth and blurred areas.Following Kazemi et al.[15], we set a threshold value to distinguish these regions, which is defined as the average spatial frequency of image patches:",
        "sentences": [
          {
            "text": "1) Spatial Frequency Masking: The spatial frequency is defined as the activity level of an image, which establishes a filterbank based on the visual stimulus and is in accordance with HVS[16].",
            "label": 0
          },
          {
            "text": "In this paper, we propose to apply spatial frequency as an effective contrast criterion to banding measurement.",
            "label": 0
          },
          {
            "text": "Specifically, given an image of size I W × I H , divided into N × N patches, where I W and I H denote the number of columns and rows respectively.",
            "label": 0
          },
          {
            "text": "The column (CF k ) and row (RF k ) frequencies of the image patches are given by where I(x, y) is the pixel value of the image patch.",
            "label": 0
          },
          {
            "text": "Then, the resulting spatial frequency of an N × N patch is computed as where k is the number of patches (1 ≤ k ≤ I W I H N 2 ).",
            "label": 0
          },
          {
            "text": "Since most banding regions are likely to have large contrast including edges and textures, which should be assigned greater weights than the smooth and blurred areas.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment",
        "section": 35,
        "paragraph_id": 35,
        "full_text": "Accordingly, we design a banding visibility transfer function to express spatial frequency masking as a function of the local textural feature.The final spatial frequency masking weight is calculated at each patch as where γ is the scaling constant factor chosen to tune the shape of the transfer function.We used γ = 1.5 in our implementation.",
        "sentences": [
          {
            "text": "Accordingly, we design a banding visibility transfer function to express spatial frequency masking as a function of the local textural feature.",
            "label": 0
          },
          {
            "text": "The final spatial frequency masking weight is calculated at each patch as where γ is the scaling constant factor chosen to tune the shape of the transfer function.",
            "label": 0
          },
          {
            "text": "We used γ = 1.5 in our implementation.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment",
        "section": 36,
        "paragraph_id": 36,
        "full_text": "2) Building a Banding Metric: The visibility of banding artifacts depends on the combination of multiple visual mechanisms.In this paper, we propose a simple but effective product model for attribute integration at each predicted banding patch to obtain the entire banding map (BM): where P k denotes the predicted label of k-th patch and w k is the weight parameter that scales the visibility of measured contours, i.e., gradient magnitude of the high-frequency map, |HFM k (i, j) | at region (i, j).Furthermore, inspired by previous psychovisual findings that the QoE of observers is dominated by those regions having poor quality[14],[42], we thereby leverage the worst p% percentile visual pooling to calculate an average banding score from the generated BM, where p is set to 80 in this experiment.As a result, the perceptual score of the overall banding image is defined as where M is the total number of patches in image I. T p% denotes the index set of the top p% non-zero pixel-wise value contained in k-th patch of the BM.",
        "sentences": [
          {
            "text": "2) Building a Banding Metric: The visibility of banding artifacts depends on the combination of multiple visual mechanisms.",
            "label": 0
          },
          {
            "text": "In this paper, we propose a simple but effective product model for attribute integration at each predicted banding patch to obtain the entire banding map (BM): where P k denotes the predicted label of k-th patch and w k is the weight parameter that scales the visibility of measured contours, i.e., gradient magnitude of the high-frequency map, |HFM k (i, j) | at region (i, j).",
            "label": 0
          },
          {
            "text": "Furthermore, inspired by previous psychovisual findings that the QoE of observers is dominated by those regions having poor quality[14],[42], we thereby leverage the worst p% percentile visual pooling to calculate an average banding score from the generated BM, where p is set to 80 in this experiment.",
            "label": 0
          },
          {
            "text": "As a result, the perceptual score of the overall banding image is defined as where M is the total number of patches in image I.",
            "label": 0
          },
          {
            "text": "T p% denotes the index set of the top p% non-zero pixel-wise value contained in k-th patch of the BM.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment",
        "section": 37,
        "paragraph_id": 37,
        "full_text": "In this section, we first present the experimental protocol in detail and then evaluate the performance of the proposed method on two tasks, namely patch-level banding classification and banding image quality assessment.After that, the ablation study and cross-database validation are conducted to prove the robustness and effectiveness of the proposed method.Finally, we test the computational efficiency of our method.",
        "sentences": [
          {
            "text": "In this section, we first present the experimental protocol in detail and then evaluate the performance of the proposed method on two tasks, namely patch-level banding classification and banding image quality assessment.",
            "label": 0
          },
          {
            "text": "After that, the ablation study and cross-database validation are conducted to prove the robustness and effectiveness of the proposed method.",
            "label": 0
          },
          {
            "text": "Finally, we test the computational efficiency of our method.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment",
        "section": 38,
        "paragraph_id": 38,
        "full_text": "A. Experimental Protocol 1) Databases and Settings: We choose two databases to train and test the effectiveness of the proposed banding IQA method, which are the database released in[43]and our proposed BAND-2k database.The detail information of these two datasets can be found in TableI.The proposed model is implemented by PyTorch[13].Before training, we randomly split the training, validation, and testing set into 8:1:1 (as shown in TableII).We use the Adam optimizer with the initial learning rate set as 1e-4 and set the batch size as 32.The training process is stopped after 25 epochs.The resolution of each cropped patch is fixed to 235×235.All experiments on both the[43]database and the BAND-2k database are conducted repeatedly 10 times to obtain the mean performance.",
        "sentences": [
          {
            "text": "A. Experimental Protocol 1) Databases and Settings: We choose two databases to train and test the effectiveness of the proposed banding IQA method, which are the database released in[43]and our proposed BAND-2k database.",
            "label": 0
          },
          {
            "text": "The detail information of these two datasets can be found in TableI.",
            "label": 1
          },
          {
            "text": "The proposed model is implemented by PyTorch[13].",
            "label": 1
          },
          {
            "text": "Before training, we randomly split the training, validation, and testing set into 8:1:1 (as shown in TableII).",
            "label": 0
          },
          {
            "text": "We use the Adam optimizer with the initial learning rate set as 1e-4 and set the batch size as 32.",
            "label": 1
          },
          {
            "text": "The training process is stopped after 25 epochs.",
            "label": 0
          },
          {
            "text": "The resolution of each cropped patch is fixed to 235×235.",
            "label": 0
          },
          {
            "text": "All experiments on both the[43]database and the BAND-2k database are conducted repeatedly 10 times to obtain the mean performance.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment",
        "section": 39,
        "paragraph_id": 39,
        "full_text": "2) Baseline Algorithms: We include a number of representative IQA algorithms in our evaluation as references to be compared against.These baseline methods include: • General FR IQA methods: We choose PSNR, SSIM[12], MS-SSIM[11], LPIPS[10]as baselines.These are the most commonly used FR IQA metrics in practical applications such as video coding, image enhancement, etc. • General NR IQA methods: BRISQUE[37], NIQE[36], NIMA[8], DBCNN[35], HyberIQA[9], and StairIQA[1].These are general-purpose NR IQA methods that are not limited by distortion types.• Banding IQA methods: Considering that there exists few research on banding detection and quality assessment, we barely select the BBAND[42], CAMBI[41], VMAF BA[40]and DBI[43]metrics as comparisons.Among them, only VMAF BA is FR method, while the rest of them are NR.3) Evaluation Criteria: To evaluate the IQA methods comprehensively, a total of seven evaluation indexes in two categories are adopted.For patch-level banding classification, we follow the common procedures as in[43]and utilize the area under the receiver operating characteristics (AUROC), the area under the precision-recall curve (AUPRC), and accuracy as the classification performance metrics.For banding image quality assessment, four mainstream metrics are selected as the evaluation criteria: Spearman rank-order correlation coefficient (SRCC) and Kendall rank-order correlation coefficient (KRCC) measure the prediction monotonicity, while Pearson linear correlation coefficient (PLCC) and root mean square error (RMSE) are calculated to assess prediction consistency.Considering the potential nonlinear mapping characteristics between the objective scores and the subjective scores, we perform score alignment by mapping the predicted value using the five-parameter logistic function before calculating PLCC and RMSE values[7].",
        "sentences": [
          {
            "text": "2) Baseline Algorithms: We include a number of representative IQA algorithms in our evaluation as references to be compared against.",
            "label": 0
          },
          {
            "text": "These baseline methods include: • General FR IQA methods: We choose PSNR, SSIM[12], MS-SSIM[11], LPIPS[10]as baselines.",
            "label": 0
          },
          {
            "text": "These are the most commonly used FR IQA metrics in practical applications such as video coding, image enhancement, etc. • General NR IQA methods: BRISQUE[37], NIQE[36], NIMA[8], DBCNN[35], HyberIQA[9], and StairIQA[1].",
            "label": 0
          },
          {
            "text": "• Banding IQA methods: Considering that there exists few research on banding detection and quality assessment, we barely select the BBAND[42], CAMBI[41], VMAF BA[40]and DBI[43]metrics as comparisons.",
            "label": 0
          },
          {
            "text": "3) Evaluation Criteria: To evaluate the IQA methods comprehensively, a total of seven evaluation indexes in two categories are adopted.",
            "label": 0
          },
          {
            "text": "For patch-level banding classification, we follow the common procedures as in[43]and utilize the area under the receiver operating characteristics (AUROC), the area under the precision-recall curve (AUPRC), and accuracy as the classification performance metrics.",
            "label": 0
          },
          {
            "text": "For banding image quality assessment, four mainstream metrics are selected as the evaluation criteria: Spearman rank-order correlation coefficient (SRCC) and Kendall rank-order correlation coefficient (KRCC) measure the prediction monotonicity, while Pearson linear correlation coefficient (PLCC) and root mean square error (RMSE) are calculated to assess prediction consistency.",
            "label": 0
          },
          {
            "text": "Considering the potential nonlinear mapping characteristics between the objective scores and the subjective scores, we perform score alignment by mapping the predicted value using the five-parameter logistic function before calculating PLCC and RMSE values[7].",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment",
        "section": 40,
        "paragraph_id": 40,
        "full_text": "Since our goal is to develop an effective banding IQA approach, we regard the identification of banding patches as an important preceding process to achieve accurate banding quality prediction.However, there exist few methods that are designed for banding classification and nearly all IQA methods produce scalar values only while failing in classifying banding regions directly.Therefore, we adopt a thresholding step to convert the single quality value into binary classification results as[43]does.Concretely, a half-interval search algorithm[48]is employed to find the optimal threshold value that can generate the best classification result.",
        "sentences": [
          {
            "text": "Since our goal is to develop an effective banding IQA approach, we regard the identification of banding patches as an important preceding process to achieve accurate banding quality prediction.",
            "label": 0
          },
          {
            "text": "However, there exist few methods that are designed for banding classification and nearly all IQA methods produce scalar values only while failing in classifying banding regions directly.",
            "label": 0
          },
          {
            "text": "Therefore, we adopt a thresholding step to convert the single quality value into binary classification results as[43]does.",
            "label": 0
          },
          {
            "text": "Concretely, a half-interval search algorithm[48]is employed to find the optimal threshold value that can generate the best classification result.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment",
        "section": 41,
        "paragraph_id": 41,
        "full_text": "Based on the above premise, TableIIIreports the experimental results on both the database from[43]and the BAND-2k database.We highlight the best results in boldface.As compared to other state-of-the-art IQA methods, our proposed method yields the best overall performance in terms of AUROC, AUPRC, and accuracy.It is shown that most general FR IQA and NR IQA models perform poorly on the patch-level banding classification task while performing fairly well on other IQA tasks[1],[9],[35], indicating that the current approaches are not sensitive to banding distortion.Benefiting from the powerful feature extraction ability of CNNs, our proposed method and the customized NR IQA models for banding artifacts detection (DBI[43]) reach a significant performance in the discrimination of false contours.However, the performance of banding IQA method BBAND, CAMBI, and VMAF BA is surprisingly poor compared with other methods, which shows their vulnerability in identifying local banding artifacts from texture regions and are not suitable for patch-level banding identification.In addition, we investigate the computational complexity in terms of execution time per image patch.It can be observed that except for those traditional FR IQA models, our method achieves comparable speed in patch-level banding classification, which determines the prediction efficiency of the subsequent image-level quality assessment, making it a favorable choice in time-constrained scenarios.",
        "sentences": [
          {
            "text": "Based on the above premise, TableIIIreports the experimental results on both the database from[43]and the BAND-2k database.",
            "label": 1
          },
          {
            "text": "We highlight the best results in boldface.",
            "label": 0
          },
          {
            "text": "As compared to other state-of-the-art IQA methods, our proposed method yields the best overall performance in terms of AUROC, AUPRC, and accuracy.",
            "label": 0
          },
          {
            "text": "It is shown that most general FR IQA and NR IQA models perform poorly on the patch-level banding classification task while performing fairly well on other IQA tasks[1],[9],[35], indicating that the current approaches are not sensitive to banding distortion.",
            "label": 0
          },
          {
            "text": "Benefiting from the powerful feature extraction ability of CNNs, our proposed method and the customized NR IQA models for banding artifacts detection (DBI[43]) reach a significant performance in the discrimination of false contours.",
            "label": 0
          },
          {
            "text": "However, the performance of banding IQA method BBAND, CAMBI, and VMAF BA is surprisingly poor compared with other methods, which shows their vulnerability in identifying local banding artifacts from texture regions and are not suitable for patch-level banding identification.",
            "label": 0
          },
          {
            "text": "In addition, we investigate the computational complexity in terms of execution time per image patch.",
            "label": 0
          },
          {
            "text": "It can be observed that except for those traditional FR IQA models, our method achieves comparable speed in patch-level banding classification, which determines the prediction efficiency of the subsequent image-level quality assessment, making it a favorable choice in time-constrained scenarios.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment",
        "section": 42,
        "paragraph_id": 42,
        "full_text": "Considering that there exist no image banding databases with subjective scores attached in the public domain, we merely compare the performance of the proposed method with the baseline approaches on the proposed BAND-2k database.The results are shown in TableIV, from which we can observe that our proposed method largely surpasses all baselines in terms of SRCC, KRCC, and PLCC except for the RMSE.Compared to the secondbest model, our method achieves 18.07%SRCC improvements, 24.91% KRCC improvements, and 18.01%PLCC improvements on the BAND-2k database.We also present scatter plots of predictions versus MOS for better visualization in Fig.8. Overall,  (a) PSNR, (b) SSIM[12], (c) MS-SSIM[11], (d) LPIPS[10], (e) VMAF BA[40], (f) BRISQUE[37], (g) NIQE[36], (h) NIMA[8], (i) DBCNN[35], (j) HyperIQA[9], (k) StairIQA[1], (l) BBAND[42], (m) CAMBI[41], (n) DBI[43], and (o) Ours.",
        "sentences": [
          {
            "text": "Considering that there exist no image banding databases with subjective scores attached in the public domain, we merely compare the performance of the proposed method with the baseline approaches on the proposed BAND-2k database.",
            "label": 1
          },
          {
            "text": "The results are shown in TableIV, from which we can observe that our proposed method largely surpasses all baselines in terms of SRCC, KRCC, and PLCC except for the RMSE.",
            "label": 1
          },
          {
            "text": "Compared to the secondbest model, our method achieves 18.07%SRCC improvements, 24.91% KRCC improvements, and 18.01%PLCC improvements on the BAND-2k database.",
            "label": 1
          },
          {
            "text": "We also present scatter plots of predictions versus MOS for better visualization in Fig.8.",
            "label": 0
          },
          {
            "text": "Overall,  (a) PSNR, (b) SSIM[12], (c) MS-SSIM[11], (d) LPIPS[10], (e) VMAF BA[40], (f) BRISQUE[37], (g) NIQE[36], (h) NIMA[8], (i) DBCNN[35], (j) HyperIQA[9], (k) StairIQA[1], (l) BBAND[42], (m) CAMBI[41], (n) DBI[43], and (o) Ours.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment",
        "section": 43,
        "paragraph_id": 43,
        "full_text": "Fig.9. Visual comparisons of the banding map results.From top to bottom are banding images and their corresponding banding maps generated by BBAND[42], DBI[43], and our proposed method, respectively.The first five columns of images from left to right are from BAND-2k, while the rest images are from[43]. the performance of traditional FR IQA models such as PSNR, SSIM, and MS-SSIM is remarkably inferior in banding images assessment and is uncorrelated with the MOS, which is consistent with the numerical results presented in TableIV.The reason is that PSNR and SSIM-based methods do perform not well on tiny, regional, and content-independent banding distortion while lacking the consideration for the mechanism of HVS.",
        "sentences": [
          {
            "text": "Fig.9. Visual comparisons of the banding map results.",
            "label": 0
          },
          {
            "text": "From top to bottom are banding images and their corresponding banding maps generated by BBAND[42], DBI[43], and our proposed method, respectively.",
            "label": 1
          },
          {
            "text": "The first five columns of images from left to right are from BAND-2k, while the rest images are from[43].",
            "label": 0
          },
          {
            "text": "the performance of traditional FR IQA models such as PSNR, SSIM, and MS-SSIM is remarkably inferior in banding images assessment and is uncorrelated with the MOS, which is consistent with the numerical results presented in TableIV.",
            "label": 0
          },
          {
            "text": "The reason is that PSNR and SSIM-based methods do perform not well on tiny, regional, and content-independent banding distortion while lacking the consideration for the mechanism of HVS.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment",
        "section": 44,
        "paragraph_id": 44,
        "full_text": "It is also worth mentioning that the deep CNN architecturebased methods (DBCNN[35], HyperIQA[9], and StairIQA[1]), despite performing well on LIVEC[6], KonIQ-10k[29], and other universal image quality evaluation databases[4],[5], underperformed our proposed model by a notable margin on the BAND-2k database.Here we make two arguments to try to explain the observations above: (1) banding exacerbated image quality is intrinsically correlated with the coverage of banding contour.(2) the perception of banding artifacts is explicitly intensity-aware.These are the issues that the CNN-based approaches above do not take into account.To some extent, banding distortion is more like a kind of local distortion than common global distortion such as Gaussian noise, blur, and dither.This suggests that it is potentially valuable to integrate some local texture, edge, contrast, or other visibility-related features into quality prediction models when assessing banding images.Fortunately, with the help of the proposed scoring strategy, our method gains the ability to convert the area range of identified banding regions to indicative annoying scores.",
        "sentences": [
          {
            "text": "It is also worth mentioning that the deep CNN architecturebased methods (DBCNN[35], HyperIQA[9], and StairIQA[1]), despite performing well on LIVEC[6], KonIQ-10k[29], and other universal image quality evaluation databases[4],[5], underperformed our proposed model by a notable margin on the BAND-2k database.",
            "label": 0
          },
          {
            "text": "(2) the perception of banding artifacts is explicitly intensity-aware.",
            "label": 0
          },
          {
            "text": "These are the issues that the CNN-based approaches above do not take into account.",
            "label": 0
          },
          {
            "text": "To some extent, banding distortion is more like a kind of local distortion than common global distortion such as Gaussian noise, blur, and dither.",
            "label": 0
          },
          {
            "text": "This suggests that it is potentially valuable to integrate some local texture, edge, contrast, or other visibility-related features into quality prediction models when assessing banding images.",
            "label": 0
          },
          {
            "text": "Fortunately, with the help of the proposed scoring strategy, our method gains the ability to convert the area range of identified banding regions to indicative annoying scores.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment",
        "section": 45,
        "paragraph_id": 45,
        "full_text": "Surprisingly, the banding IQA methods BBAND[42], CAMBI[41], and VMAF BA[40], however, did not perform very well on the BAND-2k dataset.We infer that this is due to differences in the test environment.First, the source videos selected in CAMBI and VMAF BA are from the internal Netflix 4K catalogue while the source contents in BAND-2k are collected from the public streaming websites with different resolutions, leading to an uneven quality level.Second, the methods of artificially introducing banding distortion are different.In the databases used by CAMBI and VMAF BA , only AV1 and H.264 compression techniques are used to generate the banding artifacts, limiting the adaptive ability of the algorithm in other scenarios.For further investigation, we compare our method with other two methods, i.e., BBAND[42], DBI[43], and visually study the generated banding maps to verify their effectiveness in banding detection.The visualization results are shown in Fig.9, where seven representative banding images are selected for reference.As shown, the banding maps generated from BBAND are mostly disordered and have a lot of discontinuity while the banding maps generated by DBI are too vague to locate the exact location of banding artifacts, making it difficult to develop pixel-level debanding techniques.Overall, we can compendiously conclude that: (1) in comparison with BBAND and DBI, banding maps computed by our proposed method could provide an accurate, clear indication for banding artifacts; (2) in comparison with BBAND and DBI, the quality prediction of banding images guided by our method could achieve a high consistency with HVS.",
        "sentences": [
          {
            "text": "Surprisingly, the banding IQA methods BBAND[42], CAMBI[41], and VMAF BA[40], however, did not perform very well on the BAND-2k dataset.",
            "label": 0
          },
          {
            "text": "We infer that this is due to differences in the test environment.",
            "label": 1
          },
          {
            "text": "First, the source videos selected in CAMBI and VMAF BA are from the internal Netflix 4K catalogue while the source contents in BAND-2k are collected from the public streaming websites with different resolutions, leading to an uneven quality level.",
            "label": 1
          },
          {
            "text": "Second, the methods of artificially introducing banding distortion are different.",
            "label": 1
          },
          {
            "text": "264 compression techniques are used to generate the banding artifacts, limiting the adaptive ability of the algorithm in other scenarios.",
            "label": 1
          },
          {
            "text": "For further investigation, we compare our method with other two methods, i.e., BBAND[42], DBI[43], and visually study the generated banding maps to verify their effectiveness in banding detection.",
            "label": 0
          },
          {
            "text": "The visualization results are shown in Fig.9, where seven representative banding images are selected for reference.",
            "label": 0
          },
          {
            "text": "As shown, the banding maps generated from BBAND are mostly disordered and have a lot of discontinuity while the banding maps generated by DBI are too vague to locate the exact location of banding artifacts, making it difficult to develop pixel-level debanding techniques.",
            "label": 0
          },
          {
            "text": "Overall, we can compendiously conclude that: (1) in comparison with BBAND and DBI, banding maps computed by our proposed method could provide an accurate, clear indication for banding artifacts; (2) in comparison with BBAND and DBI, the quality prediction of banding images guided by our method could achieve a high consistency with HVS.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment",
        "section": 46,
        "paragraph_id": 46,
        "full_text": "Moreover, to make a statistically meaningful comparison among different IQA methods, we further conduct the widely used F-test[2],[3]to assess the statistical significance of the proposed method.Based on the assumption that the model's prediction residuals follow the Gaussian distribution, the lefttailed F-test with a confidence level of 95% is performed on the residuals of every two IQA models.The results of significance tests on the BAND-2k database are shown in Fig.10.A value of '1' (colored in green) indicates that the model in the row is significantly better than the model in the column, while a value of '0' (colored in red) indicates that the model in the row is not significantly better than the model in the column.It is shown that our proposed method performs significantly better than other models, which is consistent with the observations from the above comparison experiments.",
        "sentences": [
          {
            "text": "Moreover, to make a statistically meaningful comparison among different IQA methods, we further conduct the widely used F-test[2],[3]to assess the statistical significance of the proposed method.",
            "label": 0
          },
          {
            "text": "Based on the assumption that the model's prediction residuals follow the Gaussian distribution, the lefttailed F-test with a confidence level of 95% is performed on the residuals of every two IQA models.",
            "label": 0
          },
          {
            "text": "The results of significance tests on the BAND-2k database are shown in Fig.10.",
            "label": 0
          },
          {
            "text": "A value of '1' (colored in green) indicates that the model in the row is significantly better than the model in the column, while a value of '0' (colored in red) indicates that the model in the row is not significantly better than the model in the column.",
            "label": 0
          },
          {
            "text": "It is shown that our proposed method performs significantly better than other models, which is consistent with the observations from the above comparison experiments.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment",
        "section": 47,
        "paragraph_id": 47,
        "full_text": "In this section, we explore the effectiveness of our model's design philosophy.To verify the importance of the dual-branch (DB) architecture, the baseline variants use the same backbone as the proposed method, except that only a single branch (SB) is reserved.Then, we use the original banding images as the input while removing the frequency map generation module (SB-I).Besides, the high-frequency maps and low-frequency maps generation modules are retained respectively (SB-HFM and SB-LFM).It can be observed from Table V that SB-LFM achieved the worst results, since the low-frequency map filters out the highfrequency banding information to a certain extent, which reduces the ability of the model to identify false contours.SB-I performs significantly better than SB-HFM and SB-LFM, resulting from that it contains richer image information, but it is still inferior to our method, which demonstrates the effectiveness of the dualbranch scheme.",
        "sentences": [
          {
            "text": "In this section, we explore the effectiveness of our model's design philosophy.",
            "label": 0
          },
          {
            "text": "To verify the importance of the dual-branch (DB) architecture, the baseline variants use the same backbone as the proposed method, except that only a single branch (SB) is reserved.",
            "label": 0
          },
          {
            "text": "Then, we use the original banding images as the input while removing the frequency map generation module (SB-I).",
            "label": 0
          },
          {
            "text": "Besides, the high-frequency maps and low-frequency maps generation modules are retained respectively (SB-HFM and SB-LFM).",
            "label": 0
          },
          {
            "text": "It can be observed from Table V that SB-LFM achieved the worst results, since the low-frequency map filters out the highfrequency banding information to a certain extent, which reduces the ability of the model to identify false contours.",
            "label": 0
          },
          {
            "text": "SB-I performs significantly better than SB-HFM and SB-LFM, resulting from that it contains richer image information, but it is still inferior to our method, which demonstrates the effectiveness of the dualbranch scheme.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment",
        "section": 48,
        "paragraph_id": 48,
        "full_text": "To investigate the effect of the banding feature extraction, we further design two variants with different input combinations of frequency maps.First, the high-frequency map was taken as the input of both the CN N H and CN N L layers (DB-HFM).Then, we replace the inputs with the low-frequency maps (DB-LFM).As shown in TableV, the performance of DB-HFM and DB-LFM is far apart from our approach, which matches our hypothesis that the high-frequency texture information contained in HFM and the low-frequency background information contained in LFM are crucial to enhance the capacity of discernment for banding artifacts.Therefore, we may conclude that our model is the most suitable model among those compared variants in terms of both banding classification and IQA applications.",
        "sentences": [
          {
            "text": "To investigate the effect of the banding feature extraction, we further design two variants with different input combinations of frequency maps.",
            "label": 0
          },
          {
            "text": "First, the high-frequency map was taken as the input of both the CN N H and CN N L layers (DB-HFM).",
            "label": 0
          },
          {
            "text": "Then, we replace the inputs with the low-frequency maps (DB-LFM).",
            "label": 0
          },
          {
            "text": "As shown in TableV, the performance of DB-HFM and DB-LFM is far apart from our approach, which matches our hypothesis that the high-frequency texture information contained in HFM and the low-frequency background information contained in LFM are crucial to enhance the capacity of discernment for banding artifacts.",
            "label": 0
          },
          {
            "text": "Therefore, we may conclude that our model is the most suitable model among those compared variants in terms of both banding classification and IQA applications.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment",
        "section": 49,
        "paragraph_id": 49,
        "full_text": "Due to the effects of different compression techniques, shooting equipment, scenes, etc., the image content and banding distortions may vary significantly in practical applications.For the database[43], it only includes limited types of image sources and means of triggering banding distortion.As a result, we conduct a cross-database validation to verify the generalizability of the proposed model, wherein the database presented by[43]and BAND-2k are included.That is, we trained the model on one full database and report the test performance on the other.We mainly compare the proposed method with four learningbased models, i.e., DBCNN[35], HyperIQA[9], StairIQA[1], and DBI[43].Since MOS information is not provided in the database[43], which is an essential part of methods training, we condensed part of the experiments.TableVIand VII report the experimental results in terms of patch-level banding classification and image quality assessment.We can observe that our proposed method generalization between database[43]and BAND-2k was surprisingly good.Besides, it is worth noting that the performance of these methods trained on the BAND-2k has improved a little compared to the previous versions that were trained on the database[43], which further demonstrates the superiority of the proposed database BAND-2k.",
        "sentences": [
          {
            "text": "Due to the effects of different compression techniques, shooting equipment, scenes, etc., the image content and banding distortions may vary significantly in practical applications.",
            "label": 0
          },
          {
            "text": "For the database[43], it only includes limited types of image sources and means of triggering banding distortion.",
            "label": 0
          },
          {
            "text": "As a result, we conduct a cross-database validation to verify the generalizability of the proposed model, wherein the database presented by[43]and BAND-2k are included.",
            "label": 1
          },
          {
            "text": "That is, we trained the model on one full database and report the test performance on the other.",
            "label": 1
          },
          {
            "text": "We mainly compare the proposed method with four learningbased models, i.e., DBCNN[35], HyperIQA[9], StairIQA[1], and DBI[43].",
            "label": 0
          },
          {
            "text": "Since MOS information is not provided in the database[43], which is an essential part of methods training, we condensed part of the experiments.",
            "label": 0
          },
          {
            "text": "TableVIand VII report the experimental results in terms of patch-level banding classification and image quality assessment.",
            "label": 0
          },
          {
            "text": "We can observe that our proposed method generalization between database[43]and BAND-2k was surprisingly good.",
            "label": 0
          },
          {
            "text": "Besides, it is worth noting that the performance of these methods trained on the BAND-2k has improved a little compared to the previous versions that were trained on the database[43], which further demonstrates the superiority of the proposed database BAND-2k.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment",
        "section": 50,
        "paragraph_id": 50,
        "full_text": "The efficiency of an image quality prediction model is of great importance in practical industrial deployments.Therefore, we measured the average running time of the compared IQA models, as shown in TableVIII.The experiments were performed in MATLAB R2021a and Python 3.7 under Windows 10 64bit system on a Lenovo laptop with Intel Core i5-9300HF CPU@2.4GHz,16GB RAM, and NVIDIA GTX 1660Ti 6G GPU.It can be observed that the proposed method achieves a reasonable running time among the FR, NR, and other bandingspecified IQA algorithms.Generally, the execution time of classical IQA algorithms is significantly less than learning-based methods.Simpler NSS-based models such as BRISQUE and NIQE still show competitive efficiency relative to CNN models while exhibiting inferior performance in banding image quality assessment.For CAMBI and VMAF BA , we use the officially launched software package, which is based on the stand-alone C library libvmaf and therefore surpasses other methods in speed.Moreover, unlike the general quality evaluation using regression to predict scores, the patch-wise prediction strategy that we adopted may increase the complexity.Note that although we deployed a more complex network structure, a nearly 10 times speedup has been seen when comparing DBI with our method since the sliding window mechanism[43]is removed.",
        "sentences": [
          {
            "text": "The efficiency of an image quality prediction model is of great importance in practical industrial deployments.",
            "label": 1
          },
          {
            "text": "Therefore, we measured the average running time of the compared IQA models, as shown in TableVIII.",
            "label": 0
          },
          {
            "text": "The experiments were performed in MATLAB R2021a and Python 3.7 under Windows 10 64bit system on a Lenovo laptop with Intel Core i5-9300HF CPU@2.4GHz,16GB RAM, and NVIDIA GTX 1660Ti 6G GPU.",
            "label": 0
          },
          {
            "text": "It can be observed that the proposed method achieves a reasonable running time among the FR, NR, and other bandingspecified IQA algorithms.",
            "label": 0
          },
          {
            "text": "Generally, the execution time of classical IQA algorithms is significantly less than learning-based methods.",
            "label": 0
          },
          {
            "text": "Simpler NSS-based models such as BRISQUE and NIQE still show competitive efficiency relative to CNN models while exhibiting inferior performance in banding image quality assessment.",
            "label": 0
          },
          {
            "text": "For CAMBI and VMAF BA , we use the officially launched software package, which is based on the stand-alone C library libvmaf and therefore surpasses other methods in speed.",
            "label": 0
          },
          {
            "text": "Moreover, unlike the general quality evaluation using regression to predict scores, the patch-wise prediction strategy that we adopted may increase the complexity.",
            "label": 0
          },
          {
            "text": "Note that although we deployed a more complex network structure, a nearly 10 times speedup has been seen when comparing DBI with our method since the sliding window mechanism[43]is removed.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment",
        "section": 51,
        "paragraph_id": 51,
        "full_text": "In this paper, we conduct a comprehensive exploration of banding images from both subjective and objective perspectives.Specifically, we construct the largest ecologically valid banding IQA database to date named BAND-2k database, which consists of 2,000 banding images generated by fifteen compression and quantization schemes, achieving several times larger in number and diversity than the existing banding dataset.The construction process of the database, including distortion content preparation, subjective test procedure, and the removal of outlying data, is described in detail in this paper.Relying on this database, we proposed a novel banding evaluator using the frequency characteristic of banding artifacts, which models the banding as high-frequency artifacts that contained in the low-frequency smoothing region.A dual-branch CNN is devised to extract hierarchical features to classify the banding regions, upon which we introduce the spatial frequency masking to refine and compute an overall banding score.Experimental results show that our proposed method outperforms the baseline algorithms significantly in patch-level banding classification and banding IQA tasks.We believe that our study will benefit further development, calibration, and benchmarking of banding IQA models.",
        "sentences": [
          {
            "text": "In this paper, we conduct a comprehensive exploration of banding images from both subjective and objective perspectives.",
            "label": 0
          },
          {
            "text": "Specifically, we construct the largest ecologically valid banding IQA database to date named BAND-2k database, which consists of 2,000 banding images generated by fifteen compression and quantization schemes, achieving several times larger in number and diversity than the existing banding dataset.",
            "label": 0
          },
          {
            "text": "The construction process of the database, including distortion content preparation, subjective test procedure, and the removal of outlying data, is described in detail in this paper.",
            "label": 1
          },
          {
            "text": "Relying on this database, we proposed a novel banding evaluator using the frequency characteristic of banding artifacts, which models the banding as high-frequency artifacts that contained in the low-frequency smoothing region.",
            "label": 1
          },
          {
            "text": "A dual-branch CNN is devised to extract hierarchical features to classify the banding regions, upon which we introduce the spatial frequency masking to refine and compute an overall banding score.",
            "label": 0
          },
          {
            "text": "Experimental results show that our proposed method outperforms the baseline algorithms significantly in patch-level banding classification and banding IQA tasks.",
            "label": 0
          },
          {
            "text": "We believe that our study will benefit further development, calibration, and benchmarking of banding IQA models.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Benchmark dataset and instance generator for Real_World Three_Dimensional Bin Packing Problems",
        "section": 0,
        "paragraph_id": 0,
        "full_text": "Abstract: In this article, a benchmark for real-world bin packing problems is proposed.This dataset consists of 12 instances of varying levels of complexity regarding size (with the number of packages ranging from 38 to 53) and user-defined requirements.In fact, several real-world-oriented restrictions were taken into account to build these instances: i) item and bin dimensions, ii) weight restrictions, iii) affinities among package categories iv) preferences for package ordering and v) load balancing.Besides the data, we also offer an own developed Python script for the dataset generation, coined Q4RealBPP-DataGen.The benchmark was initially proposed to evaluate the performance of quantum solvers.Therefore, the characteristics of this set of instances were designed according to the current limitations of quantum devices.Additionally, the dataset generator is included to allow the construction of general-purpose benchmarks.The data introduced in this article provides a baseline that will encourage quantum computing researchers to work on real-world bin packing problems.",
        "sentences": [
          {
            "text": "Abstract: In this article, a benchmark for real-world bin packing problems is proposed.",
            "label": 0
          },
          {
            "text": "This dataset consists of 12 instances of varying levels of complexity regarding size (with the number of packages ranging from 38 to 53) and user-defined requirements.",
            "label": 1
          },
          {
            "text": "In fact, several real-world-oriented restrictions were taken into account to build these instances: i) item and bin dimensions, ii) weight restrictions, iii) affinities among package categories iv) preferences for package ordering and v) load balancing.",
            "label": 1
          },
          {
            "text": "Besides the data, we also offer an own developed Python script for the dataset generation, coined Q4RealBPP-DataGen.",
            "label": 1
          },
          {
            "text": "The benchmark was initially proposed to evaluate the performance of quantum solvers.",
            "label": 0
          },
          {
            "text": "Therefore, the characteristics of this set of instances were designed according to the current limitations of quantum devices.",
            "label": 1
          },
          {
            "text": "Additionally, the dataset generator is included to allow the construction of general-purpose benchmarks.",
            "label": 1
          },
          {
            "text": "The data introduced in this article provides a baseline that will encourage quantum computing researchers to work on real-world bin packing problems.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Benchmark dataset and instance generator for Real_World Three_Dimensional Bin Packing Problems",
        "section": 1,
        "paragraph_id": 1,
        "full_text": "The whole benchmark has been generated using Q4RealBPP-DataGen, an automatic instance generator developed ad hoc for this research.The generator, implemented in Python 3.9, automatically saves the instance files in .txtformat.The packages that compose each solution are generated following the size distribution proposed in[1].",
        "sentences": [
          {
            "text": "The whole benchmark has been generated using Q4RealBPP-DataGen, an automatic instance generator developed ad hoc for this research.",
            "label": 1
          },
          {
            "text": "The generator, implemented in Python 3.9, automatically saves the instance files in .txtformat.",
            "label": 1
          },
          {
            "text": "The packages that compose each solution are generated following the size distribution proposed in[1].",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Benchmark dataset and instance generator for Real_World Three_Dimensional Bin Packing Problems",
        "section": 2,
        "paragraph_id": 2,
        "full_text": "The data has been generated in a laboratory environment through the Q4RealBPP-DataGen script.The dataset is useful to validate solvers against industrial use cases: item sizes are compliant with the proposal presented in[1], and a list of real-world oriented requirements is specified (activated or deactivated) for further analysis on problem complexity.",
        "sentences": [
          {
            "text": "The data has been generated in a laboratory environment through the Q4RealBPP-DataGen script.",
            "label": 1
          },
          {
            "text": "The dataset is useful to validate solvers against industrial use cases: item sizes are compliant with the proposal presented in[1], and a list of real-world oriented requirements is specified (activated or deactivated) for further analysis on problem complexity.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Benchmark dataset and instance generator for Real_World Three_Dimensional Bin Packing Problems",
        "section": 3,
        "paragraph_id": 3,
        "full_text": "Furthermore, the Q4RealBPP-DataGen data generator is also provided, which allows the user to create new instances to enrich the evaluation with customized use cases.",
        "sentences": [
          {
            "text": "Furthermore, the Q4RealBPP-DataGen data generator is also provided, which allows the user to create new instances to enrich the evaluation with customized use cases.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Benchmark dataset and instance generator for Real_World Three_Dimensional Bin Packing Problems",
        "section": 4,
        "paragraph_id": 4,
        "full_text": "The data has been synthetically generated by means of the generator in a laboratory located in TECNALIA, Basque Research and Technology Alliance (BRTA), 48160 Derio, Bizkaia, Spain.The information contained in the benchmark instances has no geographic reference.",
        "sentences": [
          {
            "text": "The data has been synthetically generated by means of the generator in a laboratory located in TECNALIA, Basque Research and Technology Alliance (BRTA), 48160 Derio, Bizkaia, Spain.",
            "label": 1
          },
          {
            "text": "The information contained in the benchmark instances has no geographic reference.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Benchmark dataset and instance generator for Real_World Three_Dimensional Bin Packing Problems",
        "section": 5,
        "paragraph_id": 5,
        "full_text": "The whole dataset and the Q4RealBPP-DataGen generator are available in a Mendeley public repository: Repository Name: Benchmark dataset and instance generator for Real-World 3dBPP.Data identification number: doi: 10.17632/y258s6d939.2Direct URL to data: http://dx.doi.org/10.17632/y258s6d939.2",
        "sentences": [
          {
            "text": "The whole dataset and the Q4RealBPP-DataGen generator are available in a Mendeley public repository: Repository Name: Benchmark dataset and instance generator for Real-World 3dBPP.",
            "label": 1
          },
          {
            "text":"Data identification number: doi: 10.17632/y258s6d939.2Direct URL to data: http://dx.doi.org/10.17632/y258s6d939.2",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Benchmark dataset and instance generator for Real_World Three_Dimensional Bin Packing Problems",
        "section": 6,
        "paragraph_id": 6,
        "full_text": "• The dataset includes 12 instances of the three-dimensional Bin Packing Problem (3dBPP,[2]).All the packages that compose each instance have been randomly generated using our own instance generator Q4RealBPP-DataGen to avoid any bias.The benchmark is useful for measuring the performance of solvers developed for the same purpose, especially if the solvers rely on a Quantum Processing Unit (QPU).",
        "sentences": [
          {
            "text": "• The dataset includes 12 instances of the three-dimensional Bin Packing Problem (3dBPP,[2]).",
            "label": 1
          },
          {
            "text": "All the packages that compose each instance have been randomly generated using our own instance generator Q4RealBPP-DataGen to avoid any bias.",
            "label": 1
          },
          {
            "text": "The benchmark is useful for measuring the performance of solvers developed for the same purpose, especially if the solvers rely on a Quantum Processing Unit (QPU).",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Benchmark dataset and instance generator for Real_World Three_Dimensional Bin Packing Problems",
        "section": 7,
        "paragraph_id": 7,
        "full_text": "• Along with the instances, the benchmark also includes a Python script to generate synthetic datasets for the problem.With this generator, researchers can create their own instances for benchmarking purposes.",
        "sentences": [
          {
            "text": "• Along with the instances, the benchmark also includes a Python script to generate synthetic datasets for the problem.",
            "label": 1
          },
          {
            "text": "With this generator, researchers can create their own instances for benchmarking purposes.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Benchmark dataset and instance generator for Real_World Three_Dimensional Bin Packing Problems",
        "section": 8,
        "paragraph_id": 8,
        "full_text": "• Classical Bin Packing related benchmarks are usually composed of large instances, containing few small-sized cases (if any)[3].Falkenauer U or Schwerin datasets are wellknown examples that confirm this situation, in which smallest instances count with 120 and 100 items.For this reason, researchers working in the quantum computing field can specially benefit from the benchmark proposed in this data article.This is so because of the size of each instance, which is adapted to be solved with current quantum devices.",
        "sentences": [
          {
            "text": "• Classical Bin Packing related benchmarks are usually composed of large instances, containing few small-sized cases (if any)[3].",
            "label": 0
          },
          {
            "text": "Falkenauer U or Schwerin datasets are wellknown examples that confirm this situation, in which smallest instances count with 120 and 100 items.",
            "label": 0
          },
          {
            "text": "For this reason, researchers working in the quantum computing field can specially benefit from the benchmark proposed in this data article.",
            "label": 0
          },
          {
            "text": "This is so because of the size of each instance, which is adapted to be solved with current quantum devices.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Benchmark dataset and instance generator for Real_World Three_Dimensional Bin Packing Problems",
        "section": 9,
        "paragraph_id": 9,
        "full_text": "• Both the instances and the data generator are open source, so they can be modified or extended to other Bin Packing Problem variants[4]with the aim of pushing forward the research in this field.",
        "sentences": [
          {
            "text": "• Both the instances and the data generator are open source, so they can be modified or extended to other Bin Packing Problem variants[4]with the aim of pushing forward the research in this field.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Benchmark dataset and instance generator for Real_World Three_Dimensional Bin Packing Problems",
        "section": 10,
        "paragraph_id": 10,
        "full_text": "• The benchmark also includes the results obtained in each instance using the Leap Constrained Quadratic Model Hybrid Solver of D-Wave (LeapCQMHybrid,[5]).These results are provided in both image and text format.",
        "sentences": [
          {
            "text": "• The benchmark also includes the results obtained in each instance using the Leap Constrained Quadratic Model Hybrid Solver of D-Wave (LeapCQMHybrid,[5]).",
            "label": 1
          },
          {
            "text": "These results are provided in both image and text format.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Benchmark dataset and instance generator for Real_World Three_Dimensional Bin Packing Problems",
        "section": 11,
        "paragraph_id": 11,
        "full_text": "The benchmark described in this data article provides 12 instances of real-world oriented 3dBPP scenarios.To properly characterize these realistic industrial use cases, the following requirements have been taken into consideration: i) overweight restrictions, ii) affinities among package categories, iii) preferences in relative positioning and iv) load balancing.This is the first quantumcomputing oriented benchmark for dealing with the real-world 3dBPP.This is so, because of the sizes of the generated instances, which are adapted to the capacities of current quantum devices.Additionally, there is no benchmark in the literature that addresses all the characteristics covered in this study.In addition to the data provided, and equally important, we present a data generation script, coined Q4RealBPP-DataGen, to create new instances.",
        "sentences": [
          {
            "text": "The benchmark described in this data article provides 12 instances of real-world oriented 3dBPP scenarios.",
            "label": 1
          },
          {
            "text": "To properly characterize these realistic industrial use cases, the following requirements have been taken into consideration: i) overweight restrictions, ii) affinities among package categories, iii) preferences in relative positioning and iv) load balancing.",
            "label": 1
          },
          {
            "text": "This is the first quantumcomputing oriented benchmark for dealing with the real-world 3dBPP.",
            "label": 0
          },
          {
            "text": "This is so, because of the sizes of the generated instances, which are adapted to the capacities of current quantum devices.",
            "label": 1
          },
          {
            "text": "Additionally, there is no benchmark in the literature that addresses all the characteristics covered in this study.",
            "label": 0
          },
          {
            "text": "In addition to the data provided, and equally important, we present a data generation script, coined Q4RealBPP-DataGen, to create new instances.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Benchmark dataset and instance generator for Real_World Three_Dimensional Bin Packing Problems",
        "section": 12,
        "paragraph_id": 12,
        "full_text": "The dataset consists of 12 instances for the 3dBPP, each one considering different real-world oriented restrictions.These are the constraints introduced in the benchmark: • Item and bin dimensions: being a three-dimensional problem, packages and bins have an associated length, width, and height, representing dimensions ,  and , respectively.Items stored in a bin must not exceed its capacity in terms of dimensions, and all the bins in the same instance have the same predefined [, , ] dimensions. • Overweight restrictions: each item has an associated weight, and bins have a maximum capacity.This restriction requires that the total weight of the stored items assigned to a bin not exceed its maximum capacity. • Affinities among package categories: this restriction introduces positive and negative affinities (incompatibilities) among item categories.This means that items that share a positive affinity must be packed together, while incompatible packages must be assigned to different bins. • Preferences in relative positioning: relative positioning lets the user establish a sorting strategy by package-category location in given axis.For instance, load-bearing must govern the placement of the items with respect to the -axis.For the sake of simplicity, this could be attained by applying a simple rule: sort the packages based on the mass ratio between packages to decide what item should rest on which one.Anyway, these preferences can accommodate other positioning patterns, such as sorting in -axis according to the delivery schedule. • Load balancing: center of mass to distribute the stored items according to one reference point.",
        "sentences": [
          {
            "text": "The dataset consists of 12 instances for the 3dBPP, each one considering different real-world oriented restrictions.",
            "label": 1
          },
          {
            "text": "These are the constraints introduced in the benchmark: • Item and bin dimensions: being a three-dimensional problem, packages and bins have an associated length, width, and height, representing dimensions ,  and , respectively.",
            "label": 1
          },
          {
            "text": "Items stored in a bin must not exceed its capacity in terms of dimensions, and all the bins in the same instance have the same predefined [, , ] dimensions.",
            "label": 1
          },
          {
            "text": "• Overweight restrictions: each item has an associated weight, and bins have a maximum capacity.",
            "label": 1
          },
          {
            "text": "This restriction requires that the total weight of the stored items assigned to a bin not exceed its maximum capacity.",
            "label": 1
          },
          {
            "text": "• Affinities among package categories: this restriction introduces positive and negative affinities (incompatibilities) among item categories.",
            "label": 1
          },
          {
            "text": "This means that items that share a positive affinity must be packed together, while incompatible packages must be assigned to different bins.",
            "label": 1
          },
          {
            "text": "• Preferences in relative positioning: relative positioning lets the user establish a sorting strategy by package-category location in given axis.",
            "label": 1
          },
          {
            "text": "For instance, load-bearing must govern the placement of the items with respect to the -axis.",
            "label": 1
          },
          {
            "text": "For the sake of simplicity, this could be attained by applying a simple rule: sort the packages based on the mass ratio between packages to decide what item should rest on which one.",
            "label": 1
          },
          {
            "text": "Anyway, these preferences can accommodate other positioning patterns, such as sorting in -axis according to the delivery schedule.",
            "label": 1
          },
          {
            "text": "• Load balancing: center of mass to distribute the stored items according to one reference point.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Benchmark dataset and instance generator for Real_World Three_Dimensional Bin Packing Problems",
        "section": 13,
        "paragraph_id": 13,
        "full_text": "It should be noted that the units of measurement have not been specified as they are not relevant for the study.In search of instances that maximize the difference in performance, each instance has its own particularities, which are summarized in Table1.Also, Table2describes in detail each instance.Regarding the format of each instance, for the sake of clarity, we depict in Figure3the structure of the 3dBPP_11 instance.This format is an evolution of the one proposed by D-Wave in[6], which in fact served as inspiration for our work.Thus, to build an instance, eight different characteristics should be considered.Table3lists these features.",
        "sentences": [
          {
            "text": "It should be noted that the units of measurement have not been specified as they are not relevant for the study.",
            "label": 1
          },
          {
            "text": "In search of instances that maximize the difference in performance, each instance has its own particularities, which are summarized in Table1.",
            "label": 1
          },
          {
            "text": "Also, Table2describes in detail each instance.",
            "label": 1
          },
          {
            "text": "Regarding the format of each instance, for the sake of clarity, we depict in Figure3the structure of the 3dBPP_11 instance.",
            "label": 1
          },
          {
            "text": "This format is an evolution of the one proposed by D-Wave in[6], which in fact served as inspiration for our work.",
            "label": 1
          },
          {
            "text": "Thus, to build an instance, eight different characteristics should be considered.",
            "label": 1
          },
          {
            "text": "Table3lists these features.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Benchmark dataset and instance generator for Real_World Three_Dimensional Bin Packing Problems",
        "section": 14,
        "paragraph_id": 14,
        "full_text": "It should be highlighted at this point that, given the previous settings for package definition, and for the sake of simplicity, the constraints are imposed on the item's IDs, which means that the rules described by the constraints apply to all items with the same ID.If users preferred package level assignments, they would have to simply create a dedicated ID for each package.",
        "sentences": [
          {
            "text": "It should be highlighted at this point that, given the previous settings for package definition, and for the sake of simplicity, the constraints are imposed on the item's IDs, which means that the rules described by the constraints apply to all items with the same ID.",
            "label": 1
          },
          {
            "text": "If users preferred package level assignments, they would have to simply create a dedicated ID for each package.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Benchmark dataset and instance generator for Real_World Three_Dimensional Bin Packing Problems",
        "section": 15,
        "paragraph_id": 15,
        "full_text": "This value is represented as a dictionary [: ], in which  stands for the relative positioning that the pairs of integers comprised in list  must follow.As an example, {6: (5,1) (2,1)} means that packages with ID=5 and ID=2 must have the relative position  = 6 regarding items with ID=1.In this regard, and for the sake of understandability,  = 1 represents \"at the left\";  = 2 stands for \"behind\",  = 3 is \"below\",  = 4 depicts \"at the right\",  = 5 means \"in front\", and  = 6 represents \"above\".",
        "sentences": [
          {
            "text": "This value is represented as a dictionary [: ], in which  stands for the relative positioning that the pairs of integers comprised in list  must follow.",
            "label": 1
          },
          {
            "text": "As an example, {6: (5,1) (2,1)} means that packages with ID=5 and ID=2 must have the relative position  = 6 regarding items with ID=1.",
            "label": 1
          },
          {
            "text": "In this regard, and for the sake of understandability,  = 1 represents \"at the left\";  = 2 stands for \"behind\",  = 3 is \"below\",  = 4 depicts \"at the right\",  = 5 means \"in front\", and  = 6 represents \"above\".",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Benchmark dataset and instance generator for Real_World Three_Dimensional Bin Packing Problems",
        "section": 18,
        "paragraph_id": 18,
        "full_text": "Pair of integers This pair of integers are introduced for load balancing purposes, and they represent the X and Y coordinates in which the items should gravitate.",
        "sentences": [
          {
            "text": "Pair of integers This pair of integers are introduced for load balancing purposes, and they represent the X and Y coordinates in which the items should gravitate.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Benchmark dataset and instance generator for Real_World Three_Dimensional Bin Packing Problems",
        "section": 19,
        "paragraph_id": 19,
        "full_text": "List of items This list has an entry for each item category available.For each category, six different values should be introduced: the category ID, the number of packages for each category, and the length, width, height, and weight of all the packages in the category.All these values must be integers.",
        "sentences": [
          {
            "text": "List of items This list has an entry for each item category available.",
            "label": 1
          },
          {
            "text": "For each category, six different values should be introduced: the category ID, the number of packages for each category, and the length, width, height, and weight of all the packages in the category.",
            "label": 1
          },
          {
            "text": "All these values must be integers.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Benchmark dataset and instance generator for Real_World Three_Dimensional Bin Packing Problems",
        "section": 20,
        "paragraph_id": 20,
        "full_text": "The main contribution of the benchmark proposed in this data article is twofold.First, as mentioned before, thanks to the sizes of the generated instances, this is the first quantum-oriented benchmark for solving the 3dBPP.Delving deeper into this aspect, quantum optimization has generated a significant impact in the scientific community.The advances made in the related hardware and the democratization of its access have contributed to the promotion of this scientific area.Anyway, research is restricted by the status of the hardware.There are some limitations on current quantum computers that have a negative impact on their performance.The current state of quantum computing is known as the noisy intermediate-scale quantum (NISQ,[7]) era.Quantum devices available in this NISQ era are distinguished by not being fully able to tackle large problems reliably.The evaluation of quantum or hybrid approaches is hampered by this condition, due to the fact that researchers are pushed to build ad-hoc problem instances adapted to the limited capacity of quantum computers.This holds true even when tackling well-known optimization problems, and this circumstance has a direct impact on the capacity to replicate and compare different techniques.More specifically, and focusing on the 3dBPP, the LeapCQMHybrid solver of D-Wave, which is one of the most powerful quantum solvers currently available, struggles when dealing with instances composed of more than 75 packages, making the existing datasets not practical for dealing with quantum devices.For this reason, we present in this data article a common-use benchmark for the 3dBPP approachable by the different quantum computers available, and that facilitates the comparison and replicability of the newly proposed methods in the field of quantum optimization.",
        "sentences": [
          {
            "text": "The main contribution of the benchmark proposed in this data article is twofold.",
            "label": 1
          },
          {
            "text": "First, as mentioned before, thanks to the sizes of the generated instances, this is the first quantum-oriented benchmark for solving the 3dBPP.",
            "label": 0
          },
          {
            "text": "Delving deeper into this aspect, quantum optimization has generated a significant impact in the scientific community.",
            "label": 0
          },
          {
            "text": "The advances made in the related hardware and the democratization of its access have contributed to the promotion of this scientific area.",
            "label": 0
          },
          {
            "text": "Anyway, research is restricted by the status of the hardware.",
            "label": 0
          },
          {
            "text": "There are some limitations on current quantum computers that have a negative impact on their performance.",
            "label": 0
          },
          {
            "text": "The current state of quantum computing is known as the noisy intermediate-scale quantum (NISQ,[7]) era.",
            "label": 0
          },
          {
            "text": "Quantum devices available in this NISQ era are distinguished by not being fully able to tackle large problems reliably.",
            "label": 0
          },
          {
            "text": "The evaluation of quantum or hybrid approaches is hampered by this condition, due to the fact that researchers are pushed to build ad-hoc problem instances adapted to the limited capacity of quantum computers.",
            "label": 0
          },
          {
            "text": "This holds true even when tackling well-known optimization problems, and this circumstance has a direct impact on the capacity to replicate and compare different techniques.",
            "label": 0
          },
          {
            "text": "More specifically, and focusing on the 3dBPP, the LeapCQMHybrid solver of D-Wave, which is one of the most powerful quantum solvers currently available, struggles when dealing with instances composed of more than 75 packages, making the existing datasets not practical for dealing with quantum devices.",
            "label": 0
          },
          {
            "text": "For this reason, we present in this data article a common-use benchmark for the 3dBPP approachable by the different quantum computers available, and that facilitates the comparison and replicability of the newly proposed methods in the field of quantum optimization.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Benchmark dataset and instance generator for Real_World Three_Dimensional Bin Packing Problems",
        "section": 21,
        "paragraph_id": 21,
        "full_text": "Secondly, most of the 3dBPP instances that can be openly found in the literature are usually focused on basic variants of the problem, considering just the dimension and weight restrictions 1,2 .",
        "sentences": [
          {
            "text": "Secondly, most of the 3dBPP instances that can be openly found in the literature are usually focused on basic variants of the problem, considering just the dimension and weight restrictions 1,2 .",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Benchmark dataset and instance generator for Real_World Three_Dimensional Bin Packing Problems",
        "section": 22,
        "paragraph_id": 22,
        "full_text": "In this benchmark, affinities among package categories, preferences for package ordering, and load balancing are considered.Also, thanks to the developed Q4RealBPP-DataGen, users can generate tailored instances by activating/deactivating constraints suitable for their preferences.",
        "sentences": [
          {
            "text": "In this benchmark, affinities among package categories, preferences for package ordering, and load balancing are considered.",
            "label": 0
          },
          {
            "text": "Also, thanks to the developed Q4RealBPP-DataGen, users can generate tailored instances by activating/deactivating constraints suitable for their preferences.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Benchmark dataset and instance generator for Real_World Three_Dimensional Bin Packing Problems",
        "section": 24,
        "paragraph_id": 24,
        "full_text": "The whole benchmark described in this data article has been built using an ad-hoc Python script (named Q4RealBPP-DataGen).Thanks to this script, a user can easily generate additional instances compliant with what is exposed in this article.Q4RealBPP-DataGen gives the user the possibility of taking a pre-computed pool of packages (openly available at https://github.com/Wadaboa/3d-bpp)or creating a new set of items from scratch (following the criteria described in[1]).To do so, these parameters have to be set accordingly: _, _, _ℎ -_ℎ, _ℎ -_ℎ, _ℎℎ -_ℎℎ, and _ℎ -_ℎ.The rest of the problem is characterised by the following parameters: _, _,  -_, _, _, __, and .",
        "sentences": [
          {
            "text": "The whole benchmark described in this data article has been built using an ad-hoc Python script (named Q4RealBPP-DataGen).",
            "label": 1
          },
          {
            "text": "Thanks to this script, a user can easily generate additional instances compliant with what is exposed in this article.",
            "label": 1
          },
          {
            "text": "Q4RealBPP-DataGen gives the user the possibility of taking a pre-computed pool of packages (openly available at https://github.com/Wadaboa/3d-bpp)or creating a new set of items from scratch (following the criteria described in[1]).",
            "label": 1
          },
          {
            "text": "To do so, these parameters have to be set accordingly: _, _, _ℎ -_ℎ, _ℎ -_ℎ, _ℎℎ -_ℎℎ, and _ℎ -_ℎ.",
            "label": 1
          },
          {
            "text": "The rest of the problem is characterised by the following parameters: _, _,  -_, _, _, __, and .",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Benchmark dataset and instance generator for Real_World Three_Dimensional Bin Packing Problems",
        "section": 25,
        "paragraph_id": 25,
        "full_text": "Note that incompatibilities and positive affinities are randomly generated, and it is only required for the user to indicate the number of constraints of this nature in Q4RealBPP-DataGen.However, this script is conceived to alleviate the creation of a benchmark, and it is not compulsory.Contrarily, a user could opt for manually defining the specific characteristics of the instance directly on a file to enrich the diversity.If the user wants to make use of the load-bearing constraint, he/she should specify the order manually in the dictionary   for  = 6 or take advantage of _ parameter shortcut, which will help fill automatically the list in the dictionary.",
        "sentences": [
          {
            "text": "Note that incompatibilities and positive affinities are randomly generated, and it is only required for the user to indicate the number of constraints of this nature in Q4RealBPP-DataGen.",
            "label": 1
          },
          {
            "text": "However, this script is conceived to alleviate the creation of a benchmark, and it is not compulsory.",
            "label": 1
          },
          {
            "text": "Contrarily, a user could opt for manually defining the specific characteristics of the instance directly on a file to enrich the diversity.",
            "label": 1
          },
          {
            "text": "If the user wants to make use of the load-bearing constraint, he/she should specify the order manually in the dictionary   for  = 6 or take advantage of _ parameter shortcut, which will help fill automatically the list in the dictionary.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Benchmark dataset and instance generator for Real_World Three_Dimensional Bin Packing Problems",
        "section": 26,
        "paragraph_id": 26,
        "full_text": "As a summary, in order to properly use Q4RealBPP-DataGen, the parameters described in Table4should be considered.",
        "sentences": [
          {
            "text": "As a summary, in order to properly use Q4RealBPP-DataGen, the parameters described in Table4should be considered.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Benchmark dataset and instance generator for Real_World Three_Dimensional Bin Packing Problems",
        "section": 27,
        "paragraph_id": 27,
        "full_text": "If _ = , the maximum and minimum width for each package category.",
        "sentences": [
          {
            "text": "If _ = , the maximum and minimum width for each package category.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Benchmark dataset and instance generator for Real_World Three_Dimensional Bin Packing Problems",
        "section": 28,
        "paragraph_id": 28,
        "full_text": "If _ = , the maximum and minimum length for each package category.",
        "sentences": [
          {
            "text": "If _ = , the maximum and minimum length for each package category.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Benchmark dataset and instance generator for Real_World Three_Dimensional Bin Packing Problems",
        "section": 33,
        "paragraph_id": 33,
        "full_text": "Finally, with the intention of demonstrating the functionality of Q4RealBPP-DataGen, Figure3shows an example of the parameterization of the script, and the instance generated after running it.Also, we show in Figure4a possible solution for the instances generated for showcasing purposes.We share this instance as well as its solution in the dedicated repository (http://dx.doi.org/10.17632/y258s6d939.1),labeled as 3dBPP_test.",
        "sentences": [
          {
            "text": "Finally, with the intention of demonstrating the functionality of Q4RealBPP-DataGen, Figure3shows an example of the parameterization of the script, and the instance generated after running it.",
            "label": 1
          },
          {
            "text": "Also, we show in Figure4a possible solution for the instances generated for showcasing purposes.",
            "label": 1
          },
          {
            "text": "We share this instance as well as its solution in the dedicated repository (http://dx.doi.org/10.17632/y258s6d939.1),labeled as 3dBPP_test.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "BERT is not The Count_ Learning to Match Mathematical Statements with Proofs",
        "section": 0,
        "paragraph_id": 0,
        "full_text": "Abstract: We introduce a task consisting in matching a proof to a given mathematical statement.The task fits well within current research on Mathematical Information Retrieval and, more generally, mathematical article analysis (Mathematical Sciences, 2014).We present a dataset for the task (the MATCH dataset) consisting of over 180k statement-proof pairs extracted from modern mathematical research articles. 1We find this dataset highly representative of our task, as it consists of relatively new findings useful to mathematicians.We propose a bilinear similarity model and two decoding methods to match statements to proofs effectively.While the first decoding method matches a proof to a statement without being aware of other statements or proofs, the second method treats the task as a global matching problem.Through a symbol replacement procedure, we analyze the \"insights\" that pretrained language models have in such mathematical article analysis and show that while these models perform well on this task with the best performing mean reciprocal rank of 73.7, they follow a relatively shallow symbolic analysis and matching to achieve that performance. 2* Work mostly done at the University of Edinburgh. 1 Our dataset and code are available at https:// github.com/waylonli/MATcH.",
        "sentences": [
          {
            "text": "Abstract: We introduce a task consisting in matching a proof to a given mathematical statement.",
            "label": 0
          },
          {
            "text": "The task fits well within current research on Mathematical Information Retrieval and, more generally, mathematical article analysis (Mathematical Sciences, 2014).",
            "label": 0
          },
          {
            "text": "We present a dataset for the task (the MATCH dataset) consisting of over 180k statement-proof pairs extracted from modern mathematical research articles.",
            "label": 1
          },
          {
            "text": "1We find this dataset highly representative of our task, as it consists of relatively new findings useful to mathematicians.",
            "label": 1
          },
          {
            "text": "We propose a bilinear similarity model and two decoding methods to match statements to proofs effectively.",
            "label": 0
          },
          {
            "text": "While the first decoding method matches a proof to a statement without being aware of other statements or proofs, the second method treats the task as a global matching problem.",
            "label": 0
          },
          {
            "text": "Through a symbol replacement procedure, we analyze the \"insights\" that pretrained language models have in such mathematical article analysis and show that while these models perform well on this task with the best performing mean reciprocal rank of 73.7, they follow a relatively shallow symbolic analysis and matching to achieve that performance.",
            "label": 0
          },
          {
            "text": " 2* Work mostly done at the University of Edinburgh.",
            "label": 0
          },
          {
            "text": " 1 Our dataset and code are available at https:// github.com/waylonli/MATcH.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "BERT is not The Count_ Learning to Match Mathematical Statements with Proofs",
        "section": 1,
        "paragraph_id": 1,
        "full_text": "2 Like Bert, The Count (or Count von Count; ) is a character from the television show Sesame Street.The Count likes counting, and his main role in the show is to teach this skill to children.",
        "sentences": [
          {
            "text": "2 Like Bert, The Count (or Count von Count; ) is a character from the television show Sesame Street.",
            "label": 0
          },
          {
            "text": "The Count likes counting, and his main role in the show is to teach this skill to children.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "BERT is not The Count_ Learning to Match Mathematical Statements with Proofs",
        "section": 2,
        "paragraph_id": 2,
        "full_text": "Research-level mathematical discourse is a challenging domain for Natural Language Processing (NLP).Mathematical articles frequently switch between natural language and mathematical formulae, and a semantic analysis of mathematical text needs to solve relationships (e.g.coreference) between mathematical symbols and concepts.Moreover, mathematical writing follows many conventions, Statement.When m = 0 we have E 0 rg = ∅, and when m = 0 we have E 0 rg = E 0 .Proof.When m = 0, the image of r is {1}.Hence E 0 rg = ∅.When m = 0, the map r is a surjective proper map.Hence E 0 rg = E 0 .such as variable naming or typography that are implicit, and may differ between subfields.",
        "sentences": [
          {
            "text": "Research-level mathematical discourse is a challenging domain for Natural Language Processing (NLP).",
            "label": 0
          },
          {
            "text": "Mathematical articles frequently switch between natural language and mathematical formulae, and a semantic analysis of mathematical text needs to solve relationships (e.g.coreference) between mathematical symbols and concepts.",
            "label": 0
          },
          {
            "text": "Moreover, mathematical writing follows many conventions, Statement.",
            "label": 0
          },
          {
            "text": "When m = 0 we have E 0 rg = ∅, and when m = 0 we have E 0 rg = E 0 .Proof.",
            "label": 0
          },
          {
            "text": "When m = 0, the image of r is {1}.Hence E 0 rg = ∅.When m = 0, the map r is a surjective proper map.Hence E 0 rg = E 0 .such as variable naming or typography that are implicit, and may differ between subfields.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "BERT is not The Count_ Learning to Match Mathematical Statements with Proofs",
        "section": 3,
        "paragraph_id": 3,
        "full_text": "However, mathematical research can benefit from NLP(Mathematical Sciences, 2014), in particular as concerns bibliographical research: researchers need tools to find work relevant to their research.Indeed, prior NLP work on mathematical research articles focused on Mathematical Information Retrieval (MIR) and related tools or data(Zanibbi et al., 2016;Stathopoulos andTeufel, 2016, 2015).",
        "sentences": [
          {
            "text": "However, mathematical research can benefit from NLP(Mathematical Sciences, 2014), in particular as concerns bibliographical research: researchers need tools to find work relevant to their research.",
            "label": 0
          },
          {
            "text": "Indeed, prior NLP work on mathematical research articles focused on Mathematical Information Retrieval (MIR) and related tools or data(Zanibbi et al., 2016;Stathopoulos andTeufel, 2016, 2015).",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "BERT is not The Count_ Learning to Match Mathematical Statements with Proofs",
        "section": 4,
        "paragraph_id": 4,
        "full_text": "We introduce a task aimed at improving the processing of research-level mathematical articles and make a step towards the modeling of mathematical reasoning.Given a collection of mathematical statements and a collection of mathematical proofs of the same size, the task consists in finding and assigning a proof to each mathematical statement.We construct and release a dataset for the task (MATCH), by collecting over 180k statementproof pairs from mathematical research articles (an example is given in Figure1).",
        "sentences": [
          {
            "text": "We introduce a task aimed at improving the processing of research-level mathematical articles and make a step towards the modeling of mathematical reasoning.",
            "label": 0
          },
          {
            "text": "Given a collection of mathematical statements and a collection of mathematical proofs of the same size, the task consists in finding and assigning a proof to each mathematical statement.",
            "label": 0
          },
          {
            "text": "We construct and release a dataset for the task (MATCH), by collecting over 180k statementproof pairs from mathematical research articles (an example is given in Figure1).",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "BERT is not The Count_ Learning to Match Mathematical Statements with Proofs",
        "section": 5,
        "paragraph_id": 5,
        "full_text": "Related datasets, such as LEANSTEP(Han et al., 2021)and the synthetic dataset ofPolu and Sutskever (2020)do not include natural language.NaturalProofs(Welleck et al., 2021), another related dataset, only consists of 32k theorem-proof pairs from ProofWiki, 3 some sub-topics in algebraic geometry and two textbooks.Our dataset is over five times larger and contains pairs extracted from advanced academic mathematical papers.",
        "sentences": [
          {
            "text": "Related datasets, such as LEANSTEP(Han et al., 2021)and the synthetic dataset ofPolu and Sutskever (2020)do not include natural language.",
            "label": 1
          },
          {
            "text": "NaturalProofs(Welleck et al., 2021), another related dataset, only consists of 32k theorem-proof pairs from ProofWiki, 3 some sub-topics in algebraic geometry and two textbooks.",
            "label": 1
          },
          {
            "text": "Our dataset is over five times larger and contains pairs extracted from advanced academic mathematical papers.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "BERT is not The Count_ Learning to Match Mathematical Statements with Proofs",
        "section": 6,
        "paragraph_id": 6,
        "full_text": "There are multiple motivations for the design of the task and our dataset.We believe it may help MIR by serving as a proxy for the search for the existence of a mathematical result, or for theorems and proofs related to one another (e.g. using the same proof technique), an important search tool for any digital mathematical library(Mathematical Sciences, 2014).Learning to match statements and proofs would also benefit computer-assisted theorem proving, as it is akin to tasks such as premise selection, also recently addressed with NLP methods(Piotrowski and Urban, 2019).",
        "sentences": [
          {
            "text": "There are multiple motivations for the design of the task and our dataset.",
            "label": 0
          },
          {
            "text": "We believe it may help MIR by serving as a proxy for the search for the existence of a mathematical result, or for theorems and proofs related to one another (e.g. using the same proof technique), an important search tool for any digital mathematical library(Mathematical Sciences, 2014).",
            "label": 0
          },
          {
            "text": "Learning to match statements and proofs would also benefit computer-assisted theorem proving, as it is akin to tasks such as premise selection, also recently addressed with NLP methods(Piotrowski and Urban, 2019).",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "BERT is not The Count_ Learning to Match Mathematical Statements with Proofs",
        "section": 7,
        "paragraph_id": 7,
        "full_text": "We provide first results on our proposed task with an array of neural models, aimed at scoring the likelihood of relationship between a statement and its proof.An analysis through a symbol replacement procedure provides insight on what such neural models are capable of learning about mathematical equations and text.",
        "sentences": [
          {
            "text": "We provide first results on our proposed task with an array of neural models, aimed at scoring the likelihood of relationship between a statement and its proof.",
            "label": 0
          },
          {
            "text": "An analysis through a symbol replacement procedure provides insight on what such neural models are capable of learning about mathematical equations and text.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "BERT is not The Count_ Learning to Match Mathematical Statements with Proofs",
        "section": 8,
        "paragraph_id": 8,
        "full_text": "We provide two methods for decoding, one is local decoding, matching a proof to a theorem in a greedy way, and one that provides a global bipartite matching based on a structured max-margin objective.Such an architecture may have applications to other NLP problems that can be cast as maximum bipartite matching problems (for a recent similar use in a different context, seeShao et al. 2023).",
        "sentences": [
          {
            "text": "We provide two methods for decoding, one is local decoding, matching a proof to a theorem in a greedy way, and one that provides a global bipartite matching based on a structured max-margin objective.",
            "label": 0
          },
          {
            "text": "Such an architecture may have applications to other NLP problems that can be cast as maximum bipartite matching problems (for a recent similar use in a different context, seeShao et al. 2023).",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "BERT is not The Count_ Learning to Match Mathematical Statements with Proofs",
        "section": 9,
        "paragraph_id": 9,
        "full_text": "Our analysis shows that pre-trained language models do not obtain significant \"mathematical insight\" for performing this matching, but rather rely on shallow matching.However, this does not prevent them from performing the matching relatively well in several carefully crafted scenarios, reaching an MRR of 73.7.",
        "sentences": [
          {
            "text": "Our analysis shows that pre-trained language models do not obtain significant \"mathematical insight\" for performing this matching, but rather rely on shallow matching.",
            "label": 0
          },
          {
            "text": "However, this does not prevent them from performing the matching relatively well in several carefully crafted scenarios, reaching an MRR of 73.7.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "BERT is not The Count_ Learning to Match Mathematical Statements with Proofs",
        "section": 10,
        "paragraph_id": 10,
        "full_text": "This section describes the construction of the MATCH dataset of statement-proof pairs (see Figure1for an example).",
        "sentences": [
          {
            "text": "This section describes the construction of the MATCH dataset of statement-proof pairs (see Figure1for an example).",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "BERT is not The Count_ Learning to Match Mathematical Statements with Proofs",
        "section": 11,
        "paragraph_id": 11,
        "full_text": "Source Corpus We use the MREC corpus4(Líška et al., 2011)as a source.The MREC corpus contains around 450k articles from ArxMLiV(Stamerjohanns et al., 2010), an on-going project aiming at converting the arXiv5repository from L A T E X to XML, a format more suited to machine processing.In this collection, mathematical formulae are represented in the MathML6format, a markup language.",
        "sentences": [
          {
            "text": "Source Corpus We use the MREC corpus4(Líška et al., 2011)as a source.",
            "label": 1
          },
          {
            "text": "The MREC corpus contains around 450k articles from ArxMLiV(Stamerjohanns et al., 2010), an on-going project aiming at converting the arXiv5repository from L A T E X to XML, a format more suited to machine processing.",
            "label": 1
          },
          {
            "text": "In this collection, mathematical formulae are represented in the MathML6format, a markup language.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "BERT is not The Count_ Learning to Match Mathematical Statements with Proofs",
        "section": 12,
        "paragraph_id": 12,
        "full_text": "Statistics We extract statement-proof pairs as described in Appendix A. Our processing of MREC includes the identification of statement-proof pairs through meta tags and the linearization of the representation of mathematical equations. We report in are in average 6.6 statement-proof pairs per article.",
        "sentences": [
          {
            "text": "Statistics We extract statement-proof pairs as described in Appendix A.",
            "label": 1
          },
          {
            "text": "Our processing of MREC includes the identification of statement-proof pairs through meta tags and the linearization of the representation of mathematical equations.",
            "label": 1
          },
          {
            "text": "We report in are in average 6.6 statement-proof pairs per article.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "BERT is not The Count_ Learning to Match Mathematical Statements with Proofs",
        "section": 13,
        "paragraph_id": 13,
        "full_text": "We report statistics about the size of statements and proofs in the number of tokens in Table2.We report the number of tokens in formulae (math), in the text itself (text) and in both (text+math).On average, proofs are much longer than statements.Statements and proofs have approximately the same proportion of text and math.Overall, the variation in the number of tokens across statements and proofs is extremely high, as illustrated by the standard deviation (SD) of all presented metrics.",
        "sentences": [
          {
            "text": "We report statistics about the size of statements and proofs in the number of tokens in Table2.",
            "label": 1
          },
          {
            "text": "We report the number of tokens in formulae (math), in the text itself (text) and in both (text+math).",
            "label": 1
          },
          {
            "text": "On average, proofs are much longer than statements.",
            "label": 1
          },
          {
            "text": "Statements and proofs have approximately the same proportion of text and math.",
            "label": 1
          },
          {
            "text": "Overall, the variation in the number of tokens across statements and proofs is extremely high, as illustrated by the standard deviation (SD) of all presented metrics.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "BinaryVQA_ A Versatile Test Set to Evaluate the Out_of_Distribution Generalization of VQA Models",
        "section": 0,
        "paragraph_id": 0,
        "full_text": "Abstract: We introduce a new test set for visual question answering (VQA) called BinaryVQA to push the limits of VQA models.Our dataset includes 7,800 questions across 1,024 images and covers a wide variety of objects, topics, and concepts.For easy model evaluation, we only consider binary questions.Questions and answers are formulated and verified carefully and manually.Around 63% of the questions have positive answers.The median number of questions per image and question length are 7 and 5, respectively.The state of the art OFA model achieves 75% accuracy on BinaryVQA dataset, which is significantly lower than its performance on the VQA v2 test-dev dataset (94.7%).We also analyze the model behavior along several dimensions including: a) performance over different categories such as text, counting and gaze direction, b) model interpretability, c) the effect of question length on accuracy, d) bias of models towards positive answers and introduction of a new score called the \"ShuffleAcc\", and e) sensitivity to spelling and grammar errors.Our investigation demonstrates the difficulty of our dataset and shows that it can challenge VQA models for next few years.Data and code are publicly available at: DATA and CODE.",
        "sentences": [
          {
            "text": "Abstract: We introduce a new test set for visual question answering (VQA) called BinaryVQA to push the limits of VQA models.",
            "label": 0
          },
          {
            "text": "Our dataset includes 7,800 questions across 1,024 images and covers a wide variety of objects, topics, and concepts.",
            "label": 1
          },
          {
            "text": "For easy model evaluation, we only consider binary questions.",
            "label": 1
          },
          {
            "text": "Questions and answers are formulated and verified carefully and manually.",
            "label": 1
          },
          {
            "text": "Around 63% of the questions have positive answers.",
            "label": 1
          },
          {
            "text": "The median number of questions per image and question length are 7 and 5, respectively.",
            "label": 1
          },
          {
            "text": "The state of the art OFA model achieves 75% accuracy on BinaryVQA dataset, which is significantly lower than its performance on the VQA v2 test-dev dataset (94.7%).",
            "label": 0
          },
          {
            "text": "We also analyze the model behavior along several dimensions including: a) performance over different categories such as text, counting and gaze direction, b) model interpretability, c) the effect of question length on accuracy, d) bias of models towards positive answers and introduction of a new score called the \"ShuffleAcc\", and e) sensitivity to spelling and grammar errors.",
            "label": 0
          },
          {
            "text": "Our investigation demonstrates the difficulty of our dataset and shows that it can challenge VQA models for next few years.",
            "label": 0
          },
          {
            "text": "Data and code are publicly available at: DATA and CODE.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "BinaryVQA_ A Versatile Test Set to Evaluate the Out_of_Distribution Generalization of VQA Models",
        "section": 1,
        "paragraph_id": 1,
        "full_text": "Visual question answering[5,10]is a multidisciplinary task at the intersection of computer vision, NLP, knowledge representation, reasoning, common sense knowledge, etcetra.The goal is to answer a text-based question given an input still image or a video.",
        "sentences": [
          {
            "text": "Visual question answering[5,10]is a multidisciplinary task at the intersection of computer vision, NLP, knowledge representation, reasoning, common sense knowledge, etcetra.",
            "label": 0
          },
          {
            "text": "The goal is to answer a text-based question given an input still image or a video.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "BinaryVQA_ A Versatile Test Set to Evaluate the Out_of_Distribution Generalization of VQA Models",
        "section": 2,
        "paragraph_id": 2,
        "full_text": "Recent VQA models are able to answer binary questions above 95% accuracy, which is astonishing considering that in principle, any questions can be asked on an image.At the same time, though, this alarms that perhaps we are not using the test sets that have the right level of difficulty.Using the same test set over the years has the risk of overfitting, as researchers often tune their models towards the statistics of the test sets (even when the annotations are held hidden).To mitigate this issue, it is crucial to have sev-Figure1.Samples from our dataset.Our dataset covers a wide variety of concepts including counting, crowd, emotions, drawings, paintings, camouflage, clothing, time, weather, body parts, age, text, gaze direction, etc.It also includes questions that address spatial understanding of models (e.g. the blue rectangle in the last image of the 3rd row).See Appendix A for more examples. eral versatile independent test sets to evaluate models and to track the progress.While several test sets are available for problems such as image classification (e.g.[6,14,28]) and object detection (e.g.[20,22,24]), the VQA field lacks enough difficult test sets.Our study is an effort in this direction.This discussion naturally relates to the out-ofdistribution studies showing that models are biased towards the test sets that are similar to the sets over which they have been trained on.Likewise, they underperform over test sets that are even slightly different[28,32,35].In this regard, here we also testing the out-of-distribution performance of the VQA models.",
        "sentences": [
          {
            "text": "Recent VQA models are able to answer binary questions above 95% accuracy, which is astonishing considering that in principle, any questions can be asked on an image.",
            "label": 0
          },
          {
            "text": "At the same time, though, this alarms that perhaps we are not using the test sets that have the right level of difficulty.",
            "label": 0
          },
          {
            "text": "Using the same test set over the years has the risk of overfitting, as researchers often tune their models towards the statistics of the test sets (even when the annotations are held hidden).",
            "label": 1
          },
          {
            "text": "To mitigate this issue, it is crucial to have sev-Figure1.",
            "label": 0
          },
          {
            "text": "Samples from our dataset.",
            "label": 1
          },
          {
            "text": "Our dataset covers a wide variety of concepts including counting, crowd, emotions, drawings, paintings, camouflage, clothing, time, weather, body parts, age, text, gaze direction, etc.It also includes questions that address spatial understanding of models (e.g. the blue rectangle in the last image of the 3rd row).",
            "label": 1
          },
          {
            "text": "See Appendix A for more examples. eral versatile independent test sets to evaluate models and to track the progress.",
            "label": 0
          },
          {
            "text": "While several test sets are available for problems such as image classification (e.g.[6,14,28]) and object detection (e.g.[20,22,24]), the VQA field lacks enough difficult test sets.",
            "label": 0
          },
          {
            "text": "Our study is an effort in this direction.",
            "label": 0
          },
          {
            "text": "This discussion naturally relates to the out-ofdistribution studies showing that models are biased towards the test sets that are similar to the sets over which they have been trained on.",
            "label": 0
          },
          {
            "text": "Likewise, they underperform over test sets that are even slightly different[28,32,35].",
            "label": 0
          },
          {
            "text": "In this regard, here we also testing the out-of-distribution performance of the VQA models.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "BinaryVQA_ A Versatile Test Set to Evaluate the Out_of_Distribution Generalization of VQA Models",
        "section": 3,
        "paragraph_id": 3,
        "full_text": "Our test set contains 1,024 images crawled from publicly-available and free-to-distribute sources.We used Google and Bing search engines with different search phrases to collect the images.We made sure that no im- age contains sensitive material, has poor resolution, or violates the copyright law1.The gathered data encompass a wide variety of visual concepts over both RGB images, paintings, drawings, cartoons, and clip arts (Fig.1).We have made sure that all the questions are unambiguous and answers are correct.Our test set contains more questions per image (∼7) than the VQA v2 test set (∼3).We only consider the binary questions, since essentially any question can be converted to a \"yes/no\" question.This simplifies the model evaluation and eliminates the complicated process of matching sentences of predicted answers with actual answers.Notice that this argument does not necessarily mean that we only need models that give binary answers.",
        "sentences": [
          {
            "text": "Our test set contains 1,024 images crawled from publicly-available and free-to-distribute sources.",
            "label": 1
          },
          {
            "text": "We used Google and Bing search engines with different search phrases to collect the images.",
            "label": 1
          },
          {
            "text": "We made sure that no im- age contains sensitive material, has poor resolution, or violates the copyright law1.",
            "label": 1
          },
          {
            "text": "The gathered data encompass a wide variety of visual concepts over both RGB images, paintings, drawings, cartoons, and clip arts (Fig.1).",
            "label": 1
          },
          {
            "text": "We have made sure that all the questions are unambiguous and answers are correct.",
            "label": 1
          },
          {
            "text": "Our test set contains more questions per image (∼7) than the VQA v2 test set (∼3).",
            "label": 1
          },
          {
            "text": "We only consider the binary questions, since essentially any question can be converted to a \"yes/no\" question.",
            "label": 1
          },
          {
            "text": "This simplifies the model evaluation and eliminates the complicated process of matching sentences of predicted answers with actual answers.",
            "label": 1
          },
          {
            "text": "Notice that this argument does not necessarily mean that we only need models that give binary answers.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "BinaryVQA_ A Versatile Test Set to Evaluate the Out_of_Distribution Generalization of VQA Models",
        "section": 4,
        "paragraph_id": 4,
        "full_text": "Although our test set is smaller than the VQA test set, it comes with the benefit of better control over the complexity of the questions and quality of the answers.Controlling the difficulty level of the questions generated by the Amazon Mechanical Turk (AMT) workers is challenging, as workers may choose to ask simple and short questions to save time.Unlike the questions in the VQA dataset[5]that are supposed to fool a toddler, alien, or a smart robot, some Bi-naryVQA questions can even challenge adults.To answer the majority of the questions, one has to carefully analyze the images.Further, small versatile and carefully curated test sets like ours can alleviate the legal issues concerning consents, licensing, privacy and security which are harder to control in datasets containing millions of images.",
        "sentences": [
          {
            "text": "Although our test set is smaller than the VQA test set, it comes with the benefit of better control over the complexity of the questions and quality of the answers.",
            "label": 1
          },
          {
            "text": "Controlling the difficulty level of the questions generated by the Amazon Mechanical Turk (AMT) workers is challenging, as workers may choose to ask simple and short questions to save time.",
            "label": 1
          },
          {
            "text": "Unlike the questions in the VQA dataset[5]that are supposed to fool a toddler, alien, or a smart robot, some Bi-naryVQA questions can even challenge adults.",
            "label": 1
          },
          {
            "text": "To answer the majority of the questions, one has to carefully analyze the images.",
            "label": 1
          },
          {
            "text": "Further, small versatile and carefully curated test sets like ours can alleviate the legal issues concerning consents, licensing, privacy and security which are harder to control in datasets containing millions of images.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "BinaryVQA_ A Versatile Test Set to Evaluate the Out_of_Distribution Generalization of VQA Models",
        "section": 5,
        "paragraph_id": 5,
        "full_text": "In curating the BinaryVQA, we have made three choices.First, this test set is intentionally not paired with a training set.This is to encourage generalization and to prohibit models to take advantage of correlations between testing and training sets.These correlations are easily accessible to models but are not detectable by humans[9].Second, our dataset comes with a license that disallows researchers to update the parameters of any model for any reason on it.This is again to avoid over-fitting.Third, to mitigate the danger of leaking our data to other training sets, we mark every image by a one pixel green border that must be removed on the fly before testing.",
        "sentences": [
          {
            "text": "In curating the BinaryVQA, we have made three choices.",
            "label": 1
          },
          {
            "text": "First, this test set is intentionally not paired with a training set.",
            "label": 1
          },
          {
            "text": "This is to encourage generalization and to prohibit models to take advantage of correlations between testing and training sets.",
            "label": 1
          },
          {
            "text": "These correlations are easily accessible to models but are not detectable by humans[9].",
            "label": 1
          },
          {
            "text": "Second, our dataset comes with a license that disallows researchers to update the parameters of any model for any reason on it.",
            "label": 1
          },
          {
            "text": "This is again to avoid over-fitting.",
            "label": 1
          },
          {
            "text": "Third, to mitigate the danger of leaking our data to other training sets, we mark every image by a one pixel green border that must be removed on the fly before testing.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "BinaryVQA_ A Versatile Test Set to Evaluate the Out_of_Distribution Generalization of VQA Models",
        "section": 6,
        "paragraph_id": 6,
        "full_text": "In addition to the test set, we also introduce new dimensions along which VQA models can be tested, in particular sensitivity of the models to small perturbations in the questions.We find that, unlike humans, current models are highly sensitive to minor grammar mistakes.Further, we study the bias of models towards generating positive answers, whether models indeed require the image to answer the questions, and whether they choose the right image regions to do so.In a nutshell, our results show that state of the art VQA models struggle on our dataset.This suggests that, in conjunction with other datasets, our dataset can be used to push the VQA models to become better.",
        "sentences": [
          {
            "text": "In addition to the test set, we also introduce new dimensions along which VQA models can be tested, in particular sensitivity of the models to small perturbations in the questions.",
            "label": 0
          },
          {
            "text": "We find that, unlike humans, current models are highly sensitive to minor grammar mistakes.",
            "label": 0
          },
          {
            "text": "Further, we study the bias of models towards generating positive answers, whether models indeed require the image to answer the questions, and whether they choose the right image regions to do so.",
            "label": 0
          },
          {
            "text": "In a nutshell, our results show that state of the art VQA models struggle on our dataset.",
            "label": 0
          },
          {
            "text": "This suggests that, in conjunction with other datasets, our dataset can be used to push the VQA models to become better.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "BinaryVQA_ A Versatile Test Set to Evaluate the Out_of_Distribution Generalization of VQA Models",
        "section": 7,
        "paragraph_id": 7,
        "full_text": "Several VQA datasets have been introduced[18,26,40].In these datasets, images are either taken from an existing vision dataset (e.g.MSCOCO;[24]) or are artificially created (e.g.Abstract Scenes;[5], computer graphics;[4,17]).Further, questions are generated either automatically[4,17,18,25,29,41], from crowd workers[5,8,11,18,21,43], or from in-house participants[18,38].Unlike these datasets, questions in our dataset are carefully constructed by experts such that to answer them a detailed inspection of the image is necessary.Some prominent VQA datasets are listed in Table1.Relevant ones to our work are described next.",
        "sentences": [
          {
            "text": "Several VQA datasets have been introduced[18,26,40].",
            "label": 0
          },
          {
            "text": "In these datasets, images are either taken from an existing vision dataset (e.g.MSCOCO;[24]) or are artificially created (e.g.Abstract Scenes;[5], computer graphics;[4,17]).",
            "label": 0
          },
          {
            "text": "Further, questions are generated either automatically[4,17,18,25,29,41], from crowd workers[5,8,11,18,21,43], or from in-house participants[18,38].",
            "label": 0
          },
          {
            "text": "Unlike these datasets, questions in our dataset are carefully constructed by experts such that to answer them a detailed inspection of the image is necessary.",
            "label": 1
          },
          {
            "text": "Some prominent VQA datasets are listed in Table1.",
            "label": 0
          },
          {
            "text": "Relevant ones to our work are described next.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "BinaryVQA_ A Versatile Test Set to Evaluate the Out_of_Distribution Generalization of VQA Models",
        "section": 8,
        "paragraph_id": 8,
        "full_text": "COCO-QA[29]includes 123,287 images from the MSCOCO (72,783 for training and 38,948 for testing) and each image has one question/answer pair.Questions are automatically generated from the image descriptions and are categorized into four types based on the type of expected answer: object, number, color, and location.A downside of the COCO-QA dataset is that 9,072 (23.29%) of test questions also appear in the training questions.",
        "sentences": [
          {
            "text": "COCO-QA[29]includes 123,287 images from the MSCOCO (72,783 for training and 38,948 for testing) and each image has one question/answer pair.",
            "label": 0
          },
          {
            "text": "Questions are automatically generated from the image descriptions and are categorized into four types based on the type of expected answer: object, number, color, and location.",
            "label": 0
          },
          {
            "text": "A downside of the COCO-QA dataset is that 9,072 (23.29%) of test questions also appear in the training questions.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "BinaryVQA_ A Versatile Test Set to Evaluate the Out_of_Distribution Generalization of VQA Models",
        "section": 9,
        "paragraph_id": 9,
        "full_text": "VQA[5,11]is one of the most widely used datasets (https://visualqa.org/).It comprises two parts, one using natural images called VQA-real (sourced from MSCOCO), and a second one with cartoon images called VQA-abstract.The latest more comprehensive version of this dataset, VQA v2.0 consists of 1.1 million (image, question) pairs with 13 million associated answers.",
        "sentences": [
          {
            "text": "VQA[5,11]is one of the most widely used datasets (https://visualqa.org/).",
            "label": 0
          },
          {
            "text": "It comprises two parts, one using natural images called VQA-real (sourced from MSCOCO), and a second one with cartoon images called VQA-abstract.",
            "label": 0
          },
          {
            "text": "The latest more comprehensive version of this dataset, VQA v2.0 consists of 1.1 million (image, question) pairs with 13 million associated answers.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "BinaryVQA_ A Versatile Test Set to Evaluate the Out_of_Distribution Generalization of VQA Models",
        "section": 10,
        "paragraph_id": 10,
        "full_text": "Visual Genome[21]is aimed to enhance the progress on cognitive tasks, especially spatial relationship reasoning.It contains over 108K images, with about 35 objects, 26 attributes, and 21 pairwise relationships between objects.",
        "sentences": [
          {
            "text": "Visual Genome[21]is aimed to enhance the progress on cognitive tasks, especially spatial relationship reasoning.",
            "label": 0
          },
          {
            "text": "It contains over 108K images, with about 35 objects, 26 attributes, and 21 pairwise relationships between objects.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "BinaryVQA_ A Versatile Test Set to Evaluate the Out_of_Distribution Generalization of VQA Models",
        "section": 11,
        "paragraph_id": 11,
        "full_text": "Visual7W[43]includes seven types of WH questions (what, where, when, who, why, which and how) to examine capability of a model in visual understanding.Questions are asked in the multiple-choice format.There are four candidates for each question, and only one candidate is the correct answer.",
        "sentences": [
          {
            "text": "Visual7W[43]includes seven types of WH questions (what, where, when, who, why, which and how) to examine capability of a model in visual understanding.",
            "label": 0
          },
          {
            "text": "Questions are asked in the multiple-choice format.",
            "label": 0
          },
          {
            "text": "There are four candidates for each question, and only one candidate is the correct answer.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "BinaryVQA_ A Versatile Test Set to Evaluate the Out_of_Distribution Generalization of VQA Models",
        "section": 12,
        "paragraph_id": 12,
        "full_text": "Visual Madlibs[41]consists of 360,001 targeted descriptions spanned across 12 different types of templates and their corresponding images.",
        "sentences": [
          {
            "text": "Visual Madlibs[41]consists of 360,001 targeted descriptions spanned across 12 different types of templates and their corresponding images.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "BinaryVQA_ A Versatile Test Set to Evaluate the Out_of_Distribution Generalization of VQA Models",
        "section": 13,
        "paragraph_id": 13,
        "full_text": "VizWiz[13]is constructed from interactions of visually impaired users with a mobile application.It consists of 31,000 visual questions together with 10 crowdsourced answers per question.Images often have poor quality due to poor lighting, focus, and framing of the content of interest.Further, questions are on average more conversational and are sometimes incomplete.",
        "sentences": [
          {
            "text": "VizWiz[13]is constructed from interactions of visually impaired users with a mobile application.",
            "label": 0
          },
          {
            "text": "It consists of 31,000 visual questions together with 10 crowdsourced answers per question.",
            "label": 0
          },
          {
            "text": "Images often have poor quality due to poor lighting, focus, and framing of the content of interest.",
            "label": 0
          },
          {
            "text": "Further, questions are on average more conversational and are sometimes incomplete.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "BinaryVQA_ A Versatile Test Set to Evaluate the Out_of_Distribution Generalization of VQA Models",
        "section": 14,
        "paragraph_id": 14,
        "full_text": "TextVQA[33]contains 45,336 questions on 28,408 images that require reasoning about text to be answered.Images are taken from the Open Images v3 dataset[20]. TextVQA is available at https://textvqa.org.",
        "sentences": [
          {
            "text": "TextVQA[33]contains 45,336 questions on 28,408 images that require reasoning about text to be answered.",
            "label": 0
          },
          {
            "text": "Images are taken from the Open Images v3 dataset[20].",
            "label": 0
          },
          {
            "text": " TextVQA is available at https://textvqa.org.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "BinaryVQA_ A Versatile Test Set to Evaluate the Out_of_Distribution Generalization of VQA Models",
        "section": 15,
        "paragraph_id": 15,
        "full_text": "In addition to above, some non-photo-realistic datasets such as CLEVR[17], NLVR[34], and FigureQA[19]have also been introduced to study visual reasoning independent of language.Some datasets such as Fact-Based VQA[37]explicitly require external knowledge to answer questions.GQA[16]is a popular dataset, which also involves phrases to address the relations.",
        "sentences": [
          {
            "text": "In addition to above, some non-photo-realistic datasets such as CLEVR[17], NLVR[34], and FigureQA[19]have also been introduced to study visual reasoning independent of language.",
            "label": 0
          },
          {
            "text": "Some datasets such as Fact-Based VQA[37]explicitly require external knowledge to answer questions.",
            "label": 0
          },
          {
            "text": "GQA[16]is a popular dataset, which also involves phrases to address the relations.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "BinaryVQA_ A Versatile Test Set to Evaluate the Out_of_Distribution Generalization of VQA Models",
        "section": 16,
        "paragraph_id": 16,
        "full_text": "Our work relates to research that addresses the functional diagnostics of pre-trained language models (e.g.[27,30]).It also relates to works that examine adversarial robustness and out-of-distribution generalization of VQA models (e.g.[7,23]).For example,[23]shows that non-expert annotators can easily attack the best VQA models.",
        "sentences": [
          {
            "text": "Our work relates to research that addresses the functional diagnostics of pre-trained language models (e.g.[27,30]).",
            "label": 0
          },
          {
            "text": "It also relates to works that examine adversarial robustness and out-of-distribution generalization of VQA models (e.g.[7,23]).",
            "label": 0
          },
          {
            "text": "For example,[23]shows that non-expert annotators can easily attack the best VQA models.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "BinaryVQA_ A Versatile Test Set to Evaluate the Out_of_Distribution Generalization of VQA Models",
        "section": 17,
        "paragraph_id": 17,
        "full_text": "We construct an adversarial dataset to challenge the best VQA models.Although there are few such datasets for freeform VQA (e.g.VQA-CP[3]), here we show that even that answering yes/no questions is not yet solved.",
        "sentences": [
          {
            "text": "We construct an adversarial dataset to challenge the best VQA models.",
            "label": 1
          },
          {
            "text": "Although there are few such datasets for freeform VQA (e.g.VQA-CP[3]), here we show that even that answering yes/no questions is not yet solved.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "BinaryVQA_ A Versatile Test Set to Evaluate the Out_of_Distribution Generalization of VQA Models",
        "section": 18,
        "paragraph_id": 18,
        "full_text": "Our dataset contains 7,800 questions across 1,024 images.Majority of the questions start with \"Is\" and \"Are\" as shown in the sunburst plot in Fig.2. The most common terms in the questions are person, wearing, people, and image (right panel in Fig.2).We do not include WH questions and all questions have \"yes\" or \"no\" answers.We ensured that each image is valid through human review.We formulated the questions and then presented them along with their answers to three AMT workers for verification.Please see Appendix D for details.Out of all questions, only 41 QA pairs received the incorrect majority vote, which were fixed subsequently.",
        "sentences": [
          {
            "text": "Our dataset contains 7,800 questions across 1,024 images.",
            "label": 1
          },
          {
            "text": "Majority of the questions start with \"Is\" and \"Are\" as shown in the sunburst plot in Fig.2.",
            "label": 1
          },
          {
            "text": "The most common terms in the questions are person, wearing, people, and image (right panel in Fig.2).",
            "label": 1
          },
          {
            "text": "We do not include WH questions and all questions have \"yes\" or \"no\" answers.",
            "label": 1
          },
          {
            "text": "We ensured that each image is valid through human review.",
            "label": 1
          },
          {
            "text": "We formulated the questions and then presented them along with their answers to three AMT workers for verification.",
            "label": 1
          },
          {
            "text": "Please see Appendix D for details.",
            "label": 0
          },
          {
            "text": "Out of all questions, only 41 QA pairs received the incorrect majority vote, which were fixed subsequently.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "BinaryVQA_ A Versatile Test Set to Evaluate the Out_of_Distribution Generalization of VQA Models",
        "section": 19,
        "paragraph_id": 19,
        "full_text": "Statistics of the BinaryVQA dataset are shown in Fig.3. Out of the 7,800 questions, 4,897 have positive answers and the remaining 2,903 have negative answers, resulting in a ratio of about 62.7% (positive/all images).The median positive to all questions ratio per image is 0.625.38(3.7%) have all of their questions answered \"yes\", while no image has all of its questions answered \"no\".The median number of questions per image is 7 which means that half of the images have more than 7 questions.The median number of positive questions (questions with answer \"yes\") is 4 and the median number of negative questions is 3.The mean number of questions per image in BinaryVQA is 7.62 which is higher than 5.4 for VQA v2.BinaryVQA questions range from 3 to 20 words.The mean and median question length are 5.64 and 5 words, respectively.VQA v2 questions range from 4 to 10 words (average 5).The average image resolution is 840.3 × 650.4 (w × h) with the average aspect ratio of 1.32.Sample images are shown in Fig.1.BinaryVQA images and questions cover a wide variety of topics and concepts including drawings, paintings, uncommon views of objects, hybrid animals, out of context objects and odd scenes (elephant in the room, car in the swimming pool, black sheep among white sheep), weather conditions, time, interactions among people, actions (fighting, running, walking, dancing), emotions (sadness, happiness, surprise, anger), counts and quantity, gender, age, race, gaze direction, object materials, objects in the mirror, body parts (e.g.whether mouth or eyes are open, whether teeth are visible), animals, fruits, clothing (T-shirt, long sleeve, pants), shadow, color, crowd, clouds, tattoos, camouflage, illusions, non-existing objects, and logical reasoning.",
        "sentences": [
          {
            "text": "Statistics of the BinaryVQA dataset are shown in Fig.3.",
            "label": 1
          },
          {
            "text": "Out of the 7,800 questions, 4,897 have positive answers and the remaining 2,903 have negative answers, resulting in a ratio of about 62.7% (positive/all images).",
            "label": 1
          },
          {
            "text": "38(3.7%) have all of their questions answered \"yes\", while no image has all of its questions answered \"no\".",
            "label": 1
          },
          {
            "text": "The median number of questions per image is 7 which means that half of the images have more than 7 questions.",
            "label": 1
          },
          {
            "text": "The median number of positive questions (questions with answer \"yes\") is 4 and the median number of negative questions is 3.",
            "label": 1
          },
          {
            "text": "The mean number of questions per image in BinaryVQA is 7.62 which is higher than 5.4 for VQA v2.",
            "label": 1
          },
          {
            "text": "BinaryVQA questions range from 3 to 20 words.",
            "label": 1
          },
          {
            "text": "The mean and median question length are 5.64 and 5 words, respectively.",
            "label": 1
          },
          {
            "text": "VQA v2 questions range from 4 to 10 words (average 5).",
            "label": 1
          },
          {
            "text": "The average image resolution is 840.3 × 650.4 (w × h) with the average aspect ratio of 1.32.",
            "label": 1
          },
          {
            "text": "Sample images are shown in Fig.1.",
            "label": 1
          },
          {
            "text": "BinaryVQA images and questions cover a wide variety of topics and concepts including drawings, paintings, uncommon views of objects, hybrid animals, out of context objects and odd scenes (elephant in the room, car in the swimming pool, black sheep among white sheep), weather conditions, time, interactions among people, actions (fighting, running, walking, dancing), emotions (sadness, happiness, surprise, anger), counts and quantity, gender, age, race, gaze direction, object materials, objects in the mirror, body parts (e.g.whether mouth or eyes are open, whether teeth are visible), animals, fruits, clothing (T-shirt, long sleeve, pants), shadow, color, crowd, clouds, tattoos, camouflage, illusions, non-existing objects, and logical reasoning.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "BinaryVQA_ A Versatile Test Set to Evaluate the Out_of_Distribution Generalization of VQA Models",
        "section": 20,
        "paragraph_id": 20,
        "full_text": "In formulating the questions, we tried to remove any ambiguity (e.g. in giving addresses relative to the image, objects, people in the scene, or image viewer; left side of the rightmost person; left of the image).When only some people in the image (e.g.standing ones) are doing an action, we did not ask \"Are these people doing X\".Instead, we asked \"Are the standing people in this image doing X\".Some questions test whether models can tell the type of the image (e.g.\"Is this a drawing?\" and \"Is this a painting?\")and whether they can answer questions over different types of images (e.g.drawings, paintings, cartoons, clip art, black and white images).Some questions ask about the text, for example \"Is there text?\", \"Is the word X written somewhere in this image?\",\"Is the text written in English?\", \"Is the number 53813 written somewhere in the image?\".External knowledge and common sense are needed to answer some questions (e.g.\"Is this a map of Japan?, \"Is this person a celebrity?\").In order to further test the spatial understanding of the models, we placed a blue rectangle around some objects in the image and targeted the questions only on those regions (See Fig.1).An example question is \"Is the spatula inside the blue rectangle blue?\".To test the consistency of models and see whether they truly understand the image, for some images we include questions that contradict each other (e.g.\"Is the boy standing?\"vs \"Is the boy sitting?\").Some other sample questions are \"Is the whole body of the person visible?\", \"Is she holding a wine in her left hand?\", \"Are some birds printed on her skirt?\",\"Is her right hand in her right pocket?\", \"Is the person on the left taller?\",\"Is anyone looking at the camera?\",Is this person an adult?\", \"Is the sky clear?\", \"Are his feet touching the ground?\",\"Are there more X objects than Y objects?\", \"Is object X to the left of object Y?\", \"Is the person in the image female?\", and \"Is the person opening the door with his right hand?\".We clustered the questions based on the terms that appeared in them, as shown in Table2.For example, questions with words gender, man, woman, female, male, boy, girl address the gender.Notice that a question may fall into more than one category.These categories will be used later to analyze the models.",
        "sentences": [
          {
            "text": "In formulating the questions, we tried to remove any ambiguity (e.g. in giving addresses relative to the image, objects, people in the scene, or image viewer; left side of the rightmost person; left of the image).",
            "label": 1
          },
          {
            "text": "When only some people in the image (e.g.standing ones) are doing an action, we did not ask \"Are these people doing X\".",
            "label": 1
          },
          {
            "text": "Instead, we asked \"Are the standing people in this image doing X\".",
            "label": 1
          },
          {
            "text": "Some questions test whether models can tell the type of the image (e.g.\"Is this a drawing?\" and \"Is this a painting?\")and whether they can answer questions over different types of images (e.g.drawings, paintings, cartoons, clip art, black and white images).",
            "label": 1
          },
          {
            "text": "Some questions ask about the text, for example \"Is there text?\", \"Is the word X written somewhere in this image?\",\"Is the text written in English?\", \"Is the number 53813 written somewhere in the image?\".",
            "label": 1
          },
          {
            "text": "External knowledge and common sense are needed to answer some questions (e.g.\"Is this a map of Japan?, \"Is this person a celebrity?\").",
            "label": 1
          },
          {
            "text": "In order to further test the spatial understanding of the models, we placed a blue rectangle around some objects in the image and targeted the questions only on those regions (See Fig.1).",
            "label": 0
          },
          {
            "text": "An example question is \"Is the spatula inside the blue rectangle blue?\".",
            "label": 0
          },
          {
            "text": "To test the consistency of models and see whether they truly understand the image, for some images we include questions that contradict each other (e.g.\"Is the boy standing?\"vs \"Is the boy sitting?\").",
            "label": 1
          },
          {
            "text": "Some other sample questions are \"Is the whole body of the person visible?\", \"Is she holding a wine in her left hand?\", \"Are some birds printed on her skirt?\",\"Is her right hand in her right pocket?\", \"Is the person on the left taller?\",\"Is anyone looking at the camera?\",Is this person an adult?\", \"Is the sky clear?\", \"Are his feet touching the ground?\",\"Are there more X objects than Y objects?\", \"Is object X to the left of object Y?\", \"Is the person in the image female?\", and \"Is the person opening the door with his right hand?\".",
            "label": 1
          },
          {
            "text": "We clustered the questions based on the terms that appeared in them, as shown in Table2.",
            "label": 1
          },
          {
            "text": "For example, questions with words gender, man, woman, female, male, boy, girl address the gender.",
            "label": 1
          },
          {
            "text": "Notice that a question may fall into more than one category.",
            "label": 1
          },
          {
            "text": "These categories will be used later to analyze the models.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "BinaryVQA_ A Versatile Test Set to Evaluate the Out_of_Distribution Generalization of VQA Models",
        "section": 21,
        "paragraph_id": 21,
        "full_text": "We did not incorporate any bias towards gender, age, or race during data collection, and tried to be as inclusive as possible in gathering images and formulating questions.We include and balance questions that address different ages and genders.The age groups are (baby, 26), (kid, 42), (children, 26), (Teenager, 5), (Young,16), and (old, 12).The gender groups are (woman, 350), (women, 38), (man, 448), and (men, 79).We did not include any question that ask about race.These issues are more important to address over large training sets.This is because sometimes models trained on such datasets are directly deployed in the real-world.",
        "sentences": [
          {
            "text": "We did not incorporate any bias towards gender, age, or race during data collection, and tried to be as inclusive as possible in gathering images and formulating questions.",
            "label": 1
          },
          {
            "text": "We include and balance questions that address different ages and genders.",
            "label": 1
          },
          {
            "text": "The age groups are (baby, 26), (kid, 42), (children, 26), (Teenager, 5), (Young,16), and (old, 12).",
            "label": 1
          },
          {
            "text": "The gender groups are (woman, 350), (women, 38), (man, 448), and (men, 79).",
            "label": 1
          },
          {
            "text": "We did not include any question that ask about race.",
            "label": 1
          },
          {
            "text": "These issues are more important to address over large training sets.",
            "label": 1
          },
          {
            "text": "This is because sometimes models trained on such datasets are directly deployed in the real-world.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "BinaryVQA_ A Versatile Test Set to Evaluate the Out_of_Distribution Generalization of VQA Models",
        "section": 22,
        "paragraph_id": 22,
        "full_text": "The BinaryVQA dataset is substantially different from the VQA v2 validation set (the real images) measured in terms of the Fréchet Inception Distance (FID)[15].The FID is equal to 50.9 indicating a large distribution shift, and hence high diversity (using 7K images).To put this number in perspective, the FID between VQA v2's validation and its test set is approximately 23.8.Notice that the lower the FID, the more similar the two distributions.",
        "sentences": [
          {
            "text": "The BinaryVQA dataset is substantially different from the VQA v2 validation set (the real images) measured in terms of the Fréchet Inception Distance (FID)[15].",
            "label": 1
          },
          {
            "text": "The FID is equal to 50.9 indicating a large distribution shift, and hence high diversity (using 7K images).",
            "label": 0
          },
          {
            "text": "To put this number in perspective, the FID between VQA v2's validation and its test set is approximately 23.8.",
            "label": 0
          },
          {
            "text": "Notice that the lower the FID, the more similar the two distributions.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Carolina_ a General Corpus of Contemporary Brazilian Portuguese with Provenance_ Typology and Versioning Information",
        "section": 0,
        "paragraph_id": 0,
        "full_text": "Abstract: This paper presents the first publicly available version of the Carolina Corpus and discusses its future directions.Carolina is a large open corpus of Brazilian Portuguese texts under construction using web-as-corpus methodology enhanced with provenance, typology, versioning, and text integrality.The corpus aims at being used both as a reliable source for research in Linguistics and as an important resource for Computer Science research on language models, contributing towards removing Portuguese from the set of low-resource languages.Here we present the construction of the corpus methodology, comparing it with other existing methodologies, as well as the corpus current state: Carolina's first public version has 653, 322, 577 tokens, distributed over 7 broad types.Each text is annotated with several different metadata categories in its header, which we developed using TEI annotation standards.We also present ongoing derivative works and invite NLP researchers to contribute with their own.",
        "sentences": [
          {
            "text": "Abstract: This paper presents the first publicly available version of the Carolina Corpus and discusses its future directions.",
            "label": 0
          },
          {
            "text": "Carolina is a large open corpus of Brazilian Portuguese texts under construction using web-as-corpus methodology enhanced with provenance, typology, versioning, and text integrality.",
            "label": 1
          },
          {
            "text": "The corpus aims at being used both as a reliable source for research in Linguistics and as an important resource for Computer Science research on language models, contributing towards removing Portuguese from the set of low-resource languages.",
            "label": 0
          },
          {
            "text": "Here we present the construction of the corpus methodology, comparing it with other existing methodologies, as well as the corpus current state: Carolina's first public version has 653, 322, 577 tokens, distributed over 7 broad types.",
            "label": 1
          },
          {
            "text": "Each text is annotated with several different metadata categories in its header, which we developed using TEI annotation standards.",
            "label": 1
          },
          {
            "text": "We also present ongoing derivative works and invite NLP researchers to contribute with their own.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Carolina_ a General Corpus of Contemporary Brazilian Portuguese with Provenance_ Typology and Versioning Information",
        "section": 1,
        "paragraph_id": 1,
        "full_text": "Carolina (General Corpus of Contemporary Brazilian Portuguese with Provenance and Typology Information, \"Corpus Geral do Português Brasileiro Contemporâneo com Informações de Procedência e Tipologia\")1is a corpus of diverse typology with a robust volume of texts mainly written in Brazilian Portuguese after 1970, retrieved from the Web.",
        "sentences": [
          {
            "text": "Carolina (General Corpus of Contemporary Brazilian Portuguese with Provenance and Typology Information, \"Corpus Geral do Português Brasileiro Contemporâneo com Informações de Procedência e Tipologia\")1is a corpus of diverse typology with a robust volume of texts mainly written in Brazilian Portuguese after 1970, retrieved from the Web.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Carolina_ a General Corpus of Contemporary Brazilian Portuguese with Provenance_ Typology and Versioning Information",
        "section": 2,
        "paragraph_id": 2,
        "full_text": "The corpus is under continuous development since September 2020 at the Digital Humanities Virtual Lab (LaViHD) 2 as part of the Natural Language Processing of Portuguese Division (NLP2) of the Center for Artificial Intelligence (C4AI) 3 of the University of São Paulo (USP).Currently, version 1.1 is available for download and new releases are planned for the future. 4With Carolina, we aim at building a robust resource with state-of-the-art features both for research in the field of Artificial Intelligence (AI) and in the field of Linguistics, focusing on the importance of provenance and a rich typology of textual information as fundamental assets in modern data availability.",
        "sentences": [
          {
            "text": "The corpus is under continuous development since September 2020 at the Digital Humanities Virtual Lab (LaViHD) 2 as part of the Natural Language Processing of Portuguese Division (NLP2) of the Center for Artificial Intelligence (C4AI) 3 of the University of São Paulo (USP).",
            "label": 1
          },
          {
            "text": "Currently, version 1.1 is available for download and new releases are planned for the future.",
            "label": 1
          },
          {
            "text": "4With Carolina, we aim at building a robust resource with state-of-the-art features both for research in the field of Artificial Intelligence (AI) and in the field of Linguistics, focusing on the importance of provenance and a rich typology of textual information as fundamental assets in modern data availability.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Carolina_ a General Corpus of Contemporary Brazilian Portuguese with Provenance_ Typology and Versioning Information",
        "section": 3,
        "paragraph_id": 3,
        "full_text": "The construction of Carolina is based on four concepts, which combined figure as the main difference from other available Portuguese corpora (see Section 2), namely provenance, typology, versioning, and text integrality.",
        "sentences": [
          {
            "text": "The construction of Carolina is based on four concepts, which combined figure as the main difference from other available Portuguese corpora (see Section 2), namely provenance, typology, versioning, and text integrality.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Carolina_ a General Corpus of Contemporary Brazilian Portuguese with Provenance_ Typology and Versioning Information",
        "section": 4,
        "paragraph_id": 4,
        "full_text": "Textual provenance refers to information about texts' origin and processing history(Werder et al., 2022).Moreover, we see provenance control as a way to intensify the curatorial aspect of building a corpus, by selecting texts with relevant content and representativeness for the purposes of the corpus and other applications, as well as ensuring that they are being distributed and used in accordance with original licenses.Easy access to textual provenance is one of the main elements for the acknowledgment of negative biases(Ntoutsi et al., 2020).Such information allows for careful selection of texts, providing the audit trail of the data and on applications derived from it.",
        "sentences": [
          {
            "text": "Textual provenance refers to information about texts' origin and processing history(Werder et al., 2022).",
            "label": 0
          },
          {
            "text": "Moreover, we see provenance control as a way to intensify the curatorial aspect of building a corpus, by selecting texts with relevant content and representativeness for the purposes of the corpus and other applications, as well as ensuring that they are being distributed and used in accordance with original licenses.",
            "label": 0
          },
          {
            "text": "Easy access to textual provenance is one of the main elements for the acknowledgment of negative biases(Ntoutsi et al., 2020).",
            "label": 0
          },
          {
            "text": "Such information allows for careful selection of texts, providing the audit trail of the data and on applications derived from it.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Carolina_ a General Corpus of Contemporary Brazilian Portuguese with Provenance_ Typology and Versioning Information",
        "section": 5,
        "paragraph_id": 5,
        "full_text": "However, the selection of a document to a research project also involves, ideally, information about the characteristics and context of the textual piece, requiring some typological annotation about the document.This work understands typology in a broad sense, as a crucial methodological tool for text search, selection and balancing in linguistics research.This separation has also become increasingly relevant for research in AI, which is developing models specialized in specific textual typologies, as is the case of SciBERT(Beltagy et al., 2019)and BioBERT(Lee et al., 2019), for scientific and biological texts, respectively.",
        "sentences": [
          {
            "text": "However, the selection of a document to a research project also involves, ideally, information about the characteristics and context of the textual piece, requiring some typological annotation about the document.",
            "label": 0
          },
          {
            "text": "This work understands typology in a broad sense, as a crucial methodological tool for text search, selection and balancing in linguistics research.",
            "label": 0
          },
          {
            "text": "This separation has also become increasingly relevant for research in AI, which is developing models specialized in specific textual typologies, as is the case of SciBERT(Beltagy et al., 2019)and BioBERT(Lee et al., 2019), for scientific and biological texts, respectively.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Carolina_ a General Corpus of Contemporary Brazilian Portuguese with Provenance_ Typology and Versioning Information",
        "section": 6,
        "paragraph_id": 6,
        "full_text": "These models mentioned are all pre-trained models, one of the main trends in NLP today(Devlin et al., 2019;Vaswani et al., 2017).Such models require training with large volumes of complete texts.Thus, volume and text integrality are fundamental characteristics of a corpus that aims to allow the development of state-of-the-art algorithms for Portuguese.That would contribute to elevating that language to a state of high computational resources.With regard to linguistics research, text integrality is also an important factor, as fragmentary content can be detrimental to inter-phrase or inter-text associations in linguistics studies.",
        "sentences": [
          {
            "text": "These models mentioned are all pre-trained models, one of the main trends in NLP today(Devlin et al., 2019;Vaswani et al., 2017).",
            "label": 0
          },
          {
            "text": "Such models require training with large volumes of complete texts.",
            "label": 0
          },
          {
            "text": "Thus, volume and text integrality are fundamental characteristics of a corpus that aims to allow the development of state-of-the-art algorithms for Portuguese.",
            "label": 1
          },
          {
            "text": "That would contribute to elevating that language to a state of high computational resources.",
            "label": 0
          },
          {
            "text": "With regard to linguistics research, text integrality is also an important factor, as fragmentary content can be detrimental to inter-phrase or inter-text associations in linguistics studies.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Carolina_ a General Corpus of Contemporary Brazilian Portuguese with Provenance_ Typology and Versioning Information",
        "section": 7,
        "paragraph_id": 7,
        "full_text": "These properties require a longer process of construction, when compared to other Web corpora.Thus, versioning becomes a practical managerial necessity for such a dataset, especially because Carolina is intended to host the textual pieces of all C4AI resources whose development is expected in the next decade, including TaRSila(Cândido Jr et al., 2021)and POeTiSA(Pardo et al., 2021).",
        "sentences": [
          {
            "text": "These properties require a longer process of construction, when compared to other Web corpora.",
            "label": 0
          },
          {
            "text": "Thus, versioning becomes a practical managerial necessity for such a dataset, especially because Carolina is intended to host the textual pieces of all C4AI resources whose development is expected in the next decade, including TaRSila(Cândido Jr et al., 2021)and POeTiSA(Pardo et al., 2021).",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Carolina_ a General Corpus of Contemporary Brazilian Portuguese with Provenance_ Typology and Versioning Information",
        "section": 8,
        "paragraph_id": 8,
        "full_text": "We present here a corpus of complete texts with annotations of provenance and typology, built according to the WaC-wiPT methodology, that uses a strategy of versioning and continuous development to achieve, in the next years, an unprecedented volume of texts for Brazilian Portuguese corpora.The WaC-wiPT methodology was conceived based on the web-as-corpus view(Baroni et al., 2009), which has been dominant in recent developments in linguistic resource building(Fletcher, 2007), with Provenance and Typology concerns, as mentioned above.",
        "sentences": [
          {
            "text": "We present here a corpus of complete texts with annotations of provenance and typology, built according to the WaC-wiPT methodology, that uses a strategy of versioning and continuous development to achieve, in the next years, an unprecedented volume of texts for Brazilian Portuguese corpora.",
            "label": 1
          },
          {
            "text": "The WaC-wiPT methodology was conceived based on the web-as-corpus view(Baroni et al., 2009), which has been dominant in recent developments in linguistic resource building(Fletcher, 2007), with Provenance and Typology concerns, as mentioned above.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Carolina_ a General Corpus of Contemporary Brazilian Portuguese with Provenance_ Typology and Versioning Information",
        "section": 9,
        "paragraph_id": 9,
        "full_text": "Therefore, providing an open, large and diverse corpus for Portuguese, with provenance and broad typological information, and prioritizing the use of integral or minimally modified texts, has the potential of directly impacting research both on Linguistics and Computer Science in Portuguese.This is the intended goal of this corpus, whose construction we present in this work.",
        "sentences": [
          {
            "text": "Therefore, providing an open, large and diverse corpus for Portuguese, with provenance and broad typological information, and prioritizing the use of integral or minimally modified texts, has the potential of directly impacting research both on Linguistics and Computer Science in Portuguese.",
            "label": 1
          },
          {
            "text": "This is the intended goal of this corpus, whose construction we present in this work.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "CLINFO_AI_ AN OPEN_SOURCE RETRIEVAL_AUGMENTED LARGE LANGUAGE MODEL SYSTEM FOR ANSWERING MEDICAL QUESTIONS USING SCIENTIFIC LITERATURE",
        "section": 0,
        "paragraph_id": 0,
        "full_text": "Abstract: The quickly-expanding nature of published medical literature makes it challenging for clinicians and researchers to keep up with and summarize recent, relevant findings in a timely manner.While several closed-source summarization tools based on large language models (LLMs) now exist, rigorous and systematic evaluations of their outputs are lacking.Furthermore, there is a paucity of high-quality datasets and appropriate benchmark tasks with which to evaluate these tools.We address these issues with four contributions: we release Clinfo.ai,an open-source WebApp that answers clinical questions based on dynamically retrieved scientific literature; we specify an information retrieval and abstractive summarization task to evaluate the performance of such retrieval-augmented LLM systems; we release a dataset of 200 questions and corresponding answers derived from published systematic reviews, which we name PubMed Retrieval and Synthesis (PubMedRS-200); and report benchmark results for Clinfo.aiand other publicly available OpenQA systems on PubMedRS-200.",
        "sentences": [
          {
            "text": "Abstract: The quickly-expanding nature of published medical literature makes it challenging for clinicians and researchers to keep up with and summarize recent, relevant findings in a timely manner.",
            "label": 0
          },
          {
            "text": "While several closed-source summarization tools based on large language models (LLMs) now exist, rigorous and systematic evaluations of their outputs are lacking.",
            "label": 0
          },
          {
            "text": "Furthermore, there is a paucity of high-quality datasets and appropriate benchmark tasks with which to evaluate these tools.",
            "label": 0
          },
          {
            "text": "We address these issues with four contributions: we release Clinfo.ai,an open-source WebApp that answers clinical questions based on dynamically retrieved scientific literature; we specify an information retrieval and abstractive summarization task to evaluate the performance of such retrieval-augmented LLM systems; we release a dataset of 200 questions and corresponding answers derived from published systematic reviews, which we name PubMed Retrieval and Synthesis (PubMedRS-200); and report benchmark results for Clinfo.aiand other publicly available OpenQA systems on PubMedRS-200.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "CLINFO_AI_ AN OPEN_SOURCE RETRIEVAL_AUGMENTED LARGE LANGUAGE MODEL SYSTEM FOR ANSWERING MEDICAL QUESTIONS USING SCIENTIFIC LITERATURE",
        "section": 1,
        "paragraph_id": 1,
        "full_text": "To populate such a dataset, we employed E-utilities, a public API to the NCBI Entrez system[41], to access PubMed and construct question-answer pairs with their respective references.Figure1illustrates our process in detail.First, we established a comprehensive selection of medical specialties and subspecialties.Second, we formulated a query to retrieve Systematic Reviews relevant to each medical specialty/subspecialty.Upon constructing the specialty-specific queries and retrieving associated abstracts, we retrieved all papers structured in a format that can be easily converted to questions-answer pairs (as noted by Jin et al 2019[42]) namely Title, Introduction, Conclusion, and References.Third, we applied another filtering process, narrowing down to solely those publications whose titles included an explicit question (i.e., publications whose titles including question marks).The questions from these titles were extracted.",
        "sentences": [
          {
            "text": "To populate such a dataset, we employed E-utilities, a public API to the NCBI Entrez system[41], to access PubMed and construct question-answer pairs with their respective references.",
            "label": 1
          },
          {
            "text": "Figure1illustrates our process in detail.",
            "label": 0
          },
          {
            "text": "First, we established a comprehensive selection of medical specialties and subspecialties.",
            "label": 0
          },
          {
            "text": "Second, we formulated a query to retrieve Systematic Reviews relevant to each medical specialty/subspecialty.",
            "label": 0
          },
          {
            "text": "Upon constructing the specialty-specific queries and retrieving associated abstracts, we retrieved all papers structured in a format that can be easily converted to questions-answer pairs (as noted by Jin et al 2019[42]) namely Title, Introduction, Conclusion, and References.",
            "label": 1
          },
          {
            "text": "Third, we applied another filtering process, narrowing down to solely those publications whose titles included an explicit question (i.e., publications whose titles including question marks).",
            "label": 1
          },
          {
            "text": "The questions from these titles were extracted.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "CLINFO_AI_ AN OPEN_SOURCE RETRIEVAL_AUGMENTED LARGE LANGUAGE MODEL SYSTEM FOR ANSWERING MEDICAL QUESTIONS USING SCIENTIFIC LITERATURE",
        "section": 2,
        "paragraph_id": 2,
        "full_text": "Finally, two human evaluators (AL and SF) manually reviewed the retrieved questions and extracted an answer to each question using minimally modified text from the results and conclusions section of the corresponding SR abstract.",
        "sentences": [
          {
            "text": "Finally, two human evaluators (AL and SF) manually reviewed the retrieved questions and extracted an answer to each question using minimally modified text from the results and conclusions section of the corresponding SR abstract.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "CLINFO_AI_ AN OPEN_SOURCE RETRIEVAL_AUGMENTED LARGE LANGUAGE MODEL SYSTEM FOR ANSWERING MEDICAL QUESTIONS USING SCIENTIFIC LITERATURE",
        "section": 3,
        "paragraph_id": 3,
        "full_text": "Concretely, in order to generate each answer, the human reviewers removed from the Results and Conclusions section of the abstract any text describing the structure or design of the systematic review (e.g., \"We used PubMed to retrieve 100 papers\"), leaving only text that directly addressed the question extracted from the SR's title.In the process, abstracts that were lacking substantive results and abstracts that merely described research proposals (e.g.descriptions of future work) were entirely removed.",
        "sentences": [
          {
            "text": "Concretely, in order to generate each answer, the human reviewers removed from the Results and Conclusions section of the abstract any text describing the structure or design of the systematic review (e.g., \"We used PubMed to retrieve 100 papers\"), leaving only text that directly addressed the question extracted from the SR's title.",
            "label": 1
          },
          {
            "text": "In the process, abstracts that were lacking substantive results and abstracts that merely described research proposals (e.g.descriptions of future work) were entirely removed.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "CLINFO_AI_ AN OPEN_SOURCE RETRIEVAL_AUGMENTED LARGE LANGUAGE MODEL SYSTEM FOR ANSWERING MEDICAL QUESTIONS USING SCIENTIFIC LITERATURE",
        "section": 4,
        "paragraph_id": 4,
        "full_text": "We recognize that the usage of scientific literature to extract question-answer pairs comes with the possibility that an answer deemed correct at the time of acquisition may be incorrect as new discoveries are published.To ensure that a system is not rewarded for simply copy-pasting the text of a retrieved source SR nor penalized when new relevant articles are published, we consider three evaluation regimes: 1. Restricted Search (RS): The retrieval process is constrained to include publications up to one day before the publication date.While this approach may not guarantee the retrieval of all publications considered important by the authors of each source systematic review, it effectively narrows down the search space to the subset of publications that could have been retrieved and deemed relevant during the review's preparation.",
        "sentences": [
          {
            "text": "We recognize that the usage of scientific literature to extract question-answer pairs comes with the possibility that an answer deemed correct at the time of acquisition may be incorrect as new discoveries are published.",
            "label": 0
          },
          {
            "text": "To ensure that a system is not rewarded for simply copy-pasting the text of a retrieved source SR nor penalized when new relevant articles are published, we consider three evaluation regimes: 1.",
            "label": 0
          },
          {
            "text": "Restricted Search (RS): The retrieval process is constrained to include publications up to one day before the publication date.",
            "label": 0
          },
          {
            "text": "While this approach may not guarantee the retrieval of all publications considered important by the authors of each source systematic review, it effectively narrows down the search space to the subset of publications that could have been retrieved and deemed relevant during the review's preparation.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Comprehensive Named Entity Recognition on CORD_19 with Distant or Weak Supervision",
        "section": 0,
        "paragraph_id": 0,
        "full_text": "Abstract: We created this CORD-NER dataset with comprehensive named entity recognition (NER) on the COVID-19 Open Research  Dataset Challenge (CORD-19) corpus (2020-03-13).This CORD-NER dataset covers 75 fine-grained entity types: In addition to the common biomedical entity types (e.g., genes, chemicals and diseases), it covers many new entity types related explicitly to the COVID-19 studies (e.g., coronaviruses, viral proteins, evolution, materials, substrates and immune responses), which may benefit research on COVID-19 related virus, spreading mechanisms, and potential vaccines.CORD-NER annotation is a combination of four sources with different NER methods.The quality of CORD-NER annotation surpasses SciSpacy (over 10% higher on the F1 score based on a sample set of documents), a fully supervised BioNER tool.Moreover, CORD-NER supports incrementally adding new documents as well as adding new entity types when needed by adding dozens of seeds as the input examples.We will constantly update CORD-NER based on the incremental updates of the CORD-19 corpus and the improvement of our system.",
        "sentences": [
          {
            "text": "Abstract: We created this CORD-NER dataset with comprehensive named entity recognition (NER) on the COVID-19 Open Research  Dataset Challenge (CORD-19) corpus (2020-03-13).",
            "label": 1
          },
          {
            "text": "This CORD-NER dataset covers 75 fine-grained entity types: In addition to the common biomedical entity types (e.g., genes, chemicals and diseases), it covers many new entity types related explicitly to the COVID-19 studies (e.g., coronaviruses, viral proteins, evolution, materials, substrates and immune responses), which may benefit research on COVID-19 related virus, spreading mechanisms, and potential vaccines.",
            "label": 1
          },
          {
            "text": "CORD-NER annotation is a combination of four sources with different NER methods.",
            "label": 1
          },
          {
            "text": "The quality of CORD-NER annotation surpasses SciSpacy (over 10% higher on the F1 score based on a sample set of documents), a fully supervised BioNER tool.",
            "label": 1
          },
          {
            "text": "Moreover, CORD-NER supports incrementally adding new documents as well as adding new entity types when needed by adding dozens of seeds as the input examples.",
            "label": 1
          },
          {
            "text": "We will constantly update CORD-NER based on the incremental updates of the CORD-19 corpus and the improvement of our system.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Comprehensive Named Entity Recognition on CORD_19 with Distant or Weak Supervision",
        "section": 1,
        "paragraph_id": 1,
        "full_text": "Coronavirus disease 2019 (COVID-19) is an infectious disease caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2).The disease was first identified in 2019 in Wuhan, Central China, and has since spread globally, resulting in the 20192020 coronavirus pandemic.On March 16th, 2020, researchers and leaders from the Allen Institute for AI, Chan Zuckerberg Initiative (CZI), Georgetown University's Center for Security and Emerging Technology (CSET), Microsoft, and the National Library of Medicine (NLM) at the National Institutes of Health released the  Open Research Dataset (CORD-19)1of scholarly literature about COVID-19, SARS-CoV-2, and the coronavirus group.",
        "sentences": [
          {
            "text": "Coronavirus disease 2019 (COVID-19) is an infectious disease caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2).",
            "label": 0
          },
          {
            "text": "The disease was first identified in 2019 in Wuhan, Central China, and has since spread globally, resulting in the 20192020 coronavirus pandemic.",
            "label": 0
          },
          {
            "text": "On March 16th, 2020, researchers and leaders from the Allen Institute for AI, Chan Zuckerberg Initiative (CZI), Georgetown University's Center for Security and Emerging Technology (CSET), Microsoft, and the National Library of Medicine (NLM) at the National Institutes of Health released the  Open Research Dataset (CORD-19)1of scholarly literature about COVID-19, SARS-CoV-2, and the coronavirus group.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Comprehensive Named Entity Recognition on CORD_19 with Distant or Weak Supervision",
        "section": 2,
        "paragraph_id": 2,
        "full_text": "Named entity recognition (NER) is a fundamental step in text mining system development to facilitate COVID-19 studies.There is a critical need for NER methods that can quickly adapt to all the COVID-19 related new types without much human effort for training data annotation.We created this CORD-NER dataset2with comprehensive named entity annotation on theCORD-19 corpus (2020-03-13).This dataset covers 75 fine-grained named entity types.CORD-NER is automatically generated by combining the annotation results from four sources.In the following sections, we introduce the details of CORD-NER dataset construction.We also show some NER annotation results in this dataset.",
        "sentences": [
          {
            "text": "Named entity recognition (NER) is a fundamental step in text mining system development to facilitate COVID-19 studies.",
            "label": 0
          },
          {
            "text": "There is a critical need for NER methods that can quickly adapt to all the COVID-19 related new types without much human effort for training data annotation.",
            "label": 0
          },
          {
            "text": "We created this CORD-NER dataset2with comprehensive named entity annotation on theCORD-19 corpus (2020-03-13).",
            "label": 1
          },
          {
            "text": "This dataset covers 75 fine-grained named entity types.",
            "label": 1
          },
          {
            "text": "CORD-NER is automatically generated by combining the annotation results from four sources.",
            "label": 1
          },
          {
            "text": "In the following sections, we introduce the details of CORD-NER dataset construction.",
            "label": 1
          },
          {
            "text": "We also show some NER annotation results in this dataset.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Comprehensive Named Entity Recognition on CORD_19 with Distant or Weak Supervision",
        "section": 3,
        "paragraph_id": 3,
        "full_text": "The input corpus is generated from the 29,500 documents in theCORD-19 corpus (2020-03-13).",
        "sentences": [
          {
            "text": "The input corpus is generated from the 29,500 documents in theCORD-19 corpus (2020-03-13).",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Comprehensive Named Entity Recognition on CORD_19 with Distant or Weak Supervision",
        "section": 4,
        "paragraph_id": 4,
        "full_text": "We first merge all the meta-data (all sources metadata 2020-03-13.csv) with their corresponding full-text papers.Then we create a tokenized corpus (CORD-NER-corpus.json)for further NER annotations.",
        "sentences": [
          {
            "text": "We first merge all the meta-data (all sources metadata 2020-03-13.csv) with their corresponding full-text papers.",
            "label": 1
          },
          {
            "text": "Then we create a tokenized corpus (CORD-NER-corpus.json)for further NER annotations.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Comprehensive Named Entity Recognition on CORD_19 with Distant or Weak Supervision",
        "section": 5,
        "paragraph_id": 5,
        "full_text": "The input corpus is a combination of the \"title\", \"abstract\" and \"full-text\" from the CORD-19 corpus.We first conduct automatic phrase mining and tokenization on the input corpus using AutoPhrase(Shang et al., 2018a).Then we do a second round of tokenization with Spacy3on the phrase-replaced corpus.We found that keeping the AutoPhrase results will significantly improve the distantly-and weakly-supervised NER performance.",
        "sentences": [
          {
            "text": "The input corpus is a combination of the \"title\", \"abstract\" and \"full-text\" from the CORD-19 corpus.",
            "label": 1
          },
          {
            "text": "We first conduct automatic phrase mining and tokenization on the input corpus using AutoPhrase(Shang et al., 2018a).",
            "label": 1
          },
          {
            "text": "Then we do a second round of tokenization with Spacy3on the phrase-replaced corpus.",
            "label": 0
          },
          {
            "text": "We found that keeping the AutoPhrase results will significantly improve the distantly-and weakly-supervised NER performance.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Comprehensive Named Entity Recognition on CORD_19 with Distant or Weak Supervision",
        "section": 6,
        "paragraph_id": 6,
        "full_text": "CORD-NER annotation is a combination of four sources with different NER methods: 1. Pre-trained NER on 18 general entity types from Spacy using the model \"en core web sm\". 2. Pre-trained NER on 18 biomedical entity types from SciSpacy 4 using the models \"en ner bionlp13cg md\" and \"en ner bc5cdr md\".",
        "sentences": [
          {
            "text": "CORD-NER annotation is a combination of four sources with different NER methods: 1.Pre-trained NER on 18 general entity types from Spacy using the model \"en core web sm\".",
            "label": 1
          },
          {
            "text": "2.Pre-trained NER on 18 biomedical entity types from SciSpacy 4 using the models \"en ner bionlp13cg md\" and \"en ner bc5cdr md\".",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "DCQA_ DOCUMENT_LEVEL CHART QUESTION ANSWERING TOWARDS COMPLEX REASONING AND COMMON_SENSE UNDERSTANDING",
        "section": 0,
        "paragraph_id": 0,
        "full_text": "Abstract: Visually-situated languages such as charts and plots are omnipresent in real-world documents.These graphical depictions are human-readable and are often analyzed in visuallyrich documents to address a variety of questions that necessitate complex reasoning and common-sense responses.Despite the growing number of datasets that aim to answer questions over charts, most only address this task in isolation, without considering the broader context of document-level question answering.Moreover, such datasets lack adequate common-sense reasoning information in their questions.In this work, we introduce a novel task named document-level chart question answering (DCQA).The goal of this task is to conduct document-level question answering, extracting charts or plots in the document via document layout analysis (DLA) first and subsequently performing chart question answering (CQA).The newly developed benchmark dataset comprises 50,010 synthetic documents integrating charts in a wide range of styles (6 styles in contrast to 3 for PlotQA and ChartQA) and includes 699,051 questions that demand a high degree of reasoning ability and common-sense understanding.Besides, we present the development of a potent question-answer generation engine that employs table data, a rich color set, and basic question templates to produce a vast array of reasoning question-answer pairs automatically.Based on DCQA, we devise an OCR-free transformer for document-level chartoriented understanding, capable of DLA and answering complex reasoning and common-sense questions over charts in an OCR-free manner.Our DCQA dataset is expected to foster research on understanding visualizations in documents, especially for scenarios that require complex reasoning for charts in the visually-rich document.We implement and evaluate a set of baselines, and our proposed method achieves comparable results.",
        "sentences": [
            {
                "text": "Abstract: Visually-situated languages such as charts and plots are omnipresent in real-world documents.",
                "label": 0
            },
            {
                "text": "These graphical depictions are human-readable and are often analyzed in visuallyrich documents to address a variety of questions that necessitate complex reasoning and common-sense responses.",
                "label": 0
            },
            {
                "text": "Despite the growing number of datasets that aim to answer questions over charts, most only address this task in isolation, without considering the broader context of document-level question answering.",
                "label": 0
            },
            {
                "text": "Moreover, such datasets lack adequate common-sense reasoning information in their questions.",
                "label": 0
            },
            {
                "text": "In this work, we introduce a novel task named document-level chart question answering (DCQA).",
                "label": 0
            },
            {
                "text": "The goal of this task is to conduct document-level question answering, extracting charts or plots in the document via document layout analysis (DLA) first and subsequently performing chart question answering (CQA).",
                "label": 0
            },
            {
                "text": "The newly developed benchmark dataset comprises 50,010 synthetic documents integrating charts in a wide range of styles (6 styles in contrast to 3 for PlotQA and ChartQA) and includes 699,051 questions that demand a high degree of reasoning ability and common-sense understanding.",
                "label": 1
            },
            {
                "text": "Besides, we present the development of a potent question-answer generation engine that employs table data, a rich color set, and basic question templates to produce a vast array of reasoning question-answer pairs automatically.",
                "label": 1
            },
            {
                "text": "Based on DCQA, we devise an OCR-free transformer for document-level chartoriented understanding, capable of DLA and answering complex reasoning and common-sense questions over charts in an OCR-free manner.",
                "label": 0
            },
            {
                "text": "Our DCQA dataset is expected to foster research on understanding visualizations in documents, especially for scenarios that require complex reasoning for charts in the visually-rich document.",
                "label": 1
            },
            {
                "text": "We implement and evaluate a set of baselines, and our proposed method achieves comparable results.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "DCQA_ DOCUMENT_LEVEL CHART QUESTION ANSWERING TOWARDS COMPLEX REASONING AND COMMON_SENSE UNDERSTANDING",
        "section": 1,
        "paragraph_id": 1,
        "full_text": "The emergence of visual language as a novel communicative tool, characterized by a tightly integrated interplay of visual and textual components, can be attributed to a confluence of factors, notably globalization, the growing intricacy of commerce and technology, and the convergence of lexicons from diverse fields that were once disparate[7].The prevalence of visually-situated language in various document types, such as academic, business, medical, and others, is markedly high[8][9][10].Gaining a comprehensive understanding of these graphical representations, such as charts and plots, is essential in extracting valuable and pragmatic insights from data[11].To conduct data analysis, individuals frequently pose intricate queries that require common-sense and arithmetic or logical operations pertaining to graphical representations.Answering such inquiries demands a substantial level of cognitive and reasoning exertion, as individuals are required to be aware of common sense and integrate numerous logical operations, including but not limited to retrieving entities, comparing trends, calculating averages, finding extremum, etc.Typically, the chart question answering (CQA) system[12]aims to generate the desired answer by taking a chart-question pair as input, constituting a fundamental function within the domain of intelligent document understanding (IDU)[13].",
        "sentences": [
            {
                "text": "The emergence of visual language as a novel communicative tool, characterized by a tightly integrated interplay of visual and textual components, can be attributed to a confluence of factors, notably globalization, the growing intricacy of commerce and technology, and the convergence of lexicons from diverse fields that were once disparate[7].",
                "label": 0
            },
            {
                "text": "The prevalence of visually-situated language in various document types, such as academic, business, medical, and others, is markedly high[8][9][10].",
                "label": 0
            },
            {
                "text": "Gaining a comprehensive understanding of these graphical representations, such as charts and plots, is essential in extracting valuable and pragmatic insights from data[11].",
                "label": 0
            },
            {
                "text": "To conduct data analysis, individuals frequently pose intricate queries that require common-sense and arithmetic or logical operations pertaining to graphical representations.",
                "label": 0
            },
            {
                "text": "Answering such inquiries demands a substantial level of cognitive and reasoning exertion, as individuals are required to be aware of common sense and integrate numerous logical operations, including but not limited to retrieving entities, comparing trends, calculating averages, finding extremum, etc.Typically, the chart question answering (CQA) system[12]aims to generate the desired answer by taking a chart-question pair as input, constituting a fundamental function within the domain of intelligent document understanding (IDU)[13].",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "DCQA_ DOCUMENT_LEVEL CHART QUESTION ANSWERING TOWARDS COMPLEX REASONING AND COMMON_SENSE UNDERSTANDING",
        "section": 2,
        "paragraph_id": 2,
        "full_text": "Despite the CQA task has drawn ever-growing attention from visual question answering communities in recent years, existing datasets has encountered certain obstacles: (i) Notably, while charts constitute crucial components of documents, the majority of current datasets treat the CQA task solely at the question-answering level, without taking into account its significance as a document-level task.(ii) Questions generally prioritize reasoning or visual features, potentially losing sight of common sense information that individuals typically consider when posing questions, which is a misalignment with the typical questioning habits of individuals.(iii) The quantity of chart types, as exemplified by PlotQA and ChartQA datasets, is comparatively restricted (only three).Such a limited representation fails to capture the broad range of chart styles that are present in real-world documents.",
        "sentences": [
            {
                "text": "Despite the CQA task has drawn ever-growing attention from visual question answering communities in recent years, existing datasets has encountered certain obstacles: (i) Notably, while charts constitute crucial components of documents, the majority of current datasets treat the CQA task solely at the question-answering level, without taking into account its significance as a document-level task.",
                "label": 0
            },
            {
                "text":"(ii) Questions generally prioritize reasoning or visual features, potentially losing sight of common sense information that individuals typically consider when posing questions, which is a misalignment with the typical questioning habits of individuals.",
                "label": 0
            },
            {
                "text":"(iii) The quantity of chart types, as exemplified by PlotQA and ChartQA datasets, is comparatively restricted (only three).",
                "label": 0
            },
            {
                "text": "Such a limited representation fails to capture the broad range of chart styles that are present in real-world documents.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "DCQA_ DOCUMENT_LEVEL CHART QUESTION ANSWERING TOWARDS COMPLEX REASONING AND COMMON_SENSE UNDERSTANDING",
        "section": 3,
        "paragraph_id": 3,
        "full_text": "Furthermore, in real-world settings, users typically first identify the location of charts in documents before querying them.However, directly analyzing document layout poses challenges, as charts lack explicit annotations.Manually annotating datasets for document layout analysis related to charts is remarkably laborious and time-consuming.This motivates an automated system to generate annotations associated with charts, eliminating the need for costly labeled data collection.Such a system would locate charts in arbitrary documents without annotations and produce bounding boxes highlighting them, providing chart-specific layout information without human intervention.Additionally, certain baseline models rely on obtaining high-quality optical character recognition (OCR) outcomes to extract the data table structure from the chart image.Therefore, current models' efficacy generally relies upon the accuracy of OCR results.Nevertheless, incorporating an OCR-dependent approach for CQA system poses significant challenges.For one thing, commercially available OCR techniques often exhibit limited adaptability in addressing diverse languages or changes in the domain, which are commonly encountered in the context of charts.Such limitations may impede the generalization abil-ity of these methods.For another, the occurrence of errors during the OCR process is unavoidable, and such erroneous outcomes have the potential to propagate to the CQA system, thereby adversely affecting subsequent processes[14].",
        "sentences": [
            {
                "text": "Furthermore, in real-world settings, users typically first identify the location of charts in documents before querying them.",
                "label": 0
            },
            {
                "text": "However, directly analyzing document layout poses challenges, as charts lack explicit annotations.",
                "label": 0
            },
            {
                "text": "Manually annotating datasets for document layout analysis related to charts is remarkably laborious and time-consuming.",
                "label": 0
            },
            {
                "text": "This motivates an automated system to generate annotations associated with charts, eliminating the need for costly labeled data collection.",
                "label": 0
            },
            {
                "text": "Such a system would locate charts in arbitrary documents without annotations and produce bounding boxes highlighting them, providing chart-specific layout information without human intervention.",
                "label": 0
            },
            {
                "text": "Additionally, certain baseline models rely on obtaining high-quality optical character recognition (OCR) outcomes to extract the data table structure from the chart image.",
                "label": 0
            },
            {
                "text": "Therefore, current models' efficacy generally relies upon the accuracy of OCR results.",
                "label": 0
            },
            {
                "text": "Nevertheless, incorporating an OCR-dependent approach for CQA system poses significant challenges.",
                "label": 0
            },
            {
                "text": "For one thing, commercially available OCR techniques often exhibit limited adaptability in addressing diverse languages or changes in the domain, which are commonly encountered in the context of charts.",
                "label": 0
            },
            {
                "text": "Such limitations may impede the generalization abil-ity of these methods.",
                "label": 0
            },
            {
                "text": "For another, the occurrence of errors during the OCR process is unavoidable, and such erroneous outcomes have the potential to propagate to the CQA system, thereby adversely affecting subsequent processes[14].",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "DCQA_ DOCUMENT_LEVEL CHART QUESTION ANSWERING TOWARDS COMPLEX REASONING AND COMMON_SENSE UNDERSTANDING",
        "section": 4,
        "paragraph_id": 4,
        "full_text": "To alleviate above issues, we go beyond the traditional dataset by presenting a large-scale document-level chart question answering dataset (DCQA).DCQA comprises 50,010 synthetic documents and 699,051 question-answer pairs generated using our customized semantic-rich question-answer generation engine.The dataset includes questions that focus on vision, complex reasoning, and common-sense knowledge.Common-sense knowledge reasoning primarily involves evaluating the ability of CQA models to distinguish between legend labels and entity names belonging to specific parent classes, and subsequently performing reasoning operations based on this discriminative ability.Each document in the DCQA includes a chart, unrelated images and a descriptive caption related to the chart.The language used in the DCQA dataset is English.The chart types exhibit a diverse range of styles and can be broadly categorized into six major types, namely Bar chart, Line plot, Pie chart, Scatter plot, Box plot, and Mixed chart, each of which is further divided into subtypes, yielding a total of 30 chart subtypes.Figure1displays some examples from DCQA.More examples are provided in Appendix E.",
        "sentences": [
            {
                "text": "To alleviate above issues, we go beyond the traditional dataset by presenting a large-scale document-level chart question answering dataset (DCQA).",
                "label": 1
            },
            {
                "text": "DCQA comprises 50,010 synthetic documents and 699,051 question-answer pairs generated using our customized semantic-rich question-answer generation engine.",
                "label": 1
            },
            {
                "text": "The dataset includes questions that focus on vision, complex reasoning, and common-sense knowledge.",
                "label": 1
            },
            {
                "text": "Common-sense knowledge reasoning primarily involves evaluating the ability of CQA models to distinguish between legend labels and entity names belonging to specific parent classes, and subsequently performing reasoning operations based on this discriminative ability.",
                "label": 1
            },
            {
                "text": "Each document in the DCQA includes a chart, unrelated images and a descriptive caption related to the chart.",
                "label": 1
            },
            {
                "text": "The language used in the DCQA dataset is English.",
                "label": 1
            },
            {
                "text": "The chart types exhibit a diverse range of styles and can be broadly categorized into six major types, namely Bar chart, Line plot, Pie chart, Scatter plot, Box plot, and Mixed chart, each of which is further divided into subtypes, yielding a total of 30 chart subtypes.",
                "label": 1
            },
            {
                "text": "Figure1displays some examples from DCQA.",
                "label": 1
            },
            {
                "text": "More examples are provided in Appendix E.",
                "label": 1
            }
        ]
    },
    {
        "paper_name": "DCQA_ DOCUMENT_LEVEL CHART QUESTION ANSWERING TOWARDS COMPLEX REASONING AND COMMON_SENSE UNDERSTANDING",
        "section": 5,
        "paragraph_id": 5,
        "full_text": "Drawing upon the DCQA dataset, we further devise a transformer-based OCR-free architecture to perform document layout analysis and chart question answering.Initially, we exploit swin transformer[15]as the vision backbone to extract visual features of the input document.Next, the extracted features are fed into the detection component to perform document layout analysis[16].Upon successfully identifying the chart image, we extract the relevant visual content from the chart, which is then utilized as input to the textual decoder for answer prediction.This novel OCR-free architecture provides a plug-and-play solution for performing chart question answering directly from the document.",
        "sentences": [
            {
                "text": "Drawing upon the DCQA dataset, we further devise a transformer-based OCR-free architecture to perform document layout analysis and chart question answering.",
                "label": 0
            },
            {
                "text": "Initially, we exploit swin transformer[15]as the vision backbone to extract visual features of the input document.",
                "label": 0
            },
            {
                "text": "Next, the extracted features are fed into the detection component to perform document layout analysis[16].",
                "label": 0
            },
            {
                "text": "Upon successfully identifying the chart image, we extract the relevant visual content from the chart, which is then utilized as input to the textual decoder for answer prediction.",
                "label": 0
            },
            {
                "text": "This novel OCR-free architecture provides a plug-and-play solution for performing chart question answering directly from the document.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "DCQA_ DOCUMENT_LEVEL CHART QUESTION ANSWERING TOWARDS COMPLEX REASONING AND COMMON_SENSE UNDERSTANDING",
        "section": 6,
        "paragraph_id": 6,
        "full_text": "In a nutshell, our contributions are as follows: • We present a comprehensive and extensive documentlevel chart question answering dataset, DCQA, which features a wide range of chart styles and includes question-answer pairs that incorporate complex reasoning and common-sense knowledge.The dataset's scale and diversity make it a valuable resource for researchers interested in developing and evaluating chart question answering models. • We conceptualize chart question answering as a documentlevel task and propose a transformer-based OCR-free model to effectively address this task. • We perform comprehensive experiments and thorough analyses on DCQA, verifying the efficacy of our model.",
        "sentences": [
            {
                "text": "In a nutshell, our contributions are as follows: • We present a comprehensive and extensive documentlevel chart question answering dataset, DCQA, which features a wide range of chart styles and includes question-answer pairs that incorporate complex reasoning and common-sense knowledge.",
                "label": 1
            },
            {
                "text": "The dataset's scale and diversity make it a valuable resource for researchers interested in developing and evaluating chart question answering models.",
                "label": 1
            },
            {
                "text": "• We conceptualize chart question answering as a documentlevel task and propose a transformer-based OCR-free model to effectively address this task.",
                "label": 0
            },
            {
                "text": "• We perform comprehensive experiments and thorough analyses on DCQA, verifying the efficacy of our model.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "DCQA_ DOCUMENT_LEVEL CHART QUESTION ANSWERING TOWARDS COMPLEX REASONING AND COMMON_SENSE UNDERSTANDING",
        "section": 7,
        "paragraph_id": 7,
        "full_text": "To date, only a limited number of datasets have been explicitly designed for chart question answering.These datasets include FigureQA[1], DVQA[2], LEAF-QA[3], LEAFQA++[4], PlotQA[5]and ChartQA[6].Despite consisting of a diverse set of synthetic charts, FigureQA suffers from a lack of specificity in terms of chart element labeling, utilizing only generic titles and color names.Furthermore, the questions are limited to a few template-based formats with binary \"yes/no\" answers.DVQA is limited to a single chart type, namely the Bar chart, and suffers from inadequate semantic relations between the textual elements.(e.g., bar and legend labels are randomly selected words) as well as restricted Y-axis value ranges.Numeric answers are primarily integers in both the train and test sets and share the same values.As with FigureQA, all bar plots in DVQA are artificially generated, and the questions are based on a small number of templates.Both LEAF-QA and its extended version, LEAFQA++, are not publicly available.Besides, they share a significant limitation: the absence of regression question-answering pairs.This is evident from the question templates described in their reference and the discrete answer set employed.Although PlotQA is currently the largest publicly available dataset for CQA, it is limited by imbalanced question distribution, as it is heavily weighted towards data-related questions and lacks an appropriate proportion of queries pertaining to the visual characteristics of chart elements, including color and shape.",
        "sentences": [
            {
                "text": "To date, only a limited number of datasets have been explicitly designed for chart question answering.",
                "label": 0
            },
            {
                "text": "These datasets include FigureQA[1], DVQA[2], LEAF-QA[3], LEAFQA++[4], PlotQA[5]and ChartQA[6].",
                "label": 0
            },
            {
                "text": "Despite consisting of a diverse set of synthetic charts, FigureQA suffers from a lack of specificity in terms of chart element labeling, utilizing only generic titles and color names.",
                "label": 0
            },
            {
                "text": "Furthermore, the questions are limited to a few template-based formats with binary \"yes/no\" answers.",
                "label": 0
            },
            {
                "text":"DVQA is limited to a single chart type, namely the Bar chart, and suffers from inadequate semantic relations between the textual elements.(e.g., bar and legend labels are randomly selected words) as well as restricted Y-axis value ranges.",
                "label": 0
            },
            {
                "text": "Numeric answers are primarily integers in both the train and test sets and share the same values.",
                "label": 0
            },
            {
                "text": "As with FigureQA, all bar plots in DVQA are artificially generated, and the questions are based on a small number of templates.",
                "label": 0
            },
            {
                "text": "Both LEAF-QA and its extended version, LEAFQA++, are not publicly available.",
                "label": 0
            },
            {
                "text": "Besides, they share a significant limitation: the absence of regression question-answering pairs.",
                "label": 0
            },
            {
                "text": "This is evident from the question templates described in their reference and the discrete answer set employed.",
                "label": 0
            },
            {
                "text": "Although PlotQA is currently the largest publicly available dataset for CQA, it is limited by imbalanced question distribution, as it is heavily weighted towards data-related questions and lacks an appropriate proportion of queries pertaining to the visual characteristics of chart elements, including color and shape.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "DCQA_ DOCUMENT_LEVEL CHART QUESTION ANSWERING TOWARDS COMPLEX REASONING AND COMMON_SENSE UNDERSTANDING",
        "section": 8,
        "paragraph_id": 8,
        "full_text": "Regarding ChartQA, it is the pioneer dataset to compile realworld charts with a blend of human-created QA pairs and machine-generated QA pairs.However, despite its innovative contribution, ChartQA is characterized by a limited size and encompasses only three distinct plot types.Furthermore, the paucity of question-answer pairs (two) per chart undermines the potential for a comprehensive understanding of the underlying information conveyed by these visualizations.This work presents a novel and intricate CQA dataset, which diverges from prior datasets in several respects.Firstly, DCQA is introduced, which reformulates the CQA task by integrating document layout analysis and chart question answering.Secondly, in addition to visual and complex reasoning questions, DCQA incorporates common sense-aware questions.Last but not least, DCQA covers a broad range of chart types1.",
        "sentences": [
            {
                "text": "Regarding ChartQA, it is the pioneer dataset to compile realworld charts with a blend of human-created QA pairs and machine-generated QA pairs.",
                "label": 0
            },
            {
                "text": "However, despite its innovative contribution, ChartQA is characterized by a limited size and encompasses only three distinct plot types.",
                "label": 0
            },
            {
                "text": "Furthermore, the paucity of question-answer pairs (two) per chart undermines the potential for a comprehensive understanding of the underlying information conveyed by these visualizations.",
                "label": 0
            },
            {
                "text": "This work presents a novel and intricate CQA dataset, which diverges from prior datasets in several respects.",
                "label": 0
            },
            {
                "text": "Firstly, DCQA is introduced, which reformulates the CQA task by integrating document layout analysis and chart question answering.",
                "label": 0
            },
            {
                "text": "Secondly, in addition to visual and complex reasoning questions, DCQA incorporates common sense-aware questions.",
                "label": 0
            },
            {
                "text": "Last but not least, DCQA covers a broad range of chart types1.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "DCQA_ DOCUMENT_LEVEL CHART QUESTION ANSWERING TOWARDS COMPLEX REASONING AND COMMON_SENSE UNDERSTANDING",
        "section": 9,
        "paragraph_id": 9,
        "full_text": "In this section, we describe the construction of DCQA from the following four aspects:(i) Data collection, (ii) Chart generation, (iii) Automatic QA pair generation engine, and (iv) Document generation.The general workflow of the DCQA generation process is shown in Figure3.A detailed generation procedure is provided in Appendix A.",
        "sentences": [
            {
                "text": "In this section, we describe the construction of DCQA from the following four aspects:(i) Data collection, (ii) Chart generation, (iii) Automatic QA pair generation engine, and (iv) Document generation.",
                "label": 0
            },
            {
                "text": "The general workflow of the DCQA generation process is shown in Figure3.",
                "label": 1
            },
            {
                "text": "A detailed generation procedure is provided in Appendix A.",
                "label": 1
            }
        ]
    },
    {
        "paper_name": "DCQA_ DOCUMENT_LEVEL CHART QUESTION ANSWERING TOWARDS COMPLEX REASONING AND COMMON_SENSE UNDERSTANDING",
        "section": 10,
        "paragraph_id": 10,
        "full_text": "Given the variability of chart styles in real-world scenarios, integrating real-world sources and randomly generated data for producing charts can augment the models robustness and adaptability to various chart formats encountered in practical scenarios.Drawing upon this observation, charts included in our dataset was derived by two means: utilizing real-world sources and randomly generated data.The detailed process of data collection is shown in Appendix A.",
        "sentences": [
            {
                "text": "Given the variability of chart styles in real-world scenarios, integrating real-world sources and randomly generated data for producing charts can augment the models robustness and adaptability to various chart formats encountered in practical scenarios.",
                "label": 1
            },
            {
                "text": "Drawing upon this observation, charts included in our dataset was derived by two means: utilizing real-world sources and randomly generated data.",
                "label": 1
            },
            {
                "text": "The detailed process of data collection is shown in Appendix A.",
                "label": 1
            }
        ]
    },
    {
        "paper_name": "DCQA_ DOCUMENT_LEVEL CHART QUESTION ANSWERING TOWARDS COMPLEX REASONING AND COMMON_SENSE UNDERSTANDING",
        "section": 11,
        "paragraph_id": 11,
        "full_text": "We exploit Pyecharts1, a Python visualization tool library based on the Echarts[21]charting library, to generate our charts.Our DCQA contains six different chart styles: bar chart, line chart, scatter plot, box plot, pie chart, and combination chart (line and bar).These chart styles can be further divided into 30 sub-types (As shown in Figure2).The color of chart elements is randomly picked up from a color set Johndecember, which covers a wide range of colors (595).Besides, the chart presents two distinct styles of background, namely dark and light, of which the former was not previously observed in any of the CQA datasets.Detailed chart information is provided in Appendix B. Since the generated charts are from disparate data sources and encompass a wide range of topics, engaging a cadre of individuals with diverse backgrounds, experiences, and expertise is necessary to craft questions about the corresponding charts.",
        "sentences": [
            {
                "text": "We exploit Pyecharts1, a Python visualization tool library based on the Echarts[21]charting library, to generate our charts.",
                "label": 1
            },
            {
                "text": "Our DCQA contains six different chart styles: bar chart, line chart, scatter plot, box plot, pie chart, and combination chart (line and bar).",
                "label": 1
            },
            {
                "text": "These chart styles can be further divided into 30 sub-types (As shown in Figure2).",
                "label": 1
            },
            {
                "text": "The color of chart elements is randomly picked up from a color set Johndecember, which covers a wide range of colors (595).",
                "label": 1
            },
            {
                "text": "Besides, the chart presents two distinct styles of background, namely dark and light, of which the former was not previously observed in any of the CQA datasets.",
                "label": 1
            },
            {
                "text": "Detailed chart information is provided in Appendix B.",
                "label": 1
            },
            {
                "text": "Since the generated charts are from disparate data sources and encompass a wide range of topics, engaging a cadre of individuals with diverse backgrounds, experiences, and expertise is necessary to craft questions about the corresponding charts.",
                "label": 1
            }
        ]
    },
    {
        "paper_name": "DCQA_ DOCUMENT_LEVEL CHART QUESTION ANSWERING TOWARDS COMPLEX REASONING AND COMMON_SENSE UNDERSTANDING",
        "section": 12,
        "paragraph_id": 12,
        "full_text": "We have meticulously curated a corpus of 573 charts spanning six categories, comprising data extracted from real-world and randomly generated sources, which serve as paradigmatic instances from which questions can be formulated.We commission a cohort of post-graduate students affiliated with our academic institution, and employees from Huawei company , to generate ten distinct questions for each of the selected charts, with an emphasis on reasoning and common sense awareness.We have obtained 5730 questions.",
        "sentences": [
            {
                "text": "We have meticulously curated a corpus of 573 charts spanning six categories, comprising data extracted from real-world and randomly generated sources, which serve as paradigmatic instances from which questions can be formulated.",
                "label": 1
            },
            {
                "text": "We commission a cohort of post-graduate students affiliated with our academic institution, and employees from Huawei company , to generate ten distinct questions for each of the selected charts, with an emphasis on reasoning and common sense awareness.",
                "label": 1
            },
            {
                "text": "We have obtained 5730 questions.",
                "label": 1
            }
        ]
    },
    {
        "paper_name": "DCQA_ DOCUMENT_LEVEL CHART QUESTION ANSWERING TOWARDS COMPLEX REASONING AND COMMON_SENSE UNDERSTANDING",
        "section": 13,
        "paragraph_id": 13,
        "full_text": "After an exhaustive process of meticulous meetings and indepth discussions, we have successfully distilled a total of 324 question templates from the original pool of 5730 questions.Out of these templates, 204 are specifically tailored for visual and numeric reasoning, while 120 templates are dedicated to common sense reasoning.Code will be publicly available at github .Table2displays the statistics of the dataset.Details in Appendix C.2.",
        "sentences": [
            {
                "text": "After an exhaustive process of meticulous meetings and indepth discussions, we have successfully distilled a total of 324 question templates from the original pool of 5730 questions.",
                "label": 1
            },
            {
                "text": "Out of these templates, 204 are specifically tailored for visual and numeric reasoning, while 120 templates are dedicated to common sense reasoning.",
                "label": 1
            },
            {
                "text": "Code will be publicly available at github .",
                "label": 1
            },
            {
                "text": "Table2displays the statistics of the dataset.",
                "label": 1
            },
            {
                "text": "Details in Appendix C.2.",
                "label": 1
            }
        ]
    },
    {
        "paper_name": "DCQA_ DOCUMENT_LEVEL CHART QUESTION ANSWERING TOWARDS COMPLEX REASONING AND COMMON_SENSE UNDERSTANDING",
        "section": 14,
        "paragraph_id": 14,
        "full_text": "Visual and numeric reasoning: These kinds of questions necessitate combing visual elements understanding and numerical reasoning techniques (e.g., sum, multiple, average, etc.).Integrating visual and numerical reasoning inquiries can facilitate CQA systems' comprehension of chart content, as it encourages the concurrent utilization of their visual and analytic faculties, thereby enabling them to engage in a more profound exploration of the underlying message conveyed by the data and achieve a more precise interpretation of chart figures.Examples of this type of question are presented in Figure1(a) and (b).",
        "sentences": [
            {
                "text": "Visual and numeric reasoning: These kinds of questions necessitate combing visual elements understanding and numerical reasoning techniques (e.g., sum, multiple, average, etc.).",
                "label": 0
            },
            {
                "text": "Integrating visual and numerical reasoning inquiries can facilitate CQA systems' comprehension of chart content, as it encourages the concurrent utilization of their visual and analytic faculties, thereby enabling them to engage in a more profound exploration of the underlying message conveyed by the data and achieve a more precise interpretation of chart figures.",
                "label": 0
            },
            {
                "text": "Examples of this type of question are presented in Figure1(a) and (b).",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "DCQA_ DOCUMENT_LEVEL CHART QUESTION ANSWERING TOWARDS COMPLEX REASONING AND COMMON_SENSE UNDERSTANDING",
        "section": 15,
        "paragraph_id": 15,
        "full_text": "Common sense reasoning: Questions of this type demand combining common sense knowledge and numerical operation.Common sense is able to serve as a facilitator for CQA systems to gain a more profound understanding of the real-life background and context reflected by the data, thus accurately inferring the meaning behind the data.Meanwhile, numerical reasoning skills can allow readers to fathom the underlying interconnections and relationships of the data and infer potential outcomes and trends.The combination of both proficiencies can profoundly equip CQA models with diverse conceptualizations of chart content and enable them to increase the usefulness of data in comprehensively examining and scrutinizing data, identifying patterns and trends, and making predictions and decisions.Examples of this type of question are presented in Figure1 (c).",
        "sentences": [
            {
                "text": "Common sense reasoning: Questions of this type demand combining common sense knowledge and numerical operation.",
                "label": 0
            },
            {
                "text": "Common sense is able to serve as a facilitator for CQA systems to gain a more profound understanding of the real-life background and context reflected by the data, thus accurately inferring the meaning behind the data.",
                "label": 0
            },
            {
                "text": "Meanwhile, numerical reasoning skills can allow readers to fathom the underlying interconnections and relationships of the data and infer potential outcomes and trends.",
                "label": 0
            },
            {
                "text": "The combination of both proficiencies can profoundly equip CQA models with diverse conceptualizations of chart content and enable them to increase the usefulness of data in comprehensively examining and scrutinizing data, identifying patterns and trends, and making predictions and decisions.",
                "label": 0
            },
            {
                "text": "Examples of this type of question are presented in Figure1 (c).",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "DCQA_ DOCUMENT_LEVEL CHART QUESTION ANSWERING TOWARDS COMPLEX REASONING AND COMMON_SENSE UNDERSTANDING",
        "section": 16,
        "paragraph_id": 16,
        "full_text": "Construction of the hierarchical entity database: Common sense reasoning is a crucial aspect of DCQA, which primarily involves evaluating a CQA model's ability to discriminate between legend labels and entity names that belong to a specific parent class and then perform reasoning operations based on this discriminatory capacity.Therefore, a hierarchical entity database with a tree-structured architecture and a well-defined set of parent-child relationships is necessary to serve as a source for both entity names and legend labels.The construction of the hierarchical entity database is expounded upon in Appendix C.1.",
        "sentences": [
            {
                "text": "Construction of the hierarchical entity database: Common sense reasoning is a crucial aspect of DCQA, which primarily involves evaluating a CQA model's ability to discriminate between legend labels and entity names that belong to a specific parent class and then perform reasoning operations based on this discriminatory capacity.",
                "label": 0
            },
            {
                "text": "Therefore, a hierarchical entity database with a tree-structured architecture and a well-defined set of parent-child relationships is necessary to serve as a source for both entity names and legend labels.",
                "label": 0
            },
            {
                "text": "The construction of the hierarchical entity database is expounded upon in Appendix C.1.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "DCQA_ DOCUMENT_LEVEL CHART QUESTION ANSWERING TOWARDS COMPLEX REASONING AND COMMON_SENSE UNDERSTANDING",
        "section": 17,
        "paragraph_id": 17,
        "full_text": "Categorization of question difficulty levels: Additionally, the entire set of question templates has been classified into five distinct levels delineated by their respective levels of complexity, namely, beginner, elementary, intermediate, ad-  vanced, and expert.The statistic of question levels is displayed in Table3.The difficulty levels of the question templates are manually annotated based on the following criteria: (1) Beginner includes questions about the overall nature of a chart image, such as whether it is horizontal or vertical or the number of columns it contains, as well as retrieval for the value of a specific chart element.(2) Elementary primarily involves questions carrying out some form of operation on all chart elements within a chart, such as determining the maximum, minimum, median, or mean value.(3) Intermediate refers to questions that involve applying specific operations to chart elements that satisfy predetermined criteria, including but not limited to identifying the maximum, minimum, median, mean, sum, or difference of chart elements based on their color, legend, or numerical value.(4) Advanced questions demand performing composite operations on chart elements that meet a specific property (building upon the operations mentioned for intermediate questions), such as finding the sum or difference of two maximum values after they have been identified.(5) Building upon advanced questions, questions that involve common sense will be categorized as expert-level.",
        "sentences": [
            {
                "text": "Categorization of question difficulty levels: Additionally, the entire set of question templates has been classified into five distinct levels delineated by their respective levels of complexity, namely, beginner, elementary, intermediate, ad-  vanced, and expert.",
                "label": 0
            },
            {
                "text": "The statistic of question levels is displayed in Table3.",
                "label": 0
            },
            {
                "text": "The difficulty levels of the question templates are manually annotated based on the following criteria: (1) Beginner includes questions about the overall nature of a chart image, such as whether it is horizontal or vertical or the number of columns it contains, as well as retrieval for the value of a specific chart element.",
                "label": 0
            },
            {
                "text":"(2) Elementary primarily involves questions carrying out some form of operation on all chart elements within a chart, such as determining the maximum, minimum, median, or mean value.",
                "label": 0
            },
            {
                "text":"(3) Intermediate refers to questions that involve applying specific operations to chart elements that satisfy predetermined criteria, including but not limited to identifying the maximum, minimum, median, mean, sum, or difference of chart elements based on their color, legend, or numerical value.",
                "label": 0
            },
            {
                "text":"(4) Advanced questions demand performing composite operations on chart elements that meet a specific property (building upon the operations mentioned for intermediate questions), such as finding the sum or difference of two maximum values after they have been identified.",
                "label": 0
            },
            {
                "text": "(5) Building upon advanced questions, questions that involve common sense will be categorized as expert-level.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "DCQA_ DOCUMENT_LEVEL CHART QUESTION ANSWERING TOWARDS COMPLEX REASONING AND COMMON_SENSE UNDERSTANDING",
        "section": 18,
        "paragraph_id": 18,
        "full_text": "In this paper, answers are generated through an automated process based on a customized set of procedures.Specifically, a solution step is designed for each question template, with each step representing an atomic operation that is implemented using specific functions to achieve its intended functionality.When solving specific questions to generate answers, the designed solution steps are followed by invoking corresponding functions, resulting in answers for the respec- Upon completing the question-answer pair generation process, extra analysis is conducted on the distribution of every answer type.It is noted that the highest proportion of the answer type in the dataset is the binary classification yes or no.However, the ratio between yes and no is severely imbalanced, with a vastly larger number of no responses compared to yes.As a result, a post-processing adjustment is necessary to address this language bias and prevent the model from exploiting the answer distribution pattern to output the answer without paying attention to the visual content.",
        "sentences": [
            {
                "text": "In this paper, answers are generated through an automated process based on a customized set of procedures.",
                "label": 0
            },
            {
                "text": "Specifically, a solution step is designed for each question template, with each step representing an atomic operation that is implemented using specific functions to achieve its intended functionality.",
                "label": 0
            },
            {
                "text": "When solving specific questions to generate answers, the designed solution steps are followed by invoking corresponding functions, resulting in answers for the respec- Upon completing the question-answer pair generation process, extra analysis is conducted on the distribution of every answer type.",
                "label": 0
            },
            {
                "text": "It is noted that the highest proportion of the answer type in the dataset is the binary classification yes or no.",
                "label": 1
            },
            {
                "text": "However, the ratio between yes and no is severely imbalanced, with a vastly larger number of no responses compared to yes.",
                "label": 1
            },
            {
                "text": "As a result, a post-processing adjustment is necessary to address this language bias and prevent the model from exploiting the answer distribution pattern to output the answer without paying attention to the visual content.",
                "label": 1
            }
        ]
    },
    {
        "paper_name": "DCQA_ DOCUMENT_LEVEL CHART QUESTION ANSWERING TOWARDS COMPLEX REASONING AND COMMON_SENSE UNDERSTANDING",
        "section": 19,
        "paragraph_id": 19,
        "full_text": "The debiasing procedure can be concisely described as: Firstly, filter out all question templates with Yes/No answers.Secondly, count the number of Yes/No answers for each Yes/No question template in the dataset.For each Yes/No question template, determine whether the number of Yes answers exceeds the number of No answers or vice versa.For the question with a larger proportion of answers, adjust the values of the replaceable modules in the question to change the answer to the less frequent one, and iterate this process until the number of Yes and No answers for the template are equal or differ by only 1.Most of the Yes/No question templates can be balanced by modifying the answer using the above approach.However, a few questions cannot be balanced this way, and their answers are not significantly  imbalanced.Therefore, we do not handle such question templates in this paper for now.Before debiasing, the dataset's overall proportion of yes and no answers was 35.16% and 64.84%, respectively.After debiasing, the overall proportion of yes and no answers in the dataset became 49.26% and 50.74%, respectively.The effectiveness of the debiasing is compared in Figure4, where the noticeable changes in the answer distribution of \"Yes\" and \"No\" before and after debiasing demonstrate that the debiasing method employed in this study effectively addresses the answer imbalance in binary questions.",
        "sentences": [
            {
                "text": "The debiasing procedure can be concisely described as: Firstly, filter out all question templates with Yes/No answers.",
                "label": 1
            },
            {
                "text": "Secondly, count the number of Yes/No answers for each Yes/No question template in the dataset.",
                "label": 1
            },
            {
                "text": "For each Yes/No question template, determine whether the number of Yes answers exceeds the number of No answers or vice versa.",
                "label": 1
            },
            {
                "text": "For the question with a larger proportion of answers, adjust the values of the replaceable modules in the question to change the answer to the less frequent one, and iterate this process until the number of Yes and No answers for the template are equal or differ by only 1.",
                "label": 1
            },
            {
                "text": "Most of the Yes/No question templates can be balanced by modifying the answer using the above approach.",
                "label": 1
            },
            {
                "text": "However, a few questions cannot be balanced this way, and their answers are not significantly  imbalanced.",
                "label": 1
            },
            {
                "text": "Therefore, we do not handle such question templates in this paper for now.",
                "label": 1
            },
            {
                "text": "Before debiasing, the dataset's overall proportion of yes and no answers was 35.16% and 64.84%, respectively.",
                "label": 1
            },
            {
                "text": "After debiasing, the overall proportion of yes and no answers in the dataset became 49.26% and 50.74%, respectively.",
                "label": 1
            },
            {
                "text": "The effectiveness of the debiasing is compared in Figure4, where the noticeable changes in the answer distribution of \"Yes\" and \"No\" before and after debiasing demonstrate that the debiasing method employed in this study effectively addresses the answer imbalance in binary questions.",
                "label": 1
            }
        ]
    },
    {
        "paper_name": "DCQA_ DOCUMENT_LEVEL CHART QUESTION ANSWERING TOWARDS COMPLEX REASONING AND COMMON_SENSE UNDERSTANDING",
        "section": 20,
        "paragraph_id": 20,
        "full_text": "In accordance with the methodology outlined in[22], we utilize LaTex to generate synthetic documents that include diverse multimedia elements.In addition to the chart image, the generated document also incorporates other visual content 2  and textual content produced by ChatGLM[23,24].Notably, integrating these elements augments the informational value 2 https://cocodataset.org of the synthetic document beyond the mere inclusion of the chart image, which is more consistent with real-world documents.Detailed document information is provided in Appendix D.",
        "sentences": [
            {
                "text": "In accordance with the methodology outlined in[22], we utilize LaTex to generate synthetic documents that include diverse multimedia elements.",
                "label": 0
            },
            {
                "text": "In addition to the chart image, the generated document also incorporates other visual content 2  and textual content produced by ChatGLM[23,24].",
                "label": 0
            },
            {
                "text": "org of the synthetic document beyond the mere inclusion of the chart image, which is more consistent with real-world documents.",
                "label": 0
            },
            {
                "text": "Detailed document information is provided in Appendix D.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "DCQA_ DOCUMENT_LEVEL CHART QUESTION ANSWERING TOWARDS COMPLEX REASONING AND COMMON_SENSE UNDERSTANDING",
        "section": 21,
        "paragraph_id": 21,
        "full_text": "In this section, we provide a comprehensive analysis of the experimental results to establish the validity of the recently developed DCQA dataset and verify the excellent efficacy of our proposed TOT-Doctor model through a comparative evaluation against other baselines.",
        "sentences": [
            {
                "text": "In this section, we provide a comprehensive analysis of the experimental results to establish the validity of the recently developed DCQA dataset and verify the excellent efficacy of our proposed TOT-Doctor model through a comparative evaluation against other baselines.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "DCQA_ DOCUMENT_LEVEL CHART QUESTION ANSWERING TOWARDS COMPLEX REASONING AND COMMON_SENSE UNDERSTANDING",
        "section": 22,
        "paragraph_id": 22,
        "full_text": "We compare the TOT-Doctor with the classical approaches include LayoutLMv2[25], LayoutLMv3[26], Pix2Struct[9], and MATCHA[27].",
        "sentences": [
            {
                "text": "We compare the TOT-Doctor with the classical approaches include LayoutLMv2[25], LayoutLMv3[26], Pix2Struct[9], and MATCHA[27].",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "DCQA_ DOCUMENT_LEVEL CHART QUESTION ANSWERING TOWARDS COMPLEX REASONING AND COMMON_SENSE UNDERSTANDING",
        "section": 23,
        "paragraph_id": 23,
        "full_text": "• LayoutLMv2[25]is a Transformer-based encoder that models multi-modal information, including text, layout, and image, by incorporating a spatial-aware selfattention mechanism along with two innovative pretraining strategies.• LayoutLMv3[26]is a multi-modal Transformer framework without the vision backbone that leverages reconstructive objectives for cross-modal alignment learning, showcasing notable generality in the context of document vision tasks.",
        "sentences": [
            {
                "text": "• LayoutLMv3[26]is a multi-modal Transformer framework without the vision backbone that leverages reconstructive objectives for cross-modal alignment learning, showcasing notable generality in the context of document vision tasks.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "DCQA_ DOCUMENT_LEVEL CHART QUESTION ANSWERING TOWARDS COMPLEX REASONING AND COMMON_SENSE UNDERSTANDING",
        "section": 24,
        "paragraph_id": 24,
        "full_text": "• Pix2Struct[9]is an image-to-text model tailored for visual language understanding, which is pre-trained on visually-rich screenshots of web pages with screenshot parsing objective.",
        "sentences": [
            {
                "text": "• Pix2Struct[9]is an image-to-text model tailored for visual language understanding, which is pre-trained on visually-rich screenshots of web pages with screenshot parsing objective.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "DCQA_ DOCUMENT_LEVEL CHART QUESTION ANSWERING TOWARDS COMPLEX REASONING AND COMMON_SENSE UNDERSTANDING",
        "section": 25,
        "paragraph_id": 25,
        "full_text": "• MATCHA[27]is a Pix2Struct-based model pre-trained for chart underlying structure understanding and mathematical reasoning.",
        "sentences": [
            {
                "text": "• MATCHA[27]is a Pix2Struct-based model pre-trained for chart underlying structure understanding and mathematical reasoning.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "DCQA_ DOCUMENT_LEVEL CHART QUESTION ANSWERING TOWARDS COMPLEX REASONING AND COMMON_SENSE UNDERSTANDING",
        "section": 26,
        "paragraph_id": 26,
        "full_text": "Our study employs accuracy the primary evaluation metric, wherein the assessment of the predicted answers correctness depends on the nature of the answer type.In the case of textual answers, such as binary responses, entities, and integers, the evaluation criterion mandates that the predicted answer should match the ground truth exactly.For numerical answers in the form of floating-point values, it is not always feasible to expect that the predicted answer will precisely match the correct answer.Therefore, we consider the answer correct if it falls within 5% of the expected value.",
        "sentences": [
            {
                "text": "Our study employs accuracy the primary evaluation metric, wherein the assessment of the predicted answers correctness depends on the nature of the answer type.",
                "label": 0
            },
            {
                "text": "In the case of textual answers, such as binary responses, entities, and integers, the evaluation criterion mandates that the predicted answer should match the ground truth exactly.",
                "label": 0
            },
            {
                "text": "For numerical answers in the form of floating-point values, it is not always feasible to expect that the predicted answer will precisely match the correct answer.",
                "label": 0
            },
            {
                "text": "Therefore, we consider the answer correct if it falls within 5% of the expected value.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "DCQA_ DOCUMENT_LEVEL CHART QUESTION ANSWERING TOWARDS COMPLEX REASONING AND COMMON_SENSE UNDERSTANDING",
        "section": 27,
        "paragraph_id": 27,
        "full_text": "This section primarily supplements experimental configurations, including parameters such as batch size and learning rate, and outlines the preprocessing steps undertaken to ensure a fair comparison for the extractive model.",
        "sentences": [
            {
                "text": "This section primarily supplements experimental configurations, including parameters such as batch size and learning rate, and outlines the preprocessing steps undertaken to ensure a fair comparison for the extractive model.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "DCQA_ DOCUMENT_LEVEL CHART QUESTION ANSWERING TOWARDS COMPLEX REASONING AND COMMON_SENSE UNDERSTANDING",
        "section": 28,
        "paragraph_id": 28,
        "full_text": "Table4shows the detailed experimental setup, in which CE refers to Cross Entropy.All models were verified every 5000 iterations during training.In our implementation of LayoutLMv2 and v3, we employ a multi-step learning rate schedule.More specifically, we gradually decrease the learning rate by a factor of 2 after each epoch of training.For other models, we use the cosine scheduler to adjust the learning rate, where the number of warm-up steps is set to 1000.The LayoutLM series employs a model-based extraction approach, which requires the system to select answers from the optical character recognition (OCR) results.To enable the model to perform tasks such as binary classification (e.g., Yes/No), we have implemented a simple yet effective solution: we add two special characters, \"Yes\" and \"No\", to the OCR results.",
        "sentences": [
            {
                "text": "Table4shows the detailed experimental setup, in which CE refers to Cross Entropy.",
                "label": 0
            },
            {
                "text": "All models were verified every 5000 iterations during training.",
                "label": 0
            },
            {
                "text": "In our implementation of LayoutLMv2 and v3, we employ a multi-step learning rate schedule.",
                "label": 0
            },
            {
                "text": "More specifically, we gradually decrease the learning rate by a factor of 2 after each epoch of training.",
                "label": 0
            },
            {
                "text": "For other models, we use the cosine scheduler to adjust the learning rate, where the number of warm-up steps is set to 1000.",
                "label": 0
            },
            {
                "text": "The LayoutLM series employs a model-based extraction approach, which requires the system to select answers from the optical character recognition (OCR) results.",
                "label": 0
            },
            {
                "text": "To enable the model to perform tasks such as binary classification (e.g., Yes/No), we have implemented a simple yet effective solution: we add two special characters, \"Yes\" and \"No\", to the OCR results.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "DCQA_ DOCUMENT_LEVEL CHART QUESTION ANSWERING TOWARDS COMPLEX REASONING AND COMMON_SENSE UNDERSTANDING",
        "section": 29,
        "paragraph_id": 29,
        "full_text": "TOT-Doctor consists of two main components, namely the document layout analysis and the chart question answering.The model parameters of TOT-Doctor are calculated and found to be as follows: the encoder of the Swin Transformer used in the document layout analysis has 74M parameters, while the detector component has 48M parameters.The encoder of the Swin Transformer used in the chart question answering phase has 74M parameters, and the BART has 127M parameters.",
        "sentences": [
            {
                "text": "TOT-Doctor consists of two main components, namely the document layout analysis and the chart question answering.",
                "label": 0
            },
            {
                "text": "The model parameters of TOT-Doctor are calculated and found to be as follows: the encoder of the Swin Transformer used in the document layout analysis has 74M parameters, while the detector component has 48M parameters.",
                "label": 0
            },
            {
                "text": "The encoder of the Swin Transformer used in the chart question answering phase has 74M parameters, and the BART has 127M parameters.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "DCQA_ DOCUMENT_LEVEL CHART QUESTION ANSWERING TOWARDS COMPLEX REASONING AND COMMON_SENSE UNDERSTANDING",
        "section": 30,
        "paragraph_id": 30,
        "full_text": "Based on the fine-grained document element annotations present within the DCQA dataset introduced in this paper, encompassing various elements such as chart image, picture, textual content, list, caption, header, footer, and page number, the document layout analysis model of the proposed ToT-Doctor framework was trained.The conclusive results of the document layout analysis testing on the test set are presented in Table5. Notably, the detection accuracy for chart image reached an impressive 99.901%, exhibiting a near-complete precision in accurately identifying their respective spatial position.This achievement serves as a robust foundational prerequisite for facilitating subsequent stage of chart question answering task.",
        "sentences": [
            {
                "text": "Based on the fine-grained document element annotations present within the DCQA dataset introduced in this paper, encompassing various elements such as chart image, picture, textual content, list, caption, header, footer, and page number, the document layout analysis model of the proposed ToT-Doctor framework was trained.",
                "label": 1
            },
            {
                "text": "The conclusive results of the document layout analysis testing on the test set are presented in Table5.",
                "label": 0
            },
            {
                "text": "Notably, the detection accuracy for chart image reached an impressive 99.901%, exhibiting a near-complete precision in accurately identifying their respective spatial position.",
                "label": 0
            },
            {
                "text": "This achievement serves as a robust foundational prerequisite for facilitating subsequent stage of chart question answering task.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "DCQA_ DOCUMENT_LEVEL CHART QUESTION ANSWERING TOWARDS COMPLEX REASONING AND COMMON_SENSE UNDERSTANDING",
        "section": 31,
        "paragraph_id": 31,
        "full_text": "The experiment results of our proposed TOT-Doctor and other baselines are displayed in Table6.Due to the incapacity of the baseline to perform document layout analysis, we add our document layout analysis framework to them before conduct chart question answering.",
        "sentences": [
            {
                "text": "The experiment results of our proposed TOT-Doctor and other baselines are displayed in Table6.",
                "label": 0
            },
            {
                "text": "Due to the incapacity of the baseline to perform document layout analysis, we add our document layout analysis framework to them before conduct chart question answering.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "DCQA_ DOCUMENT_LEVEL CHART QUESTION ANSWERING TOWARDS COMPLEX REASONING AND COMMON_SENSE UNDERSTANDING",
        "section": 32,
        "paragraph_id": 32,
        "full_text": "Based on the data listed in Table6, it can be seen that TOT-Doctor consistently surpasses its counterparts concerning the accuracy in both the validation and test sets, corroborating the efficacy of TOT-Doctor.It is noteworthy that despite LayoutLMv2 and LayoutLMv3 being reliant on OCR for obtaining the answers, their performance continues to lag behind our OCR-free TOT-Doctor.This observation proves the robustness and OCR error mitigation capabilities of the TOT-Doctor proposed in this study.Furthermore, in comparison to the latest state-of-the-art (SOTA) pre-trained visual language understanding model Pix2Struct, TOT-Doctor demonstrates superior performance, achieving a significant improvement of approximately 21.171% on the development dataset, and a respectable enhancement of 20.796% on the test dataset, respectively.The outstanding performance of our TOT-Doctor model underscores the significance of integrating vision and language features in an OCR-free manner to address the questions posed in DCQA effectively.",
        "sentences": [
            {
                "text": "Based on the data listed in Table6, it can be seen that TOT-Doctor consistently surpasses its counterparts concerning the accuracy in both the validation and test sets, corroborating the efficacy of TOT-Doctor.",
                "label": 0
            },
            {
                "text": "It is noteworthy that despite LayoutLMv2 and LayoutLMv3 being reliant on OCR for obtaining the answers, their performance continues to lag behind our OCR-free TOT-Doctor.",
                "label": 0
            },
            {
                "text": "This observation proves the robustness and OCR error mitigation capabilities of the TOT-Doctor proposed in this study.",
                "label": 0
            },
            {
                "text": "Furthermore, in comparison to the latest state-of-the-art (SOTA) pre-trained visual language understanding model Pix2Struct, TOT-Doctor demonstrates superior performance, achieving a significant improvement of approximately 21.171% on the development dataset, and a respectable enhancement of 20.796% on the test dataset, respectively.",
                "label": 0
            },
            {
                "text": "The outstanding performance of our TOT-Doctor model underscores the significance of integrating vision and language features in an OCR-free manner to address the questions posed in DCQA effectively.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "DCQA_ DOCUMENT_LEVEL CHART QUESTION ANSWERING TOWARDS COMPLEX REASONING AND COMMON_SENSE UNDERSTANDING",
        "section": 33,
        "paragraph_id": 33,
        "full_text": "The DCQA dataset incorporates five distinct question levels.In order to better discern the effectiveness of our proposed TOT-Doctor and other baselines, we conduct a performance analysis on each question level.The results of our analysis are presented in Table7for reference.It is evident from the results that TOT-Doctor consistently outperforms other baselines in all five levels of questions.Notably, TOT-Doctor exhibits superior performance on intermediatelevel questions, demonstrating its efficacy in directly applying specific operations such as identifying maximum, minimum, median, mean, and sum to chart elements or analyzing the differences of chart elements based on their color, legend, or numerical value.However, when encountering the subdifficult advanced-level questions involving composite oper-ations and the most challenging expert-level questions that necessitate commonsense understanding, the performance of all baselines, including TOT-Doctor, considerably decreases compared to the other three more tractable question levels.This implies that the ability for complex reasoning and commonsense understanding still requires further improvement for analyzing complex documents.Fig.6: Performance comparison between common sense and numerical reasoning questions on DCQA test set.",
        "sentences": [
            {
                "text": "The DCQA dataset incorporates five distinct question levels.",
                "label": 1
            },
            {
                "text": "In order to better discern the effectiveness of our proposed TOT-Doctor and other baselines, we conduct a performance analysis on each question level.",
                "label": 0
            },
            {
                "text": "The results of our analysis are presented in Table7for reference.",
                "label": 0
            },
            {
                "text": "It is evident from the results that TOT-Doctor consistently outperforms other baselines in all five levels of questions.",
                "label": 0
            },
            {
                "text": "Notably, TOT-Doctor exhibits superior performance on intermediatelevel questions, demonstrating its efficacy in directly applying specific operations such as identifying maximum, minimum, median, mean, and sum to chart elements or analyzing the differences of chart elements based on their color, legend, or numerical value.",
                "label": 0
            },
            {
                "text": "However, when encountering the subdifficult advanced-level questions involving composite oper-ations and the most challenging expert-level questions that necessitate commonsense understanding, the performance of all baselines, including TOT-Doctor, considerably decreases compared to the other three more tractable question levels.",
                "label": 0
            },
            {
                "text": "This implies that the ability for complex reasoning and commonsense understanding still requires further improvement for analyzing complex documents.",
                "label": 0
            },
            {
                "text": "Fig.6: Performance comparison between common sense and numerical reasoning questions on DCQA test set.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "DCQA_ DOCUMENT_LEVEL CHART QUESTION ANSWERING TOWARDS COMPLEX REASONING AND COMMON_SENSE UNDERSTANDING",
        "section": 34,
        "paragraph_id": 34,
        "full_text": "We conduct additional experiments to evaluate the performance of the TOT-Doctor and baseline models on different question types.As discussed before, DCQA comprises two primary question types: visual and numeric reasoning and commonsense reasoning.Figure6presents the results of all baselines for each question type on the test set.Our proposed TOT-Doctor outperforms other baselines significantly, particularly in numerical reasoning questions.To top it all off, TOT-Doctor demonstrates more proficiency in numerical reasoning compared to commonsense understanding.",
        "sentences": [
            {
                "text": "We conduct additional experiments to evaluate the performance of the TOT-Doctor and baseline models on different question types.",
                "label": 0
            },
            {
                "text": "As discussed before, DCQA comprises two primary question types: visual and numeric reasoning and commonsense reasoning.",
                "label": 0
            },
            {
                "text": "Figure6presents the results of all baselines for each question type on the test set.",
                "label": 0
            },
            {
                "text": "Our proposed TOT-Doctor outperforms other baselines significantly, particularly in numerical reasoning questions.",
                "label": 0
            },
            {
                "text": "To top it all off, TOT-Doctor demonstrates more proficiency in numerical reasoning compared to commonsense understanding.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "DCQA_ DOCUMENT_LEVEL CHART QUESTION ANSWERING TOWARDS COMPLEX REASONING AND COMMON_SENSE UNDERSTANDING",
        "section": 35,
        "paragraph_id": 35,
        "full_text": "To further investigate our TOT-Doctor model, we assess the ability of the TOT-Doctor to generate different answer types.From Table8, we discover that except for LayoutLMv2, all other baselines perform better in answering Yes/No questions.We observe that LayoutLMv2 and LayoutLMv3 exhibit frustrating performance in generating numerical and string answers.We speculate that this is mainly because LayoutLMv2 and LayoutLMv3 are extractive models, which means that they cannot generate answers that have not appeared in the document.This precisely explains their poor performance on the DCQA dataset, where the answers are largely obtained through data reasoning and involve numerical values.It is noted that EasyOCR 3 is utilized as the OCR system for these two baselines, which deviates from the OCR employed in their original versions.Based on this observation, we posit that the accuracy of the OCR system may have contributed to the subpar performance of the models.Moreover, TOT-Doctor is well versed in generating answers in terms of numerical or string, which verifies the robustness of TOT-Doctor.However, overall, all baselines achieve abysmal accuracy in generating numerical and string answers, highlighting the significant challenge posed by document-level chart understanding, which calls for further research efforts.",
        "sentences": [
            {
                "text": "To further investigate our TOT-Doctor model, we assess the ability of the TOT-Doctor to generate different answer types.",
                "label": 0
            },
            {
                "text": "From Table8, we discover that except for LayoutLMv2, all other baselines perform better in answering Yes/No questions.",
                "label": 0
            },
            {
                "text": "We observe that LayoutLMv2 and LayoutLMv3 exhibit frustrating performance in generating numerical and string answers.",
                "label": 0
            },
            {
                "text": "We speculate that this is mainly because LayoutLMv2 and LayoutLMv3 are extractive models, which means that they cannot generate answers that have not appeared in the document.",
                "label": 0
            },
            {
                "text": "This precisely explains their poor performance on the DCQA dataset, where the answers are largely obtained through data reasoning and involve numerical values.",
                "label": 0
            },
            {
                "text": "It is noted that EasyOCR 3 is utilized as the OCR system for these two baselines, which deviates from the OCR employed in their original versions.",
                "label": 0
            },
            {
                "text": "Based on this observation, we posit that the accuracy of the OCR system may have contributed to the subpar performance of the models.",
                "label": 0
            },
            {
                "text": "Moreover, TOT-Doctor is well versed in generating answers in terms of numerical or string, which verifies the robustness of TOT-Doctor.",
                "label": 0
            },
            {
                "text": "However, overall, all baselines achieve abysmal accuracy in generating numerical and string answers, highlighting the significant challenge posed by document-level chart understanding, which calls for further research efforts.",
                "label": 0
            }
        ]
      },
      {
        "paper_name": "Debate Helps Supervise Unreliable Experts",
        "section": 0,
        "paragraph_id": 0,
        "full_text": "Abstract: As AI systems are used to answer more difficult questions and potentially help create new knowledge, judging the truthfulness of their outputs becomes more difficult and more important.How can we supervise unreliable experts-which have access to the truth but may not accurately report it-to give answers that are systematically true and don't just superficially seem true, when the supervisor can't tell the difference between the two on their own?In this work, we show that debate between two unreliable experts can help a non-expert judge more reliably identify the truth.We collect a dataset of human-written debates on hard reading comprehension questions where the judge has not read the source passage, only ever seeing expert arguments and short quotes selectively revealed by 'expert' debaters who have access to the passage.In our debates, one expert argues for the correct answer, and the other for an incorrect answer.Comparing debate to a baseline we call consultancy, where a single expert argues for only one answer which is correct half of the time, we find that debate performs significantly better, with 84% judge accuracy compared to consultancy's 74%.Debates are also more efficient, being 68% of the length of consultancies.By comparing human to AI debaters, we find evidence that with more skilled (in this case, human) debaters, the performance of debate goes up but the performance of consultancy goes down.Our error analysis also supports this trend, with 46% of errors in human debate attributable to mistakes by the honest debater (which should go away with increased skill); whereas 52% of errors in human consultancy are due to debaters obfuscating the relevant evidence from the judge (which should become worse with increased skill).Overall, these results show that debate is a promising approach for supervising increasingly capable but potentially unreliable AI systems.",
        "sentences": [
          {
            "text": "Abstract: As AI systems are used to answer more difficult questions and potentially help create new knowledge, judging the truthfulness of their outputs becomes more difficult and more important.",
            "label": 0
          },
          {
            "text": "How can we supervise unreliable experts-which have access to the truth but may not accurately report it-to give answers that are systematically true and don't just superficially seem true, when the supervisor can't tell the difference between the two on their own?",
            "label": 0
          },
          {
            "text": "In this work, we show that debate between two unreliable experts can help a non-expert judge more reliably identify the truth.",
            "label": 0
          },
          {
            "text": "We collect a dataset of human-written debates on hard reading comprehension questions where the judge has not read the source passage, only ever seeing expert arguments and short quotes selectively revealed by 'expert' debaters who have access to the passage.",
            "label": 1
          },
          {
            "text": "In our debates, one expert argues for the correct answer, and the other for an incorrect answer.",
            "label": 1
          },
          {
            "text": "Comparing debate to a baseline we call consultancy, where a single expert argues for only one answer which is correct half of the time, we find that debate performs significantly better, with 84% judge accuracy compared to consultancy's 74%.",
            "label": 1
          },
          {
            "text": "Debates are also more efficient, being 68% of the length of consultancies.",
            "label": 1
          },
          {
            "text": "By comparing human to AI debaters, we find evidence that with more skilled (in this case, human) debaters, the performance of debate goes up but the performance of consultancy goes down.",
            "label": 1
          },
          {
            "text": "Our error analysis also supports this trend, with 46% of errors in human debate attributable to mistakes by the honest debater (which should go away with increased skill); whereas 52% of errors in human consultancy are due to debaters obfuscating the relevant evidence from the judge (which should become worse with increased skill).",
            "label": 1
          },
          {
            "text": "Overall, these results show that debate is a promising approach for supervising increasingly capable but potentially unreliable AI systems.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Debate Helps Supervise Unreliable Experts",
        "section": 1,
        "paragraph_id": 1,
        "full_text": "How can we tell if an AI system is telling the truth?Current language models trained to act as AI assistants, such asGPT-4 (OpenAI, 2023a)and Claude(Anthropic, 2023b,a) can correctly answer a wide variety of questions, construct coherent essays, and perform well on academic and professional exams(Hendrycks et al., 2020;OpenAI, 2023b).But the truthfulness of their responses is not robust: Such systems are prone to making false claims, giving misleading explanations about their reasoning Figure1: High-level summary of our experimental setup.We source hard reading comprehension questions from the QuALITY dataset(Pang et al., 2022)and incentivize human judges who can't read the passage to answer them correctly.Experts who have full access to the passage are allowed to reveal snippets of it (highlighted) in addition to free-text prose.In debate, the experts simultaneously defend their assigned option in their opening statements, and following rounds are sequential.In consultancy, the non-expert judge only interacts with one expert defending one option chosen at random.In both settings, the judge chooses when to end the session; sessions average at about 1,000 words total.(Turpin et al., 2023), and reinforcing the inferred opinions of their interlocutors(Perez et al., 2022;Bang et al., 2023;Borji, 2023).",
        "sentences": [
          {
            "text": "How can we tell if an AI system is telling the truth?",
            "label": 0
          },
          {
            "text": "Current language models trained to act as AI assistants, such asGPT-4 (OpenAI, 2023a)and Claude(Anthropic, 2023b,a) can correctly answer a wide variety of questions, construct coherent essays, and perform well on academic and professional exams(Hendrycks et al., 2020;OpenAI, 2023b).",
            "label": 0
          },
          {
            "text": "But the truthfulness of their responses is not robust: Such systems are prone to making false claims, giving misleading explanations about their reasoning Figure1: High-level summary of our experimental setup.",
            "label": 0
          },
          {
            "text": "We source hard reading comprehension questions from the QuALITY dataset(Pang et al., 2022)and incentivize human judges who can't read the passage to answer them correctly.",
            "label": 1
          },
          {
            "text": "Experts who have full access to the passage are allowed to reveal snippets of it (highlighted) in addition to free-text prose.",
            "label": 1
          },
          {
            "text": "In debate, the experts simultaneously defend their assigned option in their opening statements, and following rounds are sequential.In consultancy, the non-expert judge only interacts with one expert defending one option chosen at random.",
            "label": 1
          },
          {
            "text": "In both settings, the judge chooses when to end the session; sessions average at about 1,000 words total.(Turpin et al., 2023), and reinforcing the inferred opinions of their interlocutors(Perez et al., 2022;Bang et al., 2023;Borji, 2023).",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Debate Helps Supervise Unreliable Experts",
        "section": 2,
        "paragraph_id": 2,
        "full_text": "Language models have access to a vast array of information from their training data to draw on and synthesize-far beyond the knowledge of any individual human who might be involved in supervising them.As such, they could hold the potential to help us answer increasingly difficult questions or even create new knowledge that we otherwise couldn't.However, we expect that it will be increasingly hard to verify and supervise the truthfulness of their outputs in these cases.As language models become more capable and are used in more complex settings, it is likely that subtle mistakes, deceptive arguments, or selective use of evidence will become more difficult to spot.Making sure the information they provide is reliable requires effective methods for verifying the outputs of systems that know things we don't-a task known as scalable oversight(Amodei et al., 2016).Proposals for scalable oversight often involve leveraging the AI's abilities to help evaluators, for example with recursive reward modeling(Leike et al., 2018), model self-critique(Saunders et al., 2022), and debate(Irving et al., 2018).",
        "sentences": [
          {
            "text": "Language models have access to a vast array of information from their training data to draw on and synthesize-far beyond the knowledge of any individual human who might be involved in supervising them.",
            "label": 0
          },
          {
            "text": "As such, they could hold the potential to help us answer increasingly difficult questions or even create new knowledge that we otherwise couldn't.",
            "label": 0
          },
          {
            "text": "However, we expect that it will be increasingly hard to verify and supervise the truthfulness of their outputs in these cases.",
            "label": 0
          },
          {
            "text": "As language models become more capable and are used in more complex settings, it is likely that subtle mistakes, deceptive arguments, or selective use of evidence will become more difficult to spot.",
            "label": 0
          },
          {
            "text": "Making sure the information they provide is reliable requires effective methods for verifying the outputs of systems that know things we don't-a task known as scalable oversight(Amodei et al., 2016).",
            "label": 0
          },
          {
            "text": "Proposals for scalable oversight often involve leveraging the AI's abilities to help evaluators, for example with recursive reward modeling(Leike et al., 2018), model self-critique(Saunders et al., 2022), and debate(Irving et al., 2018).",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Debate Helps Supervise Unreliable Experts",
        "section": 3,
        "paragraph_id": 3,
        "full_text": "In debate-the focus of this work-two equally-capable expert debaters (e.g., AI systems) argue with each other over the answer to a question, each aiming to convince a non-expert (human) judge of their side.With an adversarial expert pointing out flaws in its arguments, neither debater will be able to get away with claims that its opponent can convincingly refute in the eyes of the judge.Training AI systems to win such debates should incentivize them not to make such claims in the first place.AsIrving et al. (2018)argue, this means that debate would incentivize an AI to tell the truth, as long as it is harder to lie than to refute a lie-i.e., the most successful strategies for debate lead judges to make good, informed decisions, rather than, for example, tricking them, confusing them, or prolonging the debate indefinitely.",
        "sentences": [
          {
            "text": "In debate-the focus of this work-two equally-capable expert debaters (e.g., AI systems) argue with each other over the answer to a question, each aiming to convince a non-expert (human) judge of their side.",
            "label": 0
          },
          {
            "text": "With an adversarial expert pointing out flaws in its arguments, neither debater will be able to get away with claims that its opponent can convincingly refute in the eyes of the judge.",
            "label": 0
          },
          {
            "text": "Training AI systems to win such debates should incentivize them not to make such claims in the first place.",
            "label": 0
          },
          {
            "text": "AsIrving et al. (2018)argue, this means that debate would incentivize an AI to tell the truth, as long as it is harder to lie than to refute a lie-i.e., the most successful strategies for debate lead judges to make good, informed decisions, rather than, for example, tricking them, confusing them, or prolonging the debate indefinitely.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Debate Helps Supervise Unreliable Experts",
        "section": 4,
        "paragraph_id": 4,
        "full_text": "In this paper, we demonstrate for the first time that debate helps judges find truth on a realistic task, using debates on hard reading comprehension questions.To test this, we compare debate to a baseline we call consultancy, where the judge interacts with a single unreliable expert who has a 50/50 chance of arguing for the correct answer.By prompting the consultant to argue for the wrong answer half of the time, this baseline explicitly elicits dishonest behavior which may arise implicitly in Reinforcement Learning from Human Feedback (RLHF), as in cases, e.g., of sycophancy(Perez et al., 2022).To evaluate this with the strongest possible debaters, we collect and analyze a dataset of all-human debates, enlisting competitive debaters from the New York University debate team.",
        "sentences": [
          {
            "text": "In this paper, we demonstrate for the first time that debate helps judges find truth on a realistic task, using debates on hard reading comprehension questions.",
            "label": 0
          },
          {
            "text": "To test this, we compare debate to a baseline we call consultancy, where the judge interacts with a single unreliable expert who has a 50/50 chance of arguing for the correct answer.",
            "label": 0
          },
          {
            "text": "By prompting the consultant to argue for the wrong answer half of the time, this baseline explicitly elicits dishonest behavior which may arise implicitly in Reinforcement Learning from Human Feedback (RLHF), as in cases, e.g., of sycophancy(Perez et al., 2022).",
            "label": 1
          },
          {
            "text": "To evaluate this with the strongest possible debaters, we collect and analyze a dataset of all-human debates, enlisting competitive debaters from the New York University debate team.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Debate Helps Supervise Unreliable Experts",
        "section": 5,
        "paragraph_id": 5,
        "full_text": "A high-level overview of our setup is illustrated in Figure1.2For each debate, we pose a reading comprehension question from the QuALITY dataset(Pang et al., 2022)together with two answer choices (one correct, one incorrect), and allow the debaters-but not the judge-to read the story the question is about.The judge then interactively judges a debate on the question, where the debaters can back up their claims by selectively revealing short excerpts drawn from the story.Judge accuracy in these debates is 84%, compared to with 74% on consultancy (Section 4).Debate is also more efficient, being 68% of the length and requiring 61% as much ground-truth evidence, suggesting that it will be a more effective method than open-ended dialogue (cf.Bowman et al., 2022)for helping annotators efficiently supervise untrusted models that exceed their expertise.We also find that our judges are relatively calibrated overall on debates, though they struggle with overconfidence in the high-confidence regime (Figure5).While there are still cases when the judge of a debate gets the answer wrong, we find that the most common sources of error should be possible to mitigate with further judge training or stronger debaters.For example, in 33% of mistakes, the judge ended the session prematurely, either after only a single round or immediately after changing their preferred answer, giving the debaters no opportunity to refute the judge's final reasoning.In 46% of mistakes, the debater arguing for the correct answer missed an important argument or piece of evidence that they could have used (Section 5).",
        "sentences": [
          {
            "text": "A high-level overview of our setup is illustrated in Figure1.2For each debate, we pose a reading comprehension question from the QuALITY dataset(Pang et al., 2022)together with two answer choices (one correct, one incorrect), and allow the debaters-but not the judge-to read the story the question is about.",
            "label": 1
          },
          {
            "text": "The judge then interactively judges a debate on the question, where the debaters can back up their claims by selectively revealing short excerpts drawn from the story.",
            "label": 1
          },
          {
            "text": "Judge accuracy in these debates is 84%, compared to with 74% on consultancy (Section 4).",
            "label": 1
          },
          {
            "text": "Debate is also more efficient, being 68% of the length and requiring 61% as much ground-truth evidence, suggesting that it will be a more effective method than open-ended dialogue (cf.Bowman et al., 2022)for helping annotators efficiently supervise untrusted models that exceed their expertise.",
            "label": 1
          },
          {
            "text": "We also find that our judges are relatively calibrated overall on debates, though they struggle with overconfidence in the high-confidence regime (Figure5).",
            "label": 1
          },
          {
            "text": "While there are still cases when the judge of a debate gets the answer wrong, we find that the most common sources of error should be possible to mitigate with further judge training or stronger debaters.",
            "label": 1
          },
          {
            "text": "For example, in 33% of mistakes, the judge ended the session prematurely, either after only a single round or immediately after changing their preferred answer, giving the debaters no opportunity to refute the judge's final reasoning.",
            "label": 1
          },
          {
            "text": "In 46% of mistakes, the debater arguing for the correct answer missed an important argument or piece of evidence that they could have used (Section 5).",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Debate Helps Supervise Unreliable Experts",
        "section": 6,
        "paragraph_id": 6,
        "full_text": "We also include experimental results for AI debate, using GPT-4 as a debater (Section 4).In this setting, we find no difference between debate and consultancy.However, even if debate does not work better as an oversight method for current models, that may simply be because they have not yet reached human-level capabilities at deception (i.e., as a consultant) and argumentation (as a debater); it is also possible that we do not optimize GPT-4's prompt heavily enough to elicit such capabilities.It seems plausible that AI systems may soon be capable enough of argumentation and persuasion that debate will be important to incorporate into their training; in Section 8 we lay out an agenda for what this may look like, and what challenges will need to be solved to make this work.",
        "sentences": [
          {
            "text": "We also include experimental results for AI debate, using GPT-4 as a debater (Section 4).",
            "label": 1
          },
          {
            "text": "In this setting, we find no difference between debate and consultancy.",
            "label": 1
          },
          {
            "text": "However, even if debate does not work better as an oversight method for current models, that may simply be because they have not yet reached human-level capabilities at deception (i.e., as a consultant) and argumentation (as a debater); it is also possible that we do not optimize GPT-4's prompt heavily enough to elicit such capabilities.",
            "label": 0
          },
          {
            "text": "It seems plausible that AI systems may soon be capable enough of argumentation and persuasion that debate will be important to incorporate into their training; in Section 8 we lay out an agenda for what this may look like, and what challenges will need to be solved to make this work.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Debate Helps Supervise Unreliable Experts",
        "section": 7,
        "paragraph_id": 7,
        "full_text": "As we use AI systems in more difficult and complex settings, we will need stronger mechanisms to verify their arguments-ideally, methods which improve concordantly and at pace with the system's capabilities.Our results with human debaters demonstrate for the first time that debate, where equally-capable experts point out flaws with each other's arguments, can allow a non-expert judge to effectively determine the answers to questions they could not answer on their own.This suggests that debate may soon be important for effectively supervising models to truthfully answer hard questions.",
        "sentences": [
          {
            "text": "As we use AI systems in more difficult and complex settings, we will need stronger mechanisms to verify their arguments-ideally, methods which improve concordantly and at pace with the system's capabilities.",
            "label": 0
          },
          {
            "text": "Our results with human debaters demonstrate for the first time that debate, where equally-capable experts point out flaws with each other's arguments, can allow a non-expert judge to effectively determine the answers to questions they could not answer on their own.",
            "label": 1
          },
          {
            "text": "This suggests that debate may soon be important for effectively supervising models to truthfully answer hard questions.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Debate Helps Supervise Unreliable Experts",
        "section": 8,
        "paragraph_id": 8,
        "full_text": "Source Material and Questions We draw the questions to be debated from the Question Answering with Long Input Texts, Yes! (QuALITY) dataset of reading comprehension questions(Pang et al., 2022).We use the QuALITY-hard subset, where a majority of 5 annotators with only 45 seconds of access to the story got the question wrong while all annotators with untimed access to the story got the answer correct.4To focus on especially hard questions, we further restrict our results to questions that were marked by the untimed annotators as requiring more than one or two sentences of context to get correct (the idea being to avoid questions which could be easily resolved with a single quote from one of the debaters).Each question in QuALITY has four answers, one of which is correct; as our debates consider only two answer choices, we use the correct answer and the incorrect option that was labeled as the best distractor most often by the QuALITY dataset's untimed validation annotators.",
        "sentences": [
          {
            "text": "Source Material and Questions We draw the questions to be debated from the Question Answering with Long Input Texts, Yes!",
            "label": 1
          },
          {
            "text": "(QuALITY) dataset of reading comprehension questions(Pang et al., 2022).",
            "label": 1
          },
          {
            "text": "4To focus on especially hard questions, we further restrict our results to questions that were marked by the untimed annotators as requiring more than one or two sentences of context to get correct (the idea being to avoid questions which could be easily resolved with a single quote from one of the debaters).",
            "label": 1
          },
          {
            "text": "Each question in QuALITY has four answers, one of which is correct; as our debates consider only two answer choices, we use the correct answer and the incorrect option that was labeled as the best distractor most often by the QuALITY dataset's untimed validation annotators.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Debate Helps Supervise Unreliable Experts",
        "section": 9,
        "paragraph_id": 9,
        "full_text": "We only use the Project Gutenberg subset of QuALITY-hard, which consists of questions over public-domain science fiction short stories.Since the stories are entirely fictional, judges can almost never guess the answer on the basis of prior knowledge, and must rely on the information provided by the debaters.5.",
        "sentences": [
          {
            "text": "We only use the Project Gutenberg subset of QuALITY-hard, which consists of questions over public-domain science fiction short stories.",
            "label": 1
          },
          {
            "text":"Since the stories are entirely fictional, judges can almost never guess the answer on the basis of prior knowledge, and must rely on the information provided by the debaters.5.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Debate Helps Supervise Unreliable Experts",
        "section": 10,
        "paragraph_id": 10,
        "full_text": "On average, the stories used for our debates have 27.7k characters, or 6.3k tokens using the CoreNLP tokenizer(Manning et al., 2014).For each turn, we use a character limit of ℓ  = 750 and a quote limit of ℓ  = 250, meaning that on average up to 1.8% of the story could be revealed in each round of the debate.",
        "sentences": [
          {
            "text": "On average, the stories used for our debates have 27.7k characters, or 6.3k tokens using the CoreNLP tokenizer(Manning et al., 2014).",
            "label": 1
          },
          {
            "text": "For each turn, we use a character limit of ℓ  = 750 and a quote limit of ℓ  = 250, meaning that on average up to 1.8% of the story could be revealed in each round of the debate.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Debate Helps Supervise Unreliable Experts",
        "section": 11,
        "paragraph_id": 11,
        "full_text": "Experimental Conditions While our main experimental results concern human debaters, we also test with AI debaters.As the AI debater, we use the version of GPT-4 with a 32,000-token context window available through the OpenAI API as gpt-4-32k.Prompts are provided in Appendix G.We use human judges in all experiments.6This gives us four experimental conditions: human debate, human consultancy, AI debate, and AI consultancy.",
        "sentences": [
          {
            "text": "Experimental Conditions While our main experimental results concern human debaters, we also test with AI debaters.",
            "label": 0
          },
          {
            "text": "As the AI debater, we use the version of GPT-4 with a 32,000-token context window available through the OpenAI API as gpt-4-32k.",
            "label": 0
          },
          {
            "text": "Prompts are provided in Appendix G.",
            "label": 0
          },
          {
            "text": "6This gives us four experimental conditions: human debate, human consultancy, AI debate, and AI consultancy.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Debate Helps Supervise Unreliable Experts",
        "section": 12,
        "paragraph_id": 12,
        "full_text": "We recruit 19 people to serve as both debaters and judges in our experiments.Our participants, all of whom were New York University employees during data collection, include 12 undergraduates on the NYU debate team, all with at least one year of experience in competitive debate; 6 members of the research team, three of whom have at least one year of experience with competitive debate; and one NYU Master's student with 6 years of experience studying Jewish legal reasoning and argumentation.",
        "sentences": [
          {
            "text": "We recruit 19 people to serve as both debaters and judges in our experiments.",
            "label": 1
          },
          {
            "text": "Our participants, all of whom were New York University employees during data collection, include 12 undergraduates on the NYU debate team, all with at least one year of experience in competitive debate; 6 members of the research team, three of whom have at least one year of experience with competitive debate; and one NYU Master's student with 6 years of experience studying Jewish legal reasoning and argumentation.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Debate Helps Supervise Unreliable Experts",
        "section": 13,
        "paragraph_id": 13,
        "full_text": "After running initial pilots in Fall of 2022 to establish the debate protocol (see Appendix B), we collect debates according to the protocol defined in Section 2, with collection running from February to August of 2023.During collection, participants can log into our data collection platform to read stories or take turns in their debates at any time, but we also set aside specific times each week when we request that the debaters work, to facilitate near-synchronous debates.To avoid information leakage between debates, each participant is only allowed to judge one question about each story.After each debate is complete, all participants fill out a feedback survey with quantitative and qualitative observations which we use to help us analyze the results (see Appendix F).Participants cannot see the identities of the other participants in the debate until after filling out the feedback form.Data collection was not perfectly controlled between our four experimental conditions, as some components of our experimental design were developed part of the way through data collection: The consultancy baseline was only developed in June of 2023 and the AI debaters were only incorporated 4We ended up drawing 59% of our questions from the QuaLITY training set, which has 3 untimed validators per question, and 41% from the development set, which has 5 untimed validators per question.",
        "sentences": [
          {
            "text": "After running initial pilots in Fall of 2022 to establish the debate protocol (see Appendix B), we collect debates according to the protocol defined in Section 2, with collection running from February to August of 2023.",
            "label": 1
          },
          {
            "text": "During collection, participants can log into our data collection platform to read stories or take turns in their debates at any time, but we also set aside specific times each week when we request that the debaters work, to facilitate near-synchronous debates.",
            "label": 1
          },
          {
            "text": "To avoid information leakage between debates, each participant is only allowed to judge one question about each story.",
            "label": 1
          },
          {
            "text": "After each debate is complete, all participants fill out a feedback survey with quantitative and qualitative observations which we use to help us analyze the results (see Appendix F).",
            "label": 1
          },
          {
            "text": "Participants cannot see the identities of the other participants in the debate until after filling out the feedback form.",
            "label": 1
          },
          {
            "text": "Data collection was not perfectly controlled between our four experimental conditions, as some components of our experimental design were developed part of the way through data collection: The consultancy baseline was only developed in June of 2023 and the AI debaters were only incorporated 4We ended up drawing 59% of our questions from the QuaLITY training set, which has 3 untimed validators per question, and 41% from the development set, which has 5 untimed validators per question.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Debate Helps Supervise Unreliable Experts",
        "section": 14,
        "paragraph_id": 14,
        "full_text": "5In our data, judge priors were between 45%-55% in 91% of debates, and between 35%-65% in 97% of debates.",
        "sentences": [
          {
            "text": "5In our data, judge priors were between 45%-55% in 91% of debates, and between 35%-65% in 97% of debates.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Debate Helps Supervise Unreliable Experts",
        "section": 15,
        "paragraph_id": 15,
        "full_text": "6Because the QuALITY questions are drawn from public-domain texts available from Project Gutenberg, it is likely that the passages used in our experiments appear in GPT-4's training corpus.This does not pose a data contamination issue when using GPT-4 as a debater, since debaters are meant to be experts and are given full access to the text anyway.However, this does could pose issues for future work testing AI judging in our setting, since it might be difficult to guarantee that the models do not use prior knowledge of the story in their decisions, instead of relying on the debate.Quotes and characters per round measure how close debaters came to their character limits; quote totals are calculated only using new quoted material that hasn't been used yet in the debate.Bits/rd is the amount of information conveyed to the judge on average per round, calculated from the information gain between their final and initial judgment log 2 (  * , / * ,0 ),   is the judge's score as defined in Equation1, and ECE final is the expected calibration error of the judge's final judgments, calculated with a bin size of 10%. into the data collection platform in July.The set of debaters who participated in the experiment also varied over the course of these months.These factors were due to us trying to collect as much data as possible subject to practical limits on engineering capacity, annotator availability, and researcher foresight.We validate our analysis in Section 4 with partial controls in Appendix D.",
        "sentences": [
          {
            "text": "6Because the QuALITY questions are drawn from public-domain texts available from Project Gutenberg, it is likely that the passages used in our experiments appear in GPT-4's training corpus.",
            "label": 1
          },
          {
            "text": "This does not pose a data contamination issue when using GPT-4 as a debater, since debaters are meant to be experts and are given full access to the text anyway.",
            "label": 1
          },
          {
            "text": "However, this does could pose issues for future work testing AI judging in our setting, since it might be difficult to guarantee that the models do not use prior knowledge of the story in their decisions, instead of relying on the debate.",
            "label": 1
          },
          {
            "text": "Quotes and characters per round measure how close debaters came to their character limits; quote totals are calculated only using new quoted material that hasn't been used yet in the debate.",
            "label": 0
          },
          {
            "text": "Bits/rd is the amount of information conveyed to the judge on average per round, calculated from the information gain between their final and initial judgment log 2 (  * , / * ,0 ),   is the judge's score as defined in Equation1, and ECE final is the expected calibration error of the judge's final judgments, calculated with a bin size of 10%. into the data collection platform in July.",
            "label": 1
          },
          {
            "text": "The set of debaters who participated in the experiment also varied over the course of these months.",
            "label": 1
          },
          {
            "text": "These factors were due to us trying to collect as much data as possible subject to practical limits on engineering capacity, annotator availability, and researcher foresight.",
            "label": 1
          },
          {
            "text": "We validate our analysis in Section 4 with partial controls in Appendix D.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Distilling Large Language Models for Matching Patients to Clinical Trials",
        "section": 0,
        "paragraph_id": 0,
        "full_text": "Abstract: The recent success of large language models (LLMs) has paved the way for their adoption in the high-stakes domain of healthcare.Specifically, the application of LLMs in patient-trial matching, which involves assessing patient eligibility against clinical trial's nuanced inclusion and exclusion criteria, has shown promise.Recent research has shown that GPT-3.5, a widely recognized LLM developed by OpenAI, can outperform existing methods with minimal 'variable engineering' by simply comparing clinical trial information against patient summaries.However, there are significant challenges associated with using closed-source proprietary LLMs like GPT-3.5 in practical healthcare applications, such as cost, privacy and reproducibility concerns.To address these issues, this study presents the first systematic examination of the efficacy of both proprietary (GPT-3.5, and GPT-4) and open-source LLMs (LLAMA 7B,13B, and 70B) for the task of patient-trial matching.Employing a multifaceted evaluation framework, we conducted extensive automated and human-centric assessments coupled with a detailed error analysis for each model.To enhance the adaptability of open-source LLMs, we have created a specialized synthetic dataset utilizing GPT-4, enabling effective fine-tuning under constrained data conditions.Our findings reveal that open-source LLMs, when fine-tuned on this limited and synthetic dataset, demonstrate performance parity with their proprietary counterparts.This presents a massive opportunity for their deployment in real-world healthcare applications.To foster further research and applications in this field, we release both the annotated evaluation dataset along with the fine-tuned LLM -Trial-LLAMA -for public use.",
        "sentences": [
          {
            "text": "Abstract: The recent success of large language models (LLMs) has paved the way for their adoption in the high-stakes domain of healthcare.",
            "label": 0
          },
          {
            "text": "Specifically, the application of LLMs in patient-trial matching, which involves assessing patient eligibility against clinical trial's nuanced inclusion and exclusion criteria, has shown promise.",
            "label": 0
          },
          {
            "text": "Recent research has shown that GPT-3.5, a widely recognized LLM developed by OpenAI, can outperform existing methods with minimal 'variable engineering' by simply comparing clinical trial information against patient summaries.",
            "label": 0
          },
          {
            "text": "However, there are significant challenges associated with using closed-source proprietary LLMs like GPT-3.5 in practical healthcare applications, such as cost, privacy and reproducibility concerns.",
            "label": 0
          },
          {
            "text": "To address these issues, this study presents the first systematic examination of the efficacy of both proprietary (GPT-3.5, and GPT-4) and open-source LLMs (LLAMA 7B,13B, and 70B) for the task of patient-trial matching.",
            "label": 0
          },
          {
            "text": "Employing a multifaceted evaluation framework, we conducted extensive automated and human-centric assessments coupled with a detailed error analysis for each model.",
            "label": 0
          },
          {
            "text": "To enhance the adaptability of open-source LLMs, we have created a specialized synthetic dataset utilizing GPT-4, enabling effective fine-tuning under constrained data conditions.",
            "label": 1
          },
          {
            "text": "Our findings reveal that open-source LLMs, when fine-tuned on this limited and synthetic dataset, demonstrate performance parity with their proprietary counterparts.",
            "label": 0
          },
          {
            "text": "This presents a massive opportunity for their deployment in real-world healthcare applications.",
            "label": 0
          },
          {
            "text": "To foster further research and applications in this field, we release both the annotated evaluation dataset along with the fine-tuned LLM -Trial-LLAMA -for public use.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Distilling Large Language Models for Matching Patients to Clinical Trials",
        "section": 1,
        "paragraph_id": 1,
        "full_text": "Clinical trials represent both the most important and the most challenging aspect of medical advancements.These trials serve a dual function: first, as a conduit for patients to access potentially life-altering treatments, and second, as a mechanism for the iterative process of drug development and approval.However, a significant number of trials are beleaguered by extended timelines.Empirical data suggests that, on average, clinical trials take approximately twice as long as initially projected[22], with approximately 40% of trial sites failing to meet their enrollment targets[15].Apart from others, one of the major challenges in recruiting patients is matching them against suitable trials[5,20,6,26,12,28,19,29].",
        "sentences": [
          {
            "text": "Clinical trials represent both the most important and the most challenging aspect of medical advancements.",
            "label": 0
          },
          {
            "text": "These trials serve a dual function: first, as a conduit for patients to access potentially life-altering treatments, and second, as a mechanism for the iterative process of drug development and approval.However, a significant number of trials are beleaguered by extended timelines.",
            "label": 0
          },
          {
            "text": "Empirical data suggests that, on average, clinical trials take approximately twice as long as initially projected[22], with approximately 40% of trial sites failing to meet their enrollment targets[15].",
            "label": 0
          },
          {
            "text": "Apart from others, one of the major challenges in recruiting patients is matching them against suitable trials[5,20,6,26,12,28,19,29].",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Distilling Large Language Models for Matching Patients to Clinical Trials",
        "section": 2,
        "paragraph_id": 2,
        "full_text": "The process of matching a patient to trials is challenging.It requires both the meticulous analysis of electronic health records (EHRs) and the contextual interpretation of this data against the backdrop of clinical trial criteria.This is particularly challenging because a majority of this data is stored in unstructured documents written in free text.Even the structured data is difficult to query due to the increasing complexity of inclusion and exclusion criteria.",
        "sentences": [
          {
            "text": "The process of matching a patient to trials is challenging.",
            "label": 0
          },
          {
            "text": "It requires both the meticulous analysis of electronic health records (EHRs) and the contextual interpretation of this data against the backdrop of clinical trial criteria.",
            "label": 0
          },
          {
            "text": "This is particularly challenging because a majority of this data is stored in unstructured documents written in free text.",
            "label": 0
          },
          {
            "text": "Even the structured data is difficult to query due to the increasing complexity of inclusion and exclusion criteria.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Distilling Large Language Models for Matching Patients to Clinical Trials",
        "section": 3,
        "paragraph_id": 3,
        "full_text": "Automating this process can accelerate trials save healthcare providers' time spent on manual chart reviews.Current approaches primarily rely on data extraction or classification pipelines[36,49,27].Nonetheless, these methods require extensive variable engineering, which frequently results in constrained contextual comprehension and limited scalability when dealing with intricate trial criteria.",
        "sentences": [
          {
            "text": "Automating this process can accelerate trials save healthcare providers' time spent on manual chart reviews.",
            "label": 0
          },
          {
            "text": "Current approaches primarily rely on data extraction or classification pipelines[36,49,27].",
            "label": 0
          },
          {
            "text": "Nonetheless, these methods require extensive variable engineering, which frequently results in constrained contextual comprehension and limited scalability when dealing with intricate trial criteria.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Distilling Large Language Models for Matching Patients to Clinical Trials",
        "section": 4,
        "paragraph_id": 4,
        "full_text": "The emergence of Large Language Models (LLMs), such as Med-PaLM[37]and GPT-4[25], marks a paradigm shift in the domain of automated interpretation of patient health records.These models embody the cutting-edge in natural language processing (NLP), facilitating nuanced and context-aware analysis of complex medical data.Leveraging their capabilities, recent research has used these models for a variety of clinical information interpretation tasks, including patient matching[18].However, their deployment in healthcare settings presents challenges.",
        "sentences": [
          {
            "text": "The emergence of Large Language Models (LLMs), such as Med-PaLM[37]and GPT-4[25], marks a paradigm shift in the domain of automated interpretation of patient health records.",
            "label": 0
          },
          {
            "text": "These models embody the cutting-edge in natural language processing (NLP), facilitating nuanced and context-aware analysis of complex medical data.",
            "label": 0
          },
          {
            "text": "Leveraging their capabilities, recent research has used these models for a variety of clinical information interpretation tasks, including patient matching[18].",
            "label": 0
          },
          {
            "text": "However, their deployment in healthcare settings presents challenges.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Distilling Large Language Models for Matching Patients to Clinical Trials",
        "section": 5,
        "paragraph_id": 5,
        "full_text": "One primary concern relates to the risk of Protected Health Information (PHI) leakage when using such models.Most healthcare organisations prefer on-premise infrastructure for tools that handle identified patient data.However, due to the cost and computational complexity associated with these models, they often remain in centralized cloud provider environments.These challenges can make LLMs prohibitive for widespread clinical application.Moreover, despite their effectiveness, advanced models are often characterized by opacity and proprietary restrictions, which further complicate their integration into healthcare systems subject to stringent regulatory constraints.",
        "sentences": [
          {
            "text": "One primary concern relates to the risk of Protected Health Information (PHI) leakage when using such models.",
            "label": 0
          },
          {
            "text": "Most healthcare organisations prefer on-premise infrastructure for tools that handle identified patient data.",
            "label": 0
          },
          {
            "text": "However, due to the cost and computational complexity associated with these models, they often remain in centralized cloud provider environments.",
            "label": 0
          },
          {
            "text": "These challenges can make LLMs prohibitive for widespread clinical application.",
            "label": 0
          },
          {
            "text": "Moreover, despite their effectiveness, advanced models are often characterized by opacity and proprietary restrictions, which further complicate their integration into healthcare systems subject to stringent regulatory constraints.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Distilling Large Language Models for Matching Patients to Clinical Trials",
        "section": 6,
        "paragraph_id": 6,
        "full_text": "In light of these considerations, there is a growing need for the development of open-source LLMs that can match the accuracy of their proprietary counterparts but at a significantly reduced cost.This also enables healthcare organizations to seamlessly integrate these technologies into their existing infrastructures, mitigating the risk of Protected Health Information (PHI) leaks.",
        "sentences": [
          {
            "text": "In light of these considerations, there is a growing need for the development of open-source LLMs that can match the accuracy of their proprietary counterparts but at a significantly reduced cost.",
            "label": 0
          },
          {
            "text": "This also enables healthcare organizations to seamlessly integrate these technologies into their existing infrastructures, mitigating the risk of Protected Health Information (PHI) leaks.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Distilling Large Language Models for Matching Patients to Clinical Trials",
        "section": 7,
        "paragraph_id": 7,
        "full_text": "To the best of our knowledge, this study is the first comprehensive examination of the efficacy of open-source LLMs in this domain.Our contributions are as follows -",
        "sentences": [
          {
            "text": "To the best of our knowledge, this study is the first comprehensive examination of the efficacy of open-source LLMs in this domain.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Distilling Large Language Models for Matching Patients to Clinical Trials",
        "section": 8,
        "paragraph_id": 8,
        "full_text": "• Our work thoroughly compares open-source LLMs and their proprietary counterparts for patienttrial matching.",
        "sentences": [
          {
            "text": "• Our work thoroughly compares open-source LLMs and their proprietary counterparts for patienttrial matching.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Distilling Large Language Models for Matching Patients to Clinical Trials",
        "section": 9,
        "paragraph_id": 9,
        "full_text": "• We further explore and elucidate the impact of fine-tuning on various open-source LLMs for patient-trial matching.",
        "sentences": [
          {
            "text": "• We further explore and elucidate the impact of fine-tuning on various open-source LLMs for patient-trial matching.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Distilling Large Language Models for Matching Patients to Clinical Trials",
        "section": 10,
        "paragraph_id": 10,
        "full_text": "• We define the error taxonomy and thoroughly analyze the nature of errors made by the models on this task.",
        "sentences": [
          {
            "text": "• We define the error taxonomy and thoroughly analyze the nature of errors made by the models on this task.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Distilling Large Language Models for Matching Patients to Clinical Trials",
        "section": 11,
        "paragraph_id": 11,
        "full_text": "• Along with the experimental details, we publicly release the evaluation dataset and the LLM trained based on LLAMA for patient-trial matching. 2 Related Work",
        "sentences": [
          {
            "text": "• Along with the experimental details, we publicly release the evaluation dataset and the LLM trained based on LLAMA for patient-trial matching.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Distilling Large Language Models for Matching Patients to Clinical Trials",
        "section": 12,
        "paragraph_id": 12,
        "full_text": "We tested both proprietary (GPT-3.5, and GPT-4) and open-source LLMs (LLAMA-2 7B,13B, and 70B, referred to LLAMA hereafter) for the task of patient-trial matching.For GPT-3.5, we leveraged the Azure Open AI API, specifically gpt-35-turbo-16k-0613 as the model version.We set the temperature parameter to 0, aiming for deterministic outputs that would ensure consistency and repeatability in our experiments.This was coupled with a top p setting of 0.95, aligning with our goal to eliminate randomness in the model's response generation process.Additionally, we refrained from applying any frequency or repetition penalties, allowing the model's natural language generation capabilities to function without these constraints.For GPT-4, we employed a similar configuration with gpt-4-0613 as the model version.For LLAMA, we changed the configuration from Meta.We initially encountered challenges in aligning the standard versions of these models to produce outputs in the required format, particularly in the context of complex clinical trial criteria.To address this, we opted for specific versions tailored for chat applications, namely Llama-2-7b-chat-hf, Llama-2-13b-chat-hf, and Llama-2-70b-chat-hf.These versions offered a more flexible and adaptable framework for our needs.We adjusted the temperature setting to 0.4 for all LLAMA models, a decision informed by preliminary tests which indicated that a slightly higher temperature prevented the model from collapsing on certain trials where inclusion/exclusion criteria were not clearly defined.Maintaining the output format was particularly challenging when working with the base LLAMA models.Despite employing various techniques such as context-free grammar (CFG) to constrain the model's output, the results remained suboptimal.Consequently, the models were unable to generate structured output for complex clinical trials.To circumvent this, we adjusted the model's temperature to foster more exploratory behavior and executed five output generations iteratively till the output matched our JSON schema.This allowed us to generated structured output for majority of clinical trials even with base models.",
        "sentences": [
          {
            "text": "We tested both proprietary (GPT-3.5, and GPT-4) and open-source LLMs (LLAMA-2 7B,13B, and 70B, referred to LLAMA hereafter) for the task of patient-trial matching.",
            "label": 0
          },
          {
            "text": "For GPT-3.5, we leveraged the Azure Open AI API, specifically gpt-35-turbo-16k-0613 as the model version.",
            "label": 0
          },
          {
            "text": "We set the temperature parameter to 0, aiming for deterministic outputs that would ensure consistency and repeatability in our experiments.",
            "label": 0
          },
          {
            "text": "This was coupled with a top p setting of 0.95, aligning with our goal to eliminate randomness in the model's response generation process.",
            "label": 0
          },
          {
            "text": "Additionally, we refrained from applying any frequency or repetition penalties, allowing the model's natural language generation capabilities to function without these constraints.",
            "label": 0
          },
          {
            "text": "For GPT-4, we employed a similar configuration with gpt-4-0613 as the model version.",
            "label": 0
          },
          {
            "text": "For LLAMA, we changed the configuration from Meta.",
            "label": 0
          },
          {
            "text": "We initially encountered challenges in aligning the standard versions of these models to produce outputs in the required format, particularly in the context of complex clinical trial criteria.",
            "label": 0
          },
          {
            "text": "To address this, we opted for specific versions tailored for chat applications, namely Llama-2-7b-chat-hf, Llama-2-13b-chat-hf, and Llama-2-70b-chat-hf.",
            "label": 0
          },
          {
            "text": "These versions offered a more flexible and adaptable framework for our needs.",
            "label": 0
          },
          {
            "text": "We adjusted the temperature setting to 0.4 for all LLAMA models, a decision informed by preliminary tests which indicated that a slightly higher temperature prevented the model from collapsing on certain trials where inclusion/exclusion criteria were not clearly defined.",
            "label": 0
          },
          {
            "text": "Maintaining the output format was particularly challenging when working with the base LLAMA models.",
            "label": 0
          },
          {
            "text": "Despite employing various techniques such as context-free grammar (CFG) to constrain the model's output, the results remained suboptimal.Consequently, the models were unable to generate structured output for complex clinical trials.",
            "label": 0
          },
          {
            "text": "To circumvent this, we adjusted the model's temperature to foster more exploratory behavior and executed five output generations iteratively till the output matched our JSON schema.",
            "label": 0
          },
          {
            "text": "This allowed us to generated structured output for majority of clinical trials even with base models.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Distilling Large Language Models for Matching Patients to Clinical Trials",
        "section": 13,
        "paragraph_id": 13,
        "full_text": "Compute Novelty Index: noveltyIndex ← 1 -score τ 6: if noveltyIndex > 0 then end for 10: end for 11: return C novel Each criterion in C final is then annotated with a gold-standard answer and corresponding evidences.These evidential references serve as a basis for language models to substantiate their answers.To gauge the faithfulness of various models in accurately citing these pieces of evidence, we calculate precision, recall, and F1 scores for each model.Additionally, we conduct a direct comparison of model performance at the criterion level to evaluate their relative effectiveness.",
        "sentences": [
          {
            "text": "Compute Novelty Index: noveltyIndex ← 1 -score τ 6: if noveltyIndex > 0 then end for 10: end for 11: return C novel Each criterion in C final is then annotated with a gold-standard answer and corresponding evidences.",
            "label": 0
          },
          {
            "text": "These evidential references serve as a basis for language models to substantiate their answers.",
            "label": 0
          },
          {
            "text": "To gauge the faithfulness of various models in accurately citing these pieces of evidence, we calculate precision, recall, and F1 scores for each model.",
            "label": 0
          },
          {
            "text": "Additionally, we conduct a direct comparison of model performance at the criterion level to evaluate their relative effectiveness.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Distilling Large Language Models for Matching Patients to Clinical Trials",
        "section": 14,
        "paragraph_id": 14,
        "full_text": "Different from the metrics used in[18], we created two distinct aspects of Criterion-Level Accuracy (CLA), namely Explicit CLA and Implicit CLA, to holistically assess the model's performance.For Explicit CLA, our focus is on evaluating how accurately the model categorizes each criterion into the correct class, provided that the criterion has been previously identified as 'explicit' in our manual annotation exercise.This evaluation primarily concerns criteria for which the necessary information for classification is clearly and directly stated in the patient documentation, leaving minimal room for interpretation or inference.The accuracy here reflects the model's ability to comprehend and correctly apply these straightforward, unambiguous data points.",
        "sentences": [
          {
            "text": "Different from the metrics used in[18], we created two distinct aspects of Criterion-Level Accuracy (CLA), namely Explicit CLA and Implicit CLA, to holistically assess the model's performance.",
            "label": 0
          },
          {
            "text": "For Explicit CLA, our focus is on evaluating how accurately the model categorizes each criterion into the correct class, provided that the criterion has been previously identified as 'explicit' in our manual annotation exercise.",
            "label": 0
          },
          {
            "text": "This evaluation primarily concerns criteria for which the necessary information for classification is clearly and directly stated in the patient documentation, leaving minimal room for interpretation or inference.",
            "label": 0
          },
          {
            "text": "The accuracy here reflects the model's ability to comprehend and correctly apply these straightforward, unambiguous data points.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Distilling Large Language Models for Matching Patients to Clinical Trials",
        "section": 15,
        "paragraph_id": 15,
        "full_text": "On the other hand, Implicit CLA tackles a more nuanced challenge: it assesses the model's performance on criteria deemed 'implicit' by the annotators.These criteria involve situations where the required information is not explicitly stated but rather implied or inferred from the available data.This often requires connecting disparate pieces of information, understanding subtleties and nuances in the patient data, and making educated guesses based on the context.Calculating the Implicit CLA involves a thorough analysis of how well the model navigates these complexities and accurately classifies criteria based on less direct information.",
        "sentences": [
          {
            "text": "On the other hand, Implicit CLA tackles a more nuanced challenge: it assesses the model's performance on criteria deemed 'implicit' by the annotators.",
            "label": 0
          },
          {
            "text": "These criteria involve situations where the required information is not explicitly stated but rather implied or inferred from the available data.",
            "label": 0
          },
          {
            "text": "This often requires connecting disparate pieces of information, understanding subtleties and nuances in the patient data, and making educated guesses based on the context.",
            "label": 0
          },
          {
            "text": "Calculating the Implicit CLA involves a thorough analysis of how well the model navigates these complexities and accurately classifies criteria based on less direct information.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Distilling Large Language Models for Matching Patients to Clinical Trials",
        "section": 16,
        "paragraph_id": 16,
        "full_text": "Both Explicit and Implicit CLAs are pivotal in understanding the model's overall capability to process and interpret clinical trial criteria.While Explicit CLA provides insight into the model's proficiency with clear-cut, straightforward tasks, Implicit CLA sheds light on its ability to handle ambiguity and complexity -crucial aspects in the realm of clinical data interpretation.",
        "sentences": [
          {
            "text": "Both Explicit and Implicit CLAs are pivotal in understanding the model's overall capability to process and interpret clinical trial criteria.",
            "label": 0
          },
          {
            "text": "While Explicit CLA provides insight into the model's proficiency with clear-cut, straightforward tasks, Implicit CLA sheds light on its ability to handle ambiguity and complexity -crucial aspects in the realm of clinical data interpretation.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Distilling Large Language Models for Matching Patients to Clinical Trials",
        "section": 17,
        "paragraph_id": 17,
        "full_text": "For our dataset, we adopted the similar datasets as used in[18]that incorporate the SIGIR dataset[21]and both the 2021 and 2022 versions of the TREC CT cohorts[34,33], as shown in Table1.For each patient within these datasets, we extract 50 clinical trials, categorizing them into three distinct classifications: \"eligible\", \"excluded\", and \"irrelevant\".The categorization within the SIGIR dataset required a different approach, given its classification system.The SIGIR cohort's classes are as follows:",
        "sentences": [
          {
            "text": "For our dataset, we adopted the similar datasets as used in[18]that incorporate the SIGIR dataset[21]and both the 2021 and 2022 versions of the TREC CT cohorts[34,33], as shown in Table1.",
            "label": 1
          },
          {
            "text": "For each patient within these datasets, we extract 50 clinical trials, categorizing them into three distinct classifications: \"eligible\", \"excluded\", and \"irrelevant\".",
            "label": 1
          },
          {
            "text": "The categorization within the SIGIR dataset required a different approach, given its classification system.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Distilling Large Language Models for Matching Patients to Clinical Trials",
        "section": 18,
        "paragraph_id": 18,
        "full_text": "(a) \"Will not refer to the trial\": This class aligns with the 'irrelevant' category in our study. (b) \"Will refer to the trial\": Corresponds to the 'eligible' category. (c) \"May refer to the trial\": This class does not map directly to any of our predefined categories.",
        "sentences": [
          {
            "text": "(a) \"Will not refer to the trial\": This class aligns with the 'irrelevant' category in our study.",
            "label": 1
          },
          {
            "text": "(b) \"Will refer to the trial\": Corresponds to the 'eligible' category.",
            "label": 1
          },
          {
            "text": "(c) \"May refer to the trial\": This class does not map directly to any of our predefined categories.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Distilling Large Language Models for Matching Patients to Clinical Trials",
        "section": 19,
        "paragraph_id": 19,
        "full_text": "Due to this non-conformity, we excluded all trial-patient combinations classified under (c) \"May refer to the trial\", to maintain consistency.",
        "sentences": [
          {
            "text": "Due to this non-conformity, we excluded all trial-patient combinations classified under (c) \"May refer to the trial\", to maintain consistency.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Distilling Large Language Models for Matching Patients to Clinical Trials",
        "section": 20,
        "paragraph_id": 20,
        "full_text": "To facilitate the fine-tuning of our models, we partition the dataset into a training and test set, adhering to an 80:20 ratio.This division is implemented along the patient axis to ensure no test patient record gets leaked into the training set.Prior to splitting, all datasets are combined and thoroughly shuffled.The specifics of the training and test sets are displayed in Table1.It is noteworthy that despite the large volume of records in the training set, they are not fully utilized for model training.Instead, the large size of this set provides with an easy mechanism to sample diverse training samples for fine-tuning while also allowing us to save on compute costs associated with evaluation a large number of model checkpoints.Evidently, as shown in Table1the sampled dataset is more diverse than the training set.",
        "sentences": [
          {
            "text": "To facilitate the fine-tuning of our models, we partition the dataset into a training and test set, adhering to an 80:20 ratio.",
            "label": 1
          },
          {
            "text": "This division is implemented along the patient axis to ensure no test patient record gets leaked into the training set.",
            "label": 1
          },
          {
            "text": "Prior to splitting, all datasets are combined and thoroughly shuffled.",
            "label": 1
          },
          {
            "text": "The specifics of the training and test sets are displayed in Table1.",
            "label": 1
          },
          {
            "text": "It is noteworthy that despite the large volume of records in the training set, they are not fully utilized for model training.",
            "label": 1
          },
          {
            "text": "Instead, the large size of this set provides with an easy mechanism to sample diverse training samples for fine-tuning while also allowing us to save on compute costs associated with evaluation a large number of model checkpoints.",
            "label": 1
          },
          {
            "text": "Evidently, as shown in Table1the sampled dataset is more diverse than the training set.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Distilling Large Language Models for Matching Patients to Clinical Trials",
        "section": 21,
        "paragraph_id": 21,
        "full_text": "It is known that the performance of a model improves with the volume of data it is exposed to[16].Nevertheless, the quality of data plays a pivotal role in determining the output's caliber.Multiple research works have demonstrated that while the fine-tuning performance of a model initially improves rapidly, it tends to reach a saturation point beyond a certain threshold of data exposure [?].This phenomenon is consistent with our findings with the fine-tuning of the LLAMA models of different sizes.As illustrated in Figure5b, the performance of all three LLAMA variants exhibits a significant initial leap with exposure to a small data subset, followed by a gradual enhancement as they are introduced to an increasing number of examples.Notably, the largest LLAMA variant swiftly surpasses the performance of GPT-3.5.For the assessment of model performance, we utilize the metric of overall criteria level accuracy, encompassing both implicit and explicit criteria.",
        "sentences": [
          {
            "text": "It is known that the performance of a model improves with the volume of data it is exposed to[16].",
            "label": 0
          },
          {
            "text": "Nevertheless, the quality of data plays a pivotal role in determining the output's caliber.",
            "label": 0
          },
          {
            "text": "Multiple research works have demonstrated that while the fine-tuning performance of a model initially improves rapidly, it tends to reach a saturation point beyond a certain threshold of data exposure [?].",
            "label": 0
          },
          {
            "text": "This phenomenon is consistent with our findings with the fine-tuning of the LLAMA models of different sizes.",
            "label": 0
          },
          {
            "text": "As illustrated in Figure5b, the performance of all three LLAMA variants exhibits a significant initial leap with exposure to a small data subset, followed by a gradual enhancement as they are introduced to an increasing number of examples.",
            "label": 0
          },
          {
            "text": "Notably, the largest LLAMA variant swiftly surpasses the performance of GPT-3.5.",
            "label": 0
          },
          {
            "text": "For the assessment of model performance, we utilize the metric of overall criteria level accuracy, encompassing both implicit and explicit criteria.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Distilling Large Language Models for Matching Patients to Clinical Trials",
        "section": 22,
        "paragraph_id": 22,
        "full_text": "The process of fine-tuning LLMs presents both computational and methodological challenges, primarily due to the difficulty in providing a dense, multi-token signal that these models require for effective learning.While labeling for classification tasks typically involves single-token signals, enhancing model performance necessitates the provision of multi-token feedback, which is inherently more complex to curate due to its diversity and volume requirements.Despite these challenges, our experiments demonstrate that distillation techniques that have been used to enhance the dialogue capabilities of different models[46,43]can be used for the task of patient matching as well.This method significantly reduces the necessity for manually crafted examples, thereby streamlining the fine-tuning process and making it more affordable.",
        "sentences": [
          {
            "text": "The process of fine-tuning LLMs presents both computational and methodological challenges, primarily due to the difficulty in providing a dense, multi-token signal that these models require for effective learning.",
            "label": 0
          },
          {
            "text": "While labeling for classification tasks typically involves single-token signals, enhancing model performance necessitates the provision of multi-token feedback, which is inherently more complex to curate due to its diversity and volume requirements.",
            "label": 0
          },
          {
            "text": "Despite these challenges, our experiments demonstrate that distillation techniques that have been used to enhance the dialogue capabilities of different models[46,43]can be used for the task of patient matching as well.",
            "label": 0
          },
          {
            "text": "This method significantly reduces the necessity for manually crafted examples, thereby streamlining the fine-tuning process and making it more affordable.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Diverse_ Difficult_ and Odd Instances _D2O__ A New Test Set for Object Classification",
        "section": 0,
        "paragraph_id": 0,
        "full_text": "Abstract: Test sets are an integral part of evaluating models and gauging progress in object recognition, and more broadly in computer vision and AI.Existing test sets for object recognition, however, suffer from shortcomings such as bias towards the ImageNet characteristics and idiosyncrasies (e.g.ImageNet-V2), being limited to certain types of stimuli (e.g.indoor scenes in ObjectNet), and underestimating the model performance (e.g.ImageNet-A).To mitigate these problems, we introduce a new test set, called D2O, which is sufficiently different from existing test sets.Images are a mix of generated images as well as images crawled from the web.They are diverse, unmodified, and representative of real-world scenarios and cause state-of-the-art models to misclassify them with high confidence.To emphasize generalization, our dataset by design does not come paired with a training set.It contains 8,060 images spread across 36 categories, out of which 29 appear in ImageNet.The best Top-1 accuracy on our dataset is around 60% which is much lower than 91% best Top-1 accuracy on ImageNet.We find that popular vision APIs perform very poorly in detecting objects over D2O categories such as \"faces\", \"cars\", and \"cats\".Our dataset also comes with a \"miscellaneous\" category, over which we test the image tagging models.Overall, our investigations demonstrate that the D2O test set contain a mix of images with varied levels of difficulty and is predictive of the average-case performance of models.It can challenge object recognition models for years to come and can spur more research in this fundamental area.",
        "sentences": [
          {
            "text": "Abstract: Test sets are an integral part of evaluating models and gauging progress in object recognition, and more broadly in computer vision and AI.",
            "label": 0
          },
          {
            "text": "Existing test sets for object recognition, however, suffer from shortcomings such as bias towards the ImageNet characteristics and idiosyncrasies (e.g.ImageNet-V2), being limited to certain types of stimuli (e.g.indoor scenes in ObjectNet), and underestimating the model performance (e.g.ImageNet-A).",
            "label": 0
          },
          {
            "text": "To mitigate these problems, we introduce a new test set, called D2O, which is sufficiently different from existing test sets.",
            "label": 1
          },
          {
            "text": "Images are a mix of generated images as well as images crawled from the web.",
            "label": 1
          },
          {
            "text": "They are diverse, unmodified, and representative of real-world scenarios and cause state-of-the-art models to misclassify them with high confidence.",
            "label": 1
          },
          {
            "text": "To emphasize generalization, our dataset by design does not come paired with a training set.",
            "label": 1
          },
          {
            "text": "It contains 8,060 images spread across 36 categories, out of which 29 appear in ImageNet.",
            "label": 1
          },
          {
            "text": "The best Top-1 accuracy on our dataset is around 60% which is much lower than 91% best Top-1 accuracy on ImageNet.",
            "label": 1
          },
          {
            "text": "We find that popular vision APIs perform very poorly in detecting objects over D2O categories such as \"faces\", \"cars\", and \"cats\".",
            "label": 1
          },
          {
            "text": "Our dataset also comes with a \"miscellaneous\" category, over which we test the image tagging models.",
            "label": 1
          },
          {
            "text": "Overall, our investigations demonstrate that the D2O test set contain a mix of images with varied levels of difficulty and is predictive of the average-case performance of models.",
            "label": 1
          },
          {
            "text": "It can challenge object recognition models for years to come and can spur more research in this fundamental area.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Diverse_ Difficult_ and Odd Instances _D2O__ A New Test Set for Object Classification",
        "section": 1,
        "paragraph_id": 1,
        "full_text": "The object recognition problem remains in an unclear state.Despite compelling performance of state-of-the-art object recognition methods, several questions such as outof-distribution generalization[1,17,25,28,31], \"superhuman performance\"[8,10], adversarial vulnerability[9], and invariance to image transformations and distortions[13]Figure1.Sample images from D2O dataset.Images are a mix of generated images as well as images crawled from the web.Categories in order are: acorn, banana, basketball, car, cat, and clock.still persist.Raw performance on test sets has been the main indicator of the progress and the major feedback about the state of the field.Few test sets have been proposed for evaluating object recognition models.Some follow the footsteps of ImageNet[25].Some filter images based on failures of models[14].Researchers have also used controlled settings to collect data[1,4].While being invaluable, these datasets suffer from few shortcomings.For example, datasets that only include examples for which the best models fail give the worst case scenario accuracy.While being useful, they underestimate model performance.Datasets that are biased towards certain environments (e.g.indoor scenes in ObjectNet[1]), may not capture the full spectrum of visual stimuli.Most of the new datasets for object recognition have been centered on ImageNet (e.g.ImageNet-V2, ImageNet-A, ImageNet-O) and thus may have inherited its biases.This may in turn give us a biased assessment of visual recognition capability of models.We argue that a good test set should strike the right balance between sample difficulty and diversity and should reflect the average-case performance of models.We also believe that new test sets that are sufficiently different from existing ones can provide new insights into the object recognition problem.To this end, we include images that contain rich semantics and require cognitive processing (e.g.artistic scenes).Existing datasets lack enough samples of such cases.",
        "sentences": [
          {
            "text": "The object recognition problem remains in an unclear state.",
            "label": 0
          },
          {
            "text": "Despite compelling performance of state-of-the-art object recognition methods, several questions such as outof-distribution generalization[1,17,25,28,31], \"superhuman performance\"[8,10], adversarial vulnerability[9], and invariance to image transformations and distortions[13]Figure1.",
            "label": 0
          },
          {
            "text": "Sample images from D2O dataset.",
            "label": 1
          },
          {
            "text": "Images are a mix of generated images as well as images crawled from the web.",
            "label": 1
          },
          {
            "text": "still persist.",
            "label": 0
          },
          {
            "text": "Raw performance on test sets has been the main indicator of the progress and the major feedback about the state of the field.",
            "label": 0
          },
          {
            "text": "Few test sets have been proposed for evaluating object recognition models.",
            "label": 0
          },
          {
            "text": "Some follow the footsteps of ImageNet[25].",
            "label": 0
          },
          {
            "text": "Some filter images based on failures of models[14].",
            "label": 0
          },
          {
            "text": "Researchers have also used controlled settings to collect data[1,4].",
            "label": 0
          },
          {
            "text": "While being invaluable, these datasets suffer from few shortcomings.",
            "label": 0
          },
          {
            "text": "For example, datasets that only include examples for which the best models fail give the worst case scenario accuracy.",
            "label": 0
          },
          {
            "text": "While being useful, they underestimate model performance.",
            "label": 0
          },
          {
            "text": "Datasets that are biased towards certain environments (e.g.indoor scenes in ObjectNet[1]), may not capture the full spectrum of visual stimuli.",
            "label": 0
          },
          {
            "text": "Most of the new datasets for object recognition have been centered on ImageNet (e.g.ImageNet-V2, ImageNet-A, ImageNet-O) and thus may have inherited its biases.",
            "label": 0
          },
          {
            "text": "This may in turn give us a biased assessment of visual recognition capability of models.",
            "label": 0
          },
          {
            "text": "We argue that a good test set should strike the right balance between sample difficulty and diversity and should reflect the average-case performance of models.",
            "label": 0
          },
          {
            "text": "We also believe that new test sets that are sufficiently different from existing ones can provide new insights into the object recognition problem.",
            "label": 0
          },
          {
            "text": "To this end, we include images that contain rich semantics and require cognitive processing (e.g.artistic scenes).",
            "label": 1
          },
          {
            "text": "Existing datasets lack enough samples of such cases.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Diverse_ Difficult_ and Odd Instances _D2O__ A New Test Set for Object Classification",
        "section": 2,
        "paragraph_id": 2,
        "full_text": "Here, we emphasize image diversity and difficulty over scale.While scaling up test sets has a clear advantage (e.g.covering rare cases), it comes with some shortcomings.It is hard to ensure privacy, security, quality, and spurious correlations 1 in datasets containing millions of images.These problems are easier to tackle in small scale expertmade datasets.Nonetheless, both small and large datasets are needed and are complementary.Further, our dataset is one of the early efforts to use generative models for building image datasets.",
        "sentences": [
          {
            "text": "Here, we emphasize image diversity and difficulty over scale.",
            "label": 0
          },
          {
            "text": "While scaling up test sets has a clear advantage (e.g.covering rare cases), it comes with some shortcomings.",
            "label": 0
          },
          {
            "text": "It is hard to ensure privacy, security, quality, and spurious correlations 1 in datasets containing millions of images.",
            "label": 0
          },
          {
            "text": "These problems are easier to tackle in small scale expertmade datasets.",
            "label": 0
          },
          {
            "text": "Nonetheless, both small and large datasets are needed and are complementary.",
            "label": 0
          },
          {
            "text": "Further, our dataset is one of the early efforts to use generative models for building image datasets.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Diverse_ Difficult_ and Odd Instances _D2O__ A New Test Set for Object Classification",
        "section": 3,
        "paragraph_id": 3,
        "full_text": "Our dataset includes 8,060 images across 36 categories (Fig.1).Images are carefully collected, verified, and labeled.We do not limit ourselves to object recognition models proposed in academia, and also consider prominent vision APIs in industry.This allows us to test models over a wider range of categories than those available in Ima-geNet and obtain a broader sense of image understanding by models.State-of-the-art models show a 30% absolute drop in Top-1 acc on D2O test set compared to the best Im-ageNet accuracy (around 20% drop using Top-5 acc).Further, over categories for which we know humans are very good at (e.g.faces, cars), current APIs fail drastically.",
        "sentences": [
          {
            "text": "Our dataset includes 8,060 images across 36 categories (Fig.1).",
            "label": 1
          },
          {
            "text": "Images are carefully collected, verified, and labeled.",
            "label": 1
          },
          {
            "text": "We do not limit ourselves to object recognition models proposed in academia, and also consider prominent vision APIs in industry.",
            "label": 0
          },
          {
            "text": "This allows us to test models over a wider range of categories than those available in Ima-geNet and obtain a broader sense of image understanding by models.",
            "label": 0
          },
          {
            "text": "State-of-the-art models show a 30% absolute drop in Top-1 acc on D2O test set compared to the best Im-ageNet accuracy (around 20% drop using Top-5 acc).",
            "label": 1
          },
          {
            "text": "Further, over categories for which we know humans are very good at (e.g.faces, cars), current APIs fail drastically.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Diverse_ Difficult_ and Odd Instances _D2O__ A New Test Set for Object Classification",
        "section": 4,
        "paragraph_id": 4,
        "full_text": "D2O test set is intentionally not paired with a training set.It comes with a license that disallows researchers to update the parameters of any model on it.This helps avoid over-fitting on the dataset.Additionally, to mitigate the danger of leaking our data to other datasets, we mark every image by a one pixel green border which must be removed on the fly before being used.",
        "sentences": [
          {
            "text": "D2O test set is intentionally not paired with a training set.",
            "label": 1
          },
          {
            "text": "It comes with a license that disallows researchers to update the parameters of any model on it.",
            "label": 1
          },
          {
            "text": "This helps avoid over-fitting on the dataset.",
            "label": 1
          },
          {
            "text": "Additionally, to mitigate the danger of leaking our data to other datasets, we mark every image by a one pixel green border which must be removed on the fly before being used.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "ECMD_ An Event_Centric Multisensory Driving Dataset for SLAM",
        "section": 0,
        "paragraph_id": 0,
        "full_text": "Abstract: Leveraging multiple sensors enhances complex environmental perception and increases resilience to varying luminance conditions and high-speed motion patterns, achieving precise localization and mapping.This paper proposes, ECMD, an event-centric multisensory dataset containing 81 sequences and covering over 200 km of various challenging driving scenarios including high-speed motion, repetitive scenarios, dynamic objects, etc. ECMD provides data from two sets of stereo event cameras with different resolutions (640×480, 346×260), stereo industrial cameras, an infrared camera, a top-installed mechanical LiDAR with two slanted LiDARs, two consumer-level GNSS receivers, and an onboard IMU.Meanwhile, the ground-truth of the vehicle was obtained using a centimeter-level high-accuracy GNSS-RTK/INS navigation system.All sensors are well-calibrated and temporally synchronized at the hardware level, with recording data simultaneously.We additionally evaluate several state-of-the-art SLAM algorithms for benchmarking visual and LiDAR SLAM and identifying their limitations.The dataset is available at https://arclab-hku.github.io/ecmd/.",
        "sentences": [
          {
            "text": "Abstract: Leveraging multiple sensors enhances complex environmental perception and increases resilience to varying luminance conditions and high-speed motion patterns, achieving precise localization and mapping.",
            "label": 0
          },
          {
            "text": "This paper proposes, ECMD, an event-centric multisensory dataset containing 81 sequences and covering over 200 km of various challenging driving scenarios including high-speed motion, repetitive scenarios, dynamic objects, etc. ECMD provides data from two sets of stereo event cameras with different resolutions (640×480, 346×260), stereo industrial cameras, an infrared camera, a top-installed mechanical LiDAR with two slanted LiDARs, two consumer-level GNSS receivers, and an onboard IMU.",
            "label": 1
          },
          {
            "text": "Meanwhile, the ground-truth of the vehicle was obtained using a centimeter-level high-accuracy GNSS-RTK/INS navigation system.",
            "label": 1
          },
          {
            "text": "All sensors are well-calibrated and temporally synchronized at the hardware level, with recording data simultaneously.",
            "label": 1
          },
          {
            "text": "We additionally evaluate several state-of-the-art SLAM algorithms for benchmarking visual and LiDAR SLAM and identifying their limitations.",
            "label": 0
          },
          {
            "text": "The dataset is available at https://arclab-hku.github.io/ecmd/.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "ECMD_ An Event_Centric Multisensory Driving Dataset for SLAM",
        "section": 1,
        "paragraph_id": 1,
        "full_text": "V ISUAL and LiDAR simultaneous localization and map- ping (SLAM) achieved notable progress within driving scenarios in recent years.However, they encounter the challenging task of operating robustly under heterogeneous environments, such as varying lighting conditions, lowtexture scenarios, repetitive structures, diverse motion patterns, dense dynamic objects, etc. Utilizing novel sensors and integrating multiple sensors can provide a comprehensive perception and enhance the robustness of the entire system[1]-[3].These motivate us to develop a dataset that integrates novel sensors under realistic and complex driving scenarios, thereby promoting SLAM research.Event cameras have low latency (µs-level) and high dynamic range (140 dB compared to 60 dB with standard cameras) properties, which offers great opportunities for visual (VO) and visual-inertial odometry (VIO) in rough terrain, aggressive motions, and high dynamic range (HDR)[4].Unlike traditional frame-based cameras that directly capture fixed-rate intensity frames, event cameras are motionactivated sensors that capture pixel-wise intensity differences asynchronously in continuous streams.However, the widespread commercialization and implementation of event cameras in robotics are still early due to the expensive cost.In addition, event cameras confront challenges during rapid vibrations and ego-motion, as these conditions generate a substantial quantity of events, leading to intensive computations.Conversely, in cases where minimal relative motion between the event camera and the scene exists, such as under static states, they only provide limited information or even introduce noise[5].Therefore, we embark on this research effort to explore the inquiry: Are event cameras ready for autonomous driving?",
        "sentences": [
          {
            "text": "V ISUAL and LiDAR simultaneous localization and map- ping (SLAM) achieved notable progress within driving scenarios in recent years.",
            "label": 0
          },
          {
            "text": "However, they encounter the challenging task of operating robustly under heterogeneous environments, such as varying lighting conditions, lowtexture scenarios, repetitive structures, diverse motion patterns, dense dynamic objects, etc. Utilizing novel sensors and integrating multiple sensors can provide a comprehensive perception and enhance the robustness of the entire system[1]-[3].",
            "label": 0
          },
          {
            "text": "These motivate us to develop a dataset that integrates novel sensors under realistic and complex driving scenarios, thereby promoting SLAM research.",
            "label": 0
          },
          {
            "text": "Event cameras have low latency (µs-level) and high dynamic range (140 dB compared to 60 dB with standard cameras) properties, which offers great opportunities for visual (VO) and visual-inertial odometry (VIO) in rough terrain, aggressive motions, and high dynamic range (HDR)[4].",
            "label": 0
          },
          {
            "text": "Unlike traditional frame-based cameras that directly capture fixed-rate intensity frames, event cameras are motionactivated sensors that capture pixel-wise intensity differences asynchronously in continuous streams.",
            "label": 0
          },
          {
            "text": "However, the widespread commercialization and implementation of event cameras in robotics are still early due to the expensive cost.",
            "label": 0
          },
          {
            "text": "In addition, event cameras confront challenges during rapid vibrations and ego-motion, as these conditions generate a substantial quantity of events, leading to intensive computations.",
            "label": 0
          },
          {
            "text": "Conversely, in cases where minimal relative motion between the event camera and the scene exists, such as under static states, they only provide limited information or even introduce noise[5].",
            "label": 0
          },
          {
            "text": "Therefore, we embark on this research effort to explore the inquiry: Are event cameras ready for autonomous driving?",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "ECMD_ An Event_Centric Multisensory Driving Dataset for SLAM",
        "section": 2,
        "paragraph_id": 2,
        "full_text": "There exist several stereo event-based driving datasets that are worth mentioning and exploring.MVSEC[6]was the first stereo event-based driving dataset proposed for evaluating the localization performance.While MVSEC employs the low resolution of DAVIS346 which limits the feature detection for accurate localization.DSEC[7]offers stereo event streams with a high resolution of 0.31 Megapixels(MP).However, this dataset focuses on computer vision tasks segmentation, depth estimation, optical flow estimation, etc., which is not specifically designed for VO/VIO/SLAM domains.MA-VIED[8]propose a largescale driving dataset under standard urban scenarios and race track-like loops.The ground-truth trajectory relies on GNSS-RTK, which only ensures high accuracy in open-sky environments and fails to provide high accuracy in GNSSdenied scenarios such as tunnels or densely street areas.Ref.[9]focuses on collecting both stereo event data and stereo intensity images under indoor and urban driving scenes with the ground-truth of GNSS-RTK/INS.Their sequences do not encompass extremely high-speed or repetitive scenarios that could be challenging to VO/VIO/SLAM algorithms.",
        "sentences": [
          {
            "text": "There exist several stereo event-based driving datasets that are worth mentioning and exploring.",
            "label": 0
          },
          {
            "text": "MVSEC[6]was the first stereo event-based driving dataset proposed for evaluating the localization performance.",
            "label": 0
          },
          {
            "text": "While MVSEC employs the low resolution of DAVIS346 which limits the feature detection for accurate localization.",
            "label": 0
          },
          {
            "text": "DSEC[7]offers stereo event streams with a high resolution of 0.31 Megapixels(MP).",
            "label": 0
          },
          {
            "text": "However, this dataset focuses on computer vision tasks segmentation, depth estimation, optical flow estimation, etc., which is not specifically designed for VO/VIO/SLAM domains.",
            "label": 0
          },
          {
            "text": "MA-VIED[8]propose a largescale driving dataset under standard urban scenarios and race track-like loops.",
            "label": 0
          },
          {
            "text": "The ground-truth trajectory relies on GNSS-RTK, which only ensures high accuracy in open-sky environments and fails to provide high accuracy in GNSSdenied scenarios such as tunnels or densely street areas.",
            "label": 0
          },
          {
            "text": "[9]focuses on collecting both stereo event data and stereo intensity images under indoor and urban driving scenes with the ground-truth of GNSS-RTK/INS.",
            "label": 0
          },
          {
            "text": "Their sequences do not encompass extremely high-speed or repetitive scenarios that could be challenging to VO/VIO/SLAM algorithms.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "ECMD_ An Event_Centric Multisensory Driving Dataset for SLAM",
        "section": 3,
        "paragraph_id": 3,
        "full_text": "To address the above drawbacks, we propose ECMD, a dataset procured from diverse challenging driving scenarios with a comprehensive suite of sensors for benchmarking various VO/VIO/SLAM algorithms.To the best of our knowledge, this is the first event-based SLAM dataset specifically focused on densely urbanized driving scenarios.The contributions of our work can be summarized as follows: 1) Our sensor platform consists of various novel sensors shown in Fig.1, including two sets of stereo event cameras with distinct resolutions (640×480, 346×260), an infrared camera, stereo industrial cameras, three mechanical LiDARs (including two slanted LiDARs), a high-quality inertial measurement unit (IMU), and three global navigation satellite system (GNSS) receivers.For the ground-truth, we adopt a centimeter-level position system that combines the GNSS real-time kinematic (RTK) with the fiber optics gyroscope integrated inertial system as GNSS-RTK/INS.2) ECMD collects 81 sequences covering over 200 kilometers of trajectories in various driving scenarios, including dense streets, urban, tunnels, highways, bridges, and suburbs.These sequences are recorded under daylight and nighttime, providing challenging situations for Visual and LiDAR SLAM, e.g., dynamic objects, highspeed motion, repetitive scenarios, and HDR scenes.Meanwhile, we evaluate existing state-of-the-art visual and LiDAR SLAM algorithms with various sensor modalities on our datasets.Moreover, our dataset and benchmark results are released publicly available on our website.The remainder of the paper is organized as follows: Section II introduces the related works.Section III presents the sensor setup and sensor calibration.Section IV intro-duces the dataset overview.Section V demonstrates the dataset application.Section VI introduces known issues.The conclusion is given in Section VII.II.RELATED WORKS Currently, several event-based datasets combined with various sensors have been released for VO/VIO/SLAM domains, utilizing handheld devices or a variety of robotics platforms.DAVIS240C[10], TUM-VIE[11], VECtor[12], and HKU-dataset1were collected by handheld / headmounted devices under indoor environments.M2DGR[13]utilizes ground robots to collect a multi-sensor dataset with an event camera under large-scale scenes, while the event streams exhibit large noises.FusionPortable[14]proposes multi-sensor campus-scene datasets with stereo event cameras on diverse platforms (handheld, quadruped robot, and UGV).Moreover, there exist specialized event-based datasets such as UZH-FPV[15]and GRIFFIN[16], which are targeted for flying robots.",
        "sentences": [
          {
            "text": "To address the above drawbacks, we propose ECMD, a dataset procured from diverse challenging driving scenarios with a comprehensive suite of sensors for benchmarking various VO/VIO/SLAM algorithms.",
            "label": 1
          },
          {
            "text": "To the best of our knowledge, this is the first event-based SLAM dataset specifically focused on densely urbanized driving scenarios.",
            "label": 1
          },
          {
            "text": "The contributions of our work can be summarized as follows: 1) Our sensor platform consists of various novel sensors shown in Fig.1, including two sets of stereo event cameras with distinct resolutions (640×480, 346×260), an infrared camera, stereo industrial cameras, three mechanical LiDARs (including two slanted LiDARs), a high-quality inertial measurement unit (IMU), and three global navigation satellite system (GNSS) receivers.",
            "label": 1
          },
          {
            "text": "2) ECMD collects 81 sequences covering over 200 kilometers of trajectories in various driving scenarios, including dense streets, urban, tunnels, highways, bridges, and suburbs.",
            "label": 1
          },
          {
            "text": "These sequences are recorded under daylight and nighttime, providing challenging situations for Visual and LiDAR SLAM, e.g., dynamic objects, highspeed motion, repetitive scenarios, and HDR scenes.",
            "label": 1
          },
          {
            "text": "Meanwhile, we evaluate existing state-of-the-art visual and LiDAR SLAM algorithms with various sensor modalities on our datasets.",
            "label": 0
          },
          {
            "text": "Moreover, our dataset and benchmark results are released publicly available on our website.",
            "label": 1
          },
          {
            "text": "The remainder of the paper is organized as follows: Section II introduces the related works.",
            "label": 0
          },
          {
            "text": "Section III presents the sensor setup and sensor calibration.",
            "label": 0
          },
          {
            "text": "Section IV intro-duces the dataset overview.",
            "label": 0
          },
          {
            "text": "Section V demonstrates the dataset application.",
            "label": 0
          },
          {
            "text": "Section VI introduces known issues.",
            "label": 0
          },
          {
            "text": "The conclusion is given in Section VII.II.RELATED WORKS Currently, several event-based datasets combined with various sensors have been released for VO/VIO/SLAM domains, utilizing handheld devices or a variety of robotics platforms.",
            "label": 0
          },
          {
            "text": "DAVIS240C[10], TUM-VIE[11], VECtor[12], and HKU-dataset1were collected by handheld / headmounted devices under indoor environments.",
            "label": 0
          },
          {
            "text": "M2DGR[13]utilizes ground robots to collect a multi-sensor dataset with an event camera under large-scale scenes, while the event streams exhibit large noises.",
            "label": 0
          },
          {
            "text": "FusionPortable[14]proposes multi-sensor campus-scene datasets with stereo event cameras on diverse platforms (handheld, quadruped robot, and UGV).",
            "label": 0
          },
          {
            "text": "Moreover, there exist specialized event-based datasets such as UZH-FPV[15]and GRIFFIN[16], which are targeted for flying robots.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "ECMD_ An Event_Centric Multisensory Driving Dataset for SLAM",
        "section": 4,
        "paragraph_id": 4,
        "full_text": "Moreover, a number of event-based datasets are published under large-scale driving scenarios for computer vision.These autopilot datasets offer more realistic and challenging conditions, including high-speed scenarios, repetitive situations, and HDR scenes compared to datasets collected from handheld devices.The first dataset catering to driving recordings using an event camera is DDD17[17], as well as the follow-up DDD20[18], for studying the end-to-end driving application incorporating diverse vehicle control data.HATS[19], CED[20], Ref.[21], and Ref.[22]published their event-based datasets for the computer vision task of object classification, image reconstruction, and vision place recognition in driving scenarios.MVSEC[6]is a pioneering cross-modal dataset with stereo event and image cameras, as well as LiDAR.However, a limitation of MVSEC resides in the utilization of low-resolution event cameras (346×260) with a compact baseline of 10 cm, coupled with the imprecision of the ground-truth derived from GNSS or LiDAR-SLAM.DSEC[7]proposed an event-based dataset whose scenarios are similar to KITTI[23], providing higher resolution stereo event (640×480) and image, LiDAR, and IMU under various illumination conditions.M3ED[24]encompasses high-resolution event cameras (1280×720) and covers three different robotics platforms: driving, flight, and legged robot.However, both DSEC and M3ED datasets are primarily utilized for computer vision fields, such as optical flow estimation, segmentation, and disparity estimation, rather than specifically for localization or mapping problems.Besides, they do not provide sufficient challenges for SLAM, as the majority of these datasets were collected in rural or suburban areas with relatively low-lying structures, light traffic, and less dynamic objects.ViViD++[25]focuses on diverse vision sensors for handheld and driving platforms, including event, thermal, and standard cameras.MA-VIED[8]proposes a comprehensive driving dataset that encompasses race track-like loops, maneuvers, and standard driving scenarios.However, both of these datasets exclusively offer monocular data for each camera type, thereby precluding the possibility of conducting stereo visual SLAM.Ref.[9]introduces a stereo visual localization dataset that exploits both the high-resolution event and standard cameras under indoor and urban scenarios.TableI. summarizes the differences between our ECMD and other event-based datasets under autonomous driving scenarios.Compared to other datasets, our ECMD offers several advantages: (i) Capture diverse visual data format (RGB image, event stream, and infrared image) from multiple types of vision sensors in varying luminance conditions and urbanized scenarios; (ii) 1kHz-rate event streams from different resolution event cameras empower in-depth exploration of event-based perception; (iii) Based on our previous work[26], three LiDARs, including two slanted LiDARs, are employed to collect high-rising building structures for LiDAR point cloud maps generation; (iv) We employ a centimeter-level localization system, GNSS-RTK/INS, as ground-truth, enabling a comprehensive evaluation of various SLAM algorithms.",
        "sentences": [
          {
            "text": "Moreover, a number of event-based datasets are published under large-scale driving scenarios for computer vision.",
            "label": 0
          },
          {
            "text": "These autopilot datasets offer more realistic and challenging conditions, including high-speed scenarios, repetitive situations, and HDR scenes compared to datasets collected from handheld devices.",
            "label": 0
          },
          {
            "text": "The first dataset catering to driving recordings using an event camera is DDD17[17], as well as the follow-up DDD20[18], for studying the end-to-end driving application incorporating diverse vehicle control data.",
            "label": 0
          },
          {
            "text": "[22]published their event-based datasets for the computer vision task of object classification, image reconstruction, and vision place recognition in driving scenarios.",
            "label": 0
          },
          {
            "text": "MVSEC[6]is a pioneering cross-modal dataset with stereo event and image cameras, as well as LiDAR.",
            "label": 0
          },
          {
            "text": "However, a limitation of MVSEC resides in the utilization of low-resolution event cameras (346×260) with a compact baseline of 10 cm, coupled with the imprecision of the ground-truth derived from GNSS or LiDAR-SLAM.",
            "label": 0
          },
          {
            "text": "DSEC[7]proposed an event-based dataset whose scenarios are similar to KITTI[23], providing higher resolution stereo event (640×480) and image, LiDAR, and IMU under various illumination conditions.",
            "label": 0
          },
          {
            "text": "M3ED[24]encompasses high-resolution event cameras (1280×720) and covers three different robotics platforms: driving, flight, and legged robot.",
            "label": 0
          },
          {
            "text": "However, both DSEC and M3ED datasets are primarily utilized for computer vision fields, such as optical flow estimation, segmentation, and disparity estimation, rather than specifically for localization or mapping problems.",
            "label": 0
          },
          {
            "text": "Besides, they do not provide sufficient challenges for SLAM, as the majority of these datasets were collected in rural or suburban areas with relatively low-lying structures, light traffic, and less dynamic objects.",
            "label": 0
          },
          {
            "text": "ViViD++[25]focuses on diverse vision sensors for handheld and driving platforms, including event, thermal, and standard cameras.",
            "label": 0
          },
          {
            "text": "MA-VIED[8]proposes a comprehensive driving dataset that encompasses race track-like loops, maneuvers, and standard driving scenarios.",
            "label": 0
          },
          {
            "text": "However, both of these datasets exclusively offer monocular data for each camera type, thereby precluding the possibility of conducting stereo visual SLAM.",
            "label": 0
          },
          {
            "text": "[9]introduces a stereo visual localization dataset that exploits both the high-resolution event and standard cameras under indoor and urban scenarios.",
            "label": 0
          },
          {
            "text": "TableI. summarizes the differences between our ECMD and other event-based datasets under autonomous driving scenarios.",
            "label": 1
          },
          {
            "text": "Compared to other datasets, our ECMD offers several advantages: (i) Capture diverse visual data format (RGB image, event stream, and infrared image) from multiple types of vision sensors in varying luminance conditions and urbanized scenarios; (ii) 1kHz-rate event streams from different resolution event cameras empower in-depth exploration of event-based perception; (iii) Based on our previous work[26], three LiDARs, including two slanted LiDARs, are employed to collect high-rising building structures for LiDAR point cloud maps generation; (iv) We employ a centimeter-level localization system, GNSS-RTK/INS, as ground-truth, enabling a comprehensive evaluation of various SLAM algorithms.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "ECMD_ An Event_Centric Multisensory Driving Dataset for SLAM",
        "section": 5,
        "paragraph_id": 5,
        "full_text": "The data collection platform is shown in Fig.1.Our sensor suite consists of a multi-camera setup (event camera, industrial camera, and infrared camera) equipped with three LiDARs, high-quality IMU, three GNSS receivers, and GNSS-RTK/INS systems.The specific specifications of each sensor are presented in TableII.An Intel NUC (i7-1260P, 32GB RAM) and an industrial computer (i7-10610U, 32GB RAM) are used to run sensor drivers, and record data into ROS bags on the Ubuntu system.",
        "sentences": [
          {
            "text": "The data collection platform is shown in Fig.1.",
            "label": 1
          },
          {
            "text": "Our sensor suite consists of a multi-camera setup (event camera, industrial camera, and infrared camera) equipped with three LiDARs, high-quality IMU, three GNSS receivers, and GNSS-RTK/INS systems.",
            "label": 1
          },
          {
            "text": "The specific specifications of each sensor are presented in TableII.",
            "label": 1
          },
          {
            "text": "An Intel NUC (i7-1260P, 32GB RAM) and an industrial computer (i7-10610U, 32GB RAM) are used to run sensor drivers, and record data into ROS bags on the Ubuntu system.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "ECMD_ An Event_Centric Multisensory Driving Dataset for SLAM",
        "section": 6,
        "paragraph_id": 6,
        "full_text": "1) Visual Sensors: Two sets of stereo event cameras with different resolutions, DAVIS436 (346×260) and DVXplorer (640×480), are configured at a baseline of 30 cm respectively.DAVIS346 produces asynchronous events and intensity frames.In contrast, DVXplorer exclusively generates events, while its resolution surpasses that of DAVIS346, enabling the provision of more intricate scene information.Each event camera is equipped with additional infrared filters to mitigate interference from LiDAR.Two FLIR BFLY-U3-23S3C industrial cameras with a resolution of 1920×1200 are used to capture RGB images at 20 Hz in fixed exposure mode.Forward-facing stereo industrial cameras are installed with a baseline of 30 cm, ensuring fairness by maintaining consistency with the baseline of the stereo event cameras.Hikrobot MV-CI003-GL-N6 infrared camera collects thermal frames at 20 Hz, encompassing a response band of 8-14µm and equipped with a 6.3mm focal length lens.",
        "sentences": [
          {
            "text": "1) Visual Sensors: Two sets of stereo event cameras with different resolutions, DAVIS436 (346×260) and DVXplorer (640×480), are configured at a baseline of 30 cm respectively.",
            "label": 1
          },
          {
            "text": "DAVIS346 produces asynchronous events and intensity frames.",
            "label": 1
          },
          {
            "text": "In contrast, DVXplorer exclusively generates events, while its resolution surpasses that of DAVIS346, enabling the provision of more intricate scene information.",
            "label": 1
          },
          {
            "text": "Each event camera is equipped with additional infrared filters to mitigate interference from LiDAR.",
            "label": 1
          },
          {
            "text": "Two FLIR BFLY-U3-23S3C industrial cameras with a resolution of 1920×1200 are used to capture RGB images at 20 Hz in fixed exposure mode.",
            "label": 1
          },
          {
            "text": "Forward-facing stereo industrial cameras are installed with a baseline of 30 cm, ensuring fairness by maintaining consistency with the baseline of the stereo event cameras.",
            "label": 1
          },
          {
            "text": "Hikrobot MV-CI003-GL-N6 infrared camera collects thermal frames at 20 Hz, encompassing a response band of 8-14µm and equipped with a 6.3mm focal length lens.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "ECMD_ An Event_Centric Multisensory Driving Dataset for SLAM",
        "section": 7,
        "paragraph_id": 7,
        "full_text": "2) Mechanical LiDAR: We configure three mechanical LiDARs including two slanted LiDARs to collect accurate point clouds of surrounding environments.Velodyne HDL-32E is positioned on the top of the vehicle to capture the surroundings horizontally.Two slanted LiDARs, Lslidar C16 and Velodyne VLP-16, are mounted on the left and right sides of the sensor kit, respectively.This configuration facilitates the thorough recording of architectural particulars relevant to high-rising buildings in urbanized areas and all LiDAR data are collected at 10 Hz.",
        "sentences": [
          {
            "text": "2) Mechanical LiDAR: We configure three mechanical LiDARs including two slanted LiDARs to collect accurate point clouds of surrounding environments.",
            "label": 1
          },
          {
            "text": "Velodyne HDL-32E is positioned on the top of the vehicle to capture the surroundings horizontally.",
            "label": 1
          },
          {
            "text": "Two slanted LiDARs, Lslidar C16 and Velodyne VLP-16, are mounted on the left and right sides of the sensor kit, respectively.",
            "label": 1
          },
          {
            "text": "This configuration facilitates the thorough recording of architectural particulars relevant to high-rising buildings in urbanized areas and all LiDAR data are collected at 10 Hz.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "ECMD_ An Event_Centric Multisensory Driving Dataset for SLAM",
        "section": 8,
        "paragraph_id": 8,
        "full_text": "3) GNSS-RTK/INS Sensor: A tactical-level Xsens-MTI-30 IMU is employed to collect the raw acceleration and angular velocity at 400 Hz.The accurate ground-truth of localization is furnished by a centimeter-level GNSS-RTK/INS navigation system, further details can be found in Section IV-B1.",
        "sentences": [
          {
            "text": "3) GNSS-RTK/INS Sensor: A tactical-level Xsens-MTI-30 IMU is employed to collect the raw acceleration and angular velocity at 400 Hz.",
            "label": 1
          },
          {
            "text": "The accurate ground-truth of localization is furnished by a centimeter-level GNSS-RTK/INS navigation system, further details can be found in Section IV-B1.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "ECMD_ An Event_Centric Multisensory Driving Dataset for SLAM",
        "section": 9,
        "paragraph_id": 9,
        "full_text": "We use a Precision Time Protocol (PTP)[29]device to synchronize the clocks of various data collection devices across the sensor network.The PTP ensures time accuracy within nanoseconds.The synchronization device acquires the NMEA[30]output and pulse-per-second (PPS) signal from a u-blox M8T GNSS receiver to align the ROS time of the onboard computers with the GPS time.This enables sensors such as cameras, LiDAR, and IMU to record timestamps based on the synchronized GPS time.Moreover, to achieve time synchronization between different event cameras, the DAVIS346 on the rightmost side is configured as the master device and transmits trigger signal pulses to the remaining slave event cameras sequentially from left to right via external cables.",
        "sentences": [
          {
            "text": "We use a Precision Time Protocol (PTP)[29]device to synchronize the clocks of various data collection devices across the sensor network.",
            "label": 1
          },
          {
            "text": "The PTP ensures time accuracy within nanoseconds.",
            "label": 1
          },
          {
            "text": "The synchronization device acquires the NMEA[30]output and pulse-per-second (PPS) signal from a u-blox M8T GNSS receiver to align the ROS time of the onboard computers with the GPS time.",
            "label": 1
          },
          {
            "text": "This enables sensors such as cameras, LiDAR, and IMU to record timestamps based on the synchronized GPS time.",
            "label": 1
          },
          {
            "text": "Moreover, to achieve time synchronization between different event cameras, the DAVIS346 on the rightmost side is configured as the master device and transmits trigger signal pulses to the remaining slave event cameras sequentially from left to right via external cables.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "ECMD_ An Event_Centric Multisensory Driving Dataset for SLAM",
        "section": 10,
        "paragraph_id": 10,
        "full_text": "To calibrate the IMU, we position it on a level surface for a duration of three hours and record the raw measurements.Utilizing the Kalibr toolbox, we can accurately calibrate the random walk and Gaussian white noise of the IMU.",
        "sentences": [
          {
            "text": "To calibrate the IMU, we position it on a level surface for a duration of three hours and record the raw measurements.",
            "label": 1
          },
          {
            "text": "Utilizing the Kalibr toolbox, we can accurately calibrate the random walk and Gaussian white noise of the IMU.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "ECMD_ An Event_Centric Multisensory Driving Dataset for SLAM",
        "section": 11,
        "paragraph_id": 11,
        "full_text": "2) Industrial Cameras Calibration: For industrial cameras, we move the sensor platform against the 9×7 checkerboard in the XYZ-axis and collect the sequence of RGB images and IMU.Then intrinsics calibration of industrial cameras is achieved by Kalibr toolbox[31], where the pinhole and radial-tangential camera models are adopted.",
        "sentences": [
          {
            "text": "2) Industrial Cameras Calibration: For industrial cameras, we move the sensor platform against the 9×7 checkerboard in the XYZ-axis and collect the sequence of RGB images and IMU.",
            "label": 0
          },
          {
            "text": "Then intrinsics calibration of industrial cameras is achieved by Kalibr toolbox[31], where the pinhole and radial-tangential camera models are adopted.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "ECMD_ An Event_Centric Multisensory Driving Dataset for SLAM",
        "section": 12,
        "paragraph_id": 12,
        "full_text": "3) Event Cameras Calibration: For event cameras, DAVIS346 can produce fixed-rated frames, enabling imagebased calibration, while DVXplorer merely produces asynchronous event streams.Therefore, E2Calib[32][33] is used to achieve image reconstruction from event streams.With the reconstructed checkerboard images in Fig.2(b), the intrinsics of event cameras could also be calibrated by Kalibr.4) Infrared Camera Calibration: Due to infrared cameras solely capturing the temperature rather than the intensity difference, we design a distinct 9×7 checkerboard to make the pattern detectable for infrared cameras.As shown in Fig.3(a), the checkerboard intervals are affixed with aluminum materials, and then using a heating plate to raise the temperature of the checkerboard.Since the superior thermal dissipation of aluminum compared to plastic, a temperature contrast emerges between the two materials, enabling infrared cameras to distinctly capture the lattice shape of the checkerboard, as in Fig.3(b).With the special infrared image of the checkerboard, intrinsic can be calibrated by Kalibr.",
        "sentences": [
          {
            "text": "3) Event Cameras Calibration: For event cameras, DAVIS346 can produce fixed-rated frames, enabling imagebased calibration, while DVXplorer merely produces asynchronous event streams.",
            "label": 0
          },
          {
            "text": "Therefore, E2Calib[32][33] is used to achieve image reconstruction from event streams.",
            "label": 0
          },
          {
            "text": "4) Infrared Camera Calibration: Due to infrared cameras solely capturing the temperature rather than the intensity difference, we design a distinct 9×7 checkerboard to make the pattern detectable for infrared cameras.",
            "label": 0
          },
          {
            "text": "As shown in Fig.3(a), the checkerboard intervals are affixed with aluminum materials, and then using a heating plate to raise the temperature of the checkerboard.",
            "label": 0
          },
          {
            "text": "Since the superior thermal dissipation of aluminum compared to plastic, a temperature contrast emerges between the two materials, enabling infrared cameras to distinctly capture the lattice shape of the checkerboard, as in Fig.3(b).",
            "label": 0
          },
          {
            "text": "With the special infrared image of the checkerboard, intrinsic can be calibrated by Kalibr.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "ECMD_ An Event_Centric Multisensory Driving Dataset for SLAM",
        "section": 13,
        "paragraph_id": 13,
        "full_text": "After completing intrinsics calibration, we move the sensor suite in front of checkerboards along the XYZ-RPY-axis and collect data simultaneously.Subsequently, the extrinsics and the temporal offset between all cameras and IMU could be estimated using Kalibr.",
        "sentences": [
          {
            "text": "After completing intrinsics calibration, we move the sensor suite in front of checkerboards along the XYZ-RPY-axis and collect data simultaneously.",
            "label": 0
          },
          {
            "text": "Subsequently, the extrinsics and the temporal offset between all cameras and IMU could be estimated using Kalibr.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "ECMD_ An Event_Centric Multisensory Driving Dataset for SLAM",
        "section": 14,
        "paragraph_id": 14,
        "full_text": "For the calibration of mechanical LiDAR, LI-Init[34]is capable of achieving temporal and spatial calibration for LiDAR and IMU without checkerboards or extra devices in Fig.4. We rotate and move the device around the XYZ-axis to ensure sufficient excitation until the data accumulation is completed, thus we acquire the extrinsic transformation between LiDAR and IMU.",
        "sentences": [
          {
            "text": "For the calibration of mechanical LiDAR, LI-Init[34]is capable of achieving temporal and spatial calibration for LiDAR and IMU without checkerboards or extra devices in Fig.4.",
            "label": 0
          },
          {
            "text": "We rotate and move the device around the XYZ-axis to ensure sufficient excitation until the data accumulation is completed, thus we acquire the extrinsic transformation between LiDAR and IMU.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "ECMD_ An Event_Centric Multisensory Driving Dataset for SLAM",
        "section": 15,
        "paragraph_id": 15,
        "full_text": "Our dataset encompasses a wide range of driving scenes, including urban streets, urban roads, tunnels, highways, bridges, and suburban roads.We have specifically focused on scenarios where visual SLAM algorithms encounter difficulties.These scenarios involve high-speed motion (up to 110 km/h), limited texture, as well as difficult glare conditions in both daytime and nighttime driving.We also targeted situations where LiDAR SLAM encounters limitations, such as long corridors or areas with sparse geometric structures.The complete dataset is partitioned into 81 sequences to facilitate researchers in evaluating their algorithms.Each sequence has an approximate duration of 120 seconds.Additionally, we have retained a few sequences with long duration, lasting approximately 34 minutes, specifically for the evaluation of loop closure in large-scale environments and loop closure scenarios.The summary of sequence types can be found in TableIII. A. Scenarios 1) Dense Urban Street: This scenario focuses on lowspeed vehicles, around 30km/h, proceeding on highly urbanized areas and urban canyons in Hong Kong with multiple light conditions.The streets are narrow at 10m in width and buildings on both sides of the scene are dense.Meanwhile, the presence of congested traffic and dynamic crowds may produce the degradation of visual or LiDAR localization, such as Dense street day easy b.To evaluate the loop closure performance of SLAM, we remarkably recorded sequences Dense street difficult circle and Dense street difficult loop where our vehicle was circling in repeated routes.",
        "sentences": [
          {
            "text": "Our dataset encompasses a wide range of driving scenes, including urban streets, urban roads, tunnels, highways, bridges, and suburban roads.",
            "label": 0
          },
          {
            "text": "We have specifically focused on scenarios where visual SLAM algorithms encounter difficulties.",
            "label": 1
          },
          {
            "text": "These scenarios involve high-speed motion (up to 110 km/h), limited texture, as well as difficult glare conditions in both daytime and nighttime driving.",
            "label": 1
          },
          {
            "text": "We also targeted situations where LiDAR SLAM encounters limitations, such as long corridors or areas with sparse geometric structures.",
            "label": 1
          },
          {
            "text": "The complete dataset is partitioned into 81 sequences to facilitate researchers in evaluating their algorithms.",
            "label": 1
          },
          {
            "text": "Each sequence has an approximate duration of 120 seconds.",
            "label": 1
          },
          {
            "text": "Additionally, we have retained a few sequences with long duration, lasting approximately 34 minutes, specifically for the evaluation of loop closure in large-scale environments and loop closure scenarios.",
            "label": 1
          },
          {
            "text": "The summary of sequence types can be found in TableIII. A. Scenarios 1) Dense Urban Street: This scenario focuses on lowspeed vehicles, around 30km/h, proceeding on highly urbanized areas and urban canyons in Hong Kong with multiple light conditions.",
            "label": 1
          },
          {
            "text": "The streets are narrow at 10m in width and buildings on both sides of the scene are dense.",
            "label": 1
          },
          {
            "text": "Meanwhile, the presence of congested traffic and dynamic crowds may produce the degradation of visual or LiDAR localization, such as Dense street day easy b.",
            "label": 1
          },
          {
            "text": "To evaluate the loop closure performance of SLAM, we remarkably recorded sequences Dense street difficult circle and Dense street difficult loop where our vehicle was circling in repeated routes.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "ECMD_ An Event_Centric Multisensory Driving Dataset for SLAM",
        "section": 16,
        "paragraph_id": 16,
        "full_text": "2) Urban Road: This type of scenario records the vehicle traveling at an approximate speed of 60km/h on an expressway in Hong Kong with multiple weather conditions.Compared to the Dense Urban Street scenario, Urban Road sequences travel through Hong Kong city at a higher speed, while the buildings are not as tightly packed on either side and the road is more spacious with four lanes.Despite the absence of pedestrians on the road, the scene still includes vehicles overtaking, paralleling, and other situations where the relative motion is not consistent with the absolute motion.The aforementioned discrepancy might pose a challenge for the VIO or LIO system.Moreover, the sequence comprises the vehicle traveling during nighttime in rainy conditions.We record trajectories in rainy situations under nighttime like Urban road night difficult rainy a which are commonly faced in practical driving scenarios, whereas they are not present in previous datasets.",
        "sentences": [
          {
            "text": "2) Urban Road: This type of scenario records the vehicle traveling at an approximate speed of 60km/h on an expressway in Hong Kong with multiple weather conditions.",
            "label": 1
          },
          {
            "text": "Compared to the Dense Urban Street scenario, Urban Road sequences travel through Hong Kong city at a higher speed, while the buildings are not as tightly packed on either side and the road is more spacious with four lanes.",
            "label": 1
          },
          {
            "text": "Despite the absence of pedestrians on the road, the scene still includes vehicles overtaking, paralleling, and other situations where the relative motion is not consistent with the absolute motion.",
            "label": 1
          },
          {
            "text": "The aforementioned discrepancy might pose a challenge for the VIO or LIO system.",
            "label": 1
          },
          {
            "text": "Moreover, the sequence comprises the vehicle traveling during nighttime in rainy conditions.",
            "label": 1
          },
          {
            "text": "We record trajectories in rainy situations under nighttime like Urban road night difficult rainy a which are commonly faced in practical driving scenarios, whereas they are not present in previous datasets.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "ECMD_ An Event_Centric Multisensory Driving Dataset for SLAM",
        "section": 17,
        "paragraph_id": 17,
        "full_text": "3) Tunnel: Tunnel scenarios commence with a high-speed vehicle on an open-sky highway, entering an enclosed tunnel without satellite reception.Inside the tunnel, GNSS positioning is unreliable since the satellite signal is completely blocked.Meanwhile, the scenario represents a typical and challenging scene for VIO and LIO systems due to the repetitive and texture-less environments for vision sensors and LiDAR.The sequence collections end after the vehicle exits the tunnel and continues to proceed on the highway for twenty seconds.",
        "sentences": [
          {
            "text": "3) Tunnel: Tunnel scenarios commence with a high-speed vehicle on an open-sky highway, entering an enclosed tunnel without satellite reception.",
            "label": 1
          },
          {
            "text": "Inside the tunnel, GNSS positioning is unreliable since the satellite signal is completely blocked.",
            "label": 1
          },
          {
            "text": "Meanwhile, the scenario represents a typical and challenging scene for VIO and LIO systems due to the repetitive and texture-less environments for vision sensors and LiDAR.",
            "label": 1
          },
          {
            "text": "The sequence collections end after the vehicle exits the tunnel and continues to proceed on the highway for twenty seconds.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "ECMD_ An Event_Centric Multisensory Driving Dataset for SLAM",
        "section": 18,
        "paragraph_id": 18,
        "full_text": "4) Highway: The scenario involves vehicles traveling at speeds up to 100km/h on low-texture highways both during the day and night, with sparse buildings alongside the road.High speeds, rapid changes in vehicle speed, repetitive visual scenes, and low-texture environments present significant challenges for autonomous driving.Meanwhile, the vibration of the vehicle body at high-speed motion amplifies the random walk and Gaussian white noise of IMU, thereby diminishing its reliability.",
        "sentences": [
          {
            "text": "4) Highway: The scenario involves vehicles traveling at speeds up to 100km/h on low-texture highways both during the day and night, with sparse buildings alongside the road.",
            "label": 1
          },
          {
            "text": "High speeds, rapid changes in vehicle speed, repetitive visual scenes, and low-texture environments present significant challenges for autonomous driving.",
            "label": 1
          },
          {
            "text": "Meanwhile, the vibration of the vehicle body at high-speed motion amplifies the random walk and Gaussian white noise of IMU, thereby diminishing its reliability.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "ECMD_ An Event_Centric Multisensory Driving Dataset for SLAM",
        "section": 19,
        "paragraph_id": 19,
        "full_text": "5) Bridge: The motion pattern of vehicles in bridge scenarios resembles that of highways, with vehicles traveling in a straight line at high speed along the bridge.However, this scene differs as there are no buildings on either side of the bridge, only the sea surrounds it.Bridges present scenes with limited texture, and the feature information within these scenes tends to be monotonous and repetitive, which further exacerbates the challenge of achieving accurate localization.",
        "sentences": [
          {
            "text": "5) Bridge: The motion pattern of vehicles in bridge scenarios resembles that of highways, with vehicles traveling in a straight line at high speed along the bridge.",
            "label": 1
          },
          {
            "text": "However, this scene differs as there are no buildings on either side of the bridge, only the sea surrounds it.",
            "label": 1
          },
          {
            "text": "Bridges present scenes with limited texture, and the feature information within these scenes tends to be monotonous and repetitive, which further exacerbates the challenge of achieving accurate localization.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "ECMD_ An Event_Centric Multisensory Driving Dataset for SLAM",
        "section": 20,
        "paragraph_id": 20,
        "full_text": "6) Suburban Road: Suburban road scenarios present complex natural environments characterized by winding and rugged roads, steep slopes, and narrow lanes.The vehicle navigates the serpentine mountain roads at a moderate speed (approximately 50km/h), with significant altitude changes.The abundant texture information in the mountain road scene facilitates visual algorithms to extract stable features and construct effective constraints.",
        "sentences": [
          {
            "text": "6) Suburban Road: Suburban road scenarios present complex natural environments characterized by winding and rugged roads, steep slopes, and narrow lanes.",
            "label": 1
          },
          {
            "text": "The vehicle navigates the serpentine mountain roads at a moderate speed (approximately 50km/h), with significant altitude changes.",
            "label": 1
          },
          {
            "text": "The abundant texture information in the mountain road scene facilitates visual algorithms to extract stable features and construct effective constraints.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "ECMD_ An Event_Centric Multisensory Driving Dataset for SLAM",
        "section": 21,
        "paragraph_id": 21,
        "full_text": "1) Ground-truth Poses: We obtained the ground-truth positioning from the NovAtel SPAN-CPT[28], a highperformance GNSS RTK/INS integrated navigation system.The ground-truth of most existing event-based driving datasets are derived from LiDAR-SLAM[6][24], GPS/GNSS[6], GNSS-RTK[7][25][24].The ground-truth derived from LiDAR-SLAM relies on the estimation of vehicle trajectories using LiDAR SLAM which only provides relative trajectories.It is difficult to quantify the accuracy of ground-truth pose, and errors may even exceed ten meters in some cases.The complex environment or the equipment malfunctions may disrupt the satellite reception of GPS/GNSS, thus relying solely on GPS/GNSS for ground-truth pose may lead to significant drift.The GNSS-RTK device can only provide centimeter-level accuracy in the open sky[26]In contrast, our SPAN-CPT can provide continuous high accuracy aided by the internal fiber-optic gyroscopes under high-rise buildings, tunnels, and other environments with weak satellite signals.Furthermore, we post-process the ground-truth positioning from SPAN-CPT using the state-ofthe-art NovAtel Inertial Explorer[28]software to maximize the accuracy of the trajectory.For the GNSS positioning benchmark, we provide the WGS84 coordinate data for comparison.For the evaluation of SLAM algorithms, we provide the tools2to transform the ground-truth data from the WGS84 coordinates to the local frame/ENU frame based on the original points.",
        "sentences": [
          {
            "text": "1) Ground-truth Poses: We obtained the ground-truth positioning from the NovAtel SPAN-CPT[28], a highperformance GNSS RTK/INS integrated navigation system.",
            "label": 0
          },
          {
            "text": "The ground-truth of most existing event-based driving datasets are derived from LiDAR-SLAM[6][24], GPS/GNSS[6], GNSS-RTK[7][25][24].",
            "label": 0
          },
          {
            "text": "The ground-truth derived from LiDAR-SLAM relies on the estimation of vehicle trajectories using LiDAR SLAM which only provides relative trajectories.",
            "label": 0
          },
          {
            "text": "It is difficult to quantify the accuracy of ground-truth pose, and errors may even exceed ten meters in some cases.",
            "label": 0
          },
          {
            "text": "The complex environment or the equipment malfunctions may disrupt the satellite reception of GPS/GNSS, thus relying solely on GPS/GNSS for ground-truth pose may lead to significant drift.",
            "label": 0
          },
          {
            "text": "The GNSS-RTK device can only provide centimeter-level accuracy in the open sky[26]In contrast, our SPAN-CPT can provide continuous high accuracy aided by the internal fiber-optic gyroscopes under high-rise buildings, tunnels, and other environments with weak satellite signals.",
            "label": 1
          },
          {
            "text": "Furthermore, we post-process the ground-truth positioning from SPAN-CPT using the state-ofthe-art NovAtel Inertial Explorer[28]software to maximize the accuracy of the trajectory.",
            "label": 1
          },
          {
            "text": "For the GNSS positioning benchmark, we provide the WGS84 coordinate data for comparison.",
            "label": 1
          },
          {
            "text": "For the evaluation of SLAM algorithms, we provide the tools2to transform the ground-truth data from the WGS84 coordinates to the local frame/ENU frame based on the original points.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "ECMD_ An Event_Centric Multisensory Driving Dataset for SLAM",
        "section": 22,
        "paragraph_id": 22,
        "full_text": "2) LiDAR Point Cloud Maps Generation: Utilizing the ground-truth pose for each frame in conjunction with their corresponding LiDAR point clouds, we accumulate these point clouds to construct a highly accurate LiDAR point cloud map to depict the TsingMa Bridge in Fig.6.The map encompasses rich spatial information, providing a detailed 3D reconstruction of the bridge and its surrounding areas.",
        "sentences": [
          {
            "text": "2) LiDAR Point Cloud Maps Generation: Utilizing the ground-truth pose for each frame in conjunction with their corresponding LiDAR point clouds, we accumulate these point clouds to construct a highly accurate LiDAR point cloud map to depict the TsingMa Bridge in Fig.6.",
            "label": 1
          },
          {
            "text": "The map encompasses rich spatial information, providing a detailed 3D reconstruction of the bridge and its surrounding areas.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "ECMD_ An Event_Centric Multisensory Driving Dataset for SLAM",
        "section": 23,
        "paragraph_id": 23,
        "full_text": "As shown in TableIV., we evaluate the performance of VINS-MONO[35], ORB-SLAM3[36], and ESVIO[37]Fig.6.The vehicle poses ground-truth on Google map with the LiDAR point cloud maps of Tsing Ma bridge.across various scenes and lighting conditions on our dataset.The accuracy is quantified using mean position error (MPE, %), which aligns the estimated trajectory with ground-truth through 6-DOF transformation (in SE3) computed by the tool[38].For the VINS-Mono, we evaluate it separately using RGB images and infrared images.Due to the resolution provided by industrial cameras being higher in contrast to the infrared camera, we achieve superior performance when utilizing RGB images.The ORB-SLAM3 often fails to robustly track features during high-speed vehicle movements, potentially resulting in the tracking thread restarts.The ESVIO leverages the complementary advantages of event streams and RGB images, allowing it to handle the lack of texture in RGB images under broad illumination conditions to achieve higher accuracy.Fig.7compares event, RGB images, and infrared images under different lighting conditions.The RGB images offer rich texture under regular luminance scenes in contrast to events and infrared images offer comparatively limited information, e.g., the infrared image struggles to accurately discern traffic left-turn symbol on the ground.Conversely, the RGB image may lose numerous environmental features under the conditions of low light or over-exposure.The infrared camera can capture infrared radiation beyond the visible spectrum and event cameras can detect pixel-level intensity changes at low latency.Both the event camera and the infrared camera are more resilient in external varying lighting conditions, providing effective visibility compared to the industrial camera, e.g., in nighttime scenes, event cameras can capture road signs, and the infrared camera can clearly capture the surrounding bushes.",
        "sentences": [
          {
            "text": "As shown in TableIV., we evaluate the performance of VINS-MONO[35], ORB-SLAM3[36], and ESVIO[37]Fig.6.",
            "label": 0
          },
          {
            "text": "The vehicle poses ground-truth on Google map with the LiDAR point cloud maps of Tsing Ma bridge.across various scenes and lighting conditions on our dataset.",
            "label": 0
          },
          {
            "text": "The accuracy is quantified using mean position error (MPE, %), which aligns the estimated trajectory with ground-truth through 6-DOF transformation (in SE3) computed by the tool[38].",
            "label": 0
          },
          {
            "text": "For the VINS-Mono, we evaluate it separately using RGB images and infrared images.",
            "label": 0
          },
          {
            "text": "Due to the resolution provided by industrial cameras being higher in contrast to the infrared camera, we achieve superior performance when utilizing RGB images.",
            "label": 0
          },
          {
            "text": "The ORB-SLAM3 often fails to robustly track features during high-speed vehicle movements, potentially resulting in the tracking thread restarts.",
            "label": 0
          },
          {
            "text": "The ESVIO leverages the complementary advantages of event streams and RGB images, allowing it to handle the lack of texture in RGB images under broad illumination conditions to achieve higher accuracy.",
            "label": 0
          },
          {
            "text": "Fig.7compares event, RGB images, and infrared images under different lighting conditions.",
            "label": 0
          },
          {
            "text": "The RGB images offer rich texture under regular luminance scenes in contrast to events and infrared images offer comparatively limited information, e.g., the infrared image struggles to accurately discern traffic left-turn symbol on the ground.",
            "label": 0
          },
          {
            "text": "Conversely, the RGB image may lose numerous environmental features under the conditions of low light or over-exposure.",
            "label": 0
          },
          {
            "text": "The infrared camera can capture infrared radiation beyond the visible spectrum and event cameras can detect pixel-level intensity changes at low latency.",
            "label": 0
          },
          {
            "text": "Both the event camera and the infrared camera are more resilient in external varying lighting conditions, providing effective visibility compared to the industrial camera, e.g., in nighttime scenes, event cameras can capture road signs, and the infrared camera can clearly capture the surrounding bushes.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "ECMD_ An Event_Centric Multisensory Driving Dataset for SLAM",
        "section": 24,
        "paragraph_id": 24,
        "full_text": "TableV. demonstrates the performance of LIO-SAM[39], LVI-SAM[40], Fast-LIO2[41], Point-LIO[42]across various scenes on our dataset.We use the same criteria introduced in Section V-A to evaluate the localization accuracy.Due to the tilt-mounted LiDAR setups (see Fig.1), we are able to acquire point clouds of towering buildings situated on both sides of the street.This installation approach compensated for the lack of vertical point clouds compared to the horizontally mounted LiDAR.In Fig.8, red point clouds are generated from a horizontally mounted LiDAR while white and green point clouds are generated from tilt-mounted LiDARs.We evaluate the performance of LOAM[43]using three different LiDARs (center, left, and right)in Dense street day medium circle a sequences.",
        "sentences": [
          {
            "text": "TableV. demonstrates the performance of LIO-SAM[39], LVI-SAM[40], Fast-LIO2[41], Point-LIO[42]across various scenes on our dataset.",
            "label": 1
          },
          {
            "text": "We use the same criteria introduced in Section V-A to evaluate the localization accuracy.",
            "label": 0
          },
          {
            "text": "Due to the tilt-mounted LiDAR setups (see Fig.1), we are able to acquire point clouds of towering buildings situated on both sides of the street.",
            "label": 1
          },
          {
            "text": "This installation approach compensated for the lack of vertical point clouds compared to the horizontally mounted LiDAR.",
            "label": 1
          },
          {
            "text": "In Fig.8, red point clouds are generated from a horizontally mounted LiDAR while white and green point clouds are generated from tilt-mounted LiDARs.",
            "label": 1
          },
          {
            "text": "We evaluate the performance of LOAM[43]using three different LiDARs (center, left, and right)in Dense street day medium circle a sequences.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "ECMD_ An Event_Centric Multisensory Driving Dataset for SLAM",
        "section": 25,
        "paragraph_id": 25,
        "full_text": "The MPE of LOAM using center LiDAR is 1.02%, compared to 8.67% using the left LiDAR and 2.00% using the right LiDAR.LOAM using the left LiDAR exhibits significant drift since it initially captures minimal point cloud information.Although the LOAM merely using tilt-mounted LiDAR produces less accurate results compared to the center LiDAR, multi-LiDAR fusion can integrate complementary information, thereby improving localization accuracy and constructing more precise point cloud maps.Meanwhile, tunnel scenes present challenges for LiDAR SLAM.We capture three consecutive frames of LiDAR point clouds at two-second intervals in Fig.9.It is evident that these LiDAR point clouds exhibit high similarity in the tunnel environment, potentially resulting in degradation phenomena and inaccurate state estimation.",
        "sentences": [
          {
            "text": "The MPE of LOAM using center LiDAR is 1.02%, compared to 8.67% using the left LiDAR and 2.00% using the right LiDAR.",
            "label": 0
          },
          {
            "text": "LOAM using the left LiDAR exhibits significant drift since it initially captures minimal point cloud information.",
            "label": 1
          },
          {
            "text": "Although the LOAM merely using tilt-mounted LiDAR produces less accurate results compared to the center LiDAR, multi-LiDAR fusion can integrate complementary information, thereby improving localization accuracy and constructing more precise point cloud maps.",
            "label": 1
          },
          {
            "text": "Meanwhile, tunnel scenes present challenges for LiDAR SLAM.",
            "label": 1
          },
          {
            "text": "We capture three consecutive frames of LiDAR point clouds at two-second intervals in Fig.9.",
            "label": 1
          },
          {
            "text": "It is evident that these LiDAR point clouds exhibit high similarity in the tunnel environment, potentially resulting in degradation phenomena and inaccurate state estimation.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "ECMD_ An Event_Centric Multisensory Driving Dataset for SLAM",
        "section": 26,
        "paragraph_id": 26,
        "full_text": "Due to space limitations, we positioned LiDAR closer to the event cameras.As a consequence, the infrared wavelengths emitted by LiDAR directly impinge on the photoreceptor of event cameras, resulting in continuous disturbances and flickering in the captured images and event streams.To address this issue, we implement infrared filters on event cameras to counteract the effect.However, this intervention led to a compromise, resulting in a degradation of the quality of the recorded event data.",
        "sentences": [
          {
            "text": "Due to space limitations, we positioned LiDAR closer to the event cameras.",
            "label": 1
          },
          {
            "text": "As a consequence, the infrared wavelengths emitted by LiDAR directly impinge on the photoreceptor of event cameras, resulting in continuous disturbances and flickering in the captured images and event streams.",
            "label": 1
          },
          {
            "text": "To address this issue, we implement infrared filters on event cameras to counteract the effect.",
            "label": 1
          },
          {
            "text": "However, this intervention led to a compromise, resulting in a degradation of the quality of the recorded event data.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "ECMD_ An Event_Centric Multisensory Driving Dataset for SLAM",
        "section": 27,
        "paragraph_id": 27,
        "full_text": "During the night or low illumination scenarios, we observed that when event cameras were directly toward a glowing light source, such as street lights or store lighting, event streams would exhibit persistent flickering and produce artifacts around the light source.This could potentially lead to a distorted view of the observed object.We postulate this phenomenon is related to the inherent principle of event cameras, and presently, there is no known solution to address this issue.",
        "sentences": [
          {
            "text": "During the night or low illumination scenarios, we observed that when event cameras were directly toward a glowing light source, such as street lights or store lighting, event streams would exhibit persistent flickering and produce artifacts around the light source.",
            "label": 1
          },
          {
            "text": "This could potentially lead to a distorted view of the observed object.",
            "label": 1
          },
          {
            "text": "We postulate this phenomenon is related to the inherent principle of event cameras, and presently, there is no known solution to address this issue.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "ECMD_ An Event_Centric Multisensory Driving Dataset for SLAM",
        "section": 28,
        "paragraph_id": 28,
        "full_text": "In this paper, we propose an event-centric autonomous driving dataset generated with multiple sensors across various scenarios for developing SLAM algorithms.All sensors undergo meticulous calibration and are temporally synchronized at the hardware level.We employ the GNSS-RTK/INS navigation system, which provides centimeter-level accuracy, to acquire precise ground-truth of the vehicle.Furthermore, we conduct the evaluation of various state-of-the-art visual and LiDAR SLAM algorithms while identifying their constraints.We hope this dataset could contribute to the development of visual and LiDAR SLAM.In future work, we intend to expand the dataset to encompass additional tasks, including semantics, optical flow, depth estimation, etc.",
        "sentences": [
          {
            "text": "In this paper, we propose an event-centric autonomous driving dataset generated with multiple sensors across various scenarios for developing SLAM algorithms.",
            "label": 1
          },
          {
            "text": "All sensors undergo meticulous calibration and are temporally synchronized at the hardware level.",
            "label": 1
          },
          {
            "text": "We employ the GNSS-RTK/INS navigation system, which provides centimeter-level accuracy, to acquire precise ground-truth of the vehicle.",
            "label": 1
          },
          {
            "text": "Furthermore, we conduct the evaluation of various state-of-the-art visual and LiDAR SLAM algorithms while identifying their constraints.",
            "label": 1
          },
          {
            "text": "We hope this dataset could contribute to the development of visual and LiDAR SLAM.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "EGO_CH_ Dataset and Fundamental Tasks for Visitors Behavioral Understanding using Egocentric Vision",
        "section": 0,
        "paragraph_id": 0,
        "full_text": "Abstract: Equipping visitors of a cultural site with a wearable device allows to easily collect information about their preferences which can be exploited to improve the fruition of cultural goods with augmented reality.Moreover, egocentric video can be processed using computer vision and machine learning to enable an automated analysis of visitors' behavior.The inferred information can be used both online to assist the visitor and offline to support the manager of the site.Despite the positive impact such technologies can have in cultural heritage, the topic is currently understudied due to the limited number of public datasets suitable to study the considered problems.To address this issue, in this paper we propose EGOcentric-Cultural Heritage (EGO-CH), the first dataset of egocentric videos for visitors' behavior understanding in cultural sites.The dataset has been collected in two cultural sites and includes more than 27 hours of video acquired by 70 subjects, with labels for 26 environments and over 200 different Points of Interest.A large subset of the dataset, consisting of 60 videos, is associated with surveys filled out by real visitors.To encourage research on the topic, we propose 4 challenging tasks (room-based localization, point of interest/object recognition, object retrieval and survey prediction) useful to understand visitors' behavior and report baseline results on the dataset.",
        "sentences": [
          {
            "text": "Abstract: Equipping visitors of a cultural site with a wearable device allows to easily collect information about their preferences which can be exploited to improve the fruition of cultural goods with augmented reality.",
            "label": 0
          },
          {
            "text": "Moreover, egocentric video can be processed using computer vision and machine learning to enable an automated analysis of visitors' behavior.",
            "label": 0
          },
          {
            "text": "The inferred information can be used both online to assist the visitor and offline to support the manager of the site.",
            "label": 1
          },
          {
            "text": "Despite the positive impact such technologies can have in cultural heritage, the topic is currently understudied due to the limited number of public datasets suitable to study the considered problems.",
            "label": 1
          },
          {
            "text": "To address this issue, in this paper we propose EGOcentric-Cultural Heritage (EGO-CH), the first dataset of egocentric videos for visitors' behavior understanding in cultural sites.",
            "label": 1
          },
          {
            "text": "The dataset has been collected in two cultural sites and includes more than 27 hours of video acquired by 70 subjects, with labels for 26 environments and over 200 different Points of Interest.",
            "label": 1
          },
          {
            "text": "A large subset of the dataset, consisting of 60 videos, is associated with surveys filled out by real visitors.",
            "label": 1
          },
          {
            "text": "To encourage research on the topic, we propose 4 challenging tasks (room-based localization, point of interest/object recognition, object retrieval and survey prediction) useful to understand visitors' behavior and report baseline results on the dataset.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "EGO_CH_ Dataset and Fundamental Tasks for Visitors Behavioral Understanding using Egocentric Vision",
        "section": 1,
        "paragraph_id": 1,
        "full_text": "Cultural sites receive many visitors every day.For a cultural site manager, it is hence paramount to 1) provide services able to assist the visitors, and 2) analyze their behavior to measure the performance of the site and understand what can be improved.For example using indicators[1]such as: a) Attraction index: to measure how much a point of interest attracts the visitors, b) Retention index: to measure the average time spent observing information element (e.g., a caption, a video a panel, etc.), c) Sweep Rate Index (SRI): it is used to calculate if visitors move slowly or quickly through the exhibition, d) Diligent Visitor Index (DVI): the percentage of visitors who stopped in front of more than half of the points of interest.Classic approaches addressed the former task through the delivery of printed material (e.g., maps of the museum), the use of audio-guides and the installation of informative panels.Similarly, the analysis of visitors' behavior has generally been performed through the administration of questionnaires.It should be noted that such approaches often require manual intervention and are limited especially when the number of visitors is large.Recent works[2,3,4]have highlighted that the use of wearable devices such as smart glasses can provide a convenient platform to tackle the considered tasks in an automated fashion.Using such technology, it is possible to provide to the user services such as automated localization (e.g., to help visitors navigating the site) and recognition of currently observed Ponts Of Interest (POIs)1to provide more information on relevant objects and suggest what to see next.Conveniently, localization and POI recognition can be used by the manager of the cultural site to obtain information about the visitors and understand their behavior by inferring where they have been, how much time they have spent in a specific environment and what POIs have been liked most.",
        "sentences": [
          {
            "text": "Cultural sites receive many visitors every day.",
            "label": 0
          },
          {
            "text": "For a cultural site manager, it is hence paramount to 1) provide services able to assist the visitors, and 2) analyze their behavior to measure the performance of the site and understand what can be improved.",
            "label": 0
          },
          {
            "text": "For example using indicators[1]such as: a) Attraction index: to measure how much a point of interest attracts the visitors, b) Retention index: to measure the average time spent observing information element (e.g., a caption, a video a panel, etc.), c) Sweep Rate Index (SRI): it is used to calculate if visitors move slowly or quickly through the exhibition, d) Diligent Visitor Index (DVI): the percentage of visitors who stopped in front of more than half of the points of interest.",
            "label": 0
          },
          {
            "text": "Classic approaches addressed the former task through the delivery of printed material (e.g., maps of the museum), the use of audio-guides and the installation of informative panels.",
            "label": 0
          },
          {
            "text": "Similarly, the analysis of visitors' behavior has generally been performed through the administration of questionnaires.",
            "label": 0
          },
          {
            "text": "It should be noted that such approaches often require manual intervention and are limited especially when the number of visitors is large.",
            "label": 0
          },
          {
            "text": "Recent works[2,3,4]have highlighted that the use of wearable devices such as smart glasses can provide a convenient platform to tackle the considered tasks in an automated fashion.",
            "label": 0
          },
          {
            "text": "Using such technology, it is possible to provide to the user services such as automated localization (e.g., to help visitors navigating the site) and recognition of currently observed Ponts Of Interest (POIs)1to provide more information on relevant objects and suggest what to see next.",
            "label": 0
          },
          {
            "text": "Conveniently, localization and POI recognition can be used by the manager of the cultural site to obtain information about the visitors and understand their behavior by inferring where they have been, how much time they have spent in a specific environment and what POIs have been liked most.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "EGO_CH_ Dataset and Fundamental Tasks for Visitors Behavioral Understanding using Egocentric Vision",
        "section": 2,
        "paragraph_id": 2,
        "full_text": "Despite the aforementioned technologies can have a significant impact on cultural heritage, they are currently under-explored due to the lack of public benchmark datasets.To address this issue, in this paper we propose EGOcentric-Cultural Heritage (EGO-CH), the first large dataset of egocentric videos for visitors behavioral understanding in cultural sites.The dataset has been collected in two cultural sites located in Sicily, Italy: Galleria Regionale di Palazzo Bellomo2and Monastero dei Benedettini3.The overall dataset contains more than 27 hours of video, including 26 environments, over 200 Points of Interest and 70 visits.We release EGO-CH with a set of annotations useful to tackle fundamental tasks related to visitors behavior understanding in cultural sites, and specifically, temporal labels specifying the location of the visitor as well as the currently observed POI, bounding box annotations around POIs, surveys filled out by visitors at the end of each tour in the cultural site.Figure1reports some sample frames from the proposed dataset.The dataset can be publicly accessed upon request to the authors from our webpage http://iplab.dmi.unict.it/EGO-CH/.",
        "sentences": [
          {
            "text": "Despite the aforementioned technologies can have a significant impact on cultural heritage, they are currently under-explored due to the lack of public benchmark datasets.",
            "label": 1
          },
          {
            "text": "To address this issue, in this paper we propose EGOcentric-Cultural Heritage (EGO-CH), the first large dataset of egocentric videos for visitors behavioral understanding in cultural sites.",
            "label": 1
          },
          {
            "text": "The dataset has been collected in two cultural sites located in Sicily, Italy: Galleria Regionale di Palazzo Bellomo2and Monastero dei Benedettini3.",
            "label": 1
          },
          {
            "text": "The overall dataset contains more than 27 hours of video, including 26 environments, over 200 Points of Interest and 70 visits.",
            "label": 1
          },
          {
            "text": "We release EGO-CH with a set of annotations useful to tackle fundamental tasks related to visitors behavior understanding in cultural sites, and specifically, temporal labels specifying the location of the visitor as well as the currently observed POI, bounding box annotations around POIs, surveys filled out by visitors at the end of each tour in the cultural site.",
            "label": 1
          },
          {
            "text": "Figure1reports some sample frames from the proposed dataset.",
            "label": 1
          },
          {
            "text": "The dataset can be publicly accessed upon request to the authors from our webpage http://iplab.dmi.unict.it/EGO-CH/.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "EGO_CH_ Dataset and Fundamental Tasks for Visitors Behavioral Understanding using Egocentric Vision",
        "section": 3,
        "paragraph_id": 3,
        "full_text": "We propose 4 fundamental tasks for visitors behavioral understanding using egocentric vision: 1) room-based localization, consisting in recognizing the environment in which the visitor is located in each frame of the video, 2) Point of Interest recognition, which consists in correctly detecting and localizing all objects in the image frames, 3) object retrieval, which consists in matching an observed object from the egocentric point of view to a reference image contained in the museum catalogue  of all artworks, 4) survey prediction, which consists in generating the survey associated to a visit from video.We also provide baseline results for each task on the proposed dataset.The experimental results suggest that the proposed dataset is a challenging benchmark for visitors behavioral understanding using egocentric vision.",
        "sentences": [
          {
            "text": "We propose 4 fundamental tasks for visitors behavioral understanding using egocentric vision: 1) room-based localization, consisting in recognizing the environment in which the visitor is located in each frame of the video, 2) Point of Interest recognition, which consists in correctly detecting and localizing all objects in the image frames, 3) object retrieval, which consists in matching an observed object from the egocentric point of view to a reference image contained in the museum catalogue  of all artworks, 4) survey prediction, which consists in generating the survey associated to a visit from video.",
            "label": 0
          },
          {
            "text": "We also provide baseline results for each task on the proposed dataset.",
            "label": 0
          },
          {
            "text": "The experimental results suggest that the proposed dataset is a challenging benchmark for visitors behavioral understanding using egocentric vision.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "EGO_CH_ Dataset and Fundamental Tasks for Visitors Behavioral Understanding using Egocentric Vision",
        "section": 4,
        "paragraph_id": 4,
        "full_text": "In sum, the contributions of this work are: 1) we present EGO-CH, a new challenging dataset of egocentric videos acquired in two cultural sites, 2) the dataset has been labeled to tackle 4 main tasks useful to understand visitors behavior, 3) we report baseline results for each task.",
        "sentences": [
          {
            "text": "In sum, the contributions of this work are: 1) we present EGO-CH, a new challenging dataset of egocentric videos acquired in two cultural sites, 2) the dataset has been labeled to tackle 4 main tasks useful to understand visitors behavior, 3) we report baseline results for each task.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "EGO_CH_ Dataset and Fundamental Tasks for Visitors Behavioral Understanding using Egocentric Vision",
        "section": 5,
        "paragraph_id": 5,
        "full_text": "The dataset has been acquired using a headmounted Microsoft HoloLens device in two cultural sites located in Sicily, Italy: 1) Palazzo Bellomo (Table1), located in Siracusa 4 , and 2) Monastero dei Benedettini (Table2), located in Catania5.",
        "sentences": [
          {
            "text": "The dataset has been acquired using a headmounted Microsoft HoloLens device in two cultural sites located in Sicily, Italy: 1) Palazzo Bellomo (Table1), located in Siracusa 4 , and 2) Monastero dei Benedettini (Table2), located in Catania5.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "EGO_CH_ Dataset and Fundamental Tasks for Visitors Behavioral Understanding using Egocentric Vision",
        "section": 6,
        "paragraph_id": 6,
        "full_text": "Palazzo Bellomo This cultural site is composed of 22 environments and contains 191 Points of Interest (e.g., statues, paintings, etc.).6Training videos have been collected by operators instructed to walk around in order to capture images of each environment from different points of view.To simplify labeling, each training video contains  only frames from a given environment.At least one training video has been collected per environment.In the case of outdoor environments (e.g., courtyards), we collected multiple videos to include different lighting conditions.We have collected a total of 57 training video in this cultural site.Monastero dei Benedettini This dataset is composed of 4 environments and contains 35 Points Of Interest. 8Differently from \"Palazzo Bellomo\", the POIs belonging to this cultural site include both objects such as paintings and statues as well as architectural elements, such as pavements, which 7 Examples reference images for both cultural sites are included in the supplementary material. 8See the supplementary material for the list of environments and POIs.cannot be easily recognized using object detection techniques as noted in[5].See Figure1(right) for some qualitative examples of the considered objects.Training videos have been collected with the same acquisition modality considered for the \"Palazzo Bellomo\" cultural site.Figure4reports the number/percentage of frames acquired in each environment.Training and validation videos have a resolution of 1216 × 684 pixels and a frame-rate of 24 fps.Five validation videos have been collected by asking volunteers to visit the cultural site following the same protocol used for \"Palazzo Bellomo\".Additionally, we collected 60 test videos by asking real visitors inexperienced with both the research project and its goals and the HoloLens device to freely visit the cultural site.No specific instructions have been given to the visitors, who were free to explore the 4 environments and the 35 POIs.This allowed us to obtain realistic data of how a visitor would move in a cultural site.Test videos have been collected over a period of three months.Moreover, at the end of the visit, we administered the visitor a survey, the content of which is described in Section 5.2.2.The 60 test videos have a resolution of 1408×792 pixels and a frame-rate of 30.03 f ps.The average video length is 21 min, with the maximum  length being 42 min.See the supplementary material for more details about training/validation/test videos.Similarly to \"Palazzo Bellomo\", we include 35 reference images related to the considered POIs for one-shot image retrieval 7 .Please note that this set of data is adapted from and extends significantly the dataset proposed in[3], introducing 60 new labelled videos collected by real visitors.Specifically, the overall dataset presented in this work contains +1600 minutes of video, data from +70 more subjects, +91369 bounding box annotations and an additional cultural site \"Palazzo Bellomo\" comprising 22 environments and 191 points of interest.",
        "sentences": [
          {
            "text": "Palazzo Bellomo This cultural site is composed of 22 environments and contains 191 Points of Interest (e.g., statues, paintings, etc.).",
            "label": 1
          },
          {
            "text": "6Training videos have been collected by operators instructed to walk around in order to capture images of each environment from different points of view.",
            "label": 1
          },
          {
            "text": "To simplify labeling, each training video contains  only frames from a given environment.",
            "label": 1
          },
          {
            "text": "At least one training video has been collected per environment.",
            "label": 1
          },
          {
            "text": "In the case of outdoor environments (e.g., courtyards), we collected multiple videos to include different lighting conditions.",
            "label": 1
          },
          {
            "text": "We have collected a total of 57 training video in this cultural site.",
            "label": 1
          },
          {
            "text": "Monastero dei Benedettini This dataset is composed of 4 environments and contains 35 Points Of Interest.",
            "label": 1
          },
          {
            "text": " 8Differently from \"Palazzo Bellomo\", the POIs belonging to this cultural site include both objects such as paintings and statues as well as architectural elements, such as pavements, which 7 Examples reference images for both cultural sites are included in the supplementary material. 8See the supplementary material for the list of environments and POIs.cannot be easily recognized using object detection techniques as noted in[5].",
            "label": 1
          },
          {
            "text": "See Figure1(right) for some qualitative examples of the considered objects.",
            "label": 1
          },
          {
            "text": "Training videos have been collected with the same acquisition modality considered for the \"Palazzo Bellomo\" cultural site.",
            "label": 1
          },
          {
            "text": "Figure4reports the number/percentage of frames acquired in each environment.",
            "label": 1
          },
          {
            "text": "Training and validation videos have a resolution of 1216 × 684 pixels and a frame-rate of 24 fps.",
            "label": 1
          },
          {
            "text": "Five validation videos have been collected by asking volunteers to visit the cultural site following the same protocol used for \"Palazzo Bellomo\".",
            "label": 1
          },
          {
            "text": "Additionally, we collected 60 test videos by asking real visitors inexperienced with both the research project and its goals and the HoloLens device to freely visit the cultural site.",
            "label": 1
          },
          {
            "text": "No specific instructions have been given to the visitors, who were free to explore the 4 environments and the 35 POIs.",
            "label": 1
          },
          {
            "text": "This allowed us to obtain realistic data of how a visitor would move in a cultural site.",
            "label": 1
          },
          {
            "text": "Test videos have been collected over a period of three months.",
            "label": 1
          },
          {
            "text": "2.",
            "label": 1
          },
          {
            "text": "The 60 test videos have a resolution of 1408×792 pixels and a frame-rate of 30.03 f ps.",
            "label": 1
          },
          {
            "text": "The average video length is 21 min, with the maximum  length being 42 min.",
            "label": 1
          },
          {
            "text": "See the supplementary material for more details about training/validation/test videos.",
            "label": 1
          },
          {
            "text": "Similarly to \"Palazzo Bellomo\", we include 35 reference images related to the considered POIs for one-shot image retrieval 7 .",
            "label": 1
          },
          {
            "text": "Please note that this set of data is adapted from and extends significantly the dataset proposed in[3], introducing 60 new labelled videos collected by real visitors.",
            "label": 1
          },
          {
            "text": "Specifically, the overall dataset presented in this work contains +1600 minutes of video, data from +70 more subjects, +91369 bounding box annotations and an additional cultural site \"Palazzo Bellomo\" comprising 22 environments and 191 points of interest.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "EGO_CH_ Dataset and Fundamental Tasks for Visitors Behavioral Understanding using Egocentric Vision",
        "section": 7,
        "paragraph_id": 7,
        "full_text": "All test and validation videos have been temporally labeled to indicate in every frame the environment in which the visitor is located and the observed point of interest, if any.If the visitor is not located in one of the considered environment (e.g., a stair), the frame is marked as \"negative\"9.It is worth noting that there are no negative frames in \"Palazzo Bellomo\" since all environments are part of the museum, whereas negative frames are contained in \"Monastero dei Benedettini\".This is due to the different nature of the two sites: \"Palazzo Bellomo\" is a museum, consisting in a limited set of rooms, whereas \"Monastero dei Benedettini\" is a much more complex environment including many corridors and stairs which have not been labeled as locations of interest for visitors.Similarly, we mark as \"negative\" all frames in which the visitor is not observing any of the considered POIs.Each location is identified by a number that denotes a specific environment (1 -22 for \"Palazzo Bellomo\" and 1 -4 for \"Monastero dei Benedettini\").Each point of interest is denoted by a code in the form X.Y (e.g.,3.5)where \"X\" denotes the environment in which the point of interest is located and \"Y\" identifies the point of interest.See Figure1for some examples.",
        "sentences": [
          {
            "text": "All test and validation videos have been temporally labeled to indicate in every frame the environment in which the visitor is located and the observed point of interest, if any.",
            "label": 1
          },
          {
            "text": "If the visitor is not located in one of the considered environment (e.g., a stair), the frame is marked as \"negative\"9.",
            "label": 1
          },
          {
            "text": "It is worth noting that there are no negative frames in \"Palazzo Bellomo\" since all environments are part of the museum, whereas negative frames are contained in \"Monastero dei Benedettini\".",
            "label": 1
          },
          {
            "text": "This is due to the different nature of the two sites: \"Palazzo Bellomo\" is a museum, consisting in a limited set of rooms, whereas \"Monastero dei Benedettini\" is a much more complex environment including many corridors and stairs which have not been labeled as locations of interest for visitors.",
            "label": 1
          },
          {
            "text": "Similarly, we mark as \"negative\" all frames in which the visitor is not observing any of the considered POIs.",
            "label": 1
          },
          {
            "text": "Each location is identified by a number that denotes a specific environment (1 -22 for \"Palazzo Bellomo\" and 1 -4 for \"Monastero dei Benedettini\").",
            "label": 1
          },
          {
            "text": "Each point of interest is denoted by a code in the form X.",
            "label": 1
          },
          {
            "text": "Y (e.g.,3.5)where \"X\" denotes the environment in which the point of interest is located and \"Y\" identifies the point of interest.",
            "label": 1
          },
          {
            "text": "See Figure1for some examples.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "EGO_CH_ Dataset and Fundamental Tasks for Visitors Behavioral Understanding using Egocentric Vision",
        "section": 8,
        "paragraph_id": 8,
        "full_text": "Bounding Box Annotations A subset of frames from the dataset (sampled at at 1 fps) has been labeled with bounding boxes indicating the presence and locations of all POIs.Specifically, each POI has been labeled with a tuple (class, x, y, w, h) indicating the class of the POI and its bounding box information.It is worth mentioning that, as noted in[5], a POI can be an object (e.g., a painting or a statue) or a different element (e.g., a pavement or a specific location), which cannot be strictly defined as an object.Indeed, the kind of POIs contained in a cultural site depends on the nature of the site itself.In EGO-CH, \"Palazzo Bellomo\" contains only objects as POIs, whereas \"Monastero dei Benedettini\" contains both objects and other elements.Nevertheless, all elements are labeled with class type and bounding box annotations.Figure3shows examples of labeled frames from the 60 visits of \"Monastero dei Benedettini\".",
        "sentences": [
          {
            "text": "Bounding Box Annotations A subset of frames from the dataset (sampled at at 1 fps) has been labeled with bounding boxes indicating the presence and locations of all POIs.",
            "label": 1
          },
          {
            "text": "Specifically, each POI has been labeled with a tuple (class, x, y, w, h) indicating the class of the POI and its bounding box information.",
            "label": 1
          },
          {
            "text": "It is worth mentioning that, as noted in[5], a POI can be an object (e.g., a painting or a statue) or a different element (e.g., a pavement or a specific location), which cannot be strictly defined as an object.",
            "label": 1
          },
          {
            "text": "Indeed, the kind of POIs contained in a cultural site depends on the nature of the site itself.",
            "label": 1
          },
          {
            "text": "In EGO-CH, \"Palazzo Bellomo\" contains only objects as POIs, whereas \"Monastero dei Benedettini\" contains both objects and other elements.",
            "label": 1
          },
          {
            "text": "Nevertheless, all elements are labeled with class type and bounding box annotations.",
            "label": 1
          },
          {
            "text": "Figure3shows examples of labeled frames from the 60 visits of \"Monastero dei Benedettini\".",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "EGO_CH_ Dataset and Fundamental Tasks for Visitors Behavioral Understanding using Egocentric Vision",
        "section": 9,
        "paragraph_id": 9,
        "full_text": "Surveys The 60 test videos collected in the \"Monastero dei Benedettini\" are associated with surveys which have been administered to the visitors at the end of the visits.Specifically, the visitors are asked to rate a subset of 33 out of the 35 Points Of Interest (a picture of each point is shown) or specify if any of them had not been seen it during the visit.The rating is expressed as a number ranging from -7 (not liked) to +7 (liked).",
        "sentences": [
          {
            "text": "Surveys The 60 test videos collected in the \"Monastero dei Benedettini\" are associated with surveys which have been administered to the visitors at the end of the visits.",
            "label": 1
          },
          {
            "text": "Specifically, the visitors are asked to rate a subset of 33 out of the 35 Points Of Interest (a picture of each point is shown) or specify if any of them had not been seen it during the visit.",
            "label": 1
          },
          {
            "text": "The rating is expressed as a number ranging from -7 (not liked) to +7 (liked).",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "EGO_CH_ Dataset and Fundamental Tasks for Visitors Behavioral Understanding using Egocentric Vision",
        "section": 10,
        "paragraph_id": 10,
        "full_text": "The EGO-CH dataset is publicy available at our website: http://iplab.dmi.unict.it/EGO-CH/.The reader is referred to the supplementary material for more details about the dataset and the experiments.The dataset can be used only for research purposes and is available upon the acceptance of an agreement.",
        "sentences": [
          {
            "text": "The EGO-CH dataset is publicy available at our website: http://iplab.dmi.unict.it/EGO-CH/.",
            "label": 1
          },
          {
            "text": "The reader is referred to the supplementary material for more details about the dataset and the experiments.",
            "label": 1
          },
          {
            "text": "The dataset can be used only for research purposes and is available upon the acceptance of an agreement.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "EGO_CH_ Dataset and Fundamental Tasks for Visitors Behavioral Understanding using Egocentric Vision",
        "section": 11,
        "paragraph_id": 11,
        "full_text": "This document is intended for the convenience of the reader and reports additional information about the proposed dataset and the performed experiments.This supplementary material is related to the following submission: The reader is referred to the manuscript and to our web page http://iplab.dmi.unict.it/EGO-CH/for further information.",
        "sentences": [
          {
            "text": "This document is intended for the convenience of the reader and reports additional information about the proposed dataset and the performed experiments.",
            "label": 0
          },
          {
            "text": "This supplementary material is related to the following submission: The reader is referred to the manuscript and to our web page http://iplab.dmi.unict.it/EGO-CH/for further information.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "EGO_CH_ Dataset and Fundamental Tasks for Visitors Behavioral Understanding using Egocentric Vision",
        "section": 12,
        "paragraph_id": 12,
        "full_text": "The dataset has been acquired using a headmounted Microsoft HoloLens device in two cultural sites located in Sicily, Italy: 1) \"Palazzo Bellomo\", located in Siracusa [20], and 2) \"Monastero dei Benedettini\", located in Catania[21].",
        "sentences": [
          {
            "text": "The dataset has been acquired using a headmounted Microsoft HoloLens device in two cultural sites located in Sicily, Italy: 1) \"Palazzo Bellomo\", located in Siracusa [20], and 2) \"Monastero dei Benedettini\", located in Catania[21].",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "EGO_CH_ Dataset and Fundamental Tasks for Visitors Behavioral Understanding using Egocentric Vision",
        "section": 13,
        "paragraph_id": 13,
        "full_text": "1) Palazzo Bellomo.This cultural site is composed by 22 environments and contains 191 Points of Interest (e.g., statues, paintings, etc.). Figure9and Figure10report some frames related to the different environments and some points of interest.Table 14 details the list of the acquired training video.Some of the videos are related to the 22 rooms of the cultural site, whereas other are related to specific points of interest.For each video we report its total duration, the amount of required storage, the number of frames, as well as the percentage of frames with respect to the whole training set.",
        "sentences": [
          {
            "text": "1) Palazzo Bellomo.",
            "label": 1
          },
          {
            "text": "This cultural site is composed by 22 environments and contains 191 Points of Interest (e.g., statues, paintings, etc.).",
            "label": 1
          },
          {
            "text": "Figure9and Figure10report some frames related to the different environments and some points of interest.",
            "label": 1
          },
          {
            "text": "Table 14 details the list of the acquired training video.",
            "label": 1
          },
          {
            "text": "Some of the videos are related to the 22 rooms of the cultural site, whereas other are related to specific points of interest.",
            "label": 1
          },
          {
            "text": "For each video we report its total duration, the amount of required storage, the number of frames, as well as the percentage of frames with respect to the whole training set.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "EGO_CH_ Dataset and Fundamental Tasks for Visitors Behavioral Understanding using Egocentric Vision",
        "section": 14,
        "paragraph_id": 14,
        "full_text": "Table15reports the list of the 10 test videos acquired by volunteers visiting the cultural site.For each video, we report its total duration, the amount of required storage, the number of frames, the number of environments encountered in the video, as well as the sequence of environments, as visited by the subject acquiring the video.All the training/test videos have a resolution of 1280×720 pixels           and a frame-rate of 29.97 fps.We also include 191 reference images related to the considered POIs to be used for one-shot image retrieval.The images are akin to pictures generally included in museums catalog.Figure11shows some examples of such reference images.",
        "sentences": [
          {
            "text": "Table15reports the list of the 10 test videos acquired by volunteers visiting the cultural site.",
            "label": 1
          },
          {
            "text": "For each video, we report its total duration, the amount of required storage, the number of frames, the number of environments encountered in the video, as well as the sequence of environments, as visited by the subject acquiring the video.",
            "label": 1
          },
          {
            "text": "All the training/test videos have a resolution of 1280×720 pixels           and a frame-rate of 29.97 fps.",
            "label": 1
          },
          {
            "text": "We also include 191 reference images related to the considered POIs to be used for one-shot image retrieval.The images are akin to pictures generally included in museums catalog.",
            "label": 1
          },
          {
            "text": "Figure11shows some examples of such reference images.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "EGO_CH_ Dataset and Fundamental Tasks for Visitors Behavioral Understanding using Egocentric Vision",
        "section": 15,
        "paragraph_id": 15,
        "full_text": "2) Monastero dei Benedettini.This dataset is composed by 4 environments and contains 35 Points of Interest.Figure12and Figure13report some frames related to the 4 different environments and some of the points of interest.Table16reports details on the acquired training videos, highlighting the total duration of the videos, the required storage, the number of frames and the percentage of frames with respect to the whole training set.",
        "sentences": [
          {
            "text": "2) Monastero dei Benedettini.",
            "label": 1
          },
          {
            "text": "This dataset is composed by 4 environments and contains 35 Points of Interest.",
            "label": 1
          },
          {
            "text": "Figure12and Figure13report some frames related to the 4 different environments and some of the points of interest.",
            "label": 1
          },
          {
            "text": "Table16reports details on the acquired training videos, highlighting the total duration of the videos, the required storage, the number of frames and the percentage of frames with respect to the whole training set.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "EGO_CH_ Dataset and Fundamental Tasks for Visitors Behavioral Understanding using Egocentric Vision",
        "section": 16,
        "paragraph_id": 16,
        "full_text": "Training and validation videos have a resolution of 1216 × 684 pixels and a frame-rate of 24 fps.Five validation videos have been collected by asking volunteers to visit the cultural site with rules similarly to the one used for \"Palazzo Bellomo\".Table17shows the number of frames belonging to each video (left) and the number of frames belonging for each class (right).Additionally, we collected 60 test videos by asking real visitors to freely visit the cultural site.No specific instructions have been given to the visitors, who were free to explore the 4 environments and the 35 POIs.The 60 test videos have a resolution of 1408×792 pixels and a frame-rate of 30.03 f ps.The average number of frames for each video is 39296.We also include 35 reference images related to the considered POIs to be used for one-shot image retrieval.Figure14shows some example of reference images.",
        "sentences": [
          {
            "text": "Training and validation videos have a resolution of 1216 × 684 pixels and a frame-rate of 24 fps.",
            "label": 1
          },
          {
            "text": "Five validation videos have been collected by asking volunteers to visit the cultural site with rules similarly to the one used for \"Palazzo Bellomo\".",
            "label": 0
          },
          {
            "text": "Table17shows the number of frames belonging to each video (left) and the number of frames belonging for each class (right).",
            "label": 1
          },
          {
            "text": "Additionally, we collected 60 test videos by asking real visitors to freely visit the cultural site.",
            "label": 1
          },
          {
            "text": "No specific instructions have been given to the visitors, who were free to explore the 4 environments and the 35 POIs.",
            "label": 1
          },
          {
            "text": "The 60 test videos have a resolution of 1408×792 pixels and a frame-rate of 30.03 f ps.",
            "label": 1
          },
          {
            "text": "The average number of frames for each video is 39296.",
            "label": 1
          },
          {
            "text": "We also include 35 reference images related to the considered POIs to be used for one-shot image retrieval.Figure14shows some example of reference images.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "EGO_CH_ Dataset and Fundamental Tasks for Visitors Behavioral Understanding using Egocentric Vision",
        "section": 17,
        "paragraph_id": 17,
        "full_text": "In this section, we present additional details on baseline experiments related to the 4 tasks proposed in the paper.",
        "sentences": [
          {
            "text": "In this section, we present additional details on baseline experiments related to the 4 tasks proposed in the paper.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Generalized Multiple Intent Conditioned Slot Filling",
        "section": 0,
        "paragraph_id": 0,
        "full_text": "Abstract: Natural language understanding includes the tasks of intent detection (identifying a user's objectives) and slot filling (extracting the entities relevant to those objectives).Prior slot filling methods assume that each intent type cannot occur more than once within a message, however this is often not a valid assumption for real-world settings.In this work, we generalize slot filling by removing the constraint of unique intents in a message.We cast this as a JSON generation task and approach it using a language model.We create a pre-training dataset by combining DBpedia and existing slot filling datasets that we convert for JSON generation.We also generate an in-domain dataset using GPT-3.We train T5 models for this task (with and without exemplars in the prompt) and find that both training datasets improve performance, and that the model is able to generalize to intent types not seen during training.",
        "sentences": [
          {
            "text": "Abstract: Natural language understanding includes the tasks of intent detection (identifying a user's objectives) and slot filling (extracting the entities relevant to those objectives).",
            "label": 0
          },
          {
            "text": "Prior slot filling methods assume that each intent type cannot occur more than once within a message, however this is often not a valid assumption for real-world settings.",
            "label": 0
          },
          {
            "text": "In this work, we generalize slot filling by removing the constraint of unique intents in a message.",
            "label": 0
          },
          {
            "text": "We cast this as a JSON generation task and approach it using a language model.",
            "label": 0
          },
          {
            "text": "We create a pre-training dataset by combining DBpedia and existing slot filling datasets that we convert for JSON generation.",
            "label": 1
          },
          {
            "text": "We also generate an in-domain dataset using GPT-3.",
            "label": 1
          },
          {
            "text": "We train T5 models for this task (with and without exemplars in the prompt) and find that both training datasets improve performance, and that the model is able to generalize to intent types not seen during training.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Generalized Multiple Intent Conditioned Slot Filling",
        "section": 1,
        "paragraph_id": 1,
        "full_text": "Natural language understanding (NLU) describes a model's ability to understand the semantic meaning of a sequence of text.When applied to communications data, NLU includes the tasks of intent detection (identifying a user's objectives) and slot filling (extracting the entities relevant to those objectives)(Tur and Mori, 2011).",
        "sentences": [
          {
            "text": "Natural language understanding (NLU) describes a model's ability to understand the semantic meaning of a sequence of text.",
            "label": 0
          },
          {
            "text": "When applied to communications data, NLU includes the tasks of intent detection (identifying a user's objectives) and slot filling (extracting the entities relevant to those objectives)(Tur and Mori, 2011).",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Generalized Multiple Intent Conditioned Slot Filling",
        "section": 2,
        "paragraph_id": 2,
        "full_text": "Slot filling methods assume that the same intent type cannot occur more than once within a given message.However this assumption is often not valid -for example, the message in Figure1contains multiple instances of the user wanting to remove an item from an order.As shown in Figure1, the sequence tagging style approaches typically used for slot filling do not generalize when the same intent type occurs multiple times.This makes existing models unusable in many real-world scenarios.",
        "sentences": [
          {
            "text": "Slot filling methods assume that the same intent type cannot occur more than once within a given message.",
            "label": 0
          },
          {
            "text": "However this assumption is often not valid -for example, the message in Figure1contains multiple instances of the user wanting to remove an item from an order.",
            "label": 0
          },
          {
            "text": "As shown in Figure1, the sequence tagging style approaches typically used for slot filling do not generalize when the same intent type occurs multiple times.",
            "label": 0
          },
          {
            "text": "This makes existing models unusable in many real-world scenarios.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Generalized Multiple Intent Conditioned Slot Filling",
        "section": 3,
        "paragraph_id": 3,
        "full_text": "In this work, we generalize slot filling by removing the constraint of unique intents in a message.We do so by treating this as a text generation task where the goal is to generate a JSON string containing the values of the entities grouped by their intents (see Figure2for an example).",
        "sentences": [
          {
            "text": "In this work, we generalize slot filling by removing the constraint of unique intents in a message.",
            "label": 0
          },
          {
            "text": "We do so by treating this as a text generation task where the goal is to generate a JSON string containing the values of the entities grouped by their intents (see Figure2for an example).",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Generalized Multiple Intent Conditioned Slot Filling",
        "section": 4,
        "paragraph_id": 4,
        "full_text": "We approach this task using a language modelan area of research in which we have recently seen remarkable progress(Ouyang et al., 2022;Schulman et al., 2022).However to use language models in real-world settings, we often require them to produce output which obeys a pre-specified structure and to not hallucinate (i.e.produce output which is not relevant to the input).To this end, we construct a large pre-training dataset to train the model to extract entities from text and return them in JSON format.",
        "sentences": [
          {
            "text": "We approach this task using a language modelan area of research in which we have recently seen remarkable progress(Ouyang et al., 2022;Schulman et al., 2022).",
            "label": 0
          },
          {
            "text": "However to use language models in real-world settings, we often require them to produce output which obeys a pre-specified structure and to not hallucinate (i.e.produce output which is not relevant to the input).",
            "label": 0
          },
          {
            "text": "To this end, we construct a large pre-training dataset to train the model to extract entities from text and return them in JSON format.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Generalized Multiple Intent Conditioned Slot Filling",
        "section": 5,
        "paragraph_id": 5,
        "full_text": "After pre-training, we find that the model always outputs valid JSON and does not hallucinate (i.e. it only returns values which occur in the original message).However, the pre-training dataset is not sufficient for the model to work well on communications data.To address this, we also construct an in-domain dataset.Communications data is notoriously private therefore we construct a synthetic dataset using GPT-3(Brown et al., 2020), containing over 10K email-JSON pairs with 20 different intent types.",
        "sentences": [
          {
            "text": "After pre-training, we find that the model always outputs valid JSON and does not hallucinate (i.e. it only returns values which occur in the original message).",
            "label": 0
          },
          {
            "text": "However, the pre-training dataset is not sufficient for the model to work well on communications data.",
            "label": 0
          },
          {
            "text": "To address this, we also construct an in-domain dataset.",
            "label": 1
          },
          {
            "text": "Communications data is notoriously private therefore we construct a synthetic dataset using GPT-3(Brown et al., 2020), containing over 10K email-JSON pairs with 20 different intent types.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "Generalized Multiple Intent Conditioned Slot Filling",
        "section": 6,
        "paragraph_id": 6,
        "full_text": "We train T5 models(Raffel et al., 2020;Chung et al., 2022)with and without exemplars in the prompt, finding that both training datasets improve performance and that the model is able to generalize to intent types not seen during training.We release our code, data and models.1.",
        "sentences": [
          {
            "text": "We train T5 models(Raffel et al., 2020;Chung et al., 2022)with and without exemplars in the prompt, finding that both training datasets improve performance and that the model is able to generalize to intent types not seen during training.We release our code, data and models.1.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Generalized Multiple Intent Conditioned Slot Filling",
        "section": 7,
        "paragraph_id": 7,
        "full_text": "We train two models, one with zero-shot prompts and one with one-shot prompts.The exemplar for the one-shot prompted model is randomly selected from the corresponding training/validation/test set such that the exemplar shares an article type (pretraining) or intent (in-domain training) with the chosen data point (see Figure3for an example and Section 3.1.1for more detail on the prompt).Throughout, we use the large (770M parameter) variant of the Flan-T5 model, initializing the weights at the checkpoint provided byChung et al. (2022).We implement our experiments using the PyTorch(Paszke et al., 2019)and Hugging Face Transformers(Wolf et al., 2020)libraries.",
        "sentences": [
          {
            "text": "We train two models, one with zero-shot prompts and one with one-shot prompts.",
            "label": 0
          },
          {
            "text": "The exemplar for the one-shot prompted model is randomly selected from the corresponding training/validation/test set such that the exemplar shares an article type (pretraining) or intent (in-domain training) with the chosen data point (see Figure3for an example and Section 3.1.1for more detail on the prompt).",
            "label": 0
          },
          {
            "text": "Throughout, we use the large (770M parameter) variant of the Flan-T5 model, initializing the weights at the checkpoint provided byChung et al. (2022).",
            "label": 0
          },
          {
            "text": "We implement our experiments using the PyTorch(Paszke et al., 2019)and Hugging Face Transformers(Wolf et al., 2020)libraries.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Generalized Multiple Intent Conditioned Slot Filling",
        "section": 8,
        "paragraph_id": 8,
        "full_text": "We train the model using the standard language modeling maximum likelihood objective.",
        "sentences": [
          {
            "text": "We train the model using the standard language modeling maximum likelihood objective.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Generalized Multiple Intent Conditioned Slot Filling",
        "section": 9,
        "paragraph_id": 9,
        "full_text": "We first train on the pre-training dataset (DBpedia + slot filling datasets) for 400K iterations, using the Adafactor optimizer(Shazeer and Stern, 2018)with a learning rate of 10 -5 and a batch size of 8.",
        "sentences": [
          {
            "text": "We first train on the pre-training dataset (DBpedia + slot filling datasets) for 400K iterations, using the Adafactor optimizer(Shazeer and Stern, 2018)with a learning rate of 10 -5 and a batch size of 8.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Generalized Multiple Intent Conditioned Slot Filling",
        "section": 10,
        "paragraph_id": 10,
        "full_text": "We then further train on the training set of the in-domain data (GPT-3 generations) for 10K iterations, with the same optimization settings as for the first round of training.We measure the log likelihood on the validation set after every 200 iterations and take the final model as the one with the lowest validation log likelihood.When generating outputs from the trained model, we use beam search with a beam size of 3. The computational budget used for training the models is shown in Table1.",
        "sentences": [
          {
            "text": "We then further train on the training set of the in-domain data (GPT-3 generations) for 10K iterations, with the same optimization settings as for the first round of training.",
            "label": 0
          },
          {
            "text": "We measure the log likelihood on the validation set after every 200 iterations and take the final model as the one with the lowest validation log likelihood.",
            "label": 0
          },
          {
            "text": "When generating outputs from the trained model, we use beam search with a beam size of 3.",
            "label": 0
          },
          {
            "text": "The computational budget used for training the models is shown in Table1.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Generalized Multiple Intent Conditioned Slot Filling",
        "section": 11,
        "paragraph_id": 11,
        "full_text": "When evaluating the model, we found that it would sometimes generate entity-value pairs for entities which weren't requested in the prompt.This is due to the message containing tokens which are similar to entities defined for different intents.",
        "sentences": [
          {
            "text": "When evaluating the model, we found that it would sometimes generate entity-value pairs for entities which weren't requested in the prompt.",
            "label": 0
          },
          {
            "text": "This is due to the message containing tokens which are similar to entities defined for different intents.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Generalized Multiple Intent Conditioned Slot Filling",
        "section": 12,
        "paragraph_id": 12,
        "full_text": "To mitigate this, during training we augment the data by randomly dropping and/or reordering the entities both in the prompt and correspondingly in the target output string.",
        "sentences": [
          {
            "text": "To mitigate this, during training we augment the data by randomly dropping and/or reordering the entities both in the prompt and correspondingly in the target output string.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Generalized Multiple Intent Conditioned Slot Filling",
        "section": 13,
        "paragraph_id": 13,
        "full_text": "Table2shows the precision, recall and F1 scores of the zero-shot and one-shot prompted models on the test set of the in-domain data.Interestingly, the performance is actually slightly better when not showing an exemplar in the prompt.We believe this is because we are evaluating on data which is generated from the same distribution as the training data -if we were to evaluate on out-of-distribution data, having an exemplar in the prompt is more likely to be useful (see Section 6).",
        "sentences": [
          {
            "text": "Table2shows the precision, recall and F1 scores of the zero-shot and one-shot prompted models on the test set of the in-domain data.",
            "label": 0
          },
          {
            "text": "Interestingly, the performance is actually slightly better when not showing an exemplar in the prompt.",
            "label": 0
          },
          {
            "text": "We believe this is because we are evaluating on data which is generated from the same distribution as the training data -if we were to evaluate on out-of-distribution data, having an exemplar in the prompt is more likely to be useful (see Section 6).",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Generalized Multiple Intent Conditioned Slot Filling",
        "section": 14,
        "paragraph_id": 14,
        "full_text": "Failure modes When the model does fail, it tends to be on examples where the target JSON is relatively long and/or where the model gets confused between several ID-like fields; see Figure7in Appendix C for an example.",
        "sentences": [
          {
            "text": "Failure modes When the model does fail, it tends to be on examples where the target JSON is relatively long and/or where the model gets confused between several ID-like fields; see Figure7in Appendix C for an example.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Generalized Multiple Intent Conditioned Slot Filling",
        "section": 15,
        "paragraph_id": 15,
        "full_text": "Table3shows the performance on the in-domain test set after the different rounds of training.After pre-training only, although the model always outputs valid JSON, it performs very poorly at extracting the correct entities on the in-domain data.Note that in this case, the one-shot prompted model performs slightly better than the zero-shot version.",
        "sentences": [
          {
            "text": "Table3shows the performance on the in-domain test set after the different rounds of training.",
            "label": 0
          },
          {
            "text": "After pre-training only, although the model always outputs valid JSON, it performs very poorly at extracting the correct entities on the in-domain data.",
            "label": 0
          },
          {
            "text": "Note that in this case, the one-shot prompted model performs slightly better than the zero-shot version.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Generalized Multiple Intent Conditioned Slot Filling",
        "section": 16,
        "paragraph_id": 16,
        "full_text": "After in-domain training only (i.e.taking the off-the-shelf model and training directly on the indomain data, with no pre-training), the model is able to perform quite well however we find that doing both pre-training and in-domain training provides the best results.Note that we are unable to directly evaluate the off-the-shelf model without any training, because the curly brace characters ('{', '}') required for the JSON output are not included in the original vocabulary.",
        "sentences": [
          {
            "text": "After in-domain training only (i.e.taking the off-the-shelf model and training directly on the indomain data, with no pre-training), the model is able to perform quite well however we find that doing both pre-training and in-domain training provides the best results.",
            "label": 0
          },
          {
            "text": "Note that we are unable to directly evaluate the off-the-shelf model without any training, because the curly brace characters ('{', '}') required for the JSON output are not included in the original vocabulary.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Generalized Multiple Intent Conditioned Slot Filling",
        "section": 17,
        "paragraph_id": 17,
        "full_text": "Overall, these results show the benefit of the pretraining process -even though the pre-training data is semantically different from the in-domain data, it trains the model to be able to extract relevant entities from the input text and output them as JSON, which is then useful for the transfer to the downstream in-domain task.The results are shown in Table4; they are slightly worse than when all the test intents are also seen during training, but still suggest that the model is able to generalize to new intents.Again, we find that the zero-shot model outperforms the oneshot model.This is even more surprising than the results in Table2.",
        "sentences": [
          {
            "text": "Overall, these results show the benefit of the pretraining process -even though the pre-training data is semantically different from the in-domain data, it trains the model to be able to extract relevant entities from the input text and output them as JSON, which is then useful for the transfer to the downstream in-domain task.",
            "label": 0
          },
          {
            "text": "The results are shown in Table4; they are slightly worse than when all the test intents are also seen during training, but still suggest that the model is able to generalize to new intents.",
            "label": 0
          },
          {
            "text": "Again, we find that the zero-shot model outperforms the oneshot model.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 0,
        "paragraph_id": 0,
        "full_text": "Abstract: Due to the significant resemblance in visual appearance, pill misuse is prevalent and has become a critical issue, responsible for one-third of all deaths worldwide.Pill identification, thus, is a crucial concern needed to be investigated thoroughly.Recently, several attempts have been made to exploit deep learning to tackle the pill identification problem.However, most published works consider only single-pill identification and fail to distinguish hard samples with identical appearances.Also, most existing pill image datasets only feature single pill images captured in carefully controlled environments under ideal lighting conditions and clean backgrounds.In this work, we are the first to tackle the multi-pill detection problem in real-world settings, aiming at localizing and identifying pills captured by users in a pill intake.Moreover, we also introduce a multi-pill image dataset taken in unconstrained conditions.To handle hard samples, we propose a novel method for constructing heterogeneous a priori graphs incorporating three forms of inter-pill relationships, including co-occurrence likelihood, relative size, and visual semantic correlation.We then offer a framework for integrating a priori with pills' visual features to enhance detection accuracy.Our experimental results have proved the robustness, reliability, and explainability of the proposed framework.Experimentally, it outperforms all detection benchmarks in terms of all evaluation metrics.Specifically, our proposed framework improves COCO mAP metrics by 9.4% over Faster R-CNN, and 12.0% compared to vanilla YOLOv5.Our study opens up new opportunities for protecting patients from medication errors using an AI-based pill identification solution.",
        "sentences": [
          {
            "text": "Abstract: Due to the significant resemblance in visual appearance, pill misuse is prevalent and has become a critical issue, responsible for one-third of all deaths worldwide.",
            "label": 0
          },
          {
            "text": "Pill identification, thus, is a crucial concern needed to be investigated thoroughly.",
            "label": 0
          },
          {
            "text": "Recently, several attempts have been made to exploit deep learning to tackle the pill identification problem.",
            "label": 0
          },
          {
            "text": "However, most published works consider only single-pill identification and fail to distinguish hard samples with identical appearances.",
            "label": 0
          },
          {
            "text": "Also, most existing pill image datasets only feature single pill images captured in carefully controlled environments under ideal lighting conditions and clean backgrounds.",
            "label": 1
          },
          {
            "text": "In this work, we are the first to tackle the multi-pill detection problem in real-world settings, aiming at localizing and identifying pills captured by users in a pill intake.",
            "label": 1
          },
          {
            "text": "Moreover, we also introduce a multi-pill image dataset taken in unconstrained conditions.",
            "label": 1
          },
          {
            "text": "To handle hard samples, we propose a novel method for constructing heterogeneous a priori graphs incorporating three forms of inter-pill relationships, including co-occurrence likelihood, relative size, and visual semantic correlation.",
            "label": 1
          },
          {
            "text": "We then offer a framework for integrating a priori with pills' visual features to enhance detection accuracy.",
            "label": 0
          },
          {
            "text": "Our experimental results have proved the robustness, reliability, and explainability of the proposed framework.",
            "label": 0
          },
          {
            "text": "Experimentally, it outperforms all detection benchmarks in terms of all evaluation metrics.",
            "label": 0
          },
          {
            "text": "Specifically, our proposed framework improves COCO mAP metrics by 9.4% over Faster R-CNN, and 12.0% compared to vanilla YOLOv5.",
            "label": 0
          },
          {
            "text": "Our study opens up new opportunities for protecting patients from medication errors using an AI-based pill identification solution.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 1,
        "paragraph_id": 1,
        "full_text": "• Most existing works have been restricted to the classification of single-pill images.This constraint limits the solutions' application capacity, as in practice, users usually take multiple pills simultaneously, resulting in multi-pill images in most cases.• Most of the current pill image datasets (e.g., ePillID, CURE) are limited to single-pill images.Moreover, all of them were collected in tightly-controlled settings under ideal illumination and background conditions, leading to the lack of diversity.• No prior work has studied the explainability of the model.This insufficiency diminishes the trustworthiness of the solutions, hence restricting their practical applications.",
        "sentences": [
          {
            "text": "• Most existing works have been restricted to the classification of single-pill images.",
            "label": 0
          },
          {
            "text": "• Most of the current pill image datasets (e.g., ePillID, CURE) are limited to single-pill images.",
            "label": 0
          },
          {
            "text": "• No prior work has studied the explainability of the model.",
            "label": 0
          },
          {
            "text": "This insufficiency diminishes the trustworthiness of the solutions, hence restricting their practical applications.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 2,
        "paragraph_id": 2,
        "full_text": "We are, to the best of our knowledge, the first to tackle the multi-pill detection problem in real-world settings.Specifically, we focus on a practical application that recognizes pills in patients' pill intake pictures.Our targeted problem can be formulated as follows.Given an image capturing multiple pills in patients' pill intake, we aim to determine each pill's location and identity.In addition to developing a novel pill detection framework with high reliability and explainable capacity, we build a dataset of multi-pill images captured under unconstrained real-world conditions.",
        "sentences": [
          {
            "text": "We are, to the best of our knowledge, the first to tackle the multi-pill detection problem in real-world settings.",
            "label": 0
          },
          {
            "text": "Specifically, we focus on a practical application that recognizes pills in patients' pill intake pictures.",
            "label": 0
          },
          {
            "text": "Our targeted problem can be formulated as follows.",
            "label": 0
          },
          {
            "text": "Given an image capturing multiple pills in patients' pill intake, we aim to determine each pill's location and identity.",
            "label": 0
          },
          {
            "text": "In addition to developing a novel pill detection framework with high reliability and explainable capacity, we build a dataset of multi-pill images captured under unconstrained real-world conditions.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 3,
        "paragraph_id": 3,
        "full_text": "Our motivation and key ideas.One of the most significant obstacles in the pill detection problem is the existence of numerous pills with similar shapes, colors, and sizes (Fig.1).We call these hard samples whose occurrence renders the pill identification problem complicated and challenging to solve by generic object detection.We argue that relying merely on pills' appearance is insufficient to improve pill detection accuracy, if not possible.We discovered that besides the challenge (e.g., localizing pills in hard cases such as overlapping pills), the multi-pill detection problem, on the other hand, provides us with an opportunity to improve the pill recognition accuracy.Motivated by the human tendency and ability to integrate different data sources while making decisions, our proposed solution seeks to utilize external Recently, a few efforts have leveraged the two-stage object detection approach to solve the multi-pill detection challenge[10,19].In the first stage, object localization techniques are applied to determine the pills' bounding boxes.These bounding boxes are then fed into a classifier in the second stage to identify the pills.Specifically, in[19], an enhanced feature pyramid network based on the ResNet-50 backbone has been built for pill localization.After that, the pill bounding boxes are fed into an Inception-ResNet v2 for classification.Authors in[10]exploited the Mask-RCNN framework to solve the problem.",
        "sentences": [
          {
            "text": "Our motivation and key ideas.",
            "label": 1
          },
          {
            "text": "One of the most significant obstacles in the pill detection problem is the existence of numerous pills with similar shapes, colors, and sizes (Fig.1).",
            "label": 1
          },
          {
            "text": "We call these hard samples whose occurrence renders the pill identification problem complicated and challenging to solve by generic object detection.",
            "label": 1
          },
          {
            "text": "We argue that relying merely on pills' appearance is insufficient to improve pill detection accuracy, if not possible.",
            "label": 1
          },
          {
            "text": "We discovered that besides the challenge (e.g., localizing pills in hard cases such as overlapping pills), the multi-pill detection problem, on the other hand, provides us with an opportunity to improve the pill recognition accuracy.",
            "label": 1
          },
          {
            "text": "Motivated by the human tendency and ability to integrate different data sources while making decisions, our proposed solution seeks to utilize external Recently, a few efforts have leveraged the two-stage object detection approach to solve the multi-pill detection challenge[10,19].",
            "label": 1
          },
          {
            "text": "In the first stage, object localization techniques are applied to determine the pills' bounding boxes.",
            "label": 1
          },
          {
            "text": "These bounding boxes are then fed into a classifier in the second stage to identify the pills.",
            "label": 1
          },
          {
            "text": "Specifically, in[19], an enhanced feature pyramid network based on the ResNet-50 backbone has been built for pill localization.",
            "label": 1
          },
          {
            "text": "After that, the pill bounding boxes are fed into an Inception-ResNet v2 for classification.",
            "label": 1
          },
          {
            "text": "Authors in[10]exploited the Mask-RCNN framework to solve the problem.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 4,
        "paragraph_id": 4,
        "full_text": "Multi-pill detection solutions are still in their infancy.All current works only investigate images acquired in laboratories under optimal lighting and background conditions, with each pill arranged separately.In fact, existing techniques only use specific object localization models to crop the pills and then treat the issue as a typical single-pill classification problem.",
        "sentences": [
          {
            "text": "Multi-pill detection solutions are still in their infancy.",
            "label": 0
          },
          {
            "text": "All current works only investigate images acquired in laboratories under optimal lighting and background conditions, with each pill arranged separately.",
            "label": 0
          },
          {
            "text": "In fact, existing techniques only use specific object localization models to crop the pills and then treat the issue as a typical single-pill classification problem.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 5,
        "paragraph_id": 5,
        "full_text": "Pill Image Datasets.One of the most widely used pill image datasets is NIH Pill Image Dataset [1] released by the U.S. National Library of Medicine (NLM).This dataset consists of 4,000 high-quality reference pills and 133,000 pictures captured by digital cameras on mobile phones.In[13], the authors provided the CURE pill dataset consisting of 8,973 single-pill images representing 196 classes.Although taken under various backgrounds and lighting conditions, all of these images are carefully captured from a top-down view and focus on the pills.Authors in[26]contributed a pill dataset capturing about 400 commonly used tablets and capsules.Ten to twenty-five pictures were taken for each pill, resulting in 5,284 images.",
        "sentences": [
          {
            "text": "Pill Image Datasets.",
            "label": 0
          },
          {
            "text": "One of the most widely used pill image datasets is NIH Pill Image Dataset [1] released by the U.S. National Library of Medicine (NLM).",
            "label": 0
          },
          {
            "text": "This dataset consists of 4,000 high-quality reference pills and 133,000 pictures captured by digital cameras on mobile phones.",
            "label": 0
          },
          {
            "text": "In[13], the authors provided the CURE pill dataset consisting of 8,973 single-pill images representing 196 classes.",
            "label": 0
          },
          {
            "text": "Although taken under various backgrounds and lighting conditions, all of these images are carefully captured from a top-down view and focus on the pills.",
            "label": 0
          },
          {
            "text": "Authors in[26]contributed a pill dataset capturing about 400 commonly used tablets and capsules.",
            "label": 0
          },
          {
            "text": "Ten to twenty-five pictures were taken for each pill, resulting in 5,284 images.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 6,
        "paragraph_id": 6,
        "full_text": "Unfortunately, all of these datasets provide only single-pill images.Most images were captured under quite ideal conditions, e.g., pills were put on a clean background, and the images were taken from the top-down view with the camera focused on the pills.",
        "sentences": [
          {
            "text": "Unfortunately, all of these datasets provide only single-pill images.",
            "label": 0
          },
          {
            "text": "Most images were captured under quite ideal conditions, e.g., pills were put on a clean background, and the images were taken from the top-down view with the camera focused on the pills.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 7,
        "paragraph_id": 7,
        "full_text": "Figure3: Workflow of PGPNet.First, the A Priori Graph Modeling Module leverages given prescriptions to create a non-directed Medical Co-occurrence Graph Gc and then leverages Gc in conjunction with bounding box annotation information from the pill image dataset to construct a Relative Size Graph Gs.Second, the pill images are passed through the Visual Feature Extractor to determine Regions of Interest and retrieve their visual representations.This information, together with the graphs serve as the input for the Inter-Pill Relational Feature Extractor to generate a heterogeneous relational graph expressing three types of relationships between pills, namely, co-occurrence, relative size, and visual semantic relation.Finally, the heterogeneous graph-based information and visual features are combined by Multi-modal Data Fusion module to form final enhanced vectors, which will be used to determine the final detection results..",
        "sentences": [
          {
            "text": "Figure3: Workflow of PGPNet.",
            "label": 0
          },
          {
            "text": "First, the A Priori Graph Modeling Module leverages given prescriptions to create a non-directed Medical Co-occurrence Graph Gc and then leverages Gc in conjunction with bounding box annotation information from the pill image dataset to construct a Relative Size Graph Gs.",
            "label": 0
          },
          {
            "text": "Second, the pill images are passed through the Visual Feature Extractor to determine Regions of Interest and retrieve their visual representations.",
            "label": 0
          },
          {
            "text": "This information, together with the graphs serve as the input for the Inter-Pill Relational Feature Extractor to generate a heterogeneous relational graph expressing three types of relationships between pills, namely, co-occurrence, relative size, and visual semantic relation.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 8,
        "paragraph_id": 8,
        "full_text": "In this section, we propose a novel pill detection framework named PGPNet (i.e., a Priori Graph-assisted Pill Detection Network).",
        "sentences": [
          {
            "text": "In this section, we propose a novel pill detection framework named PGPNet (i.e., a Priori Graph-assisted Pill Detection Network).",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 9,
        "paragraph_id": 9,
        "full_text": "We focus on a practical application that recognizes pills in patient intake pictures.Our model receives a multiple-pill picture as input and generates both the bounding box and the identification of each pill.Here, we incur a critical challenge: how to distinguish pills with identical appearances (i.e., shape, color, and size).We believe that relying solely on the visual features of pills is insufficient to address this issue.Moreover, employing the correlation between pills, rather than counting on each pill individually, may enhance recognition accuracy.In light of this, we propose introducing two types of a priori, the first indicating the co-occurrence likelihood and the second modeling the relative size of pills.The a-priori is extracted from a given prescription and pill image training dataset and represented as heterogeneous graphs.In summary, the proposed model comprises four components: A priori graph modeling, visual feature extractor, inter-pill relational feature extractor, and multi-modal data fusion, as illustrated in Fig.3.The overall flow is as follows. •",
        "sentences": [
          {
            "text": "We focus on a practical application that recognizes pills in patient intake pictures.",
            "label": 0
          },
          {
            "text": "Our model receives a multiple-pill picture as input and generates both the bounding box and the identification of each pill.",
            "label": 0
          },
          {
            "text": "Here, we incur a critical challenge: how to distinguish pills with identical appearances (i.e., shape, color, and size).",
            "label": 0
          },
          {
            "text": "We believe that relying solely on the visual features of pills is insufficient to address this issue.",
            "label": 0
          },
          {
            "text": "Moreover, employing the correlation between pills, rather than counting on each pill individually, may enhance recognition accuracy.",
            "label": 0
          },
          {
            "text": "In light of this, we propose introducing two types of a priori, the first indicating the co-occurrence likelihood and the second modeling the relative size of pills.",
            "label": 0
          },
          {
            "text": "The a-priori is extracted from a given prescription and pill image training dataset and represented as heterogeneous graphs.",
            "label": 0
          },
          {
            "text": "In summary, the proposed model comprises four components: A priori graph modeling, visual feature extractor, inter-pill relational feature extractor, and multi-modal data fusion, as illustrated in Fig.3.",
            "label": 0
          },
          {
            "text": "The overall flow is as follows.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 10,
        "paragraph_id": 10,
        "full_text": "Step 1 -A Priori Graph Modeling.We construct two generic graphs, namely Prescription-based Medical Co-occurrence Graph (or Co-graph for short) and Relative Size Graph (Size-graph for short) that represent the relationship between all the pills in terms of co-occurrence and relative size, respectively.Concerning the former, we leverage a given set of prescriptions from which we can model the interaction between pills (i.e., which pills are likely to be used to treat the same diseases).Based on this information, we developed the Co-graph, whose nodes represent the pill classes and whose edge weights reflect the co-occurrence likelihood between the two vertices.In the meantime, using the coordinates of the bounding boxes from our training dataset for the pill detection task, we determine the area of each box and model the relative size ratios of all the pill classes in the given images.This information is then aggregated to formulate the Size-graph.Section A Priori Graph Modeling covers the details of this algorithm. •",
        "sentences": [
          {
            "text": "Step 1 -A Priori Graph Modeling.",
            "label": 0
          },
          {
            "text": "We construct two generic graphs, namely Prescription-based Medical Co-occurrence Graph (or Co-graph for short) and Relative Size Graph (Size-graph for short) that represent the relationship between all the pills in terms of co-occurrence and relative size, respectively.",
            "label": 0
          },
          {
            "text": "Concerning the former, we leverage a given set of prescriptions from which we can model the interaction between pills (i.e., which pills are likely to be used to treat the same diseases).",
            "label": 0
          },
          {
            "text": "Based on this information, we developed the Co-graph, whose nodes represent the pill classes and whose edge weights reflect the co-occurrence likelihood between the two vertices.",
            "label": 0
          },
          {
            "text": "In the meantime, using the coordinates of the bounding boxes from our training dataset for the pill detection task, we determine the area of each box and model the relative size ratios of all the pill classes in the given images.",
            "label": 0
          },
          {
            "text": "This information is then aggregated to formulate the Size-graph.",
            "label": 0
          },
          {
            "text": "Section A Priori Graph Modeling covers the details of this algorithm.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 11,
        "paragraph_id": 11,
        "full_text": "Step 2 -Visual Feature Extraction.The original image containing multiple pills is passed through a Convolutional Network (ConvNet) for extracting visual features and a Region Proposal Network (RPN) for detecting potential Regions of Interest (RoI).The outputs of the two modules are fed into an RoI pooling layer to filter out all visual presentations of pills (i.e., RoIs).It is worth noting that the Visual Feature Extractor described here follows the architecture of the two-step object detection architecture (e.g., Faster RCNN[21]).However, PGPNet can also be implemented with one-step detection architecture.",
        "sentences": [
          {
            "text": "Step 2 -Visual Feature Extraction.",
            "label": 0
          },
          {
            "text": "The original image containing multiple pills is passed through a Convolutional Network (ConvNet) for extracting visual features and a Region Proposal Network (RPN) for detecting potential Regions of Interest (RoI).",
            "label": 0
          },
          {
            "text": "The outputs of the two modules are fed into an RoI pooling layer to filter out all visual presentations of pills (i.e., RoIs).",
            "label": 0
          },
          {
            "text": "It is worth noting that the Visual Feature Extractor described here follows the architecture of the two-step object detection architecture (e.g., Faster RCNN[21]).",
            "label": 0
          },
          {
            "text": "However, PGPNet can also be implemented with one-step detection architecture.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 12,
        "paragraph_id": 12,
        "full_text": "• Step 3 -Inter-pill Relational Feature Extraction.The two a priori graphs are aggregated with the pills' visual features to yield condensed versions of the Co-graph and Size-graph that highlight the relationship between only those pills that are likely to appear in the image.Besides, the pills' visual features are leveraged to construct a so-called Visual semantic graph that captures the pills' relationships encapsulated under their appearances.",
        "sentences": [
          {
            "text": "• Step 3 -Inter-pill Relational Feature Extraction.",
            "label": 0
          },
          {
            "text": "The two a priori graphs are aggregated with the pills' visual features to yield condensed versions of the Co-graph and Size-graph that highlight the relationship between only those pills that are likely to appear in the image.",
            "label": 0
          },
          {
            "text": "Besides, the pills' visual features are leveraged to construct a so-called Visual semantic graph that captures the pills' relationships encapsulated under their appearances.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 13,
        "paragraph_id": 13,
        "full_text": "• Step 4 -Multi-modal Data Fusion.Now, the inter-pill relational and intra-pill visual features are fused to obtain enhanced feature vectors, each of which encapsulates the characteristics of a pill standalone and its relationship with other pills.These enhanced feature vectors are used to offer the final results.",
        "sentences": [
          {
            "text": "• Step 4 -Multi-modal Data Fusion.",
            "label": 0
          },
          {
            "text": "Now, the inter-pill relational and intra-pill visual features are fused to obtain enhanced feature vectors, each of which encapsulates the characteristics of a pill standalone and its relationship with other pills.",
            "label": 0
          },
          {
            "text": "These enhanced feature vectors are used to offer the final results.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 14,
        "paragraph_id": 14,
        "full_text": "In this section, we describe our method to construct the two generic graphs, namely the co-occurrence Graph (i.e., Co-graph) and relative size graph (i.e., Size-graph) in Sections Prescription-based Co-occurrence Graph Modeling and Relative Size Graph Modeling, relatively.",
        "sentences": [
          {
            "text": "In this section, we describe our method to construct the two generic graphs, namely the co-occurrence Graph (i.e., Co-graph) and relative size graph (i.e., Size-graph) in Sections Prescription-based Co-occurrence Graph Modeling and Relative Size Graph Modeling, relatively.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 15,
        "paragraph_id": 15,
        "full_text": "We propose to leverage an external source, namely prescriptions, to build the co-occurrence graph.The rationale behind our idea is that as most pills are intended to cure or alleviate certain diseases or symptoms, there is a significant likelihood that pills meant to treat the same diseases will appear concurrently.Thus, the implicit relationship between the pills can be modeled by assessing the direct interaction between medications and diseases derived through prescriptions.Our Co-graph, G c = V, E, W c , is a weighted graph whose vertices V represent pill classes, and whose edges' weights W c reflect the co-occurrence likelihood of the pills.As the association between pills do not explicitly present in the prescriptions, we model this relationship utilizing the interaction between medications and diseases using the following criteria.",
        "sentences": [
          {
            "text": "We propose to leverage an external source, namely prescriptions, to build the co-occurrence graph.",
            "label": 0
          },
          {
            "text": "The rationale behind our idea is that as most pills are intended to cure or alleviate certain diseases or symptoms, there is a significant likelihood that pills meant to treat the same diseases will appear concurrently.",
            "label": 0
          },
          {
            "text": "Thus, the implicit relationship between the pills can be modeled by assessing the direct interaction between medications and diseases derived through prescriptions.",
            "label": 0
          },
          {
            "text": "Our Co-graph, G c = V, E, W c , is a weighted graph whose vertices V represent pill classes, and whose edges' weights W c reflect the co-occurrence likelihood of the pills.",
            "label": 0
          },
          {
            "text": "As the association between pills do not explicitly present in the prescriptions, we model this relationship utilizing the interaction between medications and diseases using the following criteria.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 16,
        "paragraph_id": 16,
        "full_text": "• There is an edge between two pill classes C i and C j if and only if they have been prescribed for at least one shared disease.",
        "sentences": [
          {
            "text": "• There is an edge between two pill classes C i and C j if and only if they have been prescribed for at least one shared disease.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 17,
        "paragraph_id": 17,
        "full_text": "• The greater the weight of an edge E ij connecting pill classes C i and C j , the more likely that these two medications will be prescribed simultaneously.",
        "sentences": [
          {
            "text": "• The greater the weight of an edge E ij connecting pill classes C i and C j , the more likely that these two medications will be prescribed simultaneously.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 18,
        "paragraph_id": 18,
        "full_text": "We first define a so-called Diagnose-Pill impact factor, which reflects how important a pill is to a diagnosis.Inspired by the Term Frequency (tf) -Inverse Dense Frequency (idf) often used in the Natural Language Processing domain, we define the impact factor of a pill P j to a diagnosis D i , denoted as I(P j , D i ), as follows where S represents the set of all prescriptions, S(D j , P i ) depicts the collection of prescriptions containing both D j and P i , and S(D j ) illustrates the set of prescriptions containing D j .Intuitively, tf(D j , P i ) measures how often pill P i is prescribed for diagnosis D j , thus it reflects the significance of P i regarding treating D j .However, in practice, some pills are more popular among prescriptions (e.g., Sustenance, Dorogyne, Betaserc, etc.), which may cause negative bias when applying only the tf term.That effect can be mitigated by the term idf(P i ).",
        "sentences": [
          {
            "text": "We first define a so-called Diagnose-Pill impact factor, which reflects how important a pill is to a diagnosis.",
            "label": 0
          },
          {
            "text": "Inspired by the Term Frequency (tf) -Inverse Dense Frequency (idf) often used in the Natural Language Processing domain, we define the impact factor of a pill P j to a diagnosis D i , denoted as I(P j , D i ), as follows where S represents the set of all prescriptions, S(D j , P i ) depicts the collection of prescriptions containing both D j and P i , and S(D j ) illustrates the set of prescriptions containing D j .",
            "label": 0
          },
          {
            "text": "Intuitively, tf(D j , P i ) measures how often pill P i is prescribed for diagnosis D j , thus it reflects the significance of P i regarding treating D j .",
            "label": 0
          },
          {
            "text": "However, in practice, some pills are more popular among prescriptions (e.g., Sustenance, Dorogyne, Betaserc, etc.), which may cause negative bias when applying only the tf term.",
            "label": 0
          },
          {
            "text": "That effect can be mitigated by the term idf(P i ).",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 19,
        "paragraph_id": 19,
        "full_text": "Once finished formulating the impact factors of the pills and diagnoses, we transform each term I(P j , D i ) into a probabilistic view by a simple normalization over all diagnoses as follows , where D denotes the set of all diagnoses.Given p(P j , D i ), we define the weight W c (P i , P j ) of the edge E ij connecting vertices P i and P j as the probability p(P i , P j ) that P i and P j are prescribed for the same diseases.W c (P i , P j ) can be formulated as follows.",
        "sentences": [
          {
            "text": "Once finished formulating the impact factors of the pills and diagnoses, we transform each term I(P j , D i ) into a probabilistic view by a simple normalization over all diagnoses as follows , where D denotes the set of all diagnoses.",
            "label": 0
          },
          {
            "text": "Given p(P j , D i ), we define the weight W c (P i , P j ) of the edge E ij connecting vertices P i and P j as the probability p(P i , P j ) that P i and P j are prescribed for the same diseases.",
            "label": 0
          },
          {
            "text": "W c (P i , P j ) can be formulated as follows.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 20,
        "paragraph_id": 20,
        "full_text": "The Size-graph is represented by a directed graph G s = V, E, W s .The edge weight W s is modeled so that the weight of an edge -→ E ij connecting from P i to P j is proportional to the size ratio of P i to P j .The primary source for constructing the Size-graph is the annotations of the training dataset's bounding boxes.As the camera locations for multiple pictures are different, the exact size of each bounding box cannot be utilized directly.Therefore, we instead define a so-called size indicator, a normalized representation of pill size, which is determined as follows.",
        "sentences": [
          {
            "text": "The Size-graph is represented by a directed graph G s = V, E, W s .",
            "label": 0
          },
          {
            "text": "The edge weight W s is modeled so that the weight of an edge -→ E ij connecting from P i to P j is proportional to the size ratio of P i to P j .",
            "label": 0
          },
          {
            "text": "The primary source for constructing the Size-graph is the annotations of the training dataset's bounding boxes.",
            "label": 0
          },
          {
            "text": "As the camera locations for multiple pictures are different, the exact size of each bounding box cannot be utilized directly.",
            "label": 0
          },
          {
            "text": "Therefore, we instead define a so-called size indicator, a normalized representation of pill size, which is determined as follows.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 21,
        "paragraph_id": 21,
        "full_text": "• Step 1: We begin with an arbitrary pill class by initializing its size indicator to 1, while those of other pill classes are initialized to 0. • Step 2: From the current node P i , we traverse through all its 1-hop neighbors P j , and calculate P j 's size indicator s j as",
        "sentences": [
          {
            "text": "• Step 1: We begin with an arbitrary pill class by initializing its size indicator to 1, while those of other pill classes are initialized to 0.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 22,
        "paragraph_id": 22,
        "full_text": "|Bi| , where B i , B j are the two bounding boxes of P i and P j in a particular image in the training set.Step 2 is repeated until all the vertices of G s are traversed.",
        "sentences": [
          {
            "text": "|Bi| , where B i , B j are the two bounding boxes of P i and P j in a particular image in the training set.",
            "label": 0
          },
          {
            "text": "Step 2 is repeated until all the vertices of G s are traversed.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 23,
        "paragraph_id": 23,
        "full_text": "Given the size indicators of all vertices, we now define the weight of edge -→ E ij as the ratio of s i to s j .",
        "sentences": [
          {
            "text": "Given the size indicators of all vertices, we now define the weight of edge -→ E ij as the ratio of s i to s j .",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 24,
        "paragraph_id": 24,
        "full_text": "This block is responsible for localizing and extracting the features of Regions of Interest (RoIs).For this purpose, we adopt components from Faster RCNN[21], a conventional two-step object detector architecture.Nevertheless, our proposed framework is compatible with any alternative object detection architecture.The Visual Feature Extractor consists of three components: a Convolutional Network, a Region Proposal Network, and an RoI Pooling Layer, as depicted in Fig.3. RPN is a fully convolutional network that takes the visual feature vector from the previous module and generates proposals with various scales and aspect ratios.The RoI Pooling layer works simply by splitting each region proposal into a grid of cells and then applying the max pooling operation to each cell in the grid.The combination of the grids' values forms the visual feature vectors of the RoIs.",
        "sentences": [
          {
            "text": "This block is responsible for localizing and extracting the features of Regions of Interest (RoIs).",
            "label": 0
          },
          {
            "text": "For this purpose, we adopt components from Faster RCNN[21], a conventional two-step object detector architecture.",
            "label": 0
          },
          {
            "text": "Nevertheless, our proposed framework is compatible with any alternative object detection architecture.",
            "label": 0
          },
          {
            "text": "The Visual Feature Extractor consists of three components: a Convolutional Network, a Region Proposal Network, and an RoI Pooling Layer, as depicted in Fig.3.",
            "label": 0
          },
          {
            "text": "RPN is a fully convolutional network that takes the visual feature vector from the previous module and generates proposals with various scales and aspect ratios.",
            "label": 0
          },
          {
            "text": "The RoI Pooling layer works simply by splitting each region proposal into a grid of cells and then applying the max pooling operation to each cell in the grid.",
            "label": 0
          },
          {
            "text": "The combination of the grids' values forms the visual feature vectors of the RoIs.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 25,
        "paragraph_id": 25,
        "full_text": "To enhance the efficacy of this a priori, we observed that rather than the whole graphs representing the interaction between all pills, we should utilize sub-graphs concentrating on the ones most likely to appear in the image.Motivated by this observation, we employ the Inter-Pill Relational Feature Extractor, responsible for extracting condensed subgraphs from generic Co-graph and Size-graph.Moreover, previous studies have pointed out that the appearances of pills convey implications about their efficacy or ingredients[7].In light of this, utilizing pills' visual feature vectors, we develop a visual-based graph that models the implicit relationship between medications indicated by their visual appearance.",
        "sentences": [
          {
            "text": "To enhance the efficacy of this a priori, we observed that rather than the whole graphs representing the interaction between all pills, we should utilize sub-graphs concentrating on the ones most likely to appear in the image.",
            "label": 0
          },
          {
            "text": "Motivated by this observation, we employ the Inter-Pill Relational Feature Extractor, responsible for extracting condensed subgraphs from generic Co-graph and Size-graph.",
            "label": 0
          },
          {
            "text": "Moreover, previous studies have pointed out that the appearances of pills convey implications about their efficacy or ingredients[7].",
            "label": 0
          },
          {
            "text": "In light of this, utilizing pills' visual feature vectors, we develop a visual-based graph that models the implicit relationship between medications indicated by their visual appearance.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 26,
        "paragraph_id": 26,
        "full_text": "Condensed Co-graph and Size-graph.Our main idea is to employ a so-called Pseudo Classifier, which provides approximate classification results using solely visual features of RoIs.These temporary identification results are then (2) where σ denotes the Softmax activation function.Intuitively, the item in the i-th row and j-th column of Ãc and Ãs highlights the relationship of the iand j-th RoIs.",
        "sentences": [
          {
            "text": "Condensed Co-graph and Size-graph.",
            "label": 0
          },
          {
            "text": "Our main idea is to employ a so-called Pseudo Classifier, which provides approximate classification results using solely visual features of RoIs.",
            "label": 0
          },
          {
            "text": "These temporary identification results are then (2) where σ denotes the Softmax activation function.",
            "label": 0
          },
          {
            "text": "Intuitively, the item in the i-th row and j-th column of Ãc and Ãs highlights the relationship of the iand j-th RoIs.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 27,
        "paragraph_id": 27,
        "full_text": "Visual Semantic Graph.As mentioned above, the visual semantic graph Gv = Ṽ , Ẽv , Wv is in charge of capturing the visually semantic correlation among pills in the input image.The detailed algorithm to construct this graph is as follows.All visual feature vectors are first passed through a non-linear function F: R h → R h to transform from the original h-dimensional space into a h -dimensional latent one, where their relationship can be best presented.The latent output vectors are then directly used for calculating the correlations between RoIs.Let R i , R j are two RoIs in the input image, and z i , z j are their feature vectors created by the Visual Feature Extractor block, the weight of the edge connecting R i and R j is defined as",
        "sentences": [
          {
            "text": "Visual Semantic Graph.",
            "label": 0
          },
          {
            "text": "As mentioned above, the visual semantic graph Gv = Ṽ , Ẽv , Wv is in charge of capturing the visually semantic correlation among pills in the input image.",
            "label": 0
          },
          {
            "text": "The detailed algorithm to construct this graph is as follows.",
            "label": 0
          },
          {
            "text": "All visual feature vectors are first passed through a non-linear function F: R h → R h to transform from the original h-dimensional space into a h -dimensional latent one, where their relationship can be best presented.",
            "label": 0
          },
          {
            "text": "The latent output vectors are then directly used for calculating the correlations between RoIs.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 28,
        "paragraph_id": 28,
        "full_text": "After going through the second and third blocks, we get the visual features of the RoIs and three relational graphs representing the relationships between the RoIs.This information is now fed into the Multi-modal Data Fusion to generate the final feature vectors, each of which encapsulates both the intra-Pill visual characteristic of an RoI and the inter-Pill interaction of that RoI with the others.The Multi-modal Data Fusion comprises two steps: graph embedding and data concatenation.The former obtains the heterogeneous relational graph G and transforms it into context features in the vector space, while the latter concatenates the context feature vectors with visual features to generate the final enhanced features.We utilize the Graph Transformer Network (GTN)[32]for the graph embedding.The reason for choosing the GTN is due to its ability to handle heterogeneous input and adaptive graph structures.Before going into the detail of the GTN, it is crucial to define the node attribute of graph G.As each node of G represents an RoI, the node attribute should be the most representative characteristic of the ROIs.Using the retrieved RoI visual features to depict the relevant ROIs is the most natural solution but is not advantageous due to several factors, including the unreliability in dealing with ambiguous samples or the intra -variance in visual features of one class[29].To this end, classifier weights has been introduced as a simple yet effective alternative.According to[8], the classifier weights connected to the i-th neuron in the last layer (which is denoted as ω i = [ω 1i , ..., ω Hi ] T in Fig.4) corresponds to the i-th pill class, encapsulating the representative characteristics of this class.Let p k = [p k1 , ..., p kN ] be the logit vector of the k-th RoI, where p ki depicts the likelihood for the k-th RoI to be classified into the i-th class, we define H i=1 p ku × ω i the attribute of the k-th RoI.Intuitively, this attribute can be considered as a decomposition of the RoI's characteristic in the space of the classes' features.",
        "sentences": [
          {
            "text": "After going through the second and third blocks, we get the visual features of the RoIs and three relational graphs representing the relationships between the RoIs.",
            "label": 0
          },
          {
            "text": "This information is now fed into the Multi-modal Data Fusion to generate the final feature vectors, each of which encapsulates both the intra-Pill visual characteristic of an RoI and the inter-Pill interaction of that RoI with the others.",
            "label": 0
          },
          {
            "text": "The Multi-modal Data Fusion comprises two steps: graph embedding and data concatenation.",
            "label": 0
          },
          {
            "text": "The former obtains the heterogeneous relational graph G and transforms it into context features in the vector space, while the latter concatenates the context feature vectors with visual features to generate the final enhanced features.",
            "label": 0
          },
          {
            "text": "We utilize the Graph Transformer Network (GTN)[32]for the graph embedding.",
            "label": 0
          },
          {
            "text": "The reason for choosing the GTN is due to its ability to handle heterogeneous input and adaptive graph structures.",
            "label": 0
          },
          {
            "text": "Before going into the detail of the GTN, it is crucial to define the node attribute of graph G.",
            "label": 0
          },
          {
            "text": "As each node of G represents an RoI, the node attribute should be the most representative characteristic of the ROIs.",
            "label": 0
          },
          {
            "text": "Using the retrieved RoI visual features to depict the relevant ROIs is the most natural solution but is not advantageous due to several factors, including the unreliability in dealing with ambiguous samples or the intra -variance in visual features of one class[29].",
            "label": 0
          },
          {
            "text": "To this end, classifier weights has been introduced as a simple yet effective alternative.",
            "label": 0
          },
          {
            "text": "According to[8], the classifier weights connected to the i-th neuron in the last layer (which is denoted as ω i = [ω 1i , ..., ω Hi ] T in Fig.4) corresponds to the i-th pill class, encapsulating the representative characteristics of this class.Let p k = [p k1 , ..., p kN ] be the logit vector of the k-th RoI, where p ki depicts the likelihood for the k-th RoI to be classified into the i-th class, we define H i=1 p ku × ω i the attribute of the k-th RoI.",
            "label": 0
          },
          {
            "text": "Intuitively, this attribute can be considered as a decomposition of the RoI's characteristic in the space of the classes' features.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 29,
        "paragraph_id": 29,
        "full_text": "Figure5depicts the GTN's architecture, which consists of two phases.The former can be seen as a meta-path generator that fuses information from multiple input adjacency matrices to generate a composite graph structure.This newly generated graph serves as the second stage's input, which comprises a Graph Convolutional Network (GCN) and is responsible for producing a representation for each node.Specifically, the GTN consists of l Graph Transformer (GT) layer; the l-th layer applies the C-channel 1D convolution operation on the input graph G to obtain a stack of new graph structure where φ indicates the convolution layer, W",
        "sentences": [
          {
            "text": "Figure5depicts the GTN's architecture, which consists of two phases.",
            "label": 0
          },
          {
            "text": "The former can be seen as a meta-path generator that fuses information from multiple input adjacency matrices to generate a composite graph structure.",
            "label": 0
          },
          {
            "text": "This newly generated graph serves as the second stage's input, which comprises a Graph Convolutional Network (GCN) and is responsible for producing a representation for each node.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 30,
        "paragraph_id": 30,
        "full_text": "φ ∈ R C×1×1×K represents the parameter of φ, and K implies the number of relations contained in the original graph G.The stacked graph Q (l) 1 serve as the first component in creating length l meta-paths, while",
        "sentences": [
          {
            "text": "φ ∈ R C×1×1×K represents the parameter of φ, and K implies the number of relations contained in the original graph G.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 31,
        "paragraph_id": 31,
        "full_text": "2 .To balance computational overheads and model performances, with PGPNet, we fix l = 2. The resulting graph G(2) , together with RoIs' representative features X RoI , are then utilized as the input for the Graph Convolution Network (GCN) to generate the final node presentations.These vectors are directly concatenated with their corresponding RoIs' visual features before getting fed into the Bounding Box Regressor and Classifier to produce the final detection results.",
        "sentences": [
          {
            "text": "2 .To balance computational overheads and model performances, with PGPNet, we fix l = 2.",
            "label": 0
          },
          {
            "text": "The resulting graph G(2) , together with RoIs' representative features X RoI , are then utilized as the input for the Graph Convolution Network (GCN) to generate the final node presentations.",
            "label": 0
          },
          {
            "text": "These vectors are directly concatenated with their corresponding RoIs' visual features before getting fed into the Bounding Box Regressor and Classifier to produce the final detection results.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 32,
        "paragraph_id": 32,
        "full_text": "This section presents the details about our model's objectives and the corresponding losses to achieve those goals.",
        "sentences": [
          {
            "text": "This section presents the details about our model's objectives and the corresponding losses to achieve those goals.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 33,
        "paragraph_id": 33,
        "full_text": "The Region Proposal Network's Losses.The loss for Region Proposal Network consists of two components: classification loss combined and bounding box regression loss.Let p i , p * i be the predicted probability of an anchor i being an object and the ground truth label whether anchor i is the object, respectively; t i and t * i depict the differences of four predicted coordinates, and the ground truth coordinates with the coordinates of the anchor boxes, respectively.The classification loss L cls and bounding box regression loss L box are defined as follows. where",
        "sentences": [
          {
            "text": "The Region Proposal Network's Losses.",
            "label": 0
          },
          {
            "text": "The loss for Region Proposal Network consists of two components: classification loss combined and bounding box regression loss.",
            "label": 0
          },
          {
            "text": "Let p i , p * i be the predicted probability of an anchor i being an object and the ground truth label whether anchor i is the object, respectively; t i and t * i depict the differences of four predicted coordinates, and the ground truth coordinates with the coordinates of the anchor boxes, respectively.",
            "label": 0
          },
          {
            "text": "The classification loss L cls and bounding box regression loss L box are defined as follows.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 34,
        "paragraph_id": 34,
        "full_text": "Here the L cls is a binary classification log loss, N cls and N box are two normalization terms, where N cls is set to the mini-batch size, while N box is the number of anchor boxes.λ is a hyper-parameter, which is responsible for balancing between L cls and L box .",
        "sentences": [
          {
            "text": "Here the L cls is a binary classification log loss, N cls and N box are two normalization terms, where N cls is set to the mini-batch size, while N box is the number of anchor boxes.λ is a hyper-parameter, which is responsible for balancing between L cls and L box .",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 35,
        "paragraph_id": 35,
        "full_text": "Output's Losses.The PGPNet's final results consist of the coordinates of the RoIs' bounding boxes and predicted labels for the RoIs.We employ two distinct losses to accomplish this objective.While the loss for a bounding box regressor is equal to that of the RPN network, the classification loss L out cls is instead the cross entropy loss for the multilabel classification task, which is represented as follows Triplet Co-occurrence Enhancement Loss.",
        "sentences": [
          {
            "text": "Output's Losses.",
            "label": 0
          },
          {
            "text": "The PGPNet's final results consist of the coordinates of the RoIs' bounding boxes and predicted labels for the RoIs.",
            "label": 0
          },
          {
            "text": "We employ two distinct losses to accomplish this objective.",
            "label": 0
          },
          {
            "text": "While the loss for a bounding box regressor is equal to that of the RPN network, the classification loss L out cls is instead the cross entropy loss for the multilabel classification task, which is represented as follows Triplet Co-occurrence Enhancement Loss.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 36,
        "paragraph_id": 36,
        "full_text": "In this section, we propose an auxiliary loss named Triplet Co-occurrence Enhancement Loss which leverages the co-occurrence graph to boost the accuracy of the Pseudo Classifier.The idea behind the auxiliary loss is that it encourages the co-occurrence likelihood of pills that are close together on the co-occurrence graph.To this end, we construct our auxiliary loss as a contrastive loss that maximizes the co-occurrence probability of positive pairings (i.e., pills joined by edges with the most significant weights in the co-occurrence graphs) while minimizing the co-occurrence probability of negative pairs (i.e., pills that are not connected or connected by edges with smallest weights).In action, for each training mini-batch, PGPNet would treat all the ground truth pills in given images as the set of anchors and build up their corresponding positive as well as negative sets.After that, Triplet Co-occurrence Enhancement Loss would do its job for enhancing the robustness of Pseudo Classifier.The detail of the auxiliary loss is as follows.",
        "sentences": [
          {
            "text": "In this section, we propose an auxiliary loss named Triplet Co-occurrence Enhancement Loss which leverages the co-occurrence graph to boost the accuracy of the Pseudo Classifier.",
            "label": 0
          },
          {
            "text": "The idea behind the auxiliary loss is that it encourages the co-occurrence likelihood of pills that are close together on the co-occurrence graph.",
            "label": 0
          },
          {
            "text": "To this end, we construct our auxiliary loss as a contrastive loss that maximizes the co-occurrence probability of positive pairings (i.e., pills joined by edges with the most significant weights in the co-occurrence graphs) while minimizing the co-occurrence probability of negative pairs (i.e., pills that are not connected or connected by edges with smallest weights).",
            "label": 0
          },
          {
            "text": "In action, for each training mini-batch, PGPNet would treat all the ground truth pills in given images as the set of anchors and build up their corresponding positive as well as negative sets.",
            "label": 0
          },
          {
            "text": "After that, Triplet Co-occurrence Enhancement Loss would do its job for enhancing the robustness of Pseudo Classifier.",
            "label": 0
          },
          {
            "text": "The detail of the auxiliary loss is as follows.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 37,
        "paragraph_id": 37,
        "full_text": "Let's denote the i-th Region of Interest as R i with its corresponding label of l i .Moreover, let N i pos and N i neg be the positive and negative samples of R i , where N i pos comprises of k + 1 nearest neighbors and N i neg consists of k + 1 furthest neighbors of R i .We suppose that the groundtruth labels of N i pos and N i neg are L pos = {l 0 pos , l 1 pos , . . ., l k pos }, and L neg = {l 0 neg , l 1 neg , . . ., l k neg }, respectively.The auxiliary loss concerning the i-th RoI is defined by and those for RoI is",
        "sentences": [
          {
            "text": "Let's denote the i-th Region of Interest as R i with its corresponding label of l i .",
            "label": 0
          },
          {
            "text": "Moreover, let N i pos and N i neg be the positive and negative samples of R i , where N i pos comprises of k + 1 nearest neighbors and N i neg consists of k + 1 furthest neighbors of R i .",
            "label": 0
          },
          {
            "text": "We suppose that the groundtruth labels of N i pos and N i neg are L pos = {l 0 pos , l 1 pos , . . ., l k pos }, and L neg = {l 0 neg , l 1 neg , . . ., l k neg }, respectively.",
            "label": 0
          },
          {
            "text": "The auxiliary loss concerning the i-th RoI is defined by and those for RoI is",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 38,
        "paragraph_id": 38,
        "full_text": "In Formulas (7) M is the total number of RoIs in the image, p is the output after going through softmax activation of logits produced by Pseudo Classifier.The objective during the training process is to maximize L aux , which in turn maximizes each positive term p i (l i )p(N i pos ) while minimizing the negative opposition (1 -p i (l i ))p(N i neg ).",
        "sentences": [
          {
            "text": "In Formulas (7) M is the total number of RoIs in the image, p is the output after going through softmax activation of logits produced by Pseudo Classifier.",
            "label": 0
          },
          {
            "text": "The objective during the training process is to maximize L aux , which in turn maximizes each positive term p i (l i )p(N i pos ) while minimizing the negative opposition (1 -p i (l i ))p(N i neg ).",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 39,
        "paragraph_id": 39,
        "full_text": "We conduct extensive experiments to validate the effectiveness of the proposed approach.In the following, we first introduce our in-house pill identification dataset, called VAIPE, which will be used to evaluate the proposed approach, and then explain our evaluation metrics and experimental settings.To assess the effectiveness of the proposed method, we conducted comparative assessments against a number of established models, including the detection backbones we selected, such as Faster R-CNN[21]and YOLOv5[9], as well as other related frameworks such as SGRN[28]and the Mask RCNN-based approach described in[10].We also perform ablation studies to investigate the efficiency of key components in our framework.",
        "sentences": [
          {
            "text": "We conduct extensive experiments to validate the effectiveness of the proposed approach.",
            "label": 0
          },
          {
            "text": "In the following, we first introduce our in-house pill identification dataset, called VAIPE, which will be used to evaluate the proposed approach, and then explain our evaluation metrics and experimental settings.",
            "label": 0
          },
          {
            "text": "To assess the effectiveness of the proposed method, we conducted comparative assessments against a number of established models, including the detection backbones we selected, such as Faster R-CNN[21]and YOLOv5[9], as well as other related frameworks such as SGRN[28]and the Mask RCNN-based approach described in[10].",
            "label": 0
          },
          {
            "text": "We also perform ablation studies to investigate the efficiency of key components in our framework.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 40,
        "paragraph_id": 40,
        "full_text": "Motivation.To the best of our knowledge, previous studies on the pill identification problem[22],[33],[14],[15]only focus on datasets collected in constrained environments.For instance, existing datasets such as NIH Dataset [1] are constructed under ideal conditions in lighting, backgrounds, and equipment or devices.The CURE dataset[14]provides only one pill per image.Hence, these datasets do not reflect the real-world scenarios in which patients take an arbitrary number of drugs, and their environmental conditions (e.g., backgrounds, lighting conditions, mobile devices, etc.) are greatly varied.Additionally, many pills have nearly identical visual appearances.The fact that they appear alone in the images of these datasets will inevitably confuse the detection frameworks.Consequently, none of the existing datasets can be directly applied to the real-world pill detection problem or can only be applied with low reliability.There is no publicly available dataset of these pills images in which the pills follow intakes of actual patients.This limits the development of machine learning algorithms for the detection of pills from images as well as for building real-world medicine inspection applications.To address this challenge, we build and introduce a new, large-scale open dataset of pill images, which we called VAIPE.",
        "sentences": [
          {
            "text": "Motivation.",
            "label": 0
          },
          {
            "text": "To the best of our knowledge, previous studies on the pill identification problem[22],[33],[14],[15]only focus on datasets collected in constrained environments.",
            "label": 0
          },
          {
            "text": "For instance, existing datasets such as NIH Dataset [1] are constructed under ideal conditions in lighting, backgrounds, and equipment or devices.",
            "label": 0
          },
          {
            "text": "The CURE dataset[14]provides only one pill per image.",
            "label": 0
          },
          {
            "text": "Hence, these datasets do not reflect the real-world scenarios in which patients take an arbitrary number of drugs, and their environmental conditions (e.g., backgrounds, lighting conditions, mobile devices, etc.) are greatly varied.",
            "label": 0
          },
          {
            "text": "Additionally, many pills have nearly identical visual appearances.",
            "label": 0
          },
          {
            "text": "The fact that they appear alone in the images of these datasets will inevitably confuse the detection frameworks.",
            "label": 0
          },
          {
            "text": "Consequently, none of the existing datasets can be directly applied to the real-world pill detection problem or can only be applied with low reliability.",
            "label": 0
          },
          {
            "text": "There is no publicly available dataset of these pills images in which the pills follow intakes of actual patients.",
            "label": 0
          },
          {
            "text": "This limits the development of machine learning algorithms for the detection of pills from images as well as for building real-world medicine inspection applications.",
            "label": 1
          },
          {
            "text": "To address this challenge, we build and introduce a new, large-scale open dataset of pill images, which we called VAIPE.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 41,
        "paragraph_id": 41,
        "full_text": "Data Descriptor.The VAIPE is a large-scale and open pill image dataset for visual-based medicine inspection.The dataset contains approximately 10,000 pill images that were manually collected in unconstrained environments.In this study, no hypotheses or new interventional procedures were generated.Also, no investigational products or clinical trials were used for patients.In addition, there were no changes in treatment plans for any patients involved.Pill images were retrospectives collected, and all identifiable information of patients was de-identified.Therefore, there was no requirement for ethics approval[18].",
        "sentences": [
          {
            "text": "Data Descriptor.",
            "label": 1
          },
          {
            "text": "The VAIPE is a large-scale and open pill image dataset for visual-based medicine inspection.",
            "label": 1
          },
          {
            "text": "The dataset contains approximately 10,000 pill images that were manually collected in unconstrained environments.",
            "label": 1
          },
          {
            "text": "In this study, no hypotheses or new interventional procedures were generated.",
            "label": 1
          },
          {
            "text": "Also, no investigational products or clinical trials were used for patients.",
            "label": 1
          },
          {
            "text": "In addition, there were no changes in treatment plans for any patients involved.",
            "label": 1
          },
          {
            "text": "Pill images were retrospectives collected, and all identifiable information of patients was de-identified.",
            "label": 1
          },
          {
            "text": "Therefore, there was no requirement for ethics approval[18].",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 42,
        "paragraph_id": 42,
        "full_text": "Pill images are collected in many different contexts (e.g., various backgrounds, lighting conditions, in-hand or outof-hand, etc.) using smartphones.These images are then manually labeled using the information from the relevant prescriptions.In summary, the number of pills per image is about 5 -10, and the total number of pill images collected was 9, 426 pill images with 96 independent pill labels.To train the proposed deep learning system, the pill images from the VAIPE dataset are resized so that the shortest edges have a size of 800, with a limit of 1, 333 on the longer edge.",
        "sentences": [
          {
            "text": "Pill images are collected in many different contexts (e.g., various backgrounds, lighting conditions, in-hand or outof-hand, etc.) using smartphones.",
            "label": 1
          },
          {
            "text": "These images are then manually labeled using the information from the relevant prescriptions.",
            "label": 1
          },
          {
            "text": "In summary, the number of pills per image is about 5 -10, and the total number of pill images collected was 9, 426 pill images with 96 independent pill labels.",
            "label": 1
          },
          {
            "text": "To train the proposed deep learning system, the pill images from the VAIPE dataset are resized so that the shortest edges have a size of 800, with a limit of 1, 333 on the longer edge.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 43,
        "paragraph_id": 43,
        "full_text": "The ratios are kept the same as the original images if the max size is reached, then downscale so that the longer edge does not exceed 1, 333.",
        "sentences": [
          {
            "text": "The ratios are kept the same as the original images if the max size is reached, then downscale so that the longer edge does not exceed 1, 333.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 44,
        "paragraph_id": 44,
        "full_text": "Data Validation.Patient privacy was controlled and protected.In particular, all images were manually reviewed to ensure that all individually identifiable health information of the patients has been removed to meet the General Data Protection Regulation (GDPR)[3].Annotations of pill images were also carefully examined.Specifically, all images were manually reviewed case-by-case by a team of 20 human readers to improve the quality of the annotations. Comparison with Existing Datasets.",
        "sentences": [
          {
            "text": "Data Validation.",
            "label": 1
          },
          {
            "text": "Patient privacy was controlled and protected.",
            "label": 1
          },
          {
            "text": "In particular, all images were manually reviewed to ensure that all individually identifiable health information of the patients has been removed to meet the General Data Protection Regulation (GDPR)[3].",
            "label": 1
          },
          {
            "text": "Annotations of pill images were also carefully examined.",
            "label": 1
          },
          {
            "text": "Specifically, all images were manually reviewed case-by-case by a team of 20 human readers to improve the quality of the annotations.",
            "label": 1
          },
          {
            "text": "Comparison with Existing Datasets.",
            "label": 1
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 45,
        "paragraph_id": 45,
        "full_text": "We evaluate the proposed method and other related works by the COCO APs metrics [2].This set of metrics is widely accepted and used for evaluating state-of-the-art object detectors.Mean Average Precision (mAP), as its name suggests, is the mean of Average Precision (AP) overall C classes and all the targeted IoU thresholds in the threshold set T calculated by mAP = 1 , where Average Precision (AP i,t ) is the area under the Precision-Recall curve, calculated for the class i at a given IoU threshold t.",
        "sentences": [
          {
            "text": "We evaluate the proposed method and other related works by the COCO APs metrics [2].",
            "label": 0
          },
          {
            "text": "This set of metrics is widely accepted and used for evaluating state-of-the-art object detectors.",
            "label": 0
          },
          {
            "text": "Mean Average Precision (mAP), as its name suggests, is the mean of Average Precision (AP) overall C classes and all the targeted IoU thresholds in the threshold set T calculated by mAP = 1 , where Average Precision (AP i,t ) is the area under the Precision-Recall curve, calculated for the class i at a given IoU threshold t.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 46,
        "paragraph_id": 46,
        "full_text": "Comparison Benchmarks.To show the effectiveness of the proposed method, we conducted a comparison with the state-of-the-art object detectors, including our detection backbones: Faster R-CNN[21], YOLOv5[9], and related works: SGRN[28], Mask RCNN-based approach[10].Throughout the literature, the baseline with which PGPNet presently integrates is Faster R-CNN[21]; hence, the original framework is utilized for our comparison.We adopt two different CNNs and one Transformer-based module for visual feature extractor, namely ResNet-50-C4, ResNet-50-FPN and Swin Transformer V2 -SwinV2[16](Fig.3).Specifically, for two ConvNets, we use a single feature map produced by convolution block C4 of the ResNet-50 model in ResNet-50-C4.In ResNet-50-FPN, we replace C4's feature map with multi-scale feature maps produced by Feature Pyramid Network (FPN)[12].As for the Swin Transformer module, we are currently utilizing the SwinV2-T configuration[16]to ensure that the number of model parameters is comparable to that of ResNet-50.In addition, we also make adaption for PGPNet with YOLOv5[9]detection backbone.Two configurations of YOLOv5s and YOLOv5n are currently adopted.Also, the most relevant frameworks compared with our PGPNet are also put into comparison: a representative approach that utilizes an external knowledge graph for Object Detection task[28]; a Mask RCNN-based baseline that also proposed to solve the same task of multi-pill detection[10].For a fair comparison, a fixed set of hyper-parameters is used for PGPNet throughout all experiments.",
        "sentences": [
          {
            "text": "Comparison Benchmarks.",
            "label": 0
          },
          {
            "text": "To show the effectiveness of the proposed method, we conducted a comparison with the state-of-the-art object detectors, including our detection backbones: Faster R-CNN[21], YOLOv5[9], and related works: SGRN[28], Mask RCNN-based approach[10].",
            "label": 0
          },
          {
            "text": "Throughout the literature, the baseline with which PGPNet presently integrates is Faster R-CNN[21]; hence, the original framework is utilized for our comparison.",
            "label": 0
          },
          {
            "text": "We adopt two different CNNs and one Transformer-based module for visual feature extractor, namely ResNet-50-C4, ResNet-50-FPN and Swin Transformer V2 -SwinV2[16](Fig.3).",
            "label": 0
          },
          {
            "text": "Specifically, for two ConvNets, we use a single feature map produced by convolution block C4 of the ResNet-50 model in ResNet-50-C4.",
            "label": 0
          },
          {
            "text": "In ResNet-50-FPN, we replace C4's feature map with multi-scale feature maps produced by Feature Pyramid Network (FPN)[12].",
            "label": 0
          },
          {
            "text": "As for the Swin Transformer module, we are currently utilizing the SwinV2-T configuration[16]to ensure that the number of model parameters is comparable to that of ResNet-50.",
            "label": 0
          },
          {
            "text": "In addition, we also make adaption for PGPNet with YOLOv5[9]detection backbone.",
            "label": 0
          },
          {
            "text": "Two configurations of YOLOv5s and YOLOv5n are currently adopted.",
            "label": 0
          },
          {
            "text": "Also, the most relevant frameworks compared with our PGPNet are also put into comparison: a representative approach that utilizes an external knowledge graph for Object Detection task[28]; a Mask RCNN-based baseline that also proposed to solve the same task of multi-pill detection[10].",
            "label": 0
          },
          {
            "text": "For a fair comparison, a fixed set of hyper-parameters is used for PGPNet throughout all experiments.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 47,
        "paragraph_id": 47,
        "full_text": "We conduct all the experiments using the Pytorch (version 1.10.1) on an Intel Xeon Silver 4210 2.20GHz system with 2 × NVIDIA GeForce RTX 3090 GPUs.We train and test all targeted models on the training and testing sub-datasets provided in Table8.Specifically, we initialize all the networks with the weights achieved by pre-training them on COCO 2017 dataset[11].We then train the models in 20, 000 iterations with a batch size of 16.AdamW[17]optimizer is used with the initial learning rate of 0.001.We also augment the training data by using simple techniques such as random horizontal and vertical flips to prevent overfitting.For our PGPNet implementation, we set the dimensions of node embeddings at 64.We also design the Graph Transformer Module with only one layer and 10 channel set.",
        "sentences": [
          {
            "text": "We conduct all the experiments using the Pytorch (version 1.10.1) on an Intel Xeon Silver 4210 2.20GHz system with 2 × NVIDIA GeForce RTX 3090 GPUs.",
            "label": 0
          },
          {
            "text": "We train and test all targeted models on the training and testing sub-datasets provided in Table8.",
            "label": 0
          },
          {
            "text": "Specifically, we initialize all the networks with the weights achieved by pre-training them on COCO 2017 dataset[11].",
            "label": 0
          },
          {
            "text": "We then train the models in 20, 000 iterations with a batch size of 16.",
            "label": 0
          },
          {
            "text": "AdamW[17]optimizer is used with the initial learning rate of 0.001.",
            "label": 0
          },
          {
            "text": "We also augment the training data by using simple techniques such as random horizontal and vertical flips to prevent overfitting.",
            "label": 0
          },
          {
            "text": "For our PGPNet implementation, we set the dimensions of node embeddings at 64.",
            "label": 0
          },
          {
            "text": "We also design the Graph Transformer Module with only one layer and 10 channel set.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 48,
        "paragraph_id": 48,
        "full_text": "This section reports our experimental results.We evaluate the effectiveness of PGPNet in three aspects: robustness, reliability, and explainability.The details are described below.",
        "sentences": [
          {
            "text": "This section reports our experimental results.",
            "label": 0
          },
          {
            "text": "We evaluate the effectiveness of PGPNet in three aspects: robustness, reliability, and explainability.",
            "label": 0
          },
          {
            "text": "The details are described below.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 49,
        "paragraph_id": 49,
        "full_text": "Comparison with Faster R-CNN and YOLOv5 Detection Performance.Table ?? shows the experimental results of PGPNet and the state-of-the-art object detectors framework (Vanilla), e.g., Faster R-CNN (two-step detector), and YOLOv5[9](one-step detector) on the VAIPE dataset.As shown, PGPNet obtained better results than Faster R-CNN by large performance gaps for all evaluation metrics.Specifically, when using the ResNet-50-C4 model as the visual feature extractor model, the average precision mAP of Faster R-CNN was 62.6, while that of PGPNet was 68.3.The proposed method improves the performance over the baseline Faster R-CNN by 9.2%.Under strict metrics, e.g., AP75, PGPNet also outperforms Faster R-CNN 8 -9%.In addition, we observed similar behavior when using the ResNet-50-FPN model.The proposed PGPNet makes an improvement of 9.4% for the mAP metrics.With a Transformer-based backbone, here a Swin Transformer V2 configuration -SwinV2-T[16], the results are slightly worse compared to those produced by ResNet-based counterparts, for both the vanilla or PGPNet alternatives.However, PGPNet still show its superior when being install with this backbone, as the empirical result for AP metrics is improved by 4.8% compared to the vanilla SwinV2-T Faster R-CNN model.",
        "sentences": [
          {
            "text": "Comparison with Faster R-CNN and YOLOv5 Detection Performance.",
            "label": 0
          },
          {
            "text": "shows the experimental results of PGPNet and the state-of-the-art object detectors framework (Vanilla), e.g., Faster R-CNN (two-step detector), and YOLOv5[9](one-step detector) on the VAIPE dataset.",
            "label": 0
          },
          {
            "text": "As shown, PGPNet obtained better results than Faster R-CNN by large performance gaps for all evaluation metrics.",
            "label": 0
          },
          {
            "text": "Specifically, when using the ResNet-50-C4 model as the visual feature extractor model, the average precision mAP of Faster R-CNN was 62.6, while that of PGPNet was 68.3.",
            "label": 0
          },
          {
            "text": "The proposed method improves the performance over the baseline Faster R-CNN by 9.2%.",
            "label": 0
          },
          {
            "text": "Under strict metrics, e.g., AP75, PGPNet also outperforms Faster R-CNN 8 -9%.",
            "label": 0
          },
          {
            "text": "In addition, we observed similar behavior when using the ResNet-50-FPN model.",
            "label": 0
          },
          {
            "text": "The proposed PGPNet makes an improvement of 9.4% for the mAP metrics.",
            "label": 0
          },
          {
            "text": "With a Transformer-based backbone, here a Swin Transformer V2 configuration -SwinV2-T[16], the results are slightly worse compared to those produced by ResNet-based counterparts, for both the vanilla or PGPNet alternatives.",
            "label": 0
          },
          {
            "text": "However, PGPNet still show its superior when being install with this backbone, as the empirical result for AP metrics is improved by 4.8% compared to the vanilla SwinV2-T Faster R-CNN model.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 50,
        "paragraph_id": 50,
        "full_text": "For YOLOv5, PGPNet outperformed Vanilla by a significant margin across all performance metrics in both YOLO instances.Specifically, the average precision AP of the vanilla model with YOLOv5n was 37.9 while that of PGPNet was 43.0 (12% improvement).In the case of a larger alternative, YOLOv5s, a similar conclusion can be drawn, namely that PGPNet improves overall mAP metrics by 5.9, e.g., 10.2%.",
        "sentences": [
          {
            "text": "For YOLOv5, PGPNet outperformed Vanilla by a significant margin across all performance metrics in both YOLO instances.",
            "label": 0
          },
          {
            "text": "Specifically, the average precision AP of the vanilla model with YOLOv5n was 37.9 while that of PGPNet was 43.0 (12% improvement).",
            "label": 0
          },
          {
            "text": "In the case of a larger alternative, YOLOv5s, a similar conclusion can be drawn, namely that PGPNet improves overall mAP metrics by 5.9, e.g., 10.2%.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 51,
        "paragraph_id": 51,
        "full_text": "Figure17visualizes the AP for all classes in the dataset when using Faster R-CNN as the backbone.The first three bins denote Faster R-CNN alternatives, and the later three are the corresponding PGPNet configurations.The dots in the figure represent AP values for classes; the vertical line is the indicator for the mean value, while the rectangle bar is the 95% High-Density Interval (HDI) band.Apart from the fact that the mean AP over all classes of PGPNet variances is better than those produced by Faster R-CNN, we found that PGPNet also has more reliable and stable results over all classes.Specifically, PGPNet helps to improve the AP of classes that Faster R-CNN frequently confuses (the points with low APs in the blue and pink beans).As a result, the three beans of Faster R-CNN exhibit a large variance, i.e., the AP ranged from 0 to around 90.In contrast, the beans of PGPNet performance are more condensed and have shorter tail, i.e., the AP ranged from 40 (or 50) to around 90.",
        "sentences": [
          {
            "text": "Figure17visualizes the AP for all classes in the dataset when using Faster R-CNN as the backbone.",
            "label": 0
          },
          {
            "text": "The first three bins denote Faster R-CNN alternatives, and the later three are the corresponding PGPNet configurations.",
            "label": 0
          },
          {
            "text": "The dots in the figure represent AP values for classes; the vertical line is the indicator for the mean value, while the rectangle bar is the 95% High-Density Interval (HDI) band.",
            "label": 0
          },
          {
            "text": "Apart from the fact that the mean AP over all classes of PGPNet variances is better than those produced by Faster R-CNN, we found that PGPNet also has more reliable and stable results over all classes.",
            "label": 0
          },
          {
            "text": "Specifically, PGPNet helps to improve the AP of classes that Faster R-CNN frequently confuses (the points with low APs in the blue and pink beans).",
            "label": 0
          },
          {
            "text": "As a result, the three beans of Faster R-CNN exhibit a large variance, i.e., the AP ranged from 0 to around 90.",
            "label": 0
          },
          {
            "text": "In contrast, the beans of PGPNet performance are more condensed and have shorter tail, i.e., the AP ranged from 40 (or 50) to around 90.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 52,
        "paragraph_id": 52,
        "full_text": "Pill Classification Accuracy.To further investigate the robustness of the proposed PGPNet, we adopt the visualization techniques presented in[25]to understand the prediction accuracy ( of the pill classification task) better.In this technique, all models' predictions are categorized by their confidence scores into different bins, in which the average accuracy can be calculated.By observing the confidence-accuracy correlation, we can tell whether the models are under or over-confidence with their predictions[25].Figure18avisualize those reliability plots of Faster R-CNN and PGPNet.It implies that both models have a propensity toward over-confidence, as the average accuracy of each confidence band is lower than the mean confidence score of that bin.However, that tendency is greatly alleviated in the circumstance of PGPNet, which means that the bins' heights are much closer to the perfect Confidence-Accuracy balance line (the red dashed diagonal line).Figure18bcompares PGPNet's confidence-accuracy correlation and that of YOLOv5.With this backbone, we observed that the proposed PGPNet can produce predictions with a high level of reliability.All the heights of bins are much closer to values suggested by the perfectly-balanced line compared to Vanilla's result.",
        "sentences": [
          {
            "text": "Pill Classification Accuracy.",
            "label": 0
          },
          {
            "text": "To further investigate the robustness of the proposed PGPNet, we adopt the visualization techniques presented in[25]to understand the prediction accuracy ( of the pill classification task) better.",
            "label": 0
          },
          {
            "text": "In this technique, all models' predictions are categorized by their confidence scores into different bins, in which the average accuracy can be calculated.",
            "label": 0
          },
          {
            "text": "By observing the confidence-accuracy correlation, we can tell whether the models are under or over-confidence with their predictions[25].",
            "label": 0
          },
          {
            "text": "Figure18avisualize those reliability plots of Faster R-CNN and PGPNet.",
            "label": 0
          },
          {
            "text": "It implies that both models have a propensity toward over-confidence, as the average accuracy of each confidence band is lower than the mean confidence score of that bin.",
            "label": 0
          },
          {
            "text": "However, that tendency is greatly alleviated in the circumstance of PGPNet, which means that the bins' heights are much closer to the perfect Confidence-Accuracy balance line (the red dashed diagonal line).",
            "label": 0
          },
          {
            "text": "Figure18bcompares PGPNet's confidence-accuracy correlation and that of YOLOv5.",
            "label": 0
          },
          {
            "text": "With this backbone, we observed that the proposed PGPNet can produce predictions with a high level of reliability.",
            "label": 0
          },
          {
            "text": "All the heights of bins are much closer to values suggested by the perfectly-balanced line compared to Vanilla's result.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 53,
        "paragraph_id": 53,
        "full_text": "Our work is the first to leverage an external graph in dealing with the Pill Detection challenge; thus, none of the preceding works are genuinely tight-correlated.Indeed, earlier researches only shared some common ground to our approach: (1) About methodology or (2) about research problem.For the first group, there are works that utilized external information to solve the Object Detection problem.We adopt one of the most current studies with this direction -[28]to solve our targeted problem and serve as a baseline for PGPNet.Spatial-aware Graph Relation Network (SGRN)[28]is a framework that adaptively discovers and incorporates key semantic and spatial relationships for reasoning over each RoI.",
        "sentences": [
          {
            "text": "Our work is the first to leverage an external graph in dealing with the Pill Detection challenge; thus, none of the preceding works are genuinely tight-correlated.",
            "label": 0
          },
          {
            "text": "Indeed, earlier researches only shared some common ground to our approach: (1) About methodology or (2) about research problem.",
            "label": 0
          },
          {
            "text": "For the first group, there are works that utilized external information to solve the Object Detection problem.",
            "label": 0
          },
          {
            "text": "We adopt one of the most current studies with this direction -[28]to solve our targeted problem and serve as a baseline for PGPNet.",
            "label": 0
          },
          {
            "text": "Spatial-aware Graph Relation Network (SGRN)[28]is a framework that adaptively discovers and incorporates key semantic and spatial relationships for reasoning over each RoI.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 54,
        "paragraph_id": 54,
        "full_text": "With respect to research problem, as stated earlier, while there are many works which target single-pill detection problem[13,24,26], only a few directly solve the task of detecting multiple pills per image[10,19].We attempt to adopt the most recent technique proposed in[10]as another baseline to compare with PGPNet.In the original work, the authors purpose is somewhat different from us, since they attempt to develop a framework which is solely trained on single-pill images, since they argued that the multi-pill dataset would scale up exponentially if the number of pills inscrease.This argument is not held in our intuition, and we believe, in reality, since the pills taken together have to be prescribed by pharmacists.We keep the pipeline as the original work, with some adoption for working with our VAIPE dataset: (1) Change Mask R-CNN to Faster R-CNN;",
        "sentences": [
          {
            "text": "With respect to research problem, as stated earlier, while there are many works which target single-pill detection problem[13,24,26], only a few directly solve the task of detecting multiple pills per image[10,19].",
            "label": 0
          },
          {
            "text": "We attempt to adopt the most recent technique proposed in[10]as another baseline to compare with PGPNet.",
            "label": 0
          },
          {
            "text": "In the original work, the authors purpose is somewhat different from us, since they attempt to develop a framework which is solely trained on single-pill images, since they argued that the multi-pill dataset would scale up exponentially if the number of pills inscrease.",
            "label": 0
          },
          {
            "text": "This argument is not held in our intuition, and we believe, in reality, since the pills taken together have to be prescribed by pharmacists.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 56,
        "paragraph_id": 56,
        "full_text": "(3) The automate data labeling process are skipped.Since the original work did not name the proposed pipeline, we called it as Kwon's Pipeline for short.",
        "sentences": [
          {
            "text": "(3) The automate data labeling process are skipped.",
            "label": 0
          },
          {
            "text": "Since the original work did not name the proposed pipeline, we called it as Kwon's Pipeline for short.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 57,
        "paragraph_id": 57,
        "full_text": "Detection Performance.Table9summarizes the comparison of PGPNet, SGRN and Kwon's Pipeline when adopting the visual feature extractor architecture from Faster R-CNN with the Resnet-50-FPN model.Clearly, SGRN outperforms the baseline Faster R-CNN in terms of overall performance but could not outperforms our proposed method PGPNet.Specifically, the mAP metrics achieved by SGRN is 65.9, and PGPNet achieves the better score with a gap of nearly 4.",
        "sentences": [
          {
            "text": "Detection Performance.",
            "label": 0
          },
          {
            "text": "Table9summarizes the comparison of PGPNet, SGRN and Kwon's Pipeline when adopting the visual feature extractor architecture from Faster R-CNN with the Resnet-50-FPN model.",
            "label": 0
          },
          {
            "text": "Clearly, SGRN outperforms the baseline Faster R-CNN in terms of overall performance but could not outperforms our proposed method PGPNet.",
            "label": 0
          },
          {
            "text": "Specifically, the mAP metrics achieved by SGRN is 65.9, and PGPNet achieves the better score with a gap of nearly 4.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 58,
        "paragraph_id": 58,
        "full_text": "Upon other metrics, AP50, AP75, APs, APm, and APl, PGPNet shows its superior by enhancing the performance from 5.1% (e.g., in AP75 metrics) up to 17.1% (e.g., in APs metrics).This is an expected result because SGRN reveals a major weakness when applying to the challenge of Pill Detection.The spatial relationships between pills in an image are arbitrary and frequently changed.Such noisy and unreliable information leads to the performance of SGRN being unstable and sometimes produce not good enough results.In the case of Kwon's Pipeline, the situation is even worse, since it cannot even beat the vanilla one-step Faster RCNN trained with mutple-pill VAIPE training set.The result of this pipeline is 43.1% and 48.2% worse than vanilla Faster R-CNN and PGPNet respectively.One reason for this deficiency is owing to the quality of its training data.There are many circumstances in which overlap or occlusion occurs, which make the cropped images also contain parts of other pills.",
        "sentences": [
          {
            "text": "Upon other metrics, AP50, AP75, APs, APm, and APl, PGPNet shows its superior by enhancing the performance from 5.1% (e.g., in AP75 metrics) up to 17.1% (e.g., in APs metrics).",
            "label": 0
          },
          {
            "text": "This is an expected result because SGRN reveals a major weakness when applying to the challenge of Pill Detection.",
            "label": 0
          },
          {
            "text": "The spatial relationships between pills in an image are arbitrary and frequently changed.",
            "label": 0
          },
          {
            "text": "Such noisy and unreliable information leads to the performance of SGRN being unstable and sometimes produce not good enough results.",
            "label": 0
          },
          {
            "text": "In the case of Kwon's Pipeline, the situation is even worse, since it cannot even beat the vanilla one-step Faster RCNN trained with mutple-pill VAIPE training set.",
            "label": 0
          },
          {
            "text": "The result of this pipeline is 43.1% and 48.2% worse than vanilla Faster R-CNN and PGPNet respectively.",
            "label": 0
          },
          {
            "text": "One reason for this deficiency is owing to the quality of its training data.",
            "label": 0
          },
          {
            "text": "There are many circumstances in which overlap or occlusion occurs, which make the cropped images also contain parts of other pills.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 59,
        "paragraph_id": 59,
        "full_text": "Pill Classification Accuracy.Figure18cshows the correlation between the confidence and accuracy of PGPNet in the comparison with those of SGRN.Both the frameworks are based on the Faster R-CNN backbone and achieve similar results e.g., an over-confidence trend in every bin.All the predictions with confidence scores smaller than 0.2 are totally unreliable (with 0 accuracy).In addition, PGPNet also shows its superior over SGRN in some bins, in which the over-confidence situation is reduced effectively.We do not plot the Confidence-Accuracy of Kwon's Pipeline owing to space constraint and the obvious performance gap compared to our PGPNet.",
        "sentences": [
          {
            "text": "Pill Classification Accuracy.",
            "label": 0
          },
          {
            "text": "Figure18cshows the correlation between the confidence and accuracy of PGPNet in the comparison with those of SGRN.",
            "label": 0
          },
          {
            "text": "Both the frameworks are based on the Faster R-CNN backbone and achieve similar results e.g., an over-confidence trend in every bin.",
            "label": 0
          },
          {
            "text": "All the predictions with confidence scores smaller than 0.2 are totally unreliable (with 0 accuracy).",
            "label": 0
          },
          {
            "text": "In addition, PGPNet also shows its superior over SGRN in some bins, in which the over-confidence situation is reduced effectively.",
            "label": 0
          },
          {
            "text": "We do not plot the Confidence-Accuracy of Kwon's Pipeline owing to space constraint and the obvious performance gap compared to our PGPNet.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 60,
        "paragraph_id": 60,
        "full_text": "In the following, we investigate the ability of PGPNet in dealing with the occlusion phenomenon caused by overlapping pills, which is one of the most critical issues in dealing with multi-pill detection.To this end, we create a so-call custom occlusion sub-dataset of VAIPE, which contains images with heavy occlusion phenomena, i.e., having at least two RoIs with the IoU beyond 30% (Fig.19).We also create a custom custom non-occlusion sub-dataset which contains samples that are in the same classes that appear in the custom occlusion sub-dataset but with no occlusion.The quantitative result is summarized in Table10.The (-) mark in the table suggests the disregarded or unavailable metrics.As the numbers suggest, even in cases where heavy occlusion occurs, PGPNet still shows its superior over Faster R-CNN.Specifically, the mAP over all classes in the custom occlusion sub-dataset suggests a gap of 8.3% between the two approaches.Interestingly, with the aid of classifier weight as the distinguishing characteristic for each class, PGPNet, even when dealing with occlusion cases still enhances the performance of 1.9% compared to Faster R-CNN handling the non-occlusion case (e,g, 67.5 vs. 65.6,respectively).Figure20provides more information about the AP for each        class in the custom occlusion sub-dataset.PGPNet still outperforms Faster R-CNN in most cases with a large gap, and also produces a more reliable result by introducing a smaller variance over the AP metrics.",
        "sentences": [
          {
            "text": "In the following, we investigate the ability of PGPNet in dealing with the occlusion phenomenon caused by overlapping pills, which is one of the most critical issues in dealing with multi-pill detection.",
            "label": 1
          },
          {
            "text": "To this end, we create a so-call custom occlusion sub-dataset of VAIPE, which contains images with heavy occlusion phenomena, i.e., having at least two RoIs with the IoU beyond 30% (Fig.19).",
            "label": 1
          },
          {
            "text": "We also create a custom custom non-occlusion sub-dataset which contains samples that are in the same classes that appear in the custom occlusion sub-dataset but with no occlusion.",
            "label": 1
          },
          {
            "text": "The quantitative result is summarized in Table10.",
            "label": 1
          },
          {
            "text": "The (-) mark in the table suggests the disregarded or unavailable metrics.",
            "label": 1
          },
          {
            "text": "As the numbers suggest, even in cases where heavy occlusion occurs, PGPNet still shows its superior over Faster R-CNN.",
            "label": 0
          },
          {
            "text": "Specifically, the mAP over all classes in the custom occlusion sub-dataset suggests a gap of 8.3% between the two approaches.",
            "label": 0
          },
          {
            "text": "Interestingly, with the aid of classifier weight as the distinguishing characteristic for each class, PGPNet, even when dealing with occlusion cases still enhances the performance of 1.9% compared to Faster R-CNN handling the non-occlusion case (e,g, 67.5 vs. 65.6,respectively).",
            "label": 0
          },
          {
            "text": "Figure20provides more information about the AP for each        class in the custom occlusion sub-dataset.",
            "label": 0
          },
          {
            "text": "PGPNet still outperforms Faster R-CNN in most cases with a large gap, and also produces a more reliable result by introducing a smaller variance over the AP metrics.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 61,
        "paragraph_id": 61,
        "full_text": "This section is dedicated to analyzing the results produced by PGPNet through a specific sample.This example demonstrates that the operation of PGPNet is very congruent with our initial motivation and that our designed architecture can materialize this motivation.",
        "sentences": [
          {
            "text": "This section is dedicated to analyzing the results produced by PGPNet through a specific sample.",
            "label": 0
          },
          {
            "text": "This example demonstrates that the operation of PGPNet is very congruent with our initial motivation and that our designed architecture can materialize this motivation.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 62,
        "paragraph_id": 62,
        "full_text": "In this experiment, we choose a hard sample, namely Hexinvon-8mg, with a relatively common appearance, for investigation.Figure21visualizes Hexinvon-8mg together with other pills in our dataset with almost identical visual appearance (round shape, white tint, etc.).As illustrated, these pills are readily confused with Hexinvon-8mg.Indeed, Fig.22depicts an example in which Hexinvon-8mg is miscategorized as Alpha-Chymotrypsine by Faster R-CNN.Our PGPNet can, however, successfully distinguish Hexinvon-8mg with a high confidence score.In the following, we applied several Explainable AI techniques to explain the results inferred by our PGPNet.The image of interest consists of three pills: LIVOLIN-FORTE, Hapenxin, and Hexivon as shown in Fig.22.",
        "sentences": [
          {
            "text": "In this experiment, we choose a hard sample, namely Hexinvon-8mg, with a relatively common appearance, for investigation.",
            "label": 0
          },
          {
            "text": "Figure21visualizes Hexinvon-8mg together with other pills in our dataset with almost identical visual appearance (round shape, white tint, etc.).",
            "label": 0
          },
          {
            "text": "As illustrated, these pills are readily confused with Hexinvon-8mg.",
            "label": 0
          },
          {
            "text": "Indeed, Fig.22depicts an example in which Hexinvon-8mg is miscategorized as Alpha-Chymotrypsine by Faster R-CNN.",
            "label": 0
          },
          {
            "text": "Our PGPNet can, however, successfully distinguish Hexinvon-8mg with a high confidence score.",
            "label": 0
          },
          {
            "text": "In the following, we applied several Explainable AI techniques to explain the results inferred by our PGPNet.",
            "label": 0
          },
          {
            "text": "The image of interest consists of three pills: LIVOLIN-FORTE, Hapenxin, and Hexivon as shown in Fig.22.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 63,
        "paragraph_id": 63,
        "full_text": "We adopt the Excitation Backpropagation technique proposed by Zhang[34]to construct the saliency maps (Fig.23), which indicate what the classifier has learned to produce the final results.Firstly, for the easy samples, i.e., LIVOLIN-FORTE and Hapenxin, our model focuses precisely on those pill regions to make the prediction decision.In contrast, in the case of the hard sample, i.e., Hexinvon-8mg, however, two regions are highlighted: one at the position of Hexinvon-8mg and the other at the location of LIVOLIN-FORTE.It indicates that the classifier solely requires information about LIVOLIN-FORTE and Hapenxin to identify these pills.Nevertheless, for Hexinvon-8mg, the classifier must additionally incorporate information about its neighbor, i.e., LIVOLIN-FORTE.This hypothesis is also supported by the Probabilistic score matrix shown in Fig.24.The probabilistic score matrix represents the prediction results generated by our Pseudo Classifier, which relies mainly on the pill's visual characteristics.As demonstrated, Pseudo Classifier can accurately detect the proper labels of two simple samples, with their prediction scores approaching 1, and boost up their neighbors' probabilities (label ID 7, 17, etc.).However, with the case of Hexinvon-8mg, the probability scores are relatively low, with all RoIs being investigated achieving scores of only about 0.3.Now, we utilize another explainable AI technique named GNNExplainer[31]to investigate further the reason for identifying the hard sample, Hexinvon-8mg.GNNExplainer is a model-agnostic architecture that can provide interpretable explanations for predictions of graph-based models.Specifically, GNNExplainer may identify a subgraph and a subset of node features that have a significant role in the prediction outcomes.In our experiment, we treat our Graph Transformer Network as a module that produces regression output, i.e., the context vectors corresponding to all RoIs.For a more comprehensible result, we set the number of RoIs selected from the RPN module to ten, consisting of the five RoIs with the greatest objectness scores and the other five with the lowest score.We utilize GNNExplainer to identify the sub-graph that contributes the most in recognizing Hexinvon-8mg.The results are demonstrated in Fig.25b.In this figure, the white box depicts the RoI of Hexinvon-8mg, the two orange boxes and blue boxes represent the RoIs of LIVOLIN-FORTE, and Hapenxin, respectively, while the five gray boxes indicate the RoIs of noise.The black edges represent the vital connections, whose weights are proportionate to the width of the edges.First, there are almost no edges between the nodes representing Hexinvon-8mg and those of the noise RoIs.It implies that the noise RoIs do not cue the prediction of Hexinvon-8mg.In contrast, there are bolded linkages between the RoIs of LIVOLIN-FORTE, Hapenxin, and Hexinvon-8mg.These findings, along with the saliency map (Fig.23), interpret that PGPNet has learned both the visual characteristic of the pill itself and the relationship between that pill and the others to make the final decision.",
        "sentences": [
          {
            "text": "We adopt the Excitation Backpropagation technique proposed by Zhang[34]to construct the saliency maps (Fig.23), which indicate what the classifier has learned to produce the final results.",
            "label": 0
          },
          {
            "text": "Firstly, for the easy samples, i.e., LIVOLIN-FORTE and Hapenxin, our model focuses precisely on those pill regions to make the prediction decision.",
            "label": 0
          },
          {
            "text": "In contrast, in the case of the hard sample, i.e., Hexinvon-8mg, however, two regions are highlighted: one at the position of Hexinvon-8mg and the other at the location of LIVOLIN-FORTE.",
            "label": 0
          },
          {
            "text": "It indicates that the classifier solely requires information about LIVOLIN-FORTE and Hapenxin to identify these pills.",
            "label": 0
          },
          {
            "text": "Nevertheless, for Hexinvon-8mg, the classifier must additionally incorporate information about its neighbor, i.e., LIVOLIN-FORTE.",
            "label": 0
          },
          {
            "text": "This hypothesis is also supported by the Probabilistic score matrix shown in Fig.24.",
            "label": 0
          },
          {
            "text": "The probabilistic score matrix represents the prediction results generated by our Pseudo Classifier, which relies mainly on the pill's visual characteristics.",
            "label": 0
          },
          {
            "text": "As demonstrated, Pseudo Classifier can accurately detect the proper labels of two simple samples, with their prediction scores approaching 1, and boost up their neighbors' probabilities (label ID 7, 17, etc.).",
            "label": 0
          },
          {
            "text": "However, with the case of Hexinvon-8mg, the probability scores are relatively low, with all RoIs being investigated achieving scores of only about 0.3.",
            "label": 0
          },
          {
            "text": "Now, we utilize another explainable AI technique named GNNExplainer[31]to investigate further the reason for identifying the hard sample, Hexinvon-8mg.",
            "label": 0
          },
          {
            "text": "GNNExplainer is a model-agnostic architecture that can provide interpretable explanations for predictions of graph-based models.",
            "label": 0
          },
          {
            "text": "Specifically, GNNExplainer may identify a subgraph and a subset of node features that have a significant role in the prediction outcomes.",
            "label": 0
          },
          {
            "text": "In our experiment, we treat our Graph Transformer Network as a module that produces regression output, i.e., the context vectors corresponding to all RoIs.",
            "label": 0
          },
          {
            "text": "For a more comprehensible result, we set the number of RoIs selected from the RPN module to ten, consisting of the five RoIs with the greatest objectness scores and the other five with the lowest score.",
            "label": 0
          },
          {
            "text": "We utilize GNNExplainer to identify the sub-graph that contributes the most in recognizing Hexinvon-8mg.",
            "label": 0
          },
          {
            "text": "The results are demonstrated in Fig.25b.",
            "label": 0
          },
          {
            "text": "In this figure, the white box depicts the RoI of Hexinvon-8mg, the two orange boxes and blue boxes represent the RoIs of LIVOLIN-FORTE, and Hapenxin, respectively, while the five gray boxes indicate the RoIs of noise.",
            "label": 0
          },
          {
            "text": "The black edges represent the vital connections, whose weights are proportionate to the width of the edges.",
            "label": 0
          },
          {
            "text": "First, there are almost no edges between the nodes representing Hexinvon-8mg and those of the noise RoIs.",
            "label": 0
          },
          {
            "text": "It implies that the noise RoIs do not cue the prediction of Hexinvon-8mg.",
            "label": 0
          },
          {
            "text": "In contrast, there are bolded linkages between the RoIs of LIVOLIN-FORTE, Hapenxin, and Hexinvon-8mg.",
            "label": 0
          },
          {
            "text": "These findings, along with the saliency map (Fig.23), interpret that PGPNet has learned both the visual characteristic of the pill itself and the relationship between that pill and the others to make the final decision.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 64,
        "paragraph_id": 64,
        "full_text": "In this section, we perform extensive ablation studies to investigate the impacts of the main techniques proposed in our PGPNet and to investigate how each component in the proposed method helps to improve learning performance.Specifically, we alter the Co-occurrence Graph and observe how it affects the detection results in Section Effect of Co-occurrence Graph's Quality.We then assess the effects of using the relational graphs, the Graph transformer network, and the proposed auxiliary loss in Sections Effects of the Relational Graphs, Effects of the Multi-modal Data Fusion Block and Auxiliary Loss, respectively.",
        "sentences": [
          {
            "text": "In this section, we perform extensive ablation studies to investigate the impacts of the main techniques proposed in our PGPNet and to investigate how each component in the proposed method helps to improve learning performance.",
            "label": 0
          },
          {
            "text": "Specifically, we alter the Co-occurrence Graph and observe how it affects the detection results in Section Effect of Co-occurrence Graph's Quality.",
            "label": 0
          },
          {
            "text": "We then assess the effects of using the relational graphs, the Graph transformer network, and the proposed auxiliary loss in Sections Effects of the Relational Graphs, Effects of the Multi-modal Data Fusion Block and Auxiliary Loss, respectively.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 65,
        "paragraph_id": 65,
        "full_text": "In this section, we perform two experiments to observe how the performance is changed when the nodes set and edges set of MCG are modified respectively.",
        "sentences": [
          {
            "text": "In this section, we perform two experiments to observe how the performance is changed when the nodes set and edges set of MCG are modified respectively.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 66,
        "paragraph_id": 66,
        "full_text": "Edge Set Modification.We first observe the behavior of our PGPNet when adding noise edges and removing actual edges.We set up four scenarios which are the combinations of removing 25% and 50% of the edges in the set E 1 , and adding a number of synthesized edges corresponding to 25% and 50% of the cardinality of E 1 .",
        "sentences": [
          {
            "text": "Edge Set Modification.",
            "label": 0
          },
          {
            "text": "We first observe the behavior of our PGPNet when adding noise edges and removing actual edges.",
            "label": 0
          },
          {
            "text": "We set up four scenarios which are the combinations of removing 25% and 50% of the edges in the set E 1 , and adding a number of synthesized edges corresponding to 25% and 50% of the cardinality of E 1 .",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 67,
        "paragraph_id": 67,
        "full_text": "Figure26illustrates the performances of PGPNet with all Medical Co-occurrence Graph variances when being put into comparison with the original one.The performance here is denoted by the general metrics AP.As indicated by AP density, PGPNet with original MCG generates a more concentrated density with a smaller variance and a higher mean than other variances.In addition, when 50% of edges are eliminated, the performance is clearly inferior to when 25% of edges are eliminated.The figure concludes with the intriguing observation that eliminating edges at random would result in a greater performance decrease than adding noisy edges.This is because, even with the addition of noisy edges, PGPNet could still filter out unnecessary information through the training process.When excluding edges, the situation is different because the framework cannot learn the external knowledge contained in the eliminated edges.",
        "sentences": [
          {
            "text": "Figure26illustrates the performances of PGPNet with all Medical Co-occurrence Graph variances when being put into comparison with the original one.",
            "label": 0
          },
          {
            "text": "The performance here is denoted by the general metrics AP.",
            "label": 0
          },
          {
            "text": "As indicated by AP density, PGPNet with original MCG generates a more concentrated density with a smaller variance and a higher mean than other variances.",
            "label": 0
          },
          {
            "text": "In addition, when 50% of edges are eliminated, the performance is clearly inferior to when 25% of edges are eliminated.",
            "label": 0
          },
          {
            "text": "The figure concludes with the intriguing observation that eliminating edges at random would result in a greater performance decrease than adding noisy edges.",
            "label": 0
          },
          {
            "text": "This is because, even with the addition of noisy edges, PGPNet could still filter out unnecessary information through the training process.",
            "label": 0
          },
          {
            "text": "When excluding edges, the situation is different because the framework cannot learn the external knowledge contained in the eliminated edges.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 68,
        "paragraph_id": 68,
        "full_text": "Node Set Modification.To observe PGPNet's performance when the Medical Co-occurrence Graph lacks information on some specific nodes -classes, we design two different scenarios.In the first one, 25% nodes are removed in the original graph, this set is denoted as N A .For the latter, 50% of nodes are eliminated, and the corresponding set N B is ensured to be a superset of N A .The performances of PGPNet in two circumstances are compared with itself when having the full MCG, considering only the classes appeared in the set N A .",
        "sentences": [
          {
            "text": "Node Set Modification.",
            "label": 0
          },
          {
            "text": "To observe PGPNet's performance when the Medical Co-occurrence Graph lacks information on some specific nodes -classes, we design two different scenarios.",
            "label": 0
          },
          {
            "text": "In the first one, 25% nodes are removed in the original graph, this set is denoted as N A .",
            "label": 0
          },
          {
            "text": "For the latter, 50% of nodes are eliminated, and the corresponding set N B is ensured to be a superset of N A .",
            "label": 0
          },
          {
            "text": "The performances of PGPNet in two circumstances are compared with itself when having the full MCG, considering only the classes appeared in the set N A .",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 69,
        "paragraph_id": 69,
        "full_text": "Figure27depicts the outcome of this experiment.The AP across all N A classes is used to evaluate performance here.As indicated by the graph, node removals also result in a significant decrease in model performance.More interestingly, the more nodes being eliminated, the greater drop is captured.Specifically, the AP density in case MCG contains only 50% of remaining nodes has a great variance, with the mean value only around 60%.",
        "sentences": [
          {
            "text": "Figure27depicts the outcome of this experiment.",
            "label": 0
          },
          {
            "text": "The AP across all N A classes is used to evaluate performance here.",
            "label": 0
          },
          {
            "text": "As indicated by the graph, node removals also result in a significant decrease in model performance.",
            "label": 0
          },
          {
            "text": "More interestingly, the more nodes being eliminated, the greater drop is captured.",
            "label": 0
          },
          {
            "text": "Specifically, the AP density in case MCG contains only 50% of remaining nodes has a great variance, with the mean value only around 60%.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 70,
        "paragraph_id": 70,
        "full_text": "In the following, we study the effectiveness of the relational graphs, Graph Transformer Network (GTN) block, and auxiliary loss.The detailed configurations are presented in Table11.The + sign indicates the presence of a component in a specific version, whiledenotes the opposite.",
        "sentences": [
          {
            "text": "In the following, we study the effectiveness of the relational graphs, Graph Transformer Network (GTN) block, and auxiliary loss.",
            "label": 0
          },
          {
            "text": "The detailed configurations are presented in Table11.",
            "label": 0
          },
          {
            "text": "The + sign indicates the presence of a component in a specific version, whiledenotes the opposite.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 71,
        "paragraph_id": 71,
        "full_text": "In this section, we study the effectiveness of the Size-graph and visual-based graph.To this end, we implement two simplified versions of PGPNet, namely PGPNet-v2 and PGPNet-v3, in which we remove the Size-graph and visual-based graph, respectively.As shown in Table11, eliminating the Size-graph causes a decrease in performance from 3.9% to 11.1%, while omitting the visual-based graph reduces the accuracy from 2.8% to 8.3%.An interesting finding is that the deterioration gap when removing the size graph is more significant than those when eliminating the visual-based graph in terms of all evaluation metrics.These findings imply the effectiveness of the Size-graph over the visual-based graph.Moreover, it can be observed that mAP is the most impacted when the relational graphs are removed, followed by AP50, when comparing mAP, AP50, and AP75.This can be explained as follows.In AP75, we measure the precision of RoIs with the IoU beyond 75%, which presumably has a high degree of confidence regarding the objective.In contrast, when we reduce the IoU threshold, such as AP50 and mAP, the overlap area of the objective drops, resulting in a model with a significant degree of uncertainty.In this case, integrating relational graphs provides additional data that reduces uncertainty, thereby boosting detection accuracy.",
        "sentences": [
          {
            "text": "In this section, we study the effectiveness of the Size-graph and visual-based graph.",
            "label": 0
          },
          {
            "text": "To this end, we implement two simplified versions of PGPNet, namely PGPNet-v2 and PGPNet-v3, in which we remove the Size-graph and visual-based graph, respectively.",
            "label": 0
          },
          {
            "text": "As shown in Table11, eliminating the Size-graph causes a decrease in performance from 3.9% to 11.1%, while omitting the visual-based graph reduces the accuracy from 2.8% to 8.3%.",
            "label": 0
          },
          {
            "text": "An interesting finding is that the deterioration gap when removing the size graph is more significant than those when eliminating the visual-based graph in terms of all evaluation metrics.",
            "label": 0
          },
          {
            "text": "These findings imply the effectiveness of the Size-graph over the visual-based graph.",
            "label": 0
          },
          {
            "text": "Moreover, it can be observed that mAP is the most impacted when the relational graphs are removed, followed by AP50, when comparing mAP, AP50, and AP75.",
            "label": 0
          },
          {
            "text": "This can be explained as follows.",
            "label": 0
          },
          {
            "text": "In AP75, we measure the precision of RoIs with the IoU beyond 75%, which presumably has a high degree of confidence regarding the objective.",
            "label": 0
          },
          {
            "text": "In contrast, when we reduce the IoU threshold, such as AP50 and mAP, the overlap area of the objective drops, resulting in a model with a significant degree of uncertainty.",
            "label": 0
          },
          {
            "text": "In this case, integrating relational graphs provides additional data that reduces uncertainty, thereby boosting detection accuracy.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 72,
        "paragraph_id": 72,
        "full_text": "To investigate the effectiveness of the GTN, we implement PGPNet-v4, omitting the GTN block and relying solely on the GCN to learn the node representation.Results in Table11reveal that GTN enhances the model's accuracy from 1.0% to 11.1%.Comparing mAP, AP50, and AP75, AP50, and AP75 are slightly more influenced by GTN than mAP, but the gaps are trivial.We employ PGPNet-v5, which eliminates the proposed auxiliary loss and compare its performance with the original PGPNet.As illustrated in Table11, adopting our auxiliary loss may result in a 3 to 4 percent performance gain for most evaluation metrics.In the final ablation study, we implement PGPNet-v1, which retains only the co-occurrence graph and removes all the other components.As depicted in Table11, the detection accuracy degrades significantly, with a gap ranging from 2.9% to 19.4%.However, even with this version, PGPNet is still superior to Faster RCNN, with a performance margin of up to 7.1%.",
        "sentences": [
          {
            "text": "To investigate the effectiveness of the GTN, we implement PGPNet-v4, omitting the GTN block and relying solely on the GCN to learn the node representation.",
            "label": 0
          },
          {
            "text": "Results in Table11reveal that GTN enhances the model's accuracy from 1.0% to 11.1%.",
            "label": 0
          },
          {
            "text": "Comparing mAP, AP50, and AP75, AP50, and AP75 are slightly more influenced by GTN than mAP, but the gaps are trivial.We employ PGPNet-v5, which eliminates the proposed auxiliary loss and compare its performance with the original PGPNet.",
            "label": 0
          },
          {
            "text": "As illustrated in Table11, adopting our auxiliary loss may result in a 3 to 4 percent performance gain for most evaluation metrics.",
            "label": 0
          },
          {
            "text": "In the final ablation study, we implement PGPNet-v1, which retains only the co-occurrence graph and removes all the other components.",
            "label": 0
          },
          {
            "text": "As depicted in Table11, the detection accuracy degrades significantly, with a gap ranging from 2.9% to 19.4%.",
            "label": 0
          },
          {
            "text": "However, even with this version, PGPNet is still superior to Faster RCNN, with a performance margin of up to 7.1%.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "HIGH ACCURATE AND EXPLAINABLE MULTI_PILL DETECTION FRAMEWORK WITH GRAPH NEURAL NETWORK_ASSISTED MULTIMODAL DATA FUSION",
        "section": 73,
        "paragraph_id": 73,
        "full_text": "In conclusion, the PGPNet version with all components exhibits its superiority in all evaluation metrics.In addition, all versions of PGPNet are superior to the Faster R-CNN backbone, demonstrating the contribution of each component to the overall performance of PGPNet.",
        "sentences": [
          {
            "text": "In conclusion, the PGPNet version with all components exhibits its superiority in all evaluation metrics.",
            "label": 0
          },
          {
            "text": "In addition, all versions of PGPNet are superior to the Faster R-CNN backbone, demonstrating the contribution of each component to the overall performance of PGPNet.",
            "label": 0
          }
        ]
      },
      {
        "paper_name": "Decoding the End_to_end Writing Trajectory in Scholarly Manuscripts",
        "section": 0,
        "paragraph_id": 0,
        "full_text": "Abstract: Scholarly writing presents a complex space that generally follows a methodical procedure to plan and produce both rationally sound and creative compositions.Recent works involving large language models (LLM) demonstrate considerable success in text generation and revision tasks; however, LLMs still struggle to provide structural and creative feedback on the document level that is crucial to academic writing.In this paper, we introduce a novel taxonomy that categorizes scholarly writing behaviors according to intention, writer actions, and the information types of the written data.We also provide ManuScript, an original dataset annotated with a simplified version of our taxonomy to show writer actions and the intentions behind them.Motivated by cognitive writing theory, our taxonomy for scientific papers includes three levels of categorization in order to trace the general writing flow and identify the distinct writer activities embedded within each higher-level process.ManuScript intends to provide a complete picture of the scholarly writing process by capturing the linearity and non-linearity of writing trajectory, such that writing assistants can provide stronger feedback and suggestions on an end-to-end level.The collected writing trajectories are viewed at https://minnesotanlp.github.io/REWARD_demo/ 1.",
        "sentences": [
            {
                "text": "Abstract: Scholarly writing presents a complex space that generally follows a methodical procedure to plan and produce both rationally sound and creative compositions.",
                "label": 0
            },
            {
                "text": "Recent works involving large language models (LLM) demonstrate considerable success in text generation and revision tasks; however, LLMs still struggle to provide structural and creative feedback on the document level that is crucial to academic writing.",
                "label": 0
            },
            {
                "text": "In this paper, we introduce a novel taxonomy that categorizes scholarly writing behaviors according to intention, writer actions, and the information types of the written data.",
                "label": 0
            },
            {
                "text": "We also provide ManuScript, an original dataset annotated with a simplified version of our taxonomy to show writer actions and the intentions behind them.",
                "label": 1
            },
            {
                "text": "Motivated by cognitive writing theory, our taxonomy for scientific papers includes three levels of categorization in order to trace the general writing flow and identify the distinct writer activities embedded within each higher-level process.",
                "label": 0
            },
            {
                "text": "ManuScript intends to provide a complete picture of the scholarly writing process by capturing the linearity and non-linearity of writing trajectory, such that writing assistants can provide stronger feedback and suggestions on an end-to-end level.",
                "label": 1
            },
            {
                "text": "The collected writing trajectories are viewed at https://minnesotanlp.github.io/REWARD_demo/ 1.",
                "label": 1
            }                
        ]
    },
    {
        "paper_name": "Decoding the End_to_end Writing Trajectory in Scholarly Manuscripts",
        "section": 1,
        "paragraph_id": 1,
        "full_text": "• Human-centered computing → Human computer interaction (HCI); • Applied computing → Document analysis; • Software and its engineering → Software creation and management.",
        "sentences": [
            {
                "text": "• Human-centered computing → Human computer interaction (HCI); • Applied computing → Document analysis; • Software and its engineering → Software creation and management.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Decoding the End_to_end Writing Trajectory in Scholarly Manuscripts",
        "section": 2,
        "paragraph_id": 2,
        "full_text": "Writing is a cognitively active task involving continuous decisionmaking, heavy use of working memory, and frequent switching between multiple activities.Scholarly writing is particularly complex as it requires the author to coordinate many pieces of multiform information while also meeting the high standards of academic communication.Flower and Hayes'[6]cognitive process theory of writing organizes these tasks into three processes: planning, during which the writer generates and organizes ideas and sets writing goals; translation, during which the writer implements their plan, keeping in mind the organization of the text as well as word choice and phrasing; and reviewing, during which the writer evaluates and revises their text.Flower and Hayes emphasize that these distinct phases are non-linear and highly embedded, meaning that any process or sub-process can be embedded within any other process and move back and forth between each process.In order to provide relevant feedback at each step of the academic writing process, it is critical for writing assistants to have a strong understanding of the planning, translation, and revision stages throughout their entirety.",
        "sentences": [
            {
                "text": "Writing is a cognitively active task involving continuous decisionmaking, heavy use of working memory, and frequent switching between multiple activities.",
                "label": 0
            },
            {
                "text": "Scholarly writing is particularly complex as it requires the author to coordinate many pieces of multiform information while also meeting the high standards of academic communication.",
                "label": 0
            },
            {
                "text": "Flower and Hayes'[6]cognitive process theory of writing organizes these tasks into three processes: planning, during which the writer generates and organizes ideas and sets writing goals; translation, during which the writer implements their plan, keeping in mind the organization of the text as well as word choice and phrasing; and reviewing, during which the writer evaluates and revises their text.",
                "label": 0
            },
            {
                "text": "Flower and Hayes emphasize that these distinct phases are non-linear and highly embedded, meaning that any process or sub-process can be embedded within any other process and move back and forth between each process.",
                "label": 0
            },
            {
                "text": "In order to provide relevant feedback at each step of the academic writing process, it is critical for writing assistants to have a strong understanding of the planning, translation, and revision stages throughout their entirety.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Decoding the End_to_end Writing Trajectory in Scholarly Manuscripts",
        "section": 3,
        "paragraph_id": 3,
        "full_text": "Recent corpora for the study of writing processes exist for each of these sub-processes.Berdanier[2]demystifies the academic writing process in a study showing the \"linguistic scheme\" involving a distinct planning and crafting procedure typically followed within technical writing.Furthermore, much work has been done to study text revision using keystroke data[1,3,13], and revision history[4,5,7,11,12].More recently, Sardo et al.[9]have developed a corpus and a metric for edit-complexity that draws a complex topological structure of the writer's efforts throughout the history of the essay to study the planning and translation processes.Despite recent advancements in large language models, particularly text generation, LLMs still exhibit subpar performance for reasoning capabilities and particularly planning[10]to have any significant impact in aiding the writing process[9].Our work builds upon these previous studies to provide a dataset with annotations encompassing the writing process spanning across all three stages, as described by Flower and Hayes.",
        "sentences": [
            {
                "text": "Recent corpora for the study of writing processes exist for each of these sub-processes.",
                "label": 0
            },
            {
                "text": "Berdanier[2]demystifies the academic writing process in a study showing the \"linguistic scheme\" involving a distinct planning and crafting procedure typically followed within technical writing.",
                "label": 0
            },
            {
                "text": "Furthermore, much work has been done to study text revision using keystroke data[1,3,13], and revision history[4,5,7,11,12].",
                "label": 0
            },
            {
                "text": "More recently, Sardo et al.[9]have developed a corpus and a metric for edit-complexity that draws a complex topological structure of the writer's efforts throughout the history of the essay to study the planning and translation processes.",
                "label": 0
            },
            {
                "text": "Despite recent advancements in large language models, particularly text generation, LLMs still exhibit subpar performance for reasoning capabilities and particularly planning[10]to have any significant impact in aiding the writing process[9].",
                "label": 0
            },
            {
                "text": "Our work builds upon these previous studies to provide a dataset with annotations encompassing the writing process spanning across all three stages, as described by Flower and Hayes.",
                "label": 1
            }
        ]
    },
    {
        "paper_name": "Decoding the End_to_end Writing Trajectory in Scholarly Manuscripts",
        "section": 4,
        "paragraph_id": 4,
        "full_text": "Our contributions include ManuScript, a small dataset of scholarly writing actions, and a comprehensive taxonomy of writing processes that are applicable across various academic disciplines.ManuScript is annotated following a simplified version of our taxonomy to capture the end-to-end writing process.Our work is motivated by the idea that providing writing assistants detailed information about the writing process will help them give more appropriate suggestions to writers throughout the writing process.Applying this taxonomy to a dataset of academic writing samples give us insight into the academic writing process and provide us with data to support the generation of suggestions that align with the writer's current activity and intention.In the future, we plan to extend this work by scaling the data collection process over a longer period of time to develop a more nuanced taxonomy of writers' actions and intentions.",
        "sentences": [
            {
                "text": "Our contributions include ManuScript, a small dataset of scholarly writing actions, and a comprehensive taxonomy of writing processes that are applicable across various academic disciplines.",
                "label": 1
            },
            {
                "text": "ManuScript is annotated following a simplified version of our taxonomy to capture the end-to-end writing process.",
                "label": 1
            },
            {
                "text": "Our work is motivated by the idea that providing writing assistants detailed information about the writing process will help them give more appropriate suggestions to writers throughout the writing process.",
                "label": 1
            },
            {
                "text": "Applying this taxonomy to a dataset of academic writing samples give us insight into the academic writing process and provide us with data to support the generation of suggestions that align with the writer's current activity and intention.",
                "label": 1
            },
            {
                "text": "In the future, we plan to extend this work by scaling the data collection process over a longer period of time to develop a more nuanced taxonomy of writers' actions and intentions.",
                "label": 1
            }
        ]
    },
    {
        "paper_name": "Decoding the End_to_end Writing Trajectory in Scholarly Manuscripts",
        "section": 5,
        "paragraph_id": 5,
        "full_text": "Analyzing a final manuscript alone is intractable for capturing an author's original intentions.We have developed a taxonomy of scholarly writing trajectories illustrated by Figure2that can characterize the finer-grained actions an author takes into distinct categories but is also general enough to fully capture the author's trajectory throughout the entire writing process.The highest level of our taxonomy describes the intention informing the writer's actions, and is based on the three main processes described by Flower and Hayes[6].The middle layer describes the various writing actions that might take place to carry out the writer's intention.Each intention is associated with its own set of actions.For example, while the author is revising their work, they may be making substantive, formal, or stylistic revisions.The lowest level describes the linguistic or LaTeX unit that they are currently operating on.For example, if the writer is drafting and moving around paragraph topic sentences within a new section of their paper, their spans of keystrokes would alternate between Planning → Generation → Section and Planning → Organization → Section because they are working at the section level and switching between generating new ideas and organizing them.",
        "sentences": [
            {
                "text": "Analyzing a final manuscript alone is intractable for capturing an author's original intentions.",
                "label": 0
            },
            {
                "text": "We have developed a taxonomy of scholarly writing trajectories illustrated by Figure2that can characterize the finer-grained actions an author takes into distinct categories but is also general enough to fully capture the author's trajectory throughout the entire writing process.",
                "label": 0
            },
            {
                "text": "The highest level of our taxonomy describes the intention informing the writer's actions, and is based on the three main processes described by Flower and Hayes[6].",
                "label": 0
            },
            {
                "text": "The middle layer describes the various writing actions that might take place to carry out the writer's intention.",
                "label": 0
            },
            {
                "text": "Each intention is associated with its own set of actions.",
                "label": 0
            },
            {
                "text": "For example, while the author is revising their work, they may be making substantive, formal, or stylistic revisions.",
                "label": 0
            },
            {
                "text": "The lowest level describes the linguistic or LaTeX unit that they are currently operating on.",
                "label": 0
            },
            {
                "text": "For example, if the writer is drafting and moving around paragraph topic sentences within a new section of their paper, their spans of keystrokes would alternate between Planning → Generation → Section and Planning → Organization → Section because they are working at the section level and switching between generating new ideas and organizing them.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Decoding the End_to_end Writing Trajectory in Scholarly Manuscripts",
        "section": 6,
        "paragraph_id": 6,
        "full_text": "Data Collection.We developed a chrome extension that reverse engineers Overleaf's editing history utilizing user keystrokes to track writing actions in real-time (See details in Appendix A).From this, we can generate a playback that shows the chronological progression for each completed writing session.Our initial study involved four participants in a pilot study where they were prompted to describe their current or future research plans by responding to the available prompts or in free form over a thirty-minute writing session.",
        "sentences": [
            {
                "text": "Data Collection.",
                "label": 1
            },
            {
                "text": "We developed a chrome extension that reverse engineers Overleaf's editing history utilizing user keystrokes to track writing actions in real-time (See details in Appendix A).",
                "label": 1
            },
            {
                "text": "From this, we can generate a playback that shows the chronological progression for each completed writing session.",
                "label": 1
            },
            {
                "text": "Our initial study involved four participants in a pilot study where they were prompted to describe their current or future research plans by responding to the available prompts or in free form over a thirty-minute writing session.",
                "label": 1
            }
        ]
    },
    {
        "paper_name": "Decoding the End_to_end Writing Trajectory in Scholarly Manuscripts",
        "section": 7,
        "paragraph_id": 7,
        "full_text": "Planning The writer's intention is to get their ideas down on paper in a semi-structured manner.generation The process of adding ideas to the document.organization Structuring the generated concepts.",
        "sentences": [
            {
                "text": "Planning The writer's intention is to get their ideas down on paper in a semi-structured manner.generation The process of adding ideas to the document.organization Structuring the generated concepts.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Decoding the End_to_end Writing Trajectory in Scholarly Manuscripts",
        "section": 8,
        "paragraph_id": 8,
        "full_text": "Implementation The writer's intention is to produce highquality and persuasive text that meets their writing goals.",
        "sentences": [
            {
                "text": "Implementation The writer's intention is to produce highquality and persuasive text that meets their writing goals.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Decoding the End_to_end Writing Trajectory in Scholarly Manuscripts",
        "section": 9,
        "paragraph_id": 9,
        "full_text": "Writing coherent text where sentences are linked by the semantic relationships between words[8].",
        "sentences": [
            {
                "text": "Writing coherent text where sentences are linked by the semantic relationships between words[8].",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Decoding the End_to_end Writing Trajectory in Scholarly Manuscripts",
        "section": 10,
        "paragraph_id": 10,
        "full_text": "Revision The writer's intention is to improve the clarity, consistency, coherence, and style of the written text.syntactic Fixing grammar, spelling, and punctuation.lexical Changing words to clarify meaning or improve coherence.structural Reordering text to improve organization. Table1: Simplified annotation schema applied to our dataset.",
        "sentences": [
            {
                "text": "Revision The writer's intention is to improve the clarity, consistency, coherence, and style of the written text.syntactic Fixing grammar, spelling, and punctuation.lexical Changing words to clarify meaning or improve coherence.structural Reordering text to improve organization. Table1: Simplified annotation schema applied to our dataset.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Decoding the End_to_end Writing Trajectory in Scholarly Manuscripts",
        "section": 11,
        "paragraph_id": 11,
        "full_text": "In total, we collected four writing trajectories, including 46 discontinuous edits with 3290 recorded actions.The detailed statistics are in Appendix C. Annotation Schema.Due to the limited scope of our pilot study, we applied a reduced annotation schema, containing two levels of granularity (Table1).The higher level includes Planning, Implementation, and Revision.These labels are used to denote the general process that the writer is working in.The lower level categorizations include {idea generation, concept organization}, {lexical_chaining}, and {syntactic, lexical, structural} for each of the three processes respectively.Presently, the category of Implementation is limited in that the only sub-category is lexical_chaining.We hope to learn more about the Implementation process during our next study.",
        "sentences": [
            {
                "text": "In total, we collected four writing trajectories, including 46 discontinuous edits with 3290 recorded actions.",
                "label": 1
            },
            {
                "text": "The detailed statistics are in Appendix C.",
                "label": 1
            },
            {
                "text": "Annotation Schema.",
                "label": 0
            },
            {
                "text": "Due to the limited scope of our pilot study, we applied a reduced annotation schema, containing two levels of granularity (Table1).",
                "label": 0
            },
            {
                "text": "The higher level includes Planning, Implementation, and Revision.",
                "label": 0
            },
            {
                "text": "These labels are used to denote the general process that the writer is working in.",
                "label": 0
            },
            {
                "text": "The lower level categorizations include {idea generation, concept organization}, {lexical_chaining}, and {syntactic, lexical, structural} for each of the three processes respectively.",
                "label": 0
            },
            {
                "text": "Presently, the category of Implementation is limited in that the only sub-category is lexical_chaining.",
                "label": 0
            },
            {
                "text": "We hope to learn more about the Implementation process during our next study.",
                "label": 0
            }
        ]
      },
      {
        "paper_name": "Larger Probes Tell a Different Story_ Extending Psycholinguistic Datasets Via In_Context Learning",
        "section": 0,
        "paragraph_id": 0,
        "full_text": "Abstract: Language model probing is often used to test specific capabilities of models.However, conclusions from such studies may be limited when the probing benchmarks are small and lack statistical power.In this work, we introduce new, larger datasets for negation (NEG-1500-SIMP) and role reversal (ROLE-1500) inspired by psycholinguistic studies.We dramatically extend existing NEG-136 and ROLE-88 benchmarks using GPT3, increasing their size from 18 and 44 sentence pairs to 750 each.We also create another version of extended negation dataset (NEG-1500-SIMP-TEMP), created using template-based generation.It consists of 770 sentence pairs.We evaluate 22 models on the extended datasets, seeing model performance dip 20-57% compared to the original smaller benchmarks.We observe high levels of negation sensitivity in models like BERT and ALBERT demonstrating that previous findings might have been skewed due to smaller test sets.Finally, we observe that while GPT3 has generated all the examples in ROLE-1500 is only able to solve 24.6% of them during probing.The datasets and code are available on Github 1 .",
        "sentences": [
            {
                "text": "Abstract: Language model probing is often used to test specific capabilities of models.",
                "label": 0
            },
            {
                "text": "However, conclusions from such studies may be limited when the probing benchmarks are small and lack statistical power.",
                "label": 0
            },
            {
                "text": "In this work, we introduce new, larger datasets for negation (NEG-1500-SIMP) and role reversal (ROLE-1500) inspired by psycholinguistic studies.",
                "label": 1
            },
            {
                "text": "We dramatically extend existing NEG-136 and ROLE-88 benchmarks using GPT3, increasing their size from 18 and 44 sentence pairs to 750 each.",
                "label": 1
            },
            {
                "text": "We also create another version of extended negation dataset (NEG-1500-SIMP-TEMP), created using template-based generation.",
                "label": 1
            },
            {
                "text": "It consists of 770 sentence pairs.",
                "label": 1
            },
            {
                "text": "We evaluate 22 models on the extended datasets, seeing model performance dip 20-57% compared to the original smaller benchmarks.",
                "label": 0
            },
            {
                "text": "We observe high levels of negation sensitivity in models like BERT and ALBERT demonstrating that previous findings might have been skewed due to smaller test sets.",
                "label": 0
            },
            {
                "text": "Finally, we observe that while GPT3 has generated all the examples in ROLE-1500 is only able to solve 24.6% of them during probing.",
                "label": 0
            },
            {
                "text": "The datasets and code are available on Github 1 .",
                "label": 1
            }
        ]
    },
    {
        "paper_name": "Larger Probes Tell a Different Story_ Extending Psycholinguistic Datasets Via In_Context Learning",
        "section": 1,
        "paragraph_id": 1,
        "full_text": "Understanding the limitations of large language models (LLMs) becomes ever more important with their accelerated adoption and application to reallife tasks.After the original discovery that large LMs could perform simple NLP tasks without additional training(Radford et al., 2019), the use of these models has rapidly grown, as have their capabilities(Brown et al., 2020;Sanh et al., 2021;Chowdhery et al., 2022).As the community invests considerable effort into creating, training, and deploying these models(Zhang et al., 2022;Black et al., 2021), it is important to understand the types of data and tasks they might not be well-suited.",
        "sentences": [
            {
                "text": "Understanding the limitations of large language models (LLMs) becomes ever more important with their accelerated adoption and application to reallife tasks.",
                "label": 0
            },
            {
                "text": "After the original discovery that large LMs could perform simple NLP tasks without additional training(Radford et al., 2019), the use of these models has rapidly grown, as have their capabilities(Brown et al., 2020;Sanh et al., 2021;Chowdhery et al., 2022).",
                "label": 0
            },
            {
                "text": "As the community invests considerable effort into creating, training, and deploying these models(Zhang et al., 2022;Black et al., 2021), it is important to understand the types of data and tasks they might not be well-suited.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Larger Probes Tell a Different Story_ Extending Psycholinguistic Datasets Via In_Context Learning",
        "section": 2,
        "paragraph_id": 2,
        "full_text": "The field of analysis of pre-trained models has grown rapidly in recent years(Zagoury et al., 2021;Liu et al., 2021;Lialin et al., 2022;bench authors, 2023;Rogers et al., 2020).Methods such as attention pattern analysis(Kovaleva et al., 2019;Kobayashi et al., 2020), linear probing(Tenney et al., 2019), and zero-shot probing(Belinkov et al., 2020;Talmor et al., 2019;Ettinger, 2019;Lialin et al., 2022)allow us to evaluate specific capabilities of pre-trained models.Zero-shot methods give us arguably the most clear picture, as they directly probe what the model learned through the upstream task and allow the researcher to target very specific skills such as understanding of negation or role.",
        "sentences": [
            {
                "text": "The field of analysis of pre-trained models has grown rapidly in recent years(Zagoury et al., 2021;Liu et al., 2021;Lialin et al., 2022;bench authors, 2023;Rogers et al., 2020).",
                "label": 0
            },
            {
                "text": "Methods such as attention pattern analysis(Kovaleva et al., 2019;Kobayashi et al., 2020), linear probing(Tenney et al., 2019), and zero-shot probing(Belinkov et al., 2020;Talmor et al., 2019;Ettinger, 2019;Lialin et al., 2022)allow us to evaluate specific capabilities of pre-trained models.",
                "label": 0
            },
            {
                "text": "Zero-shot methods give us arguably the most clear picture, as they directly probe what the model learned through the upstream task and allow the researcher to target very specific skills such as understanding of negation or role.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Larger Probes Tell a Different Story_ Extending Psycholinguistic Datasets Via In_Context Learning",
        "section": 3,
        "paragraph_id": 3,
        "full_text": "However, even though these methods do not require training data, producing a good dataset for zero-shot evaluation of these language models is not an easy task.We want these datasets to be clean, diverse and to have enough statistical power to be useful for model comparison(Card et al., 2020).Many existing probing datasets struggle with at least one of these requirements.",
        "sentences": [
            {
                "text": "However, even though these methods do not require training data, producing a good dataset for zero-shot evaluation of these language models is not an easy task.",
                "label": 0
            },
            {
                "text": "We want these datasets to be clean, diverse and to have enough statistical power to be useful for model comparison(Card et al., 2020).",
                "label": 0
            },
            {
                "text": "Many existing probing datasets struggle with at least one of these requirements.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Larger Probes Tell a Different Story_ Extending Psycholinguistic Datasets Via In_Context Learning",
        "section": 4,
        "paragraph_id": 4,
        "full_text": "Psycholinguistic datasets used in a study byEttinger (2019)have been particularly interesting in that they enabled a comparison between model behavior and human response, including both N400 effects and well-reasoned cloze judgments by human speakers.Despite being used in multiple studies since(Lialin et al., 2022;Rogers et al., 2020;Zhang et al., 2020), these datasets are quite small, ranging in size from 18 sentence pairs in negation (NEG-136-SIMP) to a maximum of 44 sentence pairs in the role-reversal dataset .",
        "sentences": [
            {
                "text": "Psycholinguistic datasets used in a study byEttinger (2019)have been particularly interesting in that they enabled a comparison between model behavior and human response, including both N400 effects and well-reasoned cloze judgments by human speakers.",
                "label": 0
            },
            {
                "text": "Despite being used in multiple studies since(Lialin et al., 2022;Rogers et al., 2020;Zhang et al., 2020), these datasets are quite small, ranging in size from 18 sentence pairs in negation (NEG-136-SIMP) to a maximum of 44 sentence pairs in the role-reversal dataset .",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Larger Probes Tell a Different Story_ Extending Psycholinguistic Datasets Via In_Context Learning",
        "section": 5,
        "paragraph_id": 5,
        "full_text": "In our work, the NEG-136-SIMP and ROLE-88 datasets are dramatically extended.For each of them, we follow the original dataset collection method(Fischler et al., 1983;Chow et al., 2016), with the exception of N400 amplitude (requires electroencephalography) and cloze probability studies on humans(Federmeier and Kutas, 1999).To explore different approaches to data ex-tension, we extended negation dataset using two methods 1) a template-based and 2) using GPT3 2 with human-in-the-loop.For ROLE, we only used GPT3 to extend the dataset, as no additional role categories were available in original source paper for this data(Chow et al., 2016).",
        "sentences": [
            {
                "text": "In our work, the NEG-136-SIMP and ROLE-88 datasets are dramatically extended.",
                "label": 1
            },
            {
                "text": "For each of them, we follow the original dataset collection method(Fischler et al., 1983;Chow et al., 2016), with the exception of N400 amplitude (requires electroencephalography) and cloze probability studies on humans(Federmeier and Kutas, 1999).",
                "label": 1
            },
            {
                "text": "To explore different approaches to data ex-tension, we extended negation dataset using two methods 1) a template-based and 2) using GPT3 2 with human-in-the-loop.",
                "label": 1
            },
            {
                "text": "For ROLE, we only used GPT3 to extend the dataset, as no additional role categories were available in original source paper for this data(Chow et al., 2016).",
                "label": 1
            }
        ]
    },
    {
        "paper_name": "Larger Probes Tell a Different Story_ Extending Psycholinguistic Datasets Via In_Context Learning",
        "section": 6,
        "paragraph_id": 6,
        "full_text": "To understand how well language models perform on these extended datasets, we evaluated 22 models, including GPT3, following the methodology fromLialin et al. (2022).Compared to the original test sets, we see a significant drop (up to 57%) in accuracy for both Role and Negation tasks.At the same time most models show higher sensitivity to negation compared to the original dataset.Finally, while GPT3 has generated all of the examples in ROLE-1500, it is only able to predict the correct answer in 24.6% cases (top-5 accuracy).ALBERT-v2-xxlarge surpasses GPT3 by 4.6%.",
        "sentences": [
            {
                "text": "To understand how well language models perform on these extended datasets, we evaluated 22 models, including GPT3, following the methodology fromLialin et al. (2022).",
                "label": 0
            },
            {
                "text": "Compared to the original test sets, we see a significant drop (up to 57%) in accuracy for both Role and Negation tasks.",
                "label": 0
            },
            {
                "text": "At the same time most models show higher sensitivity to negation compared to the original dataset.",
                "label": 0
            },
            {
                "text": "Finally, while GPT3 has generated all of the examples in ROLE-1500, it is only able to predict the correct answer in 24.6% cases (top-5 accuracy).",
                "label": 0
            },
            {
                "text": "ALBERT-v2-xxlarge surpasses GPT3 by 4.6%.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Larger Probes Tell a Different Story_ Extending Psycholinguistic Datasets Via In_Context Learning",
        "section": 7,
        "paragraph_id": 7,
        "full_text": "The existing negation dataset NEG-136-SIMP from Ettinger (2019) consists of 18 sentence pairs.Each pair is made of one affirmative and one negated 2 We use text-davinci-002 version of GPT3 sentence e.g. the sentences \"A robin is a bird\" (affirmative) and \"A robin is not a tree\" (negated) form a pair.We extend this dataset using two methods: template-based and generation-based.By employing both methods, we could gauge the potential strengths and limitations inherent to each method, which we discuss in Section 5. We create 770 sentence pairs with the template-based method and generate 750 pairs using GPT3.We refer to these datasets as NEG-1500-SIMP-TEMP and NEG-1500-SIMP-GEN, respectively.",
        "sentences": [
            {
                "text": "The existing negation dataset NEG-136-SIMP from Ettinger (2019) consists of 18 sentence pairs.",
                "label": 1
            },
            {
                "text": "Each pair is made of one affirmative and one negated 2 We use text-davinci-002 version of GPT3 sentence e.g. the sentences \"A robin is a bird\" (affirmative) and \"A robin is not a tree\" (negated) form a pair.",
                "label": 1
            },
            {
                "text": "We extend this dataset using two methods: template-based and generation-based.",
                "label": 1
            },
            {
                "text": "By employing both methods, we could gauge the potential strengths and limitations inherent to each method, which we discuss in Section 5.",
                "label": 1
            },
            {
                "text": "We create 770 sentence pairs with the template-based method and generate 750 pairs using GPT3.",
                "label": 1
            },
            {
                "text": "We refer to these datasets as NEG-1500-SIMP-TEMP and NEG-1500-SIMP-GEN, respectively.",
                "label": 1
            }
        ]
    },
    {
        "paper_name": "Larger Probes Tell a Different Story_ Extending Psycholinguistic Datasets Via In_Context Learning",
        "section": 8,
        "paragraph_id": 8,
        "full_text": "NEG-1500-SIMP-TEMP Each pair in the original dataset follows a template.For affirmative sentences, the template is \"A/An {subject} is a/an {object}\".Its negated version is \"A/An {subject} is not a/an {another object}\".Battig and Montague (1969)andFischler et al. (1983)provide a template and a list of objects (e.g., robin, pine) and the corresponding categories (e.g., bird, tree) which are used to generate samples3.For instance, the category, \"bird\" in the example, \"A robin is a bird\" is replaced with another category (\"tree\") to generate the negated example \"A robin is not a tree\".",
        "sentences": [
            {
                "text": "NEG-1500-SIMP-TEMP Each pair in the original dataset follows a template.",
                "label": 1
            },
            {
                "text": "For affirmative sentences, the template is \"A/An {subject} is a/an {object}\".",
                "label": 1
            },
            {
                "text": "Its negated version is \"A/An {subject} is not a/an {another object}\".",
                "label": 1
            },
            {
                "text": "Battig and Montague (1969)andFischler et al. (1983)provide a template and a list of objects (e.g., robin, pine) and the corresponding categories (e.g., bird, tree) which are used to generate samples3.",
                "label": 1
            },
            {
                "text": "For instance, the category, \"bird\" in the example, \"A robin is a bird\" is replaced with another category (\"tree\") to generate the negated example \"A robin is not a tree\".",
                "label": 1
            }
        ]
    },
    {
        "paper_name": "Larger Probes Tell a Different Story_ Extending Psycholinguistic Datasets Via In_Context Learning",
        "section": 9,
        "paragraph_id": 9,
        "full_text": "We experimented with using WordNet to generate from this template, but found that it required a careful curation of hypernym hierarchies.Since our goal was to reduce human effort and cost, we decided against it.We used a template fromFischler et al. (1983)along with the categories and subcategories listed inBattig and Montague (1969)to create 770 sentence pairs.Similar to Ettinger (2019), we did not use multi-word category names.",
        "sentences": [
            {
                "text": "We experimented with using WordNet to generate from this template, but found that it required a careful curation of hypernym hierarchies.",
                "label": 1
            },
            {
                "text": "Since our goal was to reduce human effort and cost, we decided against it.",
                "label": 1
            },
            {
                "text": "We used a template fromFischler et al. (1983)along with the categories and subcategories listed inBattig and Montague (1969)to create 770 sentence pairs.",
                "label": 1
            },
            {
                "text": "Similar to Ettinger (2019), we did not use multi-word category names.",
                "label": 1
            }
        ]
    },
    {
        "paper_name": "Larger Probes Tell a Different Story_ Extending Psycholinguistic Datasets Via In_Context Learning",
        "section": 10,
        "paragraph_id": 10,
        "full_text": "NEG-1500-SIMP-GEN We use human-in-theloop and few-shot prompted GPT3-text-davinci-002 with default parameters, to generate 750 pairs (1500 sentences).Each GPT3 prompt consists of an instruction and four randomly selected in-context examples.The cost of generating the dataset is around $30.An example of the prompt is shown in the appendix A. After generating samples from GPT3, the dataset was manually cleaned to filter out poorly generated examples.Details about manual filtering in mentioned in Appendix B.1.",
        "sentences": [
            {
                "text": "NEG-1500-SIMP-GEN We use human-in-theloop and few-shot prompted GPT3-text-davinci-002 with default parameters, to generate 750 pairs (1500 sentences).",
                "label": 1
            },
            {
                "text": "Each GPT3 prompt consists of an instruction and four randomly selected in-context examples.",
                "label": 1
            },
            {
                "text": "The cost of generating the dataset is around $30.",
                "label": 1
            },
            {
                "text": "An example of the prompt is shown in the appendix A.",
                "label": 1
            },
            {
                "text": "After generating samples from GPT3, the dataset was manually cleaned to filter out poorly generated examples.",
                "label": 1
            },
            {
                "text": "Details about manual filtering in mentioned in Appendix B.1.",
                "label": 1
            }
        ]
    },
    {
        "paper_name": "Larger Probes Tell a Different Story_ Extending Psycholinguistic Datasets Via In_Context Learning",
        "section": 11,
        "paragraph_id": 11,
        "full_text": "We analyzed the word distribution for the extended datasets and found that for NEG-1500-SIMP-GEN, few categories are more frequent than others, for example, the highest frequent category \"animal\" has three times more examples than the lowest frequent category \"tree\".This difference is 1.5 times in NEG-1500-SIMP-TEMP.Appendix D show the top 20 categories for all datasets.",
        "sentences": [
            {
                "text": "We analyzed the word distribution for the extended datasets and found that for NEG-1500-SIMP-GEN, few categories are more frequent than others, for example, the highest frequent category \"animal\" has three times more examples than the lowest frequent category \"tree\".",
                "label": 1
            },
            {
                "text": "This difference is 1.5 times in NEG-1500-SIMP-TEMP.",
                "label": 1
            },
            {
                "text": "Appendix D show the top 20 categories for all datasets.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Larger Probes Tell a Different Story_ Extending Psycholinguistic Datasets Via In_Context Learning",
        "section": 12,
        "paragraph_id": 12,
        "full_text": "The original ROLE-88 dataset(Ettinger, 2019;Chow et al., 2016)is a role reversal dataset consisting 88 sentences (44 sentence pairs).Here is an example of a sentence pair: \"The librarian documented which journalist the celebrities had avoided.\"\"The librarian documented which celebrities the journalist had interviewed\".The first sentence has two words representing roles (here: journalist, celebrities), which are reversed in the second sentence.The target verb (the last verb in the sentence) is always preceded by the auxiliary \"had\".This dataset was created to observe the effect of role reversal on the target verb.We notice that all of the role words represent human roles (chef, worker, etc.) or animals (whale, dog, etc.).There were some pairs where the role reversal didn't change the target word, but they were semantically correct.",
        "sentences": [
            {
                "text": "The original ROLE-88 dataset(Ettinger, 2019;Chow et al., 2016)is a role reversal dataset consisting 88 sentences (44 sentence pairs).",
                "label": 1
            },
            {
                "text": "Here is an example of a sentence pair: \"The librarian documented which journalist the celebrities had avoided.\"\"The librarian documented which celebrities the journalist had interviewed\".",
                "label": 1
            },
            {
                "text": "The first sentence has two words representing roles (here: journalist, celebrities), which are reversed in the second sentence.",
                "label": 1
            },
            {
                "text": "The target verb (the last verb in the sentence) is always preceded by the auxiliary \"had\".",
                "label": 1
            },
            {
                "text": "This dataset was created to observe the effect of role reversal on the target verb.",
                "label": 1
            },
            {
                "text": "We notice that all of the role words represent human roles (chef, worker, etc.) or animals (whale, dog, etc.).",
                "label": 1
            },
            {
                "text": "There were some pairs where the role reversal didn't change the target word, but they were semantically correct.",
                "label": 1
            }
        ]
    },
    {
        "paper_name": "Larger Probes Tell a Different Story_ Extending Psycholinguistic Datasets Via In_Context Learning",
        "section": 13,
        "paragraph_id": 13,
        "full_text": "To extend ROLE dataset, we used the same method as NEG-1500-SIMP-GEN and generated 1500 sentences with temperature set to 0.644.We call this new dataset as ROLE-1500 and cost of generating it is $25.After generating samples from GPT3, the dataset was manually cleaned.An example of the prompt, details on data filtering and word distribution for ROLE-1500 is provided in the Appendix A, B.2, and D respectively.Word distribution analysis in ROLE-1500 revealed a threefold difference between the highest and lowest frequency categories, similar to the findings in NEG-1500-SIMP-GEN.",
        "sentences": [
            {
                "text": "To extend ROLE dataset, we used the same method as NEG-1500-SIMP-GEN and generated 1500 sentences with temperature set to 0.644.",
                "label": 1
            },
            {
                "text": "We call this new dataset as ROLE-1500 and cost of generating it is $25.",
                "label": 1
            },
            {
                "text": "After generating samples from GPT3, the dataset was manually cleaned.",
                "label": 1
            },
            {
                "text": "An example of the prompt, details on data filtering and word distribution for ROLE-1500 is provided in the Appendix A, B.2, and D respectively.",
                "label": 1
            },
            {
                "text": "Word distribution analysis in ROLE-1500 revealed a threefold difference between the highest and lowest frequency categories, similar to the findings in NEG-1500-SIMP-GEN.",
                "label": 1
            }
        ]
    },
    {
        "paper_name": "Larger Probes Tell a Different Story_ Extending Psycholinguistic Datasets Via In_Context Learning",
        "section": 14,
        "paragraph_id": 14,
        "full_text": "Following methodology from Lialin et al. (2022), we evaluated 22 models on the newly created negation and role reversal data (Figure1).Both negation and role reversal tasks were first converted into a masked language modeling task, where the category (the final verb) was replaced with a mask token.For GPT2 and GPT3, the task was converted into a causal language modeling task, where the target was removed, and the model had to predict the target word.The responses of the models were evaluated against the true target.",
        "sentences": [
            {
                "text": "Following methodology from Lialin et al. (2022), we evaluated 22 models on the newly created negation and role reversal data (Figure1).",
                "label": 0
            },
            {
                "text": "Both negation and role reversal tasks were first converted into a masked language modeling task, where the category (the final verb) was replaced with a mask token.",
                "label": 0
            },
            {
                "text": "For GPT2 and GPT3, the task was converted into a causal language modeling task, where the target was removed, and the model had to predict the target word.",
                "label": 0
            },
            {
                "text": "The responses of the models were evaluated against the true target.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Larger Probes Tell a Different Story_ Extending Psycholinguistic Datasets Via In_Context Learning",
        "section": 15,
        "paragraph_id": 15,
        "full_text": "For GPT3, we also performed zero-shot evaluation with two prompts: with and without instructions pre-pended to the input sentence (see Appendix A).We used the GPT3-text-davinci-002 model via OpenAI API with the default parameters with max_tokens as 1.GPT3 response was evaluated against gold label.",
        "sentences": [
            {
                "text": "For GPT3, we also performed zero-shot evaluation with two prompts: with and without instructions pre-pended to the input sentence (see Appendix A).",
                "label": 0
            },
            {
                "text": "We used the GPT3-text-davinci-002 model via OpenAI API with the default parameters with max_tokens as 1.",
                "label": 0
            },
            {
                "text": "GPT3 response was evaluated against gold label.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Larger Probes Tell a Different Story_ Extending Psycholinguistic Datasets Via In_Context Learning",
        "section": 16,
        "paragraph_id": 16,
        "full_text": "Our evaluation metric is the top-5 prediction accuracy, followingEttinger (2019).It is the percentage of the model responses where the gold label is among the top five predictions from the model.Table1shows the results for the original and extended datasets.We also included top 1, 10, and 20 accuracies in Appendix F. For the negation dataset, we used model sensitivity as an additional metric.Since we didn't have N400 amplitude and cloze probability for the extended datasets, we defined sensitivity as the percentage of sentence pairs for which the top-1 prediction changed, when \"not\" was added.Table1shows the sensitivity result.Results for all models are shown in Appendix E. We could not compute ROLE-1500 sensitivity, as in some sentence pairs the target word was the same.",
        "sentences": [
            {
                "text": "Our evaluation metric is the top-5 prediction accuracy, followingEttinger (2019).",
                "label": 0
            },
            {
                "text": "It is the percentage of the model responses where the gold label is among the top five predictions from the model.",
                "label": 0
            },
            {
                "text": "Table1shows the results for the original and extended datasets.",
                "label": 0
            },
            {
                "text": "We also included top 1, 10, and 20 accuracies in Appendix F.",
                "label": 0
            },
            {
                "text": "For the negation dataset, we used model sensitivity as an additional metric.",
                "label": 0
            },
            {
                "text": "Since we didn't have N400 amplitude and cloze probability for the extended datasets, we defined sensitivity as the percentage of sentence pairs for which the top-1 prediction changed, when \"not\" was added.",
                "label": 0
            },
            {
                "text": "Table1shows the sensitivity result.",
                "label": 0
            },
            {
                "text": "Results for all models are shown in Appendix E.",
                "label": 0
            },
            {
                "text": "We could not compute ROLE-1500 sensitivity, as in some sentence pairs the target word was the same.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Larger Probes Tell a Different Story_ Extending Psycholinguistic Datasets Via In_Context Learning",
        "section": 17,
        "paragraph_id": 17,
        "full_text": "To assess the reliability of the new datasets, we employed the methodology proposed byCard et al. (2020)to evaluate the statistical power of the datasets.Specifically, we used McNemar's test to assess the top two models on each dataset.For the ROLE dataset, our primary criterion was accuracy, while for the NEG dataset, we used the accuracy on affirmative examples (as inEttinger (2019)).Additionally, we conducted a permutation test to check the statistical significance of the differences for extended datasets.The results are discussed in Section 5.",
        "sentences": [
            {
                "text": "To assess the reliability of the new datasets, we employed the methodology proposed byCard et al. (2020)to evaluate the statistical power of the datasets.",
                "label": 0
            },
            {
                "text": "Specifically, we used McNemar's test to assess the top two models on each dataset.",
                "label": 0
            },
            {
                "text": "For the ROLE dataset, our primary criterion was accuracy, while for the NEG dataset, we used the accuracy on affirmative examples (as inEttinger (2019)).",
                "label": 0
            },
            {
                "text": "Additionally, we conducted a permutation test to check the statistical significance of the differences for extended datasets.",
                "label": 0
            },
            {
                "text": "The results are discussed in Section 5.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Larger Probes Tell a Different Story_ Extending Psycholinguistic Datasets Via In_Context Learning",
        "section": 18,
        "paragraph_id": 18,
        "full_text": "We validated our new datasets through human evaluation.For this, we randomly selected 100 samples from each of the extended datasets.Each of these sentences was presented to two annotators, who were asked to complete the sentences with a one-word response.The completion was compared against target labels.This is a method analogous to the cloze test used traditionally to gauge human language comprehension.We used Cohen's kappa to compute inter-annotator agreement.",
        "sentences": [
            {
                "text": "We validated our new datasets through human evaluation.",
                "label": 0
            },
            {
                "text": "For this, we randomly selected 100 samples from each of the extended datasets.",
                "label": 0
            },
            {
                "text": "Each of these sentences was presented to two annotators, who were asked to complete the sentences with a one-word response.",
                "label": 0
            },
            {
                "text": "The completion was compared against target labels.",
                "label": 0
            },
            {
                "text": "This is a method analogous to the cloze test used traditionally to gauge human language comprehension.",
                "label": 0
            },
            {
                "text": "We used Cohen's kappa to compute inter-annotator agreement.",
                "label": 0
            }
        ]
      },
      {
        "paper_name": "MuAViC_ A Multilingual Audio_Visual Corpus for Robust Speech Recognition and Robust Speech_to_Text Translation",
        "section": 0,
        "paragraph_id": 0,
        "full_text": "Abstract: We introduce MuAViC, a multilingual audio-visual corpus for robust speech recognition and robust speech-to-text translation providing 1200 hours of audio-visual speech in 9 languages.It is fully transcribed and covers 6 English-to-X translation as well as 6 X-to-English translation directions.To the best of our knowledge, this is the first open benchmark for audio-visual speech-to-text translation and the largest open benchmark for multilingual audio-visual speech recognition.Our baseline results show that MuAViC is effective for building noise-robust speech recognition and translation models.We make the corpus available at https://github.com/facebookresearch/muavic.",
        "sentences": [
            {
                "text": "Abstract: We introduce MuAViC, a multilingual audio-visual corpus for robust speech recognition and robust speech-to-text translation providing 1200 hours of audio-visual speech in 9 languages.",
                "label": 1
            },
            {
                "text": "It is fully transcribed and covers 6 English-to-X translation as well as 6 X-to-English translation directions.",
                "label": 1
            },
            {
                "text": "To the best of our knowledge, this is the first open benchmark for audio-visual speech-to-text translation and the largest open benchmark for multilingual audio-visual speech recognition.",
                "label": 1
            },
            {
                "text": "Our baseline results show that MuAViC is effective for building noise-robust speech recognition and translation models.",
                "label": 0
            },
            {
                "text": "We make the corpus available at https://github.com/facebookresearch/muavic.",
                "label": 1
            }
        ]
    },
    {
        "paper_name": "MuAViC_ A Multilingual Audio_Visual Corpus for Robust Speech Recognition and Robust Speech_to_Text Translation",
        "section": 1,
        "paragraph_id": 1,
        "full_text": "Visual perception of lip movement plays a crucial role in supplementing audio signals for a better understanding of spoken languages[1].Its immunity to acoustic noise makes it a valuable asset in building noise-robust speech processing systems.Audio-visual speech recognition (AVSR), which transcribes spoken utterances using both audio and visual inputs, has been shown to be effective in improving the robustness of speech recognition[2,3].The application of deep learning has greatly improved the performance of AVSR systems, as evidenced by the state-of-the-art (SOTA) models in the widely used LRS3-TED public benchmark.Typically, SOTA systems[3]are able to lower word error rate (WER) by 3 times compared to its audio-only counterpart under challenging conditions with 0 dB babble noise.",
        "sentences": [
            {
                "text": "Visual perception of lip movement plays a crucial role in supplementing audio signals for a better understanding of spoken languages[1].",
                "label": 0
            },
            {
                "text": "Its immunity to acoustic noise makes it a valuable asset in building noise-robust speech processing systems.",
                "label": 0
            },
            {
                "text": "Audio-visual speech recognition (AVSR), which transcribes spoken utterances using both audio and visual inputs, has been shown to be effective in improving the robustness of speech recognition[2,3].",
                "label": 0
            },
            {
                "text": "The application of deep learning has greatly improved the performance of AVSR systems, as evidenced by the state-of-the-art (SOTA) models in the widely used LRS3-TED public benchmark.",
                "label": 0
            },
            {
                "text": "Typically, SOTA systems[3]are able to lower word error rate (WER) by 3 times compared to its audio-only counterpart under challenging conditions with 0 dB babble noise.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "MuAViC_ A Multilingual Audio_Visual Corpus for Robust Speech Recognition and Robust Speech_to_Text Translation",
        "section": 2,
        "paragraph_id": 2,
        "full_text": "In spite of the encouraging progress mentioned earlier, modern AVSR models are heavily benchmarked under a monolingual setup, with the majority being based on English.Non-English audio-visual datasets are orders of magnitude smaller, with a common dataset, CMU-MOSEAS[4]providing less than 20 hours of audio-visual speech for each of the 4 non-English languages it covers, which is at least 20 times smaller than the English-based LRS3-TED corpus.The lack of large-scale audio-visual datasets also impedes the development of audiovisual speech-to-text translation (AVST), which is only simulated using synthetic data in prior works, but is expected to bring a noise-robust effect similar to AVSR, as will be demonstrated in this paper.With the increasing popularity of self-supervised audio-visual pre-training approaches[5], proper benchmarking of multilingual AVSR and AVST models will enable multilingual pre-training, which allows more effective use of large-scale multilingual audio-visual data such as AV-Speech[6].",
        "sentences": [
            {
                "text": "In spite of the encouraging progress mentioned earlier, modern AVSR models are heavily benchmarked under a monolingual setup, with the majority being based on English.",
                "label": 0
            },
            {
                "text": "Non-English audio-visual datasets are orders of magnitude smaller, with a common dataset, CMU-MOSEAS[4]providing less than 20 hours of audio-visual speech for each of the 4 non-English languages it covers, which is at least 20 times smaller than the English-based LRS3-TED corpus.",
                "label": 0
            },
            {
                "text": "The lack of large-scale audio-visual datasets also impedes the development of audiovisual speech-to-text translation (AVST), which is only simulated using synthetic data in prior works, but is expected to bring a noise-robust effect similar to AVSR, as will be demonstrated in this paper.",
                "label": 0
            },
            {
                "text": "With the increasing popularity of self-supervised audio-visual pre-training approaches[5], proper benchmarking of multilingual AVSR and AVST models will enable multilingual pre-training, which allows more effective use of large-scale multilingual audio-visual data such as AV-Speech[6].",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "MuAViC_ A Multilingual Audio_Visual Corpus for Robust Speech Recognition and Robust Speech_to_Text Translation",
        "section": 3,
        "paragraph_id": 3,
        "full_text": "In this paper, we present MuAViC, a multilingual audio- visual corpus for robust speech recognition and robust speechto-text translation.This corpus is sourced from TED and TEDx talks including total 1200 hours of transcribed audio-visual speech from over 8000 speakers in 9 languages: English (En), Arabic (Ar), German (De), Greek (El), Spanish (Es), French (Fr), Italian (It), Portuguese (Pt) and Russian (Ru).This makes MuAViC the largest open benchmark so far for multilingual audio-visual speech recognition (AVSR) and lipreading.In addition, we provide text translations and establish baselines for 6 English-to-X translation as well as 6 X-to-English translation directions.To the best of our knowledge, MuAViC is the first publicly available corpus for audio-visual speech-to-text translation.",
        "sentences": [
            {
                "text": "In this paper, we present MuAViC, a multilingual audio- visual corpus for robust speech recognition and robust speechto-text translation.",
                "label": 1
            },
            {
                "text": "This corpus is sourced from TED and TEDx talks including total 1200 hours of transcribed audio-visual speech from over 8000 speakers in 9 languages: English (En), Arabic (Ar), German (De), Greek (El), Spanish (Es), French (Fr), Italian (It), Portuguese (Pt) and Russian (Ru).",
                "label": 1
            },
            {
                "text": "This makes MuAViC the largest open benchmark so far for multilingual audio-visual speech recognition (AVSR) and lipreading.",
                "label": 1
            },
            {
                "text": "In addition, we provide text translations and establish baselines for 6 English-to-X translation as well as 6 X-to-English translation directions.",
                "label": 1
            },
            {
                "text": "To the best of our knowledge, MuAViC is the first publicly available corpus for audio-visual speech-to-text translation.",
                "label": 1
            }
        ]
    },
    {
        "paper_name": "MuAViC_ A Multilingual Audio_Visual Corpus for Robust Speech Recognition and Robust Speech_to_Text Translation",
        "section": 4,
        "paragraph_id": 4,
        "full_text": "MuAViC sources data from TED and TEDx talk recordings, where native or non-native speakers (only one speaker most of the time) deliver public speech on stage and cameras capture stage scenes switching among different viewpoints.We collect both audio and video tracks from the recordings, and align them with human transcriptions as well as text translations.",
        "sentences": [
            {
                "text": "MuAViC sources data from TED and TEDx talk recordings, where native or non-native speakers (only one speaker most of the time) deliver public speech on stage and cameras capture stage scenes switching among different viewpoints.",
                "label": 1
            },
            {
                "text": "We collect both audio and video tracks from the recordings, and align them with human transcriptions as well as text translations.",
                "label": 1
            }
        ]
    },
    {
        "paper_name": "MuAViC_ A Multilingual Audio_Visual Corpus for Robust Speech Recognition and Robust Speech_to_Text Translation",
        "section": 5,
        "paragraph_id": 5,
        "full_text": "For English talks, we reuse the audio-visual data from LRS3-TED[8]and follow the original data split.We find human translations for these talks from a machine translation corpus, TED2020[26]by matching transcriptions in LRS3-TED and source sentences in TED2020.Matched LRS3-TED examples are then paired with the corresponding target sentences in TED2020 for translation labels.We apply exact text matching for development set and test set examples to ensure the best accuracy.To improve matching recall on the train set, we develop a fuzzy text matching strategy: we first segment TED2020 source and target sentences by punctuation if both sides of the sentence pair contain the same amount of segments.Then we normalize TED2020 and LRS3-TED texts by punctuation removal and lowercasing.Finally, we conduct exact text matching between the two corpora.For LRS3-TED train set examples without a match from TED2020, we acquire pseudotranslation labels from a machine translation model, M2M-100 418M[7]with default decoding hyper-parameters.",
        "sentences": [
            {
                "text": "For English talks, we reuse the audio-visual data from LRS3-TED[8]and follow the original data split.",
                "label": 1
            },
            {
                "text": "We find human translations for these talks from a machine translation corpus, TED2020[26]by matching transcriptions in LRS3-TED and source sentences in TED2020.",
                "label": 1
            },
            {
                "text": "Matched LRS3-TED examples are then paired with the corresponding target sentences in TED2020 for translation labels.",
                "label": 1
            },
            {
                "text": "We apply exact text matching for development set and test set examples to ensure the best accuracy.",
                "label": 1
            },
            {
                "text": "To improve matching recall on the train set, we develop a fuzzy text matching strategy: we first segment TED2020 source and target sentences by punctuation if both sides of the sentence pair contain the same amount of segments.",
                "label": 1
            },
            {
                "text": "Then we normalize TED2020 and LRS3-TED texts by punctuation removal and lowercasing.",
                "label": 1
            },
            {
                "text": "Finally, we conduct exact text matching between the two corpora.",
                "label": 1
            },
            {
                "text": "For LRS3-TED train set examples without a match from TED2020, we acquire pseudotranslation labels from a machine translation model, M2M-100 418M[7]with default decoding hyper-parameters.",
                "label": 1
            }
        ]
    },
    {
        "paper_name": "MuAViC_ A Multilingual Audio_Visual Corpus for Robust Speech Recognition and Robust Speech_to_Text Translation",
        "section": 6,
        "paragraph_id": 6,
        "full_text": "For non-English talks, we reuse the audio-only data, transcriptions and text translations collected by mTEDx[24].Our data split also follows mTEDx.We acquire video tracks of the original recordings, and align processed video data with the audio one to form audio-visual data, similar to LRS3-TED[8].Although all the audio data in mTEDx is transcribed, only a subset of it is translated.We acquire pseudo-translation labels from M2M-100 418M[7]for the untranslated train set examples with default decoding hyper-parameters.",
        "sentences": [
            {
                "text": "For non-English talks, we reuse the audio-only data, transcriptions and text translations collected by mTEDx[24].",
                "label": 1
            },
            {
                "text": "Our data split also follows mTEDx.",
                "label": 1
            },
            {
                "text": "We acquire video tracks of the original recordings, and align processed video data with the audio one to form audio-visual data, similar to LRS3-TED[8].",
                "label": 1
            },
            {
                "text": "Although all the audio data in mTEDx is transcribed, only a subset of it is translated.",
                "label": 1
            },
            {
                "text": "We acquire pseudo-translation labels from M2M-100 418M[7]for the untranslated train set examples with default decoding hyper-parameters.",
                "label": 1
            }
        ]
    },
    {
        "paper_name": "MuAViC_ A Multilingual Audio_Visual Corpus for Robust Speech Recognition and Robust Speech_to_Text Translation",
        "section": 7,
        "paragraph_id": 7,
        "full_text": "In Table1, we provide duration and speaker statistics for each data split in MuAViC.MuAViC covers 9 languages providing total 1200 hours of transcribed audio-visual data from",
        "sentences": [
            {
                "text": "In Table1, we provide duration and speaker statistics for each data split in MuAViC.",
                "label": 1
            }
        ]
    },
    {
        "paper_name": "MuAViC_ A Multilingual Audio_Visual Corpus for Robust Speech Recognition and Robust Speech_to_Text Translation",
        "section": 8,
        "paragraph_id": 8,
        "full_text": "For both AVSR and AVST, we use an English AV-HuBERT large pre-trained model[3]1 , which is trained on the combination of LRS3-TED[8]and the English portion of Vox-Celeb2[27].We follow[3]for fine-tuning hyper-parameters, except that we fine-tune our bilingual models to 30K updates and our multilingual AVSR model to 90K updates.We freeze the pre-trained encoders for the first 4K and 24K updates for X-En AVST and En-X AVST models, respectively.Also, we randomly augment 25% of the input samples with multiple types of additive noises with a SNR (signal-to-noise ratio) of 0. The noise audio clips in the categories of \"natural\", \"music\" and \"babble\" are sampled from MUSAN dataset[28], while the overlapping \"speech\" noise samples are drawn from LRS3-TED.In creating \"speech\" and \"babble\" noise sets, we ensure there are no speaker overlap among different partitions.We remove extremely short utterances (less than 0.2 seconds) and long utterances (more than 20 seconds) for better training stability.Besides end-to-end AVST models, we also build cascaded systems composed of a AVSR model and a text-based machine 1 Randomly initialized models perform poorly (see Appendix A.1). translation model (MT).For bilingual MT, we train Transformer base[29]models using all the transcription-translation pairs in MuAViC train set.For multilingual MT, we leverage M2M-100 418M[7], which is trained on large-scale open-domain mined bitext data.For speech recognition and X-En speech translation, we also evaluate Whisper Large V2[25]as a SOTA baseline.",
        "sentences": [
            {
                "text": "For both AVSR and AVST, we use an English AV-HuBERT large pre-trained model[3]1 , which is trained on the combination of LRS3-TED[8]and the English portion of Vox-Celeb2[27].",
                "label": 0
            },
            {
                "text": "We follow[3]for fine-tuning hyper-parameters, except that we fine-tune our bilingual models to 30K updates and our multilingual AVSR model to 90K updates.",
                "label": 0
            },
            {
                "text": "We freeze the pre-trained encoders for the first 4K and 24K updates for X-En AVST and En-X AVST models, respectively.",
                "label": 0
            },
            {
                "text": "Also, we randomly augment 25% of the input samples with multiple types of additive noises with a SNR (signal-to-noise ratio) of 0.",
                "label": 0
            },
            {
                "text": "The noise audio clips in the categories of \"natural\", \"music\" and \"babble\" are sampled from MUSAN dataset[28], while the overlapping \"speech\" noise samples are drawn from LRS3-TED.",
                "label": 0
            },
            {
                "text": "In creating \"speech\" and \"babble\" noise sets, we ensure there are no speaker overlap among different partitions.",
                "label": 0
            },
            {
                "text": "We remove extremely short utterances (less than 0.2 seconds) and long utterances (more than 20 seconds) for better training stability.",
                "label": 0
            },
            {
                "text": "Besides end-to-end AVST models, we also build cascaded systems composed of a AVSR model and a text-based machine 1 Randomly initialized models perform poorly (see Appendix A.1). translation model (MT).",
                "label": 0
            },
            {
                "text": "For bilingual MT, we train Transformer base[29]models using all the transcription-translation pairs in MuAViC train set.",
                "label": 0
            },
            {
                "text": "For multilingual MT, we leverage M2M-100 418M[7], which is trained on large-scale open-domain mined bitext data.",
                "label": 0
            },
            {
                "text": "For speech recognition and X-En speech translation, we also evaluate Whisper Large V2[25]as a SOTA baseline.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "MuAViC_ A Multilingual Audio_Visual Corpus for Robust Speech Recognition and Robust Speech_to_Text Translation",
        "section": 9,
        "paragraph_id": 9,
        "full_text": "For inference, we use the best checkpoint by validation accuracy for AVSR/AVST and the best checkpoint by validation BLEU for MT.We use a beam size of 5 and default values for the other beam search decoding hyper-parameters.For AVSR, we normalize texts by punctuation removal and lowercasing[30]before calculating WER (word error rate).For AVST and MT, we use SacreBLEU[31]with default options, where texts are processed by its built-in 13a tokenizer before BLEU[32]calculation.",
        "sentences": [
            {
                "text": "For inference, we use the best checkpoint by validation accuracy for AVSR/AVST and the best checkpoint by validation BLEU for MT.",
                "label": 0
            },
            {
                "text": "We use a beam size of 5 and default values for the other beam search decoding hyper-parameters.",
                "label": 0
            },
            {
                "text": "For AVSR, we normalize texts by punctuation removal and lowercasing[30]before calculating WER (word error rate).",
                "label": 0
            },
            {
                "text": "For AVST and MT, we use SacreBLEU[31]with default options, where texts are processed by its built-in 13a tokenizer before BLEU[32]calculation.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "MuAViC_ A Multilingual Audio_Visual Corpus for Robust Speech Recognition and Robust Speech_to_Text Translation",
        "section": 10,
        "paragraph_id": 10,
        "full_text": "We evaluate AVSR models in both audio-only (\"A\") and audiovisual (\"AV\") modes, where the former leverages only audio modality in fine-tuning and inference while the latter leverages both audio and visual modalities.As shown in Table2, the English AVSR model[3]has a low test WER of 2.5 and 2.3 respectively for audio-only and audio-visual modes.For non-English AVSR, we fine-tune the pre-trained English AV-HuBERT model either separately on each language (8 monolingual models) or jointly on all the 8 non-English languages (a multilingual model), whose test WER can be found in Table3.We observe that our monolingual AVSR models in the audio- visual mode outperform a comparable ASR baseline (Transformer, monolingual) by average 52% WER reduction.They underperform the hybrid LF-MMI monolingual ASR baselines by average 14% WER increase, which leverage 4-gram language models in decoding while our models do not.Our multilingual AVSR model on average slightly outperforms monolingual AVSR models, with gains observed on some low-resource languages (Ar and De) and degradation observed on the others (El and Ru).We also observe that our multilingual AVSR model falls behind Whisper by average 20% WER due to the large gap in the amount of training data (0.7K hours compared to 680K hours).",
        "sentences": [
            {
                "text": "We evaluate AVSR models in both audio-only (\"A\") and audiovisual (\"AV\") modes, where the former leverages only audio modality in fine-tuning and inference while the latter leverages both audio and visual modalities.",
                "label": 0
            },
            {
                "text": "As shown in Table2, the English AVSR model[3]has a low test WER of 2.5 and 2.3 respectively for audio-only and audio-visual modes.",
                "label": 0
            },
            {
                "text": "For non-English AVSR, we fine-tune the pre-trained English AV-HuBERT model either separately on each language (8 monolingual models) or jointly on all the 8 non-English languages (a multilingual model), whose test WER can be found in Table3.",
                "label": 0
            },
            {
                "text": "We observe that our monolingual AVSR models in the audio- visual mode outperform a comparable ASR baseline (Transformer, monolingual) by average 52% WER reduction.",
                "label": 0
            },
            {
                "text": "They underperform the hybrid LF-MMI monolingual ASR baselines by average 14% WER increase, which leverage 4-gram language models in decoding while our models do not.",
                "label": 0
            },
            {
                "text": "Our multilingual AVSR model on average slightly outperforms monolingual AVSR models, with gains observed on some low-resource languages (Ar and De) and degradation observed on the others (El and Ru).",
                "label": 0
            },
            {
                "text": "We also observe that our multilingual AVSR model falls behind Whisper by average 20% WER due to the large gap in the amount of training data (0.7K hours compared to 680K hours).",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "MuAViC_ A Multilingual Audio_Visual Corpus for Robust Speech Recognition and Robust Speech_to_Text Translation",
        "section": 11,
        "paragraph_id": 11,
        "full_text": "The first section of Table4shows the test WER of our AVSR models in a noisy setup, where we simulate noisy environments by adding multilingual babble noises to clean speech inputs with a SNR (signal-to-noise ratio) of 0 2 .We observe that Whisper, a SOTA multilingual ASR model, performs catastrophically in this challenging setup, with a high average WER of 174.3 over the 9 languages.In contrast, our monolingual AVSR models in the audio-only mode have an average WER of 70.2 and 66.7 respectively.In the audio-visual mode, the average WER of our models drop significantly by 32%, suggesting their efficient use of visual information to alleviate the distraction of noisy environments.Our multilingual AVSR model outperforms the monolingual counterparts on every non-English language (except El) in both audio-only and audio-visual modes.",
        "sentences": [
            {
                "text": "The first section of Table4shows the test WER of our AVSR models in a noisy setup, where we simulate noisy environments by adding multilingual babble noises to clean speech inputs with a SNR (signal-to-noise ratio) of 0 2 .",
                "label": 0
            },
            {
                "text": "We observe that Whisper, a SOTA multilingual ASR model, performs catastrophically in this challenging setup, with a high average WER of 174.3 over the 9 languages.",
                "label": 0
            },
            {
                "text": "In contrast, our monolingual AVSR models in the audio-only mode have an average WER of 70.2 and 66.7 respectively.",
                "label": 0
            },
            {
                "text": "In the audio-visual mode, the average WER of our models drop significantly by 32%, suggesting their efficient use of visual information to alleviate the distraction of noisy environments.",
                "label": 0
            },
            {
                "text": "Our multilingual AVSR model outperforms the monolingual counterparts on every non-English language (except El) in both audio-only and audio-visual modes.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "MuAViC_ A Multilingual Audio_Visual Corpus for Robust Speech Recognition and Robust Speech_to_Text Translation",
        "section": 12,
        "paragraph_id": 12,
        "full_text": "We report test BLEU for En-X AVST and X-En AVST models in Table2and Table3, respectively.Besides end-to-end AV-HuBERT AVST models, we also set up bilingual and multilingual MT baselines, and build cascaded AVST baselines by 2 See Appendix A.2 for model performance under different noise types and different SNRs pipelining AV-HuBERT AVSR models with MT models.We see that our end-to-end AVST models are on par with the cascaded counterparts in the constrained data setup (\"A1+A3\" and \"B1+B3\") for both En-X and X-En directions.Similar to the case in non-English AVSR, our 6-language multilingual X-En AVST model on average perform slightly better than the corresponding bilingual AVST models.",
        "sentences": [
            {
                "text": "We report test BLEU for En-X AVST and X-En AVST models in Table2and Table3, respectively.",
                "label": 0
            },
            {
                "text": "2 for model performance under different noise types and different SNRs pipelining AV-HuBERT AVSR models with MT models.",
                "label": 0
            },
            {
                "text": "We see that our end-to-end AVST models are on par with the cascaded counterparts in the constrained data setup (\"A1+A3\" and \"B1+B3\") for both En-X and X-En directions.",
                "label": 0
            },
            {
                "text": "Similar to the case in non-English AVSR, our 6-language multilingual X-En AVST model on average perform slightly better than the corresponding bilingual AVST models.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "MuAViC_ A Multilingual Audio_Visual Corpus for Robust Speech Recognition and Robust Speech_to_Text Translation",
        "section": 13,
        "paragraph_id": 13,
        "full_text": "We evaluate our En-X AVST and X-En AVST models in a noisy setup, whose test BLEU are shown in the second and third section of Table4, respectively.We simulate noisy environments in the same approach as that for AVSR models, where multilingual babble noises are added to clean speech inputs with a SNR of 0. We observe that Whisper, a SOTA multilingual X-En speech-to-text translation model, has a catastrophic performance under this setup, with only 0.3 average BLEU over the 6 directions.Our bilingual and multilingual AVST models in the audio-only mode outperform it largely with 7.8 and 6.6 average BLEU improvement, respectively.AVST models in the audiovisual mode consistently outperform those in the audio-only mode, with an improvement of 3.2 and 3.3 on average BLEU for the bilingual and multilingual settings, respectively.",
        "sentences": [
            {
                "text": "We evaluate our En-X AVST and X-En AVST models in a noisy setup, whose test BLEU are shown in the second and third section of Table4, respectively.",
                "label": 0
            },
            {
                "text": "We simulate noisy environments in the same approach as that for AVSR models, where multilingual babble noises are added to clean speech inputs with a SNR of 0.",
                "label": 0
            },
            {
                "text": "We observe that Whisper, a SOTA multilingual X-En speech-to-text translation model, has a catastrophic performance under this setup, with only 0.3 average BLEU over the 6 directions.",
                "label": 0
            },
            {
                "text": "Our bilingual and multilingual AVST models in the audio-only mode outperform it largely with 7.8 and 6.6 average BLEU improvement, respectively.",
                "label": 0
            },
            {
                "text": "AVST models in the audiovisual mode consistently outperform those in the audio-only mode, with an improvement of 3.2 and 3.3 on average BLEU for the bilingual and multilingual settings, respectively.",
                "label": 0
            }
        ]
      },
      {
        "paper_name": "Persian Typographical Error Type Detection Using Deep Neural Networks on Algorithmically_Generated Misspellings",
        "section": 0,
        "paragraph_id": 0,
        "full_text": "Abstract: Spelling correction is a remarkable challenge in the field of natural language processing.The objective of spelling correction tasks is to recognize and rectify spelling errors automatically.The development of applications that can effectually diagnose and correct Persian spelling and grammatical errors has become more important in order to improve the quality of Persian text.The Typographical Error Type Detection in Persian is a relatively understudied area.Therefore, this paper presents a compelling approach for detecting typographical errors in Persian texts.Our work includes the presentation of a publicly available dataset called FarsTypo, which comprises 3.4 million words arranged in chronological order and tagged with their corresponding part-of-speech.These words cover a wide range of topics and linguistic styles.We develop an algorithm designed to apply Persian-specific errors to a scalable portion of these words, resulting in a parallel dataset of correct and incorrect words.By leveraging FarsTypo, we establish a strong foundation and conduct a thorough comparison of various methodologies employing different architectures.Additionally, we introduce a groundbreaking Deep Sequential Neural Network that utilizes both word and character embeddings, along with bidirectional LSTM layers, for token classification aimed at detecting typographical errors across 51 distinct classes.Our approach is contrasted with highly advanced industrial systems that, unlike this study, have been developed using a diverse range of resources.The outcomes of our final method proved to be highly competitive, achieving an accuracy of 97.62%, precision of 98.83%, recall of 98.61%, and surpassing others in terms of speed.",
        "sentences": [
            {
                "text": "Abstract: Spelling correction is a remarkable challenge in the field of natural language processing.",
                "label": 0
            },
            {
                "text": "The objective of spelling correction tasks is to recognize and rectify spelling errors automatically.",
                "label": 0
            },
            {
                "text": "The development of applications that can effectually diagnose and correct Persian spelling and grammatical errors has become more important in order to improve the quality of Persian text.",
                "label": 0
            },
            {
                "text": "The Typographical Error Type Detection in Persian is a relatively understudied area.",
                "label": 0
            },
            {
                "text": "Therefore, this paper presents a compelling approach for detecting typographical errors in Persian texts.",
                "label": 0
            },
            {
                "text": "Our work includes the presentation of a publicly available dataset called FarsTypo, which comprises 3.4 million words arranged in chronological order and tagged with their corresponding part-of-speech.",
                "label": 1
            },
            {
                "text": "These words cover a wide range of topics and linguistic styles.",
                "label": 1
            },
            {
                "text": "We develop an algorithm designed to apply Persian-specific errors to a scalable portion of these words, resulting in a parallel dataset of correct and incorrect words.",
                "label": 0
            },
            {
                "text": "By leveraging FarsTypo, we establish a strong foundation and conduct a thorough comparison of various methodologies employing different architectures.",
                "label": 0
            },
            {
                "text": "Additionally, we introduce a groundbreaking Deep Sequential Neural Network that utilizes both word and character embeddings, along with bidirectional LSTM layers, for token classification aimed at detecting typographical errors across 51 distinct classes.",
                "label": 0
            },
            {
                "text": "Our approach is contrasted with highly advanced industrial systems that, unlike this study, have been developed using a diverse range of resources.",
                "label": 0
            },
            {
                "text": "The outcomes of our final method proved to be highly competitive, achieving an accuracy of 97.62%, precision of 98.83%, recall of 98.61%, and surpassing others in terms of speed.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Persian Typographical Error Type Detection Using Deep Neural Networks on Algorithmically_Generated Misspellings",
        "section": 1,
        "paragraph_id": 1,
        "full_text": "With the emersion of the computer era, the way we communicate has undergone a deep transformation, leading to a declined demand for traditional printed newspapers and handwritten letters.The rise of digitalization has presented humanity with manifold opportunities, but it has also introduced several challenges.As an example, in 2020, the global internet witnessed over 306 billion emails being sent worldwide(Johnson 2021), while in the United States alone, almost 2.2 trillion text messages were exchanged(CTIA 2021).Despite the fact that these statistics highlight the advantages of recent technological advancement in enhancing communication convenience and accessibility, there are certain issues that arise when natural language is employed as a communication medium.One such problem is misspelling a word, also known as making a Typographical Error (Typo).Although it seems minor, typos can have a significant and often negative impact, particularly when made by an organization or a company(Muller et al. 2019) such as customer attrition(Stiff 2012).This type of error is more prevalent in text messages and can lead to major unwanted misunderstandings(Boland and Queen 2016).In the field of Natural Language Processing (NLP), researchers have been actively working on the development of various proofreading tools to overcome this issue.",
        "sentences": [
            {
                "text": "With the emersion of the computer era, the way we communicate has undergone a deep transformation, leading to a declined demand for traditional printed newspapers and handwritten letters.",
                "label": 0
            },
            {
                "text": "The rise of digitalization has presented humanity with manifold opportunities, but it has also introduced several challenges.",
                "label": 0
            },
            {
                "text": "As an example, in 2020, the global internet witnessed over 306 billion emails being sent worldwide(Johnson 2021), while in the United States alone, almost 2.2 trillion text messages were exchanged(CTIA 2021).",
                "label": 0
            },
            {
                "text": "Despite the fact that these statistics highlight the advantages of recent technological advancement in enhancing communication convenience and accessibility, there are certain issues that arise when natural language is employed as a communication medium.",
                "label": 0
            },
            {
                "text": "One such problem is misspelling a word, also known as making a Typographical Error (Typo).",
                "label": 0
            },
            {
                "text": "Although it seems minor, typos can have a significant and often negative impact, particularly when made by an organization or a company(Muller et al. 2019) such as customer attrition(Stiff 2012).",
                "label": 0
            },
            {
                "text": "This type of error is more prevalent in text messages and can lead to major unwanted misunderstandings(Boland and Queen 2016).",
                "label": 0
            },
            {
                "text": "In the field of Natural Language Processing (NLP), researchers have been actively working on the development of various proofreading tools to overcome this issue.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Persian Typographical Error Type Detection Using Deep Neural Networks on Algorithmically_Generated Misspellings",
        "section": 2,
        "paragraph_id": 2,
        "full_text": "Typically, text-editing assistants are developed to focus on correcting spelling or grammar in input texts, known as spell-and grammar-checkers, respectively.The division of text-editing assistants is influenced byKukich's categorization (1992)of orthographical mistakes into non-word, real-word, and grammatical errors.Non-word errors happen when a word is not detected in the spell-checker's lexicon (e.g., writing 'hope' as 'hoope').Real-word errors, on the other hand, involve using a correctly spelled word from the vocabulary but in an incorrect context (e.g., writing \"I lust my keys\" instead of \"I lost my keys\").Lastly, grammatical errors, which unlike the previous two we do not focus on in this study, occur when a sentence does not adhere to the predefined rules of a language.Spell-checkers, which primarily focus on resolving non-word errors and real-word errors, generally comprise four consecutive subtasks.First, the input text is tokenized (tokenization), meaning it is divided into individual units such as words or characters.Next, spell-checker identify potential errors within the text (error detection).Afterwards, it aims to correct previously detected errors (error correction).Finally, the spell-checker ranks and provides a list of possible candidate corrections for each misspelt word (candidate ranking).These subtasks are part of a hierarchical module, and the overall effectiveness of a spell-checker relies on the success of individual components.",
        "sentences": [
            {
                "text": "Typically, text-editing assistants are developed to focus on correcting spelling or grammar in input texts, known as spell-and grammar-checkers, respectively.",
                "label": 0
            },
            {
                "text": "The division of text-editing assistants is influenced byKukich's categorization (1992)of orthographical mistakes into non-word, real-word, and grammatical errors.",
                "label": 0
            },
            {
                "text": "Non-word errors happen when a word is not detected in the spell-checker's lexicon (e.g., writing 'hope' as 'hoope').",
                "label": 0
            },
            {
                "text": "Real-word errors, on the other hand, involve using a correctly spelled word from the vocabulary but in an incorrect context (e.g., writing \"I lust my keys\" instead of \"I lost my keys\").",
                "label": 0
            },
            {
                "text": "Lastly, grammatical errors, which unlike the previous two we do not focus on in this study, occur when a sentence does not adhere to the predefined rules of a language.",
                "label": 0
            },
            {
                "text": "Spell-checkers, which primarily focus on resolving non-word errors and real-word errors, generally comprise four consecutive subtasks.",
                "label": 0
            },
            {
                "text": "First, the input text is tokenized (tokenization), meaning it is divided into individual units such as words or characters.",
                "label": 0
            },
            {
                "text": "Next, spell-checker identify potential errors within the text (error detection).",
                "label": 0
            },
            {
                "text": "Afterwards, it aims to correct previously detected errors (error correction).",
                "label": 0
            },
            {
                "text": "Finally, the spell-checker ranks and provides a list of possible candidate corrections for each misspelt word (candidate ranking).",
                "label": 0
            },
            {
                "text": "These subtasks are part of a hierarchical module, and the overall effectiveness of a spell-checker relies on the success of individual components.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Persian Typographical Error Type Detection Using Deep Neural Networks on Algorithmically_Generated Misspellings",
        "section": 3,
        "paragraph_id": 3,
        "full_text": "Applying test-editing tasks, such as spell checking, to low-resource natural languages like Persian presents additional challenges.Persian, spoken mainly in Iran, Afghanistan, and Tajikistan, has its own unique characteristics.While its linguistic structure has remained relatively unchanged over time(Bijankhan et al. 2011), it has incorporated numerous words from Arabic(Haghdadi and Azizi 2018).Furthermore, Persian is a free word order language, meaning that the order of words in a sentence can be flexible.Additionally, Persian includes letters that have both joiner and non-joiner forms(Ghayoomi et al. 2010).These linguistic features and complexities pose challenges when developing effective text-editing tools for Persian.Indeed, Persian poses additional challenges for text-editing tasks like spell-checking.One of the challenges is the presence of various letters that are written differently but have similar sounds.This can create difficulties when modeling the language, as there is no unified set of rules for writing(Rasooli et al. 2011).Additionally, distinguishing between white spaces and pseudo-spaces (ZWNJ1) can be problematic(Dastgheib et al. 2016).These factors contribute to the slow pace of computational advances in Persian language processing and its inherent ambiguity(QasemiZadeh et al. 2014).Nevertheless, several studies have focused on addressing these factors and have made progress in the field of spell-checking for Persian(Dastgheib and Fakhrahmad 2019;Samani et al. 2015).In general, these studies are focused on grammar(Ehsan and Faili 2010), a specific domain(Yazdani et al. 2020), or do not offer a comprehensive approach that is highly effective.Thus, with the advancement of neural network architectures, a more accurate method can be provided and more research needs to be conducted in order to address spell-checking in Persian.",
        "sentences": [
            {
                "text": "Applying test-editing tasks, such as spell checking, to low-resource natural languages like Persian presents additional challenges.",
                "label": 0
            },
            {
                "text": "Persian, spoken mainly in Iran, Afghanistan, and Tajikistan, has its own unique characteristics.",
                "label": 0
            },
            {
                "text": "While its linguistic structure has remained relatively unchanged over time(Bijankhan et al. 2011), it has incorporated numerous words from Arabic(Haghdadi and Azizi 2018).",
                "label": 0
            },
            {
                "text": "Furthermore, Persian is a free word order language, meaning that the order of words in a sentence can be flexible.",
                "label": 0
            },
            {
                "text": "Additionally, Persian includes letters that have both joiner and non-joiner forms(Ghayoomi et al. 2010).",
                "label": 0
            },
            {
                "text": "These linguistic features and complexities pose challenges when developing effective text-editing tools for Persian.",
                "label": 0
            },
            {
                "text": "Indeed, Persian poses additional challenges for text-editing tasks like spell-checking.",
                "label": 0
            },
            {
                "text": "One of the challenges is the presence of various letters that are written differently but have similar sounds.",
                "label": 0
            },
            {
                "text": "This can create difficulties when modeling the language, as there is no unified set of rules for writing(Rasooli et al. 2011).",
                "label": 0
            },
            {
                "text": "Additionally, distinguishing between white spaces and pseudo-spaces (ZWNJ1) can be problematic(Dastgheib et al. 2016).",
                "label": 0
            },
            {
                "text": "These factors contribute to the slow pace of computational advances in Persian language processing and its inherent ambiguity(QasemiZadeh et al. 2014).",
                "label": 0
            },
            {
                "text": "Nevertheless, several studies have focused on addressing these factors and have made progress in the field of spell-checking for Persian(Dastgheib and Fakhrahmad 2019;Samani et al. 2015).",
                "label": 0
            },
            {
                "text": "In general, these studies are focused on grammar(Ehsan and Faili 2010), a specific domain(Yazdani et al. 2020), or do not offer a comprehensive approach that is highly effective.",
                "label": 0
            },
            {
                "text": "Thus, with the advancement of neural network architectures, a more accurate method can be provided and more research needs to be conducted in order to address spell-checking in Persian.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Persian Typographical Error Type Detection Using Deep Neural Networks on Algorithmically_Generated Misspellings",
        "section": 4,
        "paragraph_id": 4,
        "full_text": "Our objective in this study is to enhance the development of more accurate Persian spell-checking systems by focusing on error detection.The effectiveness of spell-checking systems relies heavily on the accurate identification of errors, which can then be corrected.This is particularly evident in situations where Persian words contain Arabic letter, silent letters, or incorrect spacing.Once these errors are detected, they can be easily being corrected using practical techniques like predefined regular expressions patterns (e.g., correcting ‫'ي'‬ to ‫'ی'‬ using re.sub (r\", \"\", text) in Python).Hence, the detection part is crucial since the correction part can be performed quickly with only one line of code and better detection of errors results in a large number of Persian misspellings being effortlessly corrected.To achieve the goal of improving error detection and subsequently correcting Persian misspellings, this study introduces an algorithm that constructs a dataset comprising different Persian-specific typographical errors.This dataset consists of pairs of Persian words, their misspelled variants, and algorithmically assigned typographical error labels.Using this parallel dataset, the study train a Deep Sequential Neural Network to predict error types.The Deep Sequential Neural Network architecture model is designed to take input text and perform token classification, specifically classifying each word into one of 51 classes representing different types of typographical errors.By training the model on this dataset, it learns to accurately identify and classify the typographical errors associated with each word in Persian text.This work introduces several novel aspects that distinguish it from the existing literature.One key difference is the approach to error detection.Unlike previous methods that involved manual checking for every possible Persian typographical error of each word at each time step, the developed architecture in this study utilizes a trained neural network to detect any possible error for every word by propagating forward through the network.Additionally, while dictionary-based methods have been also employed in the past to address this problem, they are not effective in detecting real-word errors and struggle to adapt to various types of misspellings, especially when encountering outof-vocabulary (OOV) words(Dong et al. 2019).In contrast, the proposed approach in this study consider input text at both the sentence and character levels.This comprehensive approach allows the model to capture the necessary information to detect both real-and non-word errors, effectively.In previous studies, there has been a focus on developing better confusion sets to improve performance in detecting such errors.Confusion sets consists of small groups of words that are commonly confused with each other, such as \"site\", \"sight\", \"cite\".Constructing informative and well-investigated confusion sets is crucial for the success of spell-checking systems that utilize them.Furthermore, these spell-checking systems face the challenge of selecting the correct replacement from the confusion sets in the subsequent step.Choosing the most appropriate replacement from a set of similar words requires careful consideration and can be a complex task for the system.Therefore, in this study, addressing real-word error detection and the accurate selection of replacements from confusion sets are important aspects that need to be investigated and improved to enhance the overall performance of Persian spell-checking systems.During real-word error detection, our approach intelligently executes both steps simultaneously by propagating forward our developed neural network.This makes error detection much simpler and results in a reduction in the processing time of a spell-checker.In addition to the benefits mentioned earlier, when the error type is accurately identified, it enables more effective ordering of suggestions in the candidate ranking phase of spell-checkers (candidate ranking).Furthermore, the neural architecture developed in this study is designed to detect both non-word and real-word errors simultaneously.Our comprehensive approach allows the system to handle a wider range of errors and provides a more generalized alternative compared to previous methods.By leveraging the capabilities of the neural network, the approach reduces the need for extensive hard coding and manual rule-based interventions, making the system more adaptable and flexible.",
        "sentences": [
            {
                "text": "Our objective in this study is to enhance the development of more accurate Persian spell-checking systems by focusing on error detection.",
                "label": 0
            },
            {
                "text": "The effectiveness of spell-checking systems relies heavily on the accurate identification of errors, which can then be corrected.",
                "label": 0
            },
            {
                "text": "This is particularly evident in situations where Persian words contain Arabic letter, silent letters, or incorrect spacing.",
                "label": 0
            },
            {
                "text": "Once these errors are detected, they can be easily being corrected using practical techniques like predefined regular expressions patterns (e.g., correcting ‫'ي'‬ to ‫'ی'‬ using re.sub (r\", \"\", text) in Python).",
                "label": 0
            },
            {
                "text": "Hence, the detection part is crucial since the correction part can be performed quickly with only one line of code and better detection of errors results in a large number of Persian misspellings being effortlessly corrected.",
                "label": 0
            },
            {
                "text": "To achieve the goal of improving error detection and subsequently correcting Persian misspellings, this study introduces an algorithm that constructs a dataset comprising different Persian-specific typographical errors.",
                "label": 1
            },
            {
                "text": "This dataset consists of pairs of Persian words, their misspelled variants, and algorithmically assigned typographical error labels.",
                "label": 1
            },
            {
                "text": "Using this parallel dataset, the study train a Deep Sequential Neural Network to predict error types.",
                "label": 0
            },
            {
                "text": "The Deep Sequential Neural Network architecture model is designed to take input text and perform token classification, specifically classifying each word into one of 51 classes representing different types of typographical errors.",
                "label": 0
            },
            {
                "text": "By training the model on this dataset, it learns to accurately identify and classify the typographical errors associated with each word in Persian text.",
                "label": 0
            },
            {
                "text": "This work introduces several novel aspects that distinguish it from the existing literature.",
                "label": 0
            },
            {
                "text": "One key difference is the approach to error detection.",
                "label": 0
            },
            {
                "text": "Unlike previous methods that involved manual checking for every possible Persian typographical error of each word at each time step, the developed architecture in this study utilizes a trained neural network to detect any possible error for every word by propagating forward through the network.",
                "label": 0
            },
            {
                "text": "Additionally, while dictionary-based methods have been also employed in the past to address this problem, they are not effective in detecting real-word errors and struggle to adapt to various types of misspellings, especially when encountering outof-vocabulary (OOV) words(Dong et al. 2019).",
                "label": 0
            },
            {
                "text": "In contrast, the proposed approach in this study consider input text at both the sentence and character levels.",
                "label": 0
            },
            {
                "text": "This comprehensive approach allows the model to capture the necessary information to detect both real-and non-word errors, effectively.",
                "label": 0
            },
            {
                "text": "In previous studies, there has been a focus on developing better confusion sets to improve performance in detecting such errors.",
                "label": 0
            },
            {
                "text": "Confusion sets consists of small groups of words that are commonly confused with each other, such as \"site\", \"sight\", \"cite\".",
                "label": 0
            },
            {
                "text": "Constructing informative and well-investigated confusion sets is crucial for the success of spell-checking systems that utilize them.",
                "label": 0
            },
            {
                "text": "Furthermore, these spell-checking systems face the challenge of selecting the correct replacement from the confusion sets in the subsequent step.",
                "label": 0
            },
            {
                "text": "Choosing the most appropriate replacement from a set of similar words requires careful consideration and can be a complex task for the system.",
                "label": 0
            },
            {
                "text": "Therefore, in this study, addressing real-word error detection and the accurate selection of replacements from confusion sets are important aspects that need to be investigated and improved to enhance the overall performance of Persian spell-checking systems.",
                "label": 0
            },
            {
                "text": "During real-word error detection, our approach intelligently executes both steps simultaneously by propagating forward our developed neural network.",
                "label": 0
            },
            {
                "text": "This makes error detection much simpler and results in a reduction in the processing time of a spell-checker.",
                "label": 0
            },
            {
                "text": "In addition to the benefits mentioned earlier, when the error type is accurately identified, it enables more effective ordering of suggestions in the candidate ranking phase of spell-checkers (candidate ranking).",
                "label": 0
            },
            {
                "text": "Furthermore, the neural architecture developed in this study is designed to detect both non-word and real-word errors simultaneously.",
                "label": 0
            },
            {
                "text": "Our comprehensive approach allows the system to handle a wider range of errors and provides a more generalized alternative compared to previous methods.",
                "label": 0
            },
            {
                "text": "By leveraging the capabilities of the neural network, the approach reduces the need for extensive hard coding and manual rule-based interventions, making the system more adaptable and flexible.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Persian Typographical Error Type Detection Using Deep Neural Networks on Algorithmically_Generated Misspellings",
        "section": 5,
        "paragraph_id": 5,
        "full_text": "Overall, the contributions of this paper can be summarized as follows: • Introduction of FarsTypo: The paper introduces FarsTypo, one of the largest typographical error datasets specifically designed for the Persian language.This dataset is made publicly available, enabling researchers and practitioners to utilize it for further studies and developments2.The rest of this paper is organized as follows.In section 2, we review the existing literature on relevant spell-checking systems.Next, in section 3, we provide a detailed description of the algorithm used for error detection and introduce the dataset, FarsTypo, which is utilized for training and evaluating the system.In section 4, we describe the architecture used to perform error type detection.In section 5, we present the experimental setup conducted to evaluate the performance of the proposed approach.We also established a baseline for performance evaluation and compares the results obtained from the proposed approach with those of previous works in the field.In section 6, discussion of the research is conducted.Finally, in section 7, we conclude our work.",
        "sentences": [
            {
                "text": "Overall, the contributions of this paper can be summarized as follows: • Introduction of FarsTypo: The paper introduces FarsTypo, one of the largest typographical error datasets specifically designed for the Persian language.",
                "label": 1
            },
            {
                "text": "This dataset is made publicly available, enabling researchers and practitioners to utilize it for further studies and developments2.",
                "label": 1
            },
            {
                "text": "The rest of this paper is organized as follows.",
                "label": 0
            },
            {
                "text": "In section 2, we review the existing literature on relevant spell-checking systems.",
                "label": 0
            },
            {
                "text": "Next, in section 3, we provide a detailed description of the algorithm used for error detection and introduce the dataset, FarsTypo, which is utilized for training and evaluating the system.",
                "label": 0
            },
            {
                "text": "In section 4, we describe the architecture used to perform error type detection.",
                "label": 0
            },
            {
                "text": "In section 5, we present the experimental setup conducted to evaluate the performance of the proposed approach.",
                "label": 0
            },
            {
                "text": "We also established a baseline for performance evaluation and compares the results obtained from the proposed approach with those of previous works in the field.",
                "label": 0
            },
            {
                "text": "In section 6, discussion of the research is conducted.",
                "label": 0
            },
            {
                "text": "Finally, in section 7, we conclude our work.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Persian Typographical Error Type Detection Using Deep Neural Networks on Algorithmically_Generated Misspellings",
        "section": 6,
        "paragraph_id": 6,
        "full_text": "Several datasets for Persian spell-checking have been proposed.Similar to Aspell4, FAspell Although there are a number of Persian error datasets available, they do not consider all kinds of errors.Our objective was to develop a system that is capable of identifying any type of error (common or unusual).Therefore, we generate a dataset with a broad range of errors, which is an advantage of our method.",
        "sentences": [
            {
                "text": "Several datasets for Persian spell-checking have been proposed.",
                "label": 0
            },
            {
                "text": "Similar to Aspell4, FAspell Although there are a number of Persian error datasets available, they do not consider all kinds of errors.",
                "label": 0
            },
            {
                "text": "Our objective was to develop a system that is capable of identifying any type of error (common or unusual).",
                "label": 0
            },
            {
                "text": "Therefore, we generate a dataset with a broad range of errors, which is an advantage of our method.",
                "label": 1
            }
        ]
    },
    {
        "paper_name": "Persian Typographical Error Type Detection Using Deep Neural Networks on Algorithmically_Generated Misspellings",
        "section": 7,
        "paragraph_id": 7,
        "full_text": "Using different layers and text representations, we will explore various neural architectures to establish a baseline for the task of Persian spelling error type detection.The dataset used in the study contains 3,450,918 parallel words and error types, which are divided into 60%, 20%, and 20% for training, validation, and testing.Table5lists the parameters used to train the model.The maximum length of a sequence is set at 30 words, and the model is trained in 30 epochs with a batch size of 256.The Adam optimizer is used to adjust the network's parameters and minimize the loss function, which is calculated using categorical cross entropy.The learning rate parameter has been set to 2e-5.Another factor to be compared between Tables6and7is how different layers have contributed either positively or actively towards the obtained results.The Vanilla RNN performed fastest in both training and testing, making the model light and more feasible to work with.However, this comes at the cost of an inefficient model.In contrast, the bidirectional layers served relatively more accurately using either word or character embeddings.Although the bidirectional mechanism takes longer to function, it can be negligible since it slightly differs.In this stage, we develop our final architecture by exploiting the effective components of previously-examined methods.Since their accumulation is speed-efficient in our case, both word and character embeddings are utilized simultaneously to gain insight from two different representations and their previously discussed benefits.The two embeddings are further connected to a deep architecture, comprised of LSTM layers that process text bidirectionally before connecting to a TimeDistributed output layer for the token classification purpose.In contrast to unidirectional layers, bidirectional LSTM layers account for textual information from the past and future of a time frame through both forward and backward processing.The effectiveness of this architecture is established by drawing analogies with modern methodologies.",
        "sentences": [
            {
                "text": "Using different layers and text representations, we will explore various neural architectures to establish a baseline for the task of Persian spelling error type detection.",
                "label": 0
            },
            {
                "text": "The dataset used in the study contains 3,450,918 parallel words and error types, which are divided into 60%, 20%, and 20% for training, validation, and testing.",
                "label": 1
            },
            {
                "text": "Table5lists the parameters used to train the model.",
                "label": 0
            },
            {
                "text": "The maximum length of a sequence is set at 30 words, and the model is trained in 30 epochs with a batch size of 256.",
                "label": 0
            },
            {
                "text": "The Adam optimizer is used to adjust the network's parameters and minimize the loss function, which is calculated using categorical cross entropy.",
                "label": 0
            },
            {
                "text": "The learning rate parameter has been set to 2e-5.",
                "label": 0
            },
            {
                "text": "Another factor to be compared between Tables6and7is how different layers have contributed either positively or actively towards the obtained results.",
                "label": 0
            },
            {
                "text": "The Vanilla RNN performed fastest in both training and testing, making the model light and more feasible to work with.",
                "label": 0
            },
            {
                "text": "However, this comes at the cost of an inefficient model.",
                "label": 0
            },
            {
                "text": "In contrast, the bidirectional layers served relatively more accurately using either word or character embeddings.",
                "label": 0
            },
            {
                "text": "Although the bidirectional mechanism takes longer to function, it can be negligible since it slightly differs.",
                "label": 0
            },
            {
                "text": "In this stage, we develop our final architecture by exploiting the effective components of previously-examined methods.",
                "label": 0
            },
            {
                "text": "Since their accumulation is speed-efficient in our case, both word and character embeddings are utilized simultaneously to gain insight from two different representations and their previously discussed benefits.",
                "label": 0
            },
            {
                "text": "The two embeddings are further connected to a deep architecture, comprised of LSTM layers that process text bidirectionally before connecting to a TimeDistributed output layer for the token classification purpose.",
                "label": 0
            },
            {
                "text": "In contrast to unidirectional layers, bidirectional LSTM layers account for textual information from the past and future of a time frame through both forward and backward processing.",
                "label": 0
            },
            {
                "text": "The effectiveness of this architecture is established by drawing analogies with modern methodologies.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Persian Typographical Error Type Detection Using Deep Neural Networks on Algorithmically_Generated Misspellings",
        "section": 8,
        "paragraph_id": 8,
        "full_text": "Over the years, several publicly-available Persian spell-checkers have been developed in the industry, but the same is not true in academia.Previous studies' implementations were either not released or not available at the time of writing this paper.This hindered comparisons with previous work but motivated us to establish a baseline for future studies.However, this came at the expense of competing with industrialized spell-checkers, which a plethora of technicians developed using significantly large data and computational resources.Nonetheless, we compared our final approach with four applications.Previous studies have compared themselves with Microsoft Word.Besides, Lilak 9 is another commonly used spell-checker among Persian writers which proofreads text after installing an extension in Mozilla Firefox or Google Chrome.Furthermore, we also compared our approach with two highly-developed applications called Virastman and Paknevis 10 .One dilemma when drawing comparisons is the non-identical set of error classes that these applications identify, including ours.This leaves comparisons to be made on binary classes based on whether a misspelt word is detected or not.We designated a test case consisting of 202 synthetically misspelt and 2,651 correct words (2,853 words in total) extracted from Wikipedia and news (namely, Wikipedia test case).After testing our method and prior work on our test case, we were able to achieve competitive accuracy, precision, and recall scores, while our approach outperformed others on test time (Table8).Meaning the knowledge of sentences, words, and characters that our architecture has acquired to make a token classification based on our dataset is an appropriate infrastructure for our task of interest.We also achieved competitive results when we tested our model on PerspellData and Shargh, two publicly available spell-checking test cases provided by Dadmatech11.Our approach scored higher precision (93.1%) and recall (94.7%) compared to Paknevis and Virastman on the PerspellData test case (other scores were not being reported).It is noteworthy that such results have been obtained using relatively much fewer training data.Although Algorithm 1 is adaptable so that it can be used to expand our dataset up to a much larger size so that results would consequently rise, the lack of computational power impeded us from following up the upswing trend of scores, which was witnessed by training our approach on datasets ranging from 500,000 to 3,450,918.",
        "sentences": [
            {
                "text": "Over the years, several publicly-available Persian spell-checkers have been developed in the industry, but the same is not true in academia.",
                "label": 0
            },
            {
                "text": "Previous studies' implementations were either not released or not available at the time of writing this paper.",
                "label": 0
            },
            {
                "text": "This hindered comparisons with previous work but motivated us to establish a baseline for future studies.",
                "label": 0
            },
            {
                "text": "However, this came at the expense of competing with industrialized spell-checkers, which a plethora of technicians developed using significantly large data and computational resources.",
                "label": 0
            },
            {
                "text": "Nonetheless, we compared our final approach with four applications.",
                "label": 0
            },
            {
                "text": "Previous studies have compared themselves with Microsoft Word.",
                "label": 0
            },
            {
                "text": "Besides, Lilak 9 is another commonly used spell-checker among Persian writers which proofreads text after installing an extension in Mozilla Firefox or Google Chrome.",
                "label": 0
            },
            {
                "text": "Furthermore, we also compared our approach with two highly-developed applications called Virastman and Paknevis 10 .",
                "label": 0
            },
            {
                "text": "One dilemma when drawing comparisons is the non-identical set of error classes that these applications identify, including ours.",
                "label": 0
            },
            {
                "text": "This leaves comparisons to be made on binary classes based on whether a misspelt word is detected or not.",
                "label": 0
            },
            {
                "text": "We designated a test case consisting of 202 synthetically misspelt and 2,651 correct words (2,853 words in total) extracted from Wikipedia and news (namely, Wikipedia test case).",
                "label": 0
            },
            {
                "text": "After testing our method and prior work on our test case, we were able to achieve competitive accuracy, precision, and recall scores, while our approach outperformed others on test time (Table8).",
                "label": 0
            },
            {
                "text": "Meaning the knowledge of sentences, words, and characters that our architecture has acquired to make a token classification based on our dataset is an appropriate infrastructure for our task of interest.",
                "label": 0
            },
            {
                "text": "We also achieved competitive results when we tested our model on PerspellData and Shargh, two publicly available spell-checking test cases provided by Dadmatech11.",
                "label": 0
            },
            {
                "text": "Our approach scored higher precision (93.1%) and recall (94.7%) compared to Paknevis and Virastman on the PerspellData test case (other scores were not being reported).",
                "label": 0
            },
            {
                "text": "It is noteworthy that such results have been obtained using relatively much fewer training data.",
                "label": 0
            },
            {
                "text": "Although Algorithm 1 is adaptable so that it can be used to expand our dataset up to a much larger size so that results would consequently rise, the lack of computational power impeded us from following up the upswing trend of scores, which was witnessed by training our approach on datasets ranging from 500,000 to 3,450,918.",
                "label": 0
            }
        ]
      },
      {
        "paper_name": "Inline Citation Classification using Peripheral Context and Time_evolving Augmentation",
        "section": 0,
        "paragraph_id": 0,
        "full_text": "Abstract: Citation plays a pivotal role in determining the associations among research articles.It portrays essential information in indicative, supportive, or contrastive studies.The task of inline citation classification aids in extrapolating these relationships; However, existing studies are still immature and demand further scrutiny.Current datasets and methods used for inline citation classification only use citation-marked sentences constraining the model to turn a blind eye to domain knowledge and neighboring contextual sentences.In this paper, we propose a new dataset, named 3Cext, which along with the cited sentences, provides discourse information using the vicinal sentences to analyze the contrasting and entailing relationships as well as domain information.We propose PeriCite, a Transformer-based deep neural network that fuses peripheral sentences and domain knowledge.Our model achieves the state-of-the-art on the 3Cext dataset by +0.09 F1 against the best baseline.We conduct extensive ablations to analyze the efficacy of the proposed dataset and model fusion methods.",
        "sentences": [
            {
                "text": "Abstract: Citation plays a pivotal role in determining the associations among research articles.",
                "label": 0
            },
            {
                "text": "It portrays essential information in indicative, supportive, or contrastive studies.",
                "label": 0
            },
            {
                "text": "The task of inline citation classification aids in extrapolating these relationships; However, existing studies are still immature and demand further scrutiny.",
                "label": 0
            },
            {
                "text": "Current datasets and methods used for inline citation classification only use citation-marked sentences constraining the model to turn a blind eye to domain knowledge and neighboring contextual sentences.",
                "label": 0
            },
            {
                "text": "In this paper, we propose a new dataset, named 3Cext, which along with the cited sentences, provides discourse information using the vicinal sentences to analyze the contrasting and entailing relationships as well as domain information.",
                "label": 1
            },
            {
                "text": "We propose PeriCite, a Transformer-based deep neural network that fuses peripheral sentences and domain knowledge.",
                "label": 0
            },
            {
                "text": "Our model achieves the state-of-the-art on the 3Cext dataset by +0.09 F1 against the best baseline.",
                "label": 0
            },
            {
                "text": "We conduct extensive ablations to analyze the efficacy of the proposed dataset and model fusion methods.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Inline Citation Classification using Peripheral Context and Time_evolving Augmentation",
        "section": 1,
        "paragraph_id": 1,
        "full_text": "For the past several decades, there has been an interest in citation analysis for research evaluation.Researchers have emphasized the necessity for new methodologies that take into account various components of citing sentences.A wellknown qualitative technique for assessing the scientific influence is to analyze the sentence in which the research article is mentioned to ascertain the purpose behind the citation.The context of the citation, or the text in which the cited document is mentioned, has proven to be an effective indicator of the citation's intent[25].Measuring the scientific impact of research articles requires a fundamental understanding of citation intent.A great way to gauge the significance of a scientific publication is to determine why citations are made in one's work and how significant they are.",
        "sentences": [
            {
                "text": "For the past several decades, there has been an interest in citation analysis for research evaluation.",
                "label": 0
            },
            {
                "text": "Researchers have emphasized the necessity for new methodologies that take into account various components of citing sentences.",
                "label": 0
            },
            {
                "text": "A wellknown qualitative technique for assessing the scientific influence is to analyze the sentence in which the research article is mentioned to ascertain the purpose behind the citation.",
                "label": 0
            },
            {
                "text": "The context of the citation, or the text in which the cited document is mentioned, has proven to be an effective indicator of the citation's intent[25].",
                "label": 0
            },
            {
                "text": "Measuring the scientific impact of research articles requires a fundamental understanding of citation intent.",
                "label": 0
            },
            {
                "text": "A great way to gauge the significance of a scientific publication is to determine why citations are made in one's work and how significant they are.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Inline Citation Classification using Peripheral Context and Time_evolving Augmentation",
        "section": 2,
        "paragraph_id": 2,
        "full_text": "Previous methods for citation context categorization used a range of annotation techniques with low-to-high granularity.Comparing the earlier systems is extremely difficult due to the absence of standardized methodologies and annotation schemes.The 3C shared task[12,13]used a piece of the Academic Citation Typing (ACT) dataset to categorize the reference anchor into 'function' or 'purpose' by looking at the citing sentence or the text that contains the citation[19].Only quantitative elements are considered in traditional citation analysis based solely on the citation count.One of the biggest obstacles to citation context analysis for citation identification is that there is no multidisciplinary dataset and that there isn't any medium to fine-grained schemes that adequately represent the function and its influence[8].To address this challenge, Kunnath et al.[12]provided a unified task, called the 3C Shared Task, to compare several citation classification approaches on the same dataset to address the shortcomings of citation context categorization.The main distinction in the second iteration of this task[13]was that the subtasks contained full-text datasets.However, even with the full text, the metadata associated with the citation sentence was not adequate to understand the reasoning for the citation.",
        "sentences": [
            {
                "text": "Previous methods for citation context categorization used a range of annotation techniques with low-to-high granularity.",
                "label": 0
            },
            {
                "text": "Comparing the earlier systems is extremely difficult due to the absence of standardized methodologies and annotation schemes.",
                "label": 0
            },
            {
                "text": "The 3C shared task[12,13]used a piece of the Academic Citation Typing (ACT) dataset to categorize the reference anchor into 'function' or 'purpose' by looking at the citing sentence or the text that contains the citation[19].",
                "label": 0
            },
            {
                "text": "Only quantitative elements are considered in traditional citation analysis based solely on the citation count.",
                "label": 0
            },
            {
                "text": "One of the biggest obstacles to citation context analysis for citation identification is that there is no multidisciplinary dataset and that there isn't any medium to fine-grained schemes that adequately represent the function and its influence[8].",
                "label": 0
            },
            {
                "text": "To address this challenge, Kunnath et al.[12]provided a unified task, called the 3C Shared Task, to compare several citation classification approaches on the same dataset to address the shortcomings of citation context categorization.",
                "label": 0
            },
            {
                "text": "The main distinction in the second iteration of this task[13]was that the subtasks contained full-text datasets.",
                "label": 0
            },
            {
                "text": "However, even with the full text, the metadata associated with the citation sentence was not adequate to understand the reasoning for the citation.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Inline Citation Classification using Peripheral Context and Time_evolving Augmentation",
        "section": 3,
        "paragraph_id": 3,
        "full_text": "To alleviate the above limitations, we propose a new dataset, named 3Cext, and a new model, named PeriCite that combines the advantages of augmentation and peripheral context.Experiments show that the cited sentences heavily rely on the peripheral context to strengthen an argument by contrasting or entailing information.Our main contributions are as follows 1.We extend the 3C dataset[13]-3Cext, which, along with the cited sentence, adds more discourse information by providing contrasting and entailing information using the peripheral sentences.2. We propose a novel model, PeriCite, which uses spatial fusion and crosstext attention to attend to contextual information for the peripheral sentences and time-evolving augmentation to counter class imbalance during the training time.3. We also compare our proposed model against various baselines and show the efficacy of the module along with ablation studies and error analysis.",
        "sentences": [
            {
                "text": "To alleviate the above limitations, we propose a new dataset, named 3Cext, and a new model, named PeriCite that combines the advantages of augmentation and peripheral context.",
                "label": 1
            },
            {
                "text": "Experiments show that the cited sentences heavily rely on the peripheral context to strengthen an argument by contrasting or entailing information.",
                "label": 1
            },
            {
                "text": "Our main contributions are as follows 1.We extend the 3C dataset[13]-3Cext, which, along with the cited sentence, adds more discourse information by providing contrasting and entailing information using the peripheral sentences.",
                "label": 1
            },
            {
                "text": "2. We propose a novel model, PeriCite, which uses spatial fusion and crosstext attention to attend to contextual information for the peripheral sentences and time-evolving augmentation to counter class imbalance during the training time.",
                "label": 0
            },
            {
                "text": "3. We also compare our proposed model against various baselines and show the efficacy of the module along with ablation studies and error analysis.",
                "label": 0
            },
            {
                "text": "We also compare our proposed model against various baselines and show the efficacy of the module along with ablation studies and error analysis.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Inline Citation Classification using Peripheral Context and Time_evolving Augmentation",
        "section": 4,
        "paragraph_id": 4,
        "full_text": "In this section, we discuss our proposed 3Cext dataset in detail.Kunnath et al.[12]introduced the ACT dataset, with annotations for 11, 233 citations annotated by 883 authors.The cited label was masked with \"#AUTHOR TAG\" denoting the position of the cited object.Additionally, the 3C dataset contained full text and the label denoting the class of a particle citation (c.f.Table2).",
        "sentences": [
            {
                "text": "In this section, we discuss our proposed 3Cext dataset in detail.",
                "label": 1
            },
            {
                "text": "Kunnath et al.[12]introduced the ACT dataset, with annotations for 11, 233 citations annotated by 883 authors.",
                "label": 1
            },
            {
                "text": "The cited label was masked with \"#AUTHOR TAG\" denoting the position of the cited object.",
                "label": 1
            },
            {
                "text": "Additionally, the 3C dataset contained full text and the label denoting the class of a particle citation (c.f.Table2).",
                "label": 1
            }
        ]
    },
    {
        "paper_name": "Inline Citation Classification using Peripheral Context and Time_evolving Augmentation",
        "section": 5,
        "paragraph_id": 5,
        "full_text": "In our work, we extend the 3C dataset to house more discourse information to explain better why a citation is present in a sentence.Our intuition is that the cited sentences mostly either entail or contrast the adjoining sentences.To capture the peripheral sentences, we extract the full-text files corresponding to the COREIDs (unique paper ID) in our dataset to follow through on this discovery.",
        "sentences": [
            {
                "text": "In our work, we extend the 3C dataset to house more discourse information to explain better why a citation is present in a sentence.",
                "label": 1
            },
            {
                "text": "Our intuition is that the cited sentences mostly either entail or contrast the adjoining sentences.",
                "label": 1
            },
            {
                "text": "To capture the peripheral sentences, we extract the full-text files corresponding to the COREIDs (unique paper ID) in our dataset to follow through on this discovery.",
                "label": 1
            }
        ]
      },
      {
        "paper_name": "Language Variety Identification with True Labels",
        "section": 0,
        "paragraph_id": 0,
        "full_text": "Abstract: Language identification is an important first step in many IR and NLP applications.Most publicly available language identification datasets, however, are compiled under the assumption that the gold label of each instance is determined by where texts are retrieved from.Research has shown that this is a problematic assumption, particularly in the case of very similar languages (e.g., Croatian and Serbian) and national language varieties (e.g., Brazilian and European Portuguese), where texts may contain no distinctive marker of the particular language or variety.To overcome this important limitation, this paper presents DSL True Labels (DSL-TL), the first humanannotated multilingual dataset for language variety identification.DSL-TL contains a total of 12,900 instances in Portuguese, split between European Portuguese and Brazilian Portuguese; Spanish, split between Argentine Spanish and Castilian Spanish; and English, split between American English and British English.We trained multiple models to discriminate between these language varieties, and we present the results in detail.The data and models presented in this paper provide a reliable benchmark toward the development of robust and fairer language variety identification systems.We make DSL-TL freely available to the research community.",
        "sentences": [
            {
                "text": "Abstract: Language identification is an important first step in many IR and NLP applications.",
                "label": 0
            },
            {
                "text": "Most publicly available language identification datasets, however, are compiled under the assumption that the gold label of each instance is determined by where texts are retrieved from.",
                "label": 0
            },
            {
                "text": "Research has shown that this is a problematic assumption, particularly in the case of very similar languages (e.g., Croatian and Serbian) and national language varieties (e.g., Brazilian and European Portuguese), where texts may contain no distinctive marker of the particular language or variety.",
                "label": 0
            },
            {
                "text": "To overcome this important limitation, this paper presents DSL True Labels (DSL-TL), the first humanannotated multilingual dataset for language variety identification.",
                "label": 1
            },
            {
                "text": "DSL-TL contains a total of 12,900 instances in Portuguese, split between European Portuguese and Brazilian Portuguese; Spanish, split between Argentine Spanish and Castilian Spanish; and English, split between American English and British English.",
                "label": 1
            },
            {
                "text": "We trained multiple models to discriminate between these language varieties, and we present the results in detail.",
                "label": 0
            },
            {
                "text": "The data and models presented in this paper provide a reliable benchmark toward the development of robust and fairer language variety identification systems.",
                "label": 1
            },
            {
                "text": "We make DSL-TL freely available to the research community.",
                "label": 1
            }
        ]
    },
    {
        "paper_name": "Language Variety Identification with True Labels",
        "section": 1,
        "paragraph_id": 1,
        "full_text": "Language identification is the task of automatically identifying the language of a given text or document(Jauhiainen et al., 2019c).The task is a vital pre-processing step integrated into many IR and NLP applications.Language identification is commonly modeled as a supervised text classification task where the archetypal language identification system typically follows these four main steps(Lui, 2014): (a) Representation: selects a text representation (e.g., characters, words, or a combination of the two); (b) Language Modelling: derives a model from texts for each language;",
        "sentences": [
            {
                "text": "Language identification is the task of automatically identifying the language of a given text or document(Jauhiainen et al., 2019c).",
                "label": 0
            },
            {
                "text": "The task is a vital pre-processing step integrated into many IR and NLP applications.",
                "label": 0
            },
            {
                "text":"Language identification is commonly modeled as a supervised text classification task where the archetypal language identification system typically follows these four main steps(Lui, 2014): (a) Representation: selects a text representation (e.g., characters, words, or a combination of the two); (b) Language Modelling: derives a model from texts for each language;",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Language Variety Identification with True Labels",
        "section": 2,
        "paragraph_id": 2,
        "full_text": "(c) Classification: defines a function that best represents the similarity between a text and each language model; (d) Prediction: computes the highest-scoring model to determine the language of the given text.",
        "sentences": [
            {
                "text": "(c) Classification: defines a function that best represents the similarity between a text and each language model; (d) Prediction: computes the highest-scoring model to determine the language of the given text.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Language Variety Identification with True Labels",
        "section": 3,
        "paragraph_id": 3,
        "full_text": "In the early 2000s, language identification was widely considered as a solved task(McNamee, 2005)since character n-gram language models achieve perfect performance on discriminating between sets of dissimilar languages (e.g., Arabic, English, Finish, and Japanese) in standard contemporary texts (e.g., newspaper texts).Renewed interest in the task has emerged in the last decade with more challenging scenarios of particular interest to IR applications.This includes identifying the language of very short non-standard texts from user-generated content (e.g., microblogs)(Tromp and Pechenizkiy, 2011), and web queries(Anand, 2014;Ceylan and Kim, 2009).Other challenges to state-of-the-art language identification systems arise from linguistic phenomena such as codemixing and code-switching, where two or more languages are mixed in texts or social media posts(Solorio et al., 2014;Molina et al., 2016).Discriminating between very similar languages, dialects, and national varieties of the same language is another important, challenging language identification scenario that has been addressed by several studies(Tiedemann and Ljubešić, 2012;Lui and Cook, 2013;Bouamor et al., 2019).In this scenario, systems need to model fine distinctions between a set of closely-related languages (e.g., Bulgarian and Macedonian), dialects (e.g., the different dialects of Arabic), or national varieties of the same language (e.g., Brazilian and European Portuguese) to accurately discriminate between them.This challenge has been the main topic of the workshop series on NLP for Similar Languages, Varieties, and Dialects (VarDial)(Gȃman et al., 2020;Chakravarthi et al., 2021;Aepli et al., 2022)and their associated benchmark competitions which are organized yearly since 2014.The VarDial competitions have been providing the community with multiple datasets containing a wide variety of languages and dialects, helping to establish important language identification benchmarks.",
        "sentences": [
            {
                "text": "In the early 2000s, language identification was widely considered as a solved task(McNamee, 2005)since character n-gram language models achieve perfect performance on discriminating between sets of dissimilar languages (e.g., Arabic, English, Finish, and Japanese) in standard contemporary texts (e.g., newspaper texts).",
                "label": 0
            },
            {
                "text": "Renewed interest in the task has emerged in the last decade with more challenging scenarios of particular interest to IR applications.",
                "label": 0
            },
            {
                "text": "This includes identifying the language of very short non-standard texts from user-generated content (e.g., microblogs)(Tromp and Pechenizkiy, 2011), and web queries(Anand, 2014;Ceylan and Kim, 2009).",
                "label": 0
            },
            {
                "text": "Other challenges to state-of-the-art language identification systems arise from linguistic phenomena such as codemixing and code-switching, where two or more languages are mixed in texts or social media posts(Solorio et al., 2014;Molina et al., 2016).",
                "label": 0
            },
            {
                "text": "Discriminating between very similar languages, dialects, and national varieties of the same language is another important, challenging language identification scenario that has been addressed by several studies(Tiedemann and Ljubešić, 2012;Lui and Cook, 2013;Bouamor et al., 2019).",
                "label": 0
            },
            {
                "text": "In this scenario, systems need to model fine distinctions between a set of closely-related languages (e.g., Bulgarian and Macedonian), dialects (e.g., the different dialects of Arabic), or national varieties of the same language (e.g., Brazilian and European Portuguese) to accurately discriminate between them.",
                "label": 0
            },
            {
                "text": "This challenge has been the main topic of the workshop series on NLP for Similar Languages, Varieties, and Dialects (VarDial)(Gȃman et al., 2020;Chakravarthi et al., 2021;Aepli et al., 2022)and their associated benchmark competitions which are organized yearly since 2014.",
                "label": 0
            },
            {
                "text": "The VarDial competitions have been providing the community with multiple datasets containing a wide variety of languages and dialects, helping to establish important language identification benchmarks.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Language Variety Identification with True Labels",
        "section": 4,
        "paragraph_id": 4,
        "full_text": "As discussed in Section 2, the main limitation of the datasets collected for VarDial and similar competitions is that the gold labels for each instance are not obtained through human annotation.",
        "sentences": [
            {
                "text": "As discussed in Section 2, the main limitation of the datasets collected for VarDial and similar competitions is that the gold labels for each instance are not obtained through human annotation.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Language Variety Identification with True Labels",
        "section": 5,
        "paragraph_id": 5,
        "full_text": "The most widely used one, the DSL Corpus Collection (DSLCC)(Tan et al., 2014), for example, contains news texts retrieved from multiple newspaper websites considering the domain of the website as a proxy for language variety.For example, all content retrieved from news websites hosted in country-specific domains such as .brand .ptdomains is labeled as Brazilian and European Portuguese, respectively.While this is a straightforward assumption that results in a high number of accurate gold labels, this assumption has proved to be problematic in cases of republications of articles in different countries, particularly for languages that are widely spoken throughout the world, most notably English(Zampieri et al., 2014).Furthermore, multiple studies(Ács et al., 2015;Goutte et al., 2016)have evaluated native speakers' performance in identifying language varieties using the DSLCC concluding that many instances do not include any marker that allows humans to discriminate between varieties.",
        "sentences": [
            {
                "text": "The most widely used one, the DSL Corpus Collection (DSLCC)(Tan et al., 2014), for example, contains news texts retrieved from multiple newspaper websites considering the domain of the website as a proxy for language variety.",
                "label": 0
            },
            {
                "text": "ptdomains is labeled as Brazilian and European Portuguese, respectively.",
                "label": 0
            },
            {
                "text": "While this is a straightforward assumption that results in a high number of accurate gold labels, this assumption has proved to be problematic in cases of republications of articles in different countries, particularly for languages that are widely spoken throughout the world, most notably English(Zampieri et al., 2014).",
                "label": 0
            },
            {
                "text": "Furthermore, multiple studies(Ács et al., 2015;Goutte et al., 2016)have evaluated native speakers' performance in identifying language varieties using the DSLCC concluding that many instances do not include any marker that allows humans to discriminate between varieties.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Language Variety Identification with True Labels",
        "section": 6,
        "paragraph_id": 6,
        "full_text": "To address this limitation, in this paper, we introduce DSL True Labels (DSL-TL), the first humanannotated language variety identification dataset.To the best of our knowledge, no manually annotated dataset with true labels is available for language variety identification or language identification in general, and ours fills this gap.We collect instances available in the DSLCC and in other news corpora and gather multiple human judgments for each instance through a crowdsourcing platform.Finally, we train and evaluate multiple machinelearning models on this new dataset.",
        "sentences": [
            {
                "text": "To address this limitation, in this paper, we introduce DSL True Labels (DSL-TL), the first humanannotated language variety identification dataset.",
                "label": 1
            },
            {
                "text": "To the best of our knowledge, no manually annotated dataset with true labels is available for language variety identification or language identification in general, and ours fills this gap.",
                "label": 1
            },
            {
                "text": "We collect instances available in the DSLCC and in other news corpora and gather multiple human judgments for each instance through a crowdsourcing platform.",
                "label": 1
            },
            {
                "text": "Finally, we train and evaluate multiple machinelearning models on this new dataset.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Language Variety Identification with True Labels",
        "section": 7,
        "paragraph_id": 7,
        "full_text": "The contributions of this paper are the following: 1.A novel problem formulation for language variety identification and language identification in general. 2. The release of DSL-TL, the first humanannotated language identification dataset.1 3. An evaluation of multiple language identification models on this new dataset. The remainder of this paper is organized as follows.",
        "sentences": [
            {
                "text": "The contributions of this paper are the following: 1.A novel problem formulation for language variety identification and language identification in general. ",
                "label": 0
            },
            {
                "text": "2. The release of DSL-TL, the first humanannotated language identification dataset.1",
                "label": 1
            },
            {
                "text": "3. An evaluation of multiple language identification models on this new dataset.",
                "label": 0
            },
            {
                "text": "The remainder of this paper is organized as follows.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Language Variety Identification with True Labels",
        "section": 8,
        "paragraph_id": 8,
        "full_text": "Section 2 discusses prior research in language variety identification, including the VarDial competitions and available datasets.Section 3 details the steps taken in the construction DSL-TL dataset from data collection to annotation.Section 4 describes the language identification models used in our experiments, while Section 5 presents their results on the new DSL-TL dataset.Finally, Section 6 summarizes our research and discusses avenues for future work.",
        "sentences": [
            {
                "text": "Section 2 discusses prior research in language variety identification, including the VarDial competitions and available datasets.",
                "label": 0
            },
            {
                "text": "Section 3 details the steps taken in the construction DSL-TL dataset from data collection to annotation.",
                "label": 1
            },
            {
                "text": "Section 4 describes the language identification models used in our experiments, while Section 5 presents their results on the new DSL-TL dataset.",
                "label": 0
            },
            {
                "text": "Finally, Section 6 summarizes our research and discusses avenues for future work.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Language Variety Identification with True Labels",
        "section": 9,
        "paragraph_id": 9,
        "full_text": "The datasets used in the VarDial shared tasks, as well as other similar shared tasks(Zubiaga et al., 2016;Rangel et al., 2017), contain thousands of sentences in groups of languages or dialects sampled mostly from local newspapers and social media.Examples include Portuguese, Spanish, and English(Zampieri et al., 2014(Zampieri et al., , 2015)), Arabic(Malmasi et al., 2016), Chinese(Zampieri et al., 2019), Romanian(Gȃman et al., 2020), and Italian(Aepli et al., 2022).However, as discussed in the introduction, these datasets consist of instances assigned with a ground truth label determined by where the text was published (e.g., UK, USA, etc.).Each sentence within these datasets is, therefore, either, for example, American or British English, and only one of these labels is considered correct according to the gold labels included in these datasets.",
        "sentences": [
            {
                "text": "The datasets used in the VarDial shared tasks, as well as other similar shared tasks(Zubiaga et al., 2016;Rangel et al., 2017), contain thousands of sentences in groups of languages or dialects sampled mostly from local newspapers and social media.",
                "label": 0
            },
            {
                "text": "Examples include Portuguese, Spanish, and English(Zampieri et al., 2014(Zampieri et al., , 2015)), Arabic(Malmasi et al., 2016), Chinese(Zampieri et al., 2019), Romanian(Gȃman et al., 2020), and Italian(Aepli et al., 2022).",
                "label": 0
            },
            {
                "text": "However, as discussed in the introduction, these datasets consist of instances assigned with a ground truth label determined by where the text was published (e.g., UK, USA, etc.).",
                "label": 0
            },
            {
                "text": "Each sentence within these datasets is, therefore, either, for example, American or British English, and only one of these labels is considered correct according to the gold labels included in these datasets.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Language Variety Identification with True Labels",
        "section": 10,
        "paragraph_id": 10,
        "full_text": "The problem with formulating automatic language identification in this way is that many sentences do not necessarily belong to a single language variety(Goutte et al., 2016).This is true for varieties of English and varieties of other languages.The DSL dataset from 2014(Zampieri et al., 2014), for example, contained instances with incorrect ground truth labels.Articles containing features characteristic of British English were published in American newspapers and vice-versa, resulting in mislabeled data.Goutte et al. (2016)went on to show that human annotators were unable to achieve competitive performances on the DSL 2014 and 2015 datasets(Zampieri et al., 2014(Zampieri et al., , 2015)).On average, accuracies achieved by single human annotators were just over 50%, with performances varying between languages.Relying on the source of extracts or on an individual annotator to determine binary ground truth labels is, therefore, problematic.We address this limitation with DSL-TL by collecting the first human-annotated language identification dataset containing multiple annotations per instance.3 The Dataset: DSL-TL",
        "sentences": [
            {
                "text": "The problem with formulating automatic language identification in this way is that many sentences do not necessarily belong to a single language variety(Goutte et al., 2016).",
                "label": 0
            },
            {
                "text": "This is true for varieties of English and varieties of other languages.",
                "label": 0
            },
            {
                "text": "The DSL dataset from 2014(Zampieri et al., 2014), for example, contained instances with incorrect ground truth labels.",
                "label": 0
            },
            {
                "text": "Articles containing features characteristic of British English were published in American newspapers and vice-versa, resulting in mislabeled data.",
                "label": 0
            },
            {
                "text": "Goutte et al. (2016)went on to show that human annotators were unable to achieve competitive performances on the DSL 2014 and 2015 datasets(Zampieri et al., 2014(Zampieri et al., , 2015)).",
                "label": 0
            },
            {
                "text": "On average, accuracies achieved by single human annotators were just over 50%, with performances varying between languages.",
                "label": 0
            },
            {
                "text": "Relying on the source of extracts or on an individual annotator to determine binary ground truth labels is, therefore, problematic.",
                "label": 0
            },
            {
                "text":"We address this limitation with DSL-TL by collecting the first human-annotated language identification dataset containing multiple annotations per instance.3 The Dataset: DSL-TL.",
                "label": 1
            }
        ]
    },
    {
        "paper_name": "Language Variety Identification with True Labels",
        "section": 11,
        "paragraph_id": 11,
        "full_text": "DSL-TL contains 12,900 instances split between several language varieties, as shown in Table1.These instances vary between 1 to 3 sentences in length.They consist of short extracts taken from newspaper articles.The English articles have been sourced from a collection of news articles made available byZellers et al. (2019)-henceforth True News -while the Portuguese and Spanish articles have been sourced from the DSLCC(Tan et al., 2014).Both datasets feature data retrieved from multiple newspapers from each country.We randomly selected instances from the original datasets with an even split between each language variety, being a 2,500/2,500 split for Portuguese and Spanish varieties and a 1,500/1,500 split for English varieties.",
        "sentences": [
            {
                "text": "DSL-TL contains 12,900 instances split between several language varieties, as shown in Table1.",
                "label": 1
            },
            {
                "text": "These instances vary between 1 to 3 sentences in length.",
                "label": 1
            },
            {
                "text": "They consist of short extracts taken from newspaper articles.",
                "label": 1
            },
            {
                "text": "The English articles have been sourced from a collection of news articles made available byZellers et al. (2019)-henceforth True News -while the Portuguese and Spanish articles have been sourced from the DSLCC(Tan et al., 2014).",
                "label": 1
            },
            {
                "text": "Both datasets feature data retrieved from multiple newspapers from each country.",
                "label": 1
            },
            {
                "text": "We randomly selected instances from the original datasets with an even split between each language variety, being a 2,500/2,500 split for Portuguese and Spanish varieties and a 1,500/1,500 split for English varieties.",
                "label": 1
            }
        ]
      },
      {
        "paper_name": "llm_japanese_dataset v0_ Construction of Japanese Chat Dataset for Large Language Models and its Methodology",
        "section": 0,
        "paragraph_id": 0,
        "full_text": "Abstract: This study constructed a Japanese chat dataset for tuning large language models (LLMs), which consist of about 8.4 million records.Recently, LLMs have been developed and gaining popularity.However, high-performing LLMs are usually mainly for English.There are two ways to support languages other than English by those LLMs: constructing LLMs from scratch or tuning existing models.However, in both ways, datasets are necessary parts.In this study, we focused on supporting Japanese in those LLMs and making a dataset for training or tuning LLMs in Japanese.The dataset we constructed consisted of various tasks, such as translation and knowledge tasks.In our experiment, we tuned an existing LLM using our dataset and evaluated the performance qualitatively.The results suggest that our dataset is possibly beneficial for LLMs.However, we also revealed some difficulties in constructing LLMs in languages other than English.",
        "sentences": [
            {
                "text": "Abstract: This study constructed a Japanese chat dataset for tuning large language models (LLMs), which consist of about 8.4 million records.",
                "label": 1
            },
            {
                "text": "Recently, LLMs have been developed and gaining popularity.",
                "label": 0
            },
            {
                "text": "However, high-performing LLMs are usually mainly for English.",
                "label": 0
            },
            {
                "text": "There are two ways to support languages other than English by those LLMs: constructing LLMs from scratch or tuning existing models.",
                "label": 0
            },
            {
                "text": "However, in both ways, datasets are necessary parts.",
                "label": 0
            },
            {
                "text": "In this study, we focused on supporting Japanese in those LLMs and making a dataset for training or tuning LLMs in Japanese.",
                "label": 1
            },
            {
                "text": "The dataset we constructed consisted of various tasks, such as translation and knowledge tasks.",
                "label": 1
            },
            {
                "text": "In our experiment, we tuned an existing LLM using our dataset and evaluated the performance qualitatively.",
                "label": 0
            },
            {
                "text": "The results suggest that our dataset is possibly beneficial for LLMs.However, we also revealed some difficulties in constructing LLMs in languages other than English.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "llm_japanese_dataset v0_ Construction of Japanese Chat Dataset for Large Language Models and its Methodology",
        "section": 1,
        "paragraph_id": 1,
        "full_text": "2 Dataset Construction: izumi-lab/llm-japanese-dataset v0 In this study, we created a Japanese chat dataset.The dataset 1 contains 8,393,726 data points.In the following, we describe the details of datasets and their creation process.",
        "sentences": [
            {
                "text": "2 Dataset Construction: izumi-lab/llm-japanese-dataset v0 In this study, we created a Japanese chat dataset.",
                "label": 1
            },
            {
                "text": "The dataset 1 contains 8,393,726 data points.",
                "label": 1
            },
            {
                "text": "In the following, we describe the details of datasets and their creation process.",
                "label": 1
            }
        ]
    },
    {
        "paper_name": "llm_japanese_dataset v0_ Construction of Japanese Chat Dataset for Large Language Models and its Methodology",
        "section": 2,
        "paragraph_id": 2,
        "full_text": "Large language models (LLMs) have recently achieved remarkable progress in performance and generalization.Specifically, Transformer-based LLMs such as BERT[3]and the GPT series[17,18,1]have demonstrated high-performance thanks to their pre-training.Furthermore, models that have evolved from these, such as Chat-GPT[14]and GPT4[15], have gained popularity for their remarkable performance.Other models such asBard [6], LLaMA[24], Dolly[2], Bloom[21], and Vicuna[26]have also emerged.",
        "sentences": [
            {
                "text": "Large language models (LLMs) have recently achieved remarkable progress in performance and generalization.",
                "label": 0
            },
            {
                "text": "Specifically, Transformer-based LLMs such as BERT[3]and the GPT series[17,18,1]have demonstrated high-performance thanks to their pre-training.",
                "label": 0
            },
            {
                "text": "Furthermore, models that have evolved from these, such as Chat-GPT[14]and GPT4[15], have gained popularity for their remarkable performance.",
                "label": 0
            },
            {
                "text": "Other models such asBard [6], LLaMA[24], Dolly[2], Bloom[21], and Vicuna[26]have also emerged.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "llm_japanese_dataset v0_ Construction of Japanese Chat Dataset for Large Language Models and its Methodology",
        "section": 3,
        "paragraph_id": 3,
        "full_text": "Masanori HIRANO, Masahiro SUZUKI, and Hiroki SAKAJI The University of Tokyo, 7-3-1 Hongo, Bunkyo, Tokyo 113-8656 Japan, e-mail: research@mhirano.jp,b2019msuzuki@socsim.org,sakaji@sys.t.utokyo.ac.jpSome of those models are already provided to consumers as a web service.Moreover, via API, those models and services are also now available for sub-parts of web services, and many spin-off services are emerging.",
        "sentences": [
            {
                "text": "Masanori HIRANO, Masahiro SUZUKI, and Hiroki SAKAJI The University of Tokyo, 7-3-1 Hongo, Bunkyo, Tokyo 113-8656 Japan, e-mail: research@mhirano.jp,b2019msuzuki@socsim.org,sakaji@sys.t.utokyo.ac.jpSome of those models are already provided to consumers as a web service.",
                "label": 0
            },
            {
                "text": "Moreover, via API, those models and services are also now available for sub-parts of web services, and many spin-off services are emerging.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "llm_japanese_dataset v0_ Construction of Japanese Chat Dataset for Large Language Models and its Methodology",
        "section": 4,
        "paragraph_id": 4,
        "full_text": "However, despite the prosperity of language models, there are still challenges in handling diverse prompts, including prompts written in languages other than English.For example, Alpaca[23]dataset has been proposed due to the incompleteness of LLaMA's response.However, the dataset of Alpaca is only available in English, and the incompleteness pointed out by Alpaca has not been filled yet in the other languages.Moreover, LLaMA has difficultness to respond appropriately to some prompts in languages other than English.",
        "sentences": [
            {
                "text": "However, despite the prosperity of language models, there are still challenges in handling diverse prompts, including prompts written in languages other than English.",
                "label": 0
            },
            {
                "text": "For example, Alpaca[23]dataset has been proposed due to the incompleteness of LLaMA's response.",
                "label": 0
            },
            {
                "text": "However, the dataset of Alpaca is only available in English, and the incompleteness pointed out by Alpaca has not been filled yet in the other languages.",
                "label": 0
            },
            {
                "text": "Moreover, LLaMA has difficultness to respond appropriately to some prompts in languages other than English.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "llm_japanese_dataset v0_ Construction of Japanese Chat Dataset for Large Language Models and its Methodology",
        "section": 5,
        "paragraph_id": 5,
        "full_text": "Considering these challenges, it is necessary to enhance models' performances in languages other than English.However, it is not a good idea to study a specific model in terms of performance improvements in the other language.Moreover, model development is still ongoing and very competitive, and the situation is changing dramatically recently.It is also easy to assume that newer models with better performance will emerge in a few months or even 1-2 months.Therefore, enhancing datasets that support model training may be more useful than focusing on specific models.This approach may also lower the barrier to adapting new models to languages other than English.",
        "sentences": [
            {
                "text": "Considering these challenges, it is necessary to enhance models' performances in languages other than English.",
                "label": 0
            },
            {
                "text": "However, it is not a good idea to study a specific model in terms of performance improvements in the other language.",
                "label": 0
            },
            {
                "text": "Moreover, model development is still ongoing and very competitive, and the situation is changing dramatically recently.",
                "label": 0
            },
            {
                "text": "It is also easy to assume that newer models with better performance will emerge in a few months or even 1-2 months.",
                "label": 0
            },
            {
                "text": "Therefore, enhancing datasets that support model training may be more useful than focusing on specific models.",
                "label": 0
            },
            {
                "text": "This approach may also lower the barrier to adapting new models to languages other than English.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "llm_japanese_dataset v0_ Construction of Japanese Chat Dataset for Large Language Models and its Methodology",
        "section": 6,
        "paragraph_id": 6,
        "full_text": "Therefore, this study constructed a new chat dataset in Japanese for LLM training, which contains approximately 8.4 million data points, and demonstrated the performance of the dataset qualitatively.The dataset and trained models are opensourced and publicly available.The details are as follows: • Dataset: https://huggingface.co/datasets/izumi-lab/llm-japanesedataset • Trained Models (LLaMA 1 epoch): https://huggingface.co/izumi-lab/llama-13b-japanese-lora-v0-1ep.",
        "sentences": [
            {
                "text": "Therefore, this study constructed a new chat dataset in Japanese for LLM training, which contains approximately 8.4 million data points, and demonstrated the performance of the dataset qualitatively.",
                "label": 1
            },
            {
                "text": "The dataset and trained models are opensourced and publicly available.",
                "label": 1
            },
            {
                "text":"The details are as follows: • Dataset: https://huggingface.co/datasets/izumi-lab/llm-japanesedataset • Trained Models (LLaMA 1 epoch): https://huggingface.co/izumi-lab/llama-13b-japanese-lora-v0-1ep.",
                "label": 1
            }
        ]
    },
    {
        "paper_name": "llm_japanese_dataset v0_ Construction of Japanese Chat Dataset for Large Language Models and its Methodology",
        "section": 7,
        "paragraph_id": 7,
        "full_text": "The more details are explained in the following.Moreover, data expansion and additional model training are planned as future tasks.",
        "sentences": [
            {
                "text": "The more details are explained in the following.",
                "label": 0
            },
            {
                "text": "Moreover, data expansion and additional model training are planned as future tasks.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "llm_japanese_dataset v0_ Construction of Japanese Chat Dataset for Large Language Models and its Methodology",
        "section": 8,
        "paragraph_id": 8,
        "full_text": "The format of the chat data used for model training is shown below.In the description of the dataset later, we will omit some of the introductory parts and line breaks.",
        "sentences": [
            {
                "text": "The format of the chat data used for model training is shown below.",
                "label": 0
            },
            {
                "text": "In the description of the dataset later, we will omit some of the introductory parts and line breaks.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "llm_japanese_dataset v0_ Construction of Japanese Chat Dataset for Large Language Models and its Methodology",
        "section": 9,
        "paragraph_id": 9,
        "full_text": "Below is an instruction that describes a task, paired with an input that provides further context.Write a response that appropriately completes the request. Note that, in the following examples, the underlined sentences are originally written in Japanese.",
        "sentences": [
            {
                "text": "Below is an instruction that describes a task, paired with an input that provides further context.",
                "label": 0
            },
            {
                "text": "Write a response that appropriately completes the request.",
                "label": 0
            },
            {
                "text": "Note that, in the following examples, the underlined sentences are originally written in Japanese.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "llm_japanese_dataset v0_ Construction of Japanese Chat Dataset for Large Language Models and its Methodology",
        "section": 10,
        "paragraph_id": 10,
        "full_text": "We utilized the aforementioned ParaNatCom[25]to create tasks related to our research paper.The license for the dataset is CC BY 4.0, and the size of the created dataset is 1,732. Task Example 1 (partially omitted).",
        "sentences": [
            {
                "text": "We utilized the aforementioned ParaNatCom[25]to create tasks related to our research paper.",
                "label": 0
            },
            {
                "text": "The license for the dataset is CC BY 4.0, and the size of the created dataset is 1,732.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "llm_japanese_dataset v0_ Construction of Japanese Chat Dataset for Large Language Models and its Methodology",
        "section": 11,
        "paragraph_id": 11,
        "full_text": "### Instruction: Please make a title from the abstract of the paper. 1  ### Input: Superthin nanostructures, particularly with atomic-level thicknesses, typically display unique optical properties because of their exceptional light-matter interactions.Here, we report a facile strategy for the synthesis of sulfur-doped molybdenum oxide nanorings ...",
        "sentences": [
            {
                "text": "### Instruction: Please make a title from the abstract of the paper.",
                "label": 0
            },
            {
                "text": "1  ### Input: Superthin nanostructures, particularly with atomic-level thicknesses, typically display unique optical properties because of their exceptional light-matter interactions.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "llm_japanese_dataset v0_ Construction of Japanese Chat Dataset for Large Language Models and its Methodology",
        "section": 12,
        "paragraph_id": 12,
        "full_text": "### Instruction: Please rephrase the following Japanese into easy Japanese. 1 ### Input: Bill has no sense of adventure at all. 1  ### Response: Bill has no desire to do anything dangerous. 1",
        "sentences": [
            {
                "text": "### Instruction: Please rephrase the following Japanese into easy Japanese.",
                "label": 0
            },
            {
                "text": "1 ### Input: Bill has no sense of adventure at all.",
                "label": 0
            },
            {
                "text": "1  ### Response: Bill has no desire to do anything dangerous.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "llm_japanese_dataset v0_ Construction of Japanese Chat Dataset for Large Language Models and its Methodology",
        "section": 13,
        "paragraph_id": 13,
        "full_text": "In addition, we incorporated Japanese-translated versions of existing publicly available chat datasets.The following datasets were included: • Japanese-Alpaca-LoRA17: A translation of the Alpaca[23]dataset into Japanese.The license is Apache License 2.0.The dataset size is 52,002.• databricks-dolly-15k-ja18: A Japanese-translated version of the dataset used for training Dolly[2].The license is CC BY-SA 3.0.The dataset size is 15,015.",
        "sentences": [
            {
                "text": "In addition, we incorporated Japanese-translated versions of existing publicly available chat datasets.",
                "label": 0
            },
            {
                "text": "The following datasets were included: • Japanese-Alpaca-LoRA17: A translation of the Alpaca[23]dataset into Japanese.",
                "label": 0
            },
            {
                "text": "The license is Apache License 2.0.",
                "label": 0
            },
            {
                "text": "• databricks-dolly-15k-ja18: A Japanese-translated version of the dataset used for training Dolly[2].",
                "label": 0
            },
            {
                "text": "The license is CC BY-SA 3.0.",
                "label": 0
            },
            {
                "text": "The dataset size is 15,015.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "llm_japanese_dataset v0_ Construction of Japanese Chat Dataset for Large Language Models and its Methodology",
        "section": 14,
        "paragraph_id": 14,
        "full_text": "This study used LoRA[7]as a method to fine-tune LLMs without significant performance degradations.It is because building LLMs from scratch requires a massive amount of computational resources.Furthermore, LLMs with a large number of parameters require GPU resources not only for pre-training but also for fine-tuning.",
        "sentences": [
            {
                "text": "This study used LoRA[7]as a method to fine-tune LLMs without significant performance degradations.",
                "label": 0
            },
            {
                "text": "It is because building LLMs from scratch requires a massive amount of computational resources.",
                "label": 0
            },
            {
                "text": "Furthermore, LLMs with a large number of parameters require GPU resources not only for pre-training but also for fine-tuning.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "llm_japanese_dataset v0_ Construction of Japanese Chat Dataset for Large Language Models and its Methodology",
        "section": 15,
        "paragraph_id": 15,
        "full_text": "On the other hand, LoRA updates only small parts of LLM parameters.Therefore, LoRA is a feasible option for us to evaluate the benefits of our dataset. The main parameters used in the experiment are shown below.",
        "sentences": [
            {
                "text": "On the other hand, LoRA updates only small parts of LLM parameters.",
                "label": 0
            },
            {
                "text": "Therefore, LoRA is a feasible option for us to evaluate the benefits of our dataset.",
                "label": 0
            },
            {
                "text": "The main parameters used in the experiment are shown below.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "llm_japanese_dataset v0_ Construction of Japanese Chat Dataset for Large Language Models and its Methodology",
        "section": 16,
        "paragraph_id": 16,
        "full_text": "• Base model: LLaMA 13B[24]• Learning rate: 3e-4 We used PEFT[12]and DeepSpeed ZeRO 2[19]for the implementation.This tuned model is publicly available at https://huggingface.co/izumilab/llama-13b-japanese-lora-v0-1ep.",
        "sentences": [
            {
                "text": "• Base model: LLaMA 13B[24]• Learning rate: 3e-4 We used PEFT[12]and DeepSpeed ZeRO 2[19]for the implementation.",
                "label": 0
            },
            {
                "text": "This tuned model is publicly available at https://huggingface.co/izumilab/llama-13b-japanese-lora-v0-1ep.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "llm_japanese_dataset v0_ Construction of Japanese Chat Dataset for Large Language Models and its Methodology",
        "section": 17,
        "paragraph_id": 17,
        "full_text": "In order to increase the reproducibility of the evaluation experiment, the temperature parameter for prompt generation was set to 0.0.Below are some qualitative comparisons we conducted to assess performance.The phone rings.When the call is received, the person receiving the call should receive the call. 1   Response Example(5)### Input: What are the three major festivals in Kyoto? 1 ### Output(LLaMA+LoRA): The three major festivals in Kyoto are the spring festival, the summer festival, and the autumn festival. 1(Authors' note: Correct answer is Aoi, Gion, Jidai festivals.)### Output(LLaMA): What are the three major festivals in Kyoto?What are the three major festivals in Kyoto?What are the three major festivals in Kyoto? 1",
        "sentences": [
            {
                "text": "In order to increase the reproducibility of the evaluation experiment, the temperature parameter for prompt generation was set to 0.0.",
                "label": 0
            },
            {
                "text": "Below are some qualitative comparisons we conducted to assess performance.",
                "label": 0
            },
            {
                "text": "The phone rings.",
                "label": 0
            },
            {
                "text": "When the call is received, the person receiving the call should receive the call.",
                "label": 0
            },
            {
                "text": "1   Response Example(5)### Input: What are the three major festivals in Kyoto?",
                "label": 0
            },
            {
                "text": ")### Output(LLaMA): What are the three major festivals in Kyoto?",
                "label": 0
            },
            {
                "text": "What are the three major festivals in Kyoto?",
                "label": 0
            },
            {
                "text": "What are the three major festivals in Kyoto?",
                "label": 0
            }
        ]
      },  
      {
        "paper_name": "LMR_ A Large_Scale Multi_Reference Dataset for Reference_based Super_Resolution",
        "section": 0,
        "paragraph_id": 0,
        "full_text": "Abstract: It is widely agreed that reference-based super-resolution (RefSR) achieves superior results by referring to similar high quality images, compared to single image superresolution (SISR).Intuitively, the more references, the better performance.However, previous RefSR methods have all focused on single-reference image training, while multiple reference images are often available in testing or practical applications.The root cause of such trainingtesting mismatch is the absence of publicly available multireference SR training datasets, which greatly hinders research efforts on multi-reference super-resolution.To this end, we construct a large-scale, multi-reference superresolution dataset, named LMR.It contains 112,142 groups of 300×300 training images, which is 10× of the existing largest RefSR dataset.The image size is also much larger.More importantly, each group is equipped with 5 reference images with different similarity levels.Furthermore, we propose a new baseline method for multireference super-resolution: MRefSR, including a Multi-Reference Attention Module (MAM) for feature fusion of an arbitrary number of reference images, and a Spatial Aware Filtering Module (SAFM) for the fused feature selection.The proposed MRefSR achieves significant improvements over state-of-the-art approaches on both quantitative and qualitative evaluations.https://github.com/wdmwhh/MRefSR.",
        "sentences": [
            {
                "text": "Abstract: It is widely agreed that reference-based super-resolution (RefSR) achieves superior results by referring to similar high quality images, compared to single image superresolution (SISR).",
                "label": 0
            },
            {
                "text": "Intuitively, the more references, the better performance.",
                "label": 0
            },
            {
                "text": "However, previous RefSR methods have all focused on single-reference image training, while multiple reference images are often available in testing or practical applications.",
                "label": 0
            },
            {
                "text": "The root cause of such trainingtesting mismatch is the absence of publicly available multireference SR training datasets, which greatly hinders research efforts on multi-reference super-resolution.",
                "label": 0
            },
            {
                "text": "To this end, we construct a large-scale, multi-reference superresolution dataset, named LMR.",
                "label": 1
            },
            {
                "text": "It contains 112,142 groups of 300×300 training images, which is 10× of the existing largest RefSR dataset.",
                "label": 1
            },
            {
                "text": "The image size is also much larger.",
                "label": 1
            },
            {
                "text": "More importantly, each group is equipped with 5 reference images with different similarity levels.",
                "label": 1
            },
            {
                "text": "Furthermore, we propose a new baseline method for multireference super-resolution: MRefSR, including a Multi-Reference Attention Module (MAM) for feature fusion of an arbitrary number of reference images, and a Spatial Aware Filtering Module (SAFM) for the fused feature selection.",
                "label": 0
            },
            {
                "text": "The proposed MRefSR achieves significant improvements over state-of-the-art approaches on both quantitative and qualitative evaluations.https://github.com/wdmwhh/MRefSR.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "LMR_ A Large_Scale Multi_Reference Dataset for Reference_based Super_Resolution",
        "section": 1,
        "paragraph_id": 1,
        "full_text": "Single image super-resolution (SISR) is to restore a degraded low-resolution (LR) image to a texture-realistic high-resolution (HR) image[11].SISR has a wide range of applications in surveillance[39], astronomy[8], medical imaging[7], film and television[23,14], and other industries[26,37,33].With the development of deep learning, SISR has made great progress over these years[4,5,16,18,13,40,31,19,3,22].Compared with SISR, reference-* Work done during an internship at Baidu Inc. based super-resolution (RefSR) can leverage textures from additional similar HR reference images, so it often achieves better performance.",
        "sentences": [
            {
                "text": "Single image super-resolution (SISR) is to restore a degraded low-resolution (LR) image to a texture-realistic high-resolution (HR) image[11].",
                "label": 0
            },
            {
                "text": "SISR has a wide range of applications in surveillance[39], astronomy[8], medical imaging[7], film and television[23,14], and other industries[26,37,33].",
                "label": 0
            },
            {
                "text": "With the development of deep learning, SISR has made great progress over these years[4,5,16,18,13,40,31,19,3,22].",
                "label": 0
            },
            {
                "text": "Compared with SISR, reference-* Work done during an internship at Baidu Inc.based super-resolution (RefSR) can leverage textures from additional similar HR reference images, so it often achieves better performance.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "LMR_ A Large_Scale Multi_Reference Dataset for Reference_based Super_Resolution",
        "section": 2,
        "paragraph_id": 2,
        "full_text": "Because of promising results shown by recent RefSR methods[28,36,42,41,27,29,12,20,34], it attracts more and more research interest.However, all these previous RefSR methods have focused on using a single reference image for training, but there are often multiple reference images available for testing or practical applications.To the best of our knowledge, the only RefSR training dataset currently available is CUFED5[41,32], which has only 11,871 image pairs with a small resolution of 160×160.More importantly, there is only one reference image for each LR input image.However, in practical applications, multiple reference images are often encountered.For example, testing set of CUFED5 has 126 input images and each has 5 reference images with different similarity levels.Similarly, we can also easily find multiple reference images for any real test case.Due to the limitation of the only available training dataset, previous RefSR methods do not make good use of multiple reference images in testing or practical applica-tions.The previous RefSR methods usually stitch together several reference images to get a large resolution image as one reference image to fit the models trained with only one reference image.Nevertheless, if the resolution of the reference images is too large, this way of testing will exhaust the GPU memory.Furthermore, the relationship among multiple reference images is not modeled effectively.So this is certainly much worse than a method designed specifically for multiple reference images.Therefore, a multi-reference RefSR training dataset and a simple but effective multireference RefSR method are needed.",
        "sentences": [
            {
                "text": "Because of promising results shown by recent RefSR methods[28,36,42,41,27,29,12,20,34], it attracts more and more research interest.",
                "label": 0
            },
            {
                "text": "However, all these previous RefSR methods have focused on using a single reference image for training, but there are often multiple reference images available for testing or practical applications.",
                "label": 0
            },
            {
                "text": "To the best of our knowledge, the only RefSR training dataset currently available is CUFED5[41,32], which has only 11,871 image pairs with a small resolution of 160×160.",
                "label": 0
            },
            {
                "text": "More importantly, there is only one reference image for each LR input image.",
                "label": 0
            },
            {
                "text": "However, in practical applications, multiple reference images are often encountered.",
                "label": 0
            },
            {
                "text": "For example, testing set of CUFED5 has 126 input images and each has 5 reference images with different similarity levels.",
                "label": 0
            },
            {
                "text": "Similarly, we can also easily find multiple reference images for any real test case.",
                "label": 0
            },
            {
                "text": "Due to the limitation of the only available training dataset, previous RefSR methods do not make good use of multiple reference images in testing or practical applica-tions.",
                "label": 0
            },
            {
                "text": "The previous RefSR methods usually stitch together several reference images to get a large resolution image as one reference image to fit the models trained with only one reference image.",
                "label": 0
            },
            {
                "text": "Nevertheless, if the resolution of the reference images is too large, this way of testing will exhaust the GPU memory.",
                "label": 0
            },
            {
                "text": "Furthermore, the relationship among multiple reference images is not modeled effectively.",
                "label": 0
            },
            {
                "text": "So this is certainly much worse than a method designed specifically for multiple reference images.",
                "label": 0
            },
            {
                "text": "Therefore, a multi-reference RefSR training dataset and a simple but effective multireference RefSR method are needed.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "LMR_ A Large_Scale Multi_Reference Dataset for Reference_based Super_Resolution",
        "section": 3,
        "paragraph_id": 3,
        "full_text": "In this paper, we propose a large-scale, multi-reference RefSR dataset, named LMR.The training set of LMR consists of 112,142 groups of 300×300 training images, each group containing 5 reference images of different similarity levels.LMR training dataset has 10 times images compared to CUFED5 and the image size is also much larger.Such a sufficiently large training dataset will be beneficial for improving the generalization ability of models.We believe this training dataset will greatly facilitate the RefSR research as it is the first RefSR training dataset with multiple reference images.Meanwhile, the testing set of LMR has 142 groups of images and each group with 2∼6 reference images.The side length of the testing images ranges from 800 to 1600.",
        "sentences": [
            {
                "text": "In this paper, we propose a large-scale, multi-reference RefSR dataset, named LMR.",
                "label": 1
            },
            {
                "text": "The training set of LMR consists of 112,142 groups of 300×300 training images, each group containing 5 reference images of different similarity levels.",
                "label": 1
            },
            {
                "text": "LMR training dataset has 10 times images compared to CUFED5 and the image size is also much larger.",
                "label": 1
            },
            {
                "text": "Such a sufficiently large training dataset will be beneficial for improving the generalization ability of models.",
                "label": 1
            },
            {
                "text": "We believe this training dataset will greatly facilitate the RefSR research as it is the first RefSR training dataset with multiple reference images.",
                "label": 1
            },
            {
                "text": "Meanwhile, the testing set of LMR has 142 groups of images and each group with 2∼6 reference images.",
                "label": 1
            },
            {
                "text": "The side length of the testing images ranges from 800 to 1600.",
                "label": 1
            }
        ]
    },
    {
        "paper_name": "LMR_ A Large_Scale Multi_Reference Dataset for Reference_based Super_Resolution",
        "section": 4,
        "paragraph_id": 4,
        "full_text": "With the help of LMR, we propose a new RefSR baseline method for multiple reference RefSR, named MRefSR.First, we develop a Multi-Reference Attention Module (MAM) for feature fusion from an arbitrary number of reference images.We treat the LR input feature as query, and candidate keys and values are generated from the aligned reference features corresponding to different reference images.Then, attention across different aligned reference features is conducted to fuse features from different reference images.Second, since not all LR feature points can well match the reference features, we use Spatial Aware Filtering Module (SAFM) for fused feature selection.As shown in Figure1, our MRefSR effectively utilizes information from multiple reference images to produce visually pleasing details.In summary, our contributions are threefold: • We contribute the first multi-reference RefSR dataset, named LMR, which contains 112,142 groups of 300×300 training images and each group has 5 reference images for the input image.This dataset will enable RefSR research from single-reference to multireference images and largely promote the development of the RefSR research field. • We propose a novel multi-reference baseline RefSR method MRefSR, using a multi-reference attention module for feature fusion of an arbitrary number of reference images, and a spatial aware filtering module for the fused feature selection.Our method effectively learns the relationship among multiple references and makes the best use of them, this is also thanks to the multi-reference dataset LRM. • We conduct extensive experiments which demonstrate the superiority of the proposed LMR and the potential of multi-reference RefSR methods.Our method achieves significant improvements over state-of-the-art approaches on both quantitative and qualitative evaluations.",
        "sentences": [
            {
                "text": "With the help of LMR, we propose a new RefSR baseline method for multiple reference RefSR, named MRefSR.",
                "label": 0
            },
            {
                "text": "First, we develop a Multi-Reference Attention Module (MAM) for feature fusion from an arbitrary number of reference images.",
                "label": 0
            },
            {
                "text": "We treat the LR input feature as query, and candidate keys and values are generated from the aligned reference features corresponding to different reference images.",
                "label": 0
            },
            {
                "text": "Then, attention across different aligned reference features is conducted to fuse features from different reference images.",
                "label": 0
            },
            {
                "text": "Second, since not all LR feature points can well match the reference features, we use Spatial Aware Filtering Module (SAFM) for fused feature selection.",
                "label": 0
            },
            {
                "text": "As shown in Figure1, our MRefSR effectively utilizes information from multiple reference images to produce visually pleasing details.",
                "label": 0
            },
            {
                "text": "In summary, our contributions are threefold: • We contribute the first multi-reference RefSR dataset, named LMR, which contains 112,142 groups of 300×300 training images and each group has 5 reference images for the input image.",
                "label": 1
            },
            {
                "text": "This dataset will enable RefSR research from single-reference to multireference images and largely promote the development of the RefSR research field.",
                "label": 1
            },
            {
                "text": "• We propose a novel multi-reference baseline RefSR method MRefSR, using a multi-reference attention module for feature fusion of an arbitrary number of reference images, and a spatial aware filtering module for the fused feature selection.",
                "label": 0
            },
            {
                "text": "Our method effectively learns the relationship among multiple references and makes the best use of them, this is also thanks to the multi-reference dataset LRM.",
                "label": 0
            },
            {
                "text": "• We conduct extensive experiments which demonstrate the superiority of the proposed LMR and the potential of multi-reference RefSR methods.",
                "label": 0
            },
            {
                "text": "Our method achieves significant improvements over state-of-the-art approaches on both quantitative and qualitative evaluations.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "LMR_ A Large_Scale Multi_Reference Dataset for Reference_based Super_Resolution",
        "section": 5,
        "paragraph_id": 5,
        "full_text": "To the best of our knowledge, there are five datasets commonly used in RefSR research: Sun80[28], Urban100[9], Manga109[21], WR-SR[12]and CUFED5[41,32].However, the first four are all testing sets.The Sun80 dataset contains 80 natural images, each with 20 web-search reference images, but these reference images are not very similar to the corresponding LR input, so it is not suitable as a testing set for RefSR.The Urban100 dataset contains 100 building images, lacking references.Because of self-similarity in the building image, the corresponding LR image is usually treated as the reference image.The",
        "sentences": [
            {
                "text": "To the best of our knowledge, there are five datasets commonly used in RefSR research: Sun80[28], Urban100[9], Manga109[21], WR-SR[12]and CUFED5[41,32].",
                "label": 0
            },
            {
                "text": "However, the first four are all testing sets.",
                "label": 0
            },
            {
                "text": "The Sun80 dataset contains 80 natural images, each with 20 web-search reference images, but these reference images are not very similar to the corresponding LR input, so it is not suitable as a testing set for RefSR.",
                "label": 0
            },
            {
                "text": "The Urban100 dataset contains 100 building images, lacking references.",
                "label": 0
            },
            {
                "text": "Because of self-similarity in the building image, the corresponding LR image is usually treated as the reference image.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "LMR_ A Large_Scale Multi_Reference Dataset for Reference_based Super_Resolution",
        "section": 6,
        "paragraph_id": 6,
        "full_text": "We train our network on the proposed LMR training set and evaluate it on the testing set of LMR, CUFED5[41,32], Sun80[28]and WR-SR[12].As mentioned earlier, LMR and CUFED5 are two real multi-reference testing sets.Although Sun80 has multiple reference images, these reference images are not very similar to the corresponding target images.WR-SR is a single-reference testing set.According to previous practice, due to the relatively small images of CUFED5 testing set (300×500), when testing other singlereference RefSR methods on CUFED5, we stitch multiple reference images into one large reference image for testing.However, on the LMR and Sun80, due to the large image resolution of the testing sets and the limitation of GPU memory, other RefSR methods cannot be tested by the stitching reference image together and can only use a single reference image.With the multi-reference attention module (MAM), our MRefSR can utilize multiple reference images for prediction on the LMR, CUFED5 and Sun80 testing sets.we adopt two quantitative metrics, PSNR and SSIM, both calculated on Y channel in the transformed YCbCr color space.To evaluate the results qualitatively, we show the visual results of different methods and conduct a user study for subjective visual quality comparison.",
        "sentences": [
            {
                "text": "We train our network on the proposed LMR training set and evaluate it on the testing set of LMR, CUFED5[41,32], Sun80[28]and WR-SR[12].",
                "label": 0
            },
            {
                "text": "As mentioned earlier, LMR and CUFED5 are two real multi-reference testing sets.",
                "label": 0
            },
            {
                "text": "Although Sun80 has multiple reference images, these reference images are not very similar to the corresponding target images.",
                "label": 0
            },
            {
                "text": "WR-SR is a single-reference testing set.",
                "label": 0
            },
            {
                "text": "According to previous practice, due to the relatively small images of CUFED5 testing set (300×500), when testing other singlereference RefSR methods on CUFED5, we stitch multiple reference images into one large reference image for testing.",
                "label": 0
            },
            {
                "text": "However, on the LMR and Sun80, due to the large image resolution of the testing sets and the limitation of GPU memory, other RefSR methods cannot be tested by the stitching reference image together and can only use a single reference image.",
                "label": 0
            },
            {
                "text": "we adopt two quantitative metrics, PSNR and SSIM, both calculated on Y channel in the transformed YCbCr color space.",
                "label": 0
            },
            {
                "text": "To evaluate the results qualitatively, we show the visual results of different methods and conduct a user study for subjective visual quality comparison.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "LMR_ A Large_Scale Multi_Reference Dataset for Reference_based Super_Resolution",
        "section": 7,
        "paragraph_id": 7,
        "full_text": "We compare the proposed MRefSR with previous stateof-the-art SISR methods and single-reference RefSR methods.SISR methods include SRCNN[4], EDSR[18], Table1.We report PSNR/SSIM on Y channel of YCbCR space to compare among different SR methods on the testing set of LMR, CUFED5[41,32], Sun80[28], and WR-SR[12].Methods are grouped by SISR methods (top) and reference-based methods (bottom).The best results are marked in bold and with underlines.The second best and the third best results are marked in bold and with underlines, respectively.C 2 -Matching-LMR means C 2 -Matching-rec is trained on the LMR dataset and the Ours-rec-LPF indicates that the model was finetuned using large patch size (300×300) training images.",
        "sentences": [
            {
                "text": "We compare the proposed MRefSR with previous stateof-the-art SISR methods and single-reference RefSR methods.",
                "label": 0
            },
            {
                "text": "SISR methods include SRCNN[4], EDSR[18], Table1.",
                "label": 0
            },
            {
                "text": "We report PSNR/SSIM on Y channel of YCbCR space to compare among different SR methods on the testing set of LMR, CUFED5[41,32], Sun80[28], and WR-SR[12].",
                "label": 0
            },
            {
                "text": "Methods are grouped by SISR methods (top) and reference-based methods (bottom).",
                "label": 0
            },
            {
                "text": "The best results are marked in bold and with underlines.",
                "label": 0
            },
            {
                "text": "The second best and the third best results are marked in bold and with underlines, respectively.",
                "label": 0
            },
            {
                "text": "C 2 -Matching-LMR means C 2 -Matching-rec is trained on the LMR dataset and the Ours-rec-LPF indicates that the model was finetuned using large patch size (300×300) training images.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "LMR_ A Large_Scale Multi_Reference Dataset for Reference_based Super_Resolution",
        "section": 8,
        "paragraph_id": 8,
        "full_text": "Training[40], RRDB[31]and ESRGAN[31].As for singlereference RefSR methods, Landmark[36], CrossNet[42], SRNTT[41], TTSR[35], MASA[20], C 2 -Matching[12], AMSA[34]and TDF[10]are included.For fair comparison, we retrain three high-performance SISR methods RCAN, RRDB and ESRGAN, and one open-sourced topperforming single-reference RefSR method C 2 -Matching on the training set of LMR.Quantitative evaluation.As shown in Table1, our MRefSR outperforms other methods by a large margin on two real multiple reference datasets, CUFED5 and LMR.On the most commonly used CUFED5 benchmark, MRefSR outperforms the retrained C 2 -Matching-LMR by 0.29dB.Models trained on LMR can achieve better performance on CUFED5, which also demonstrates the generalization ability and effectiveness of LMR.What's more, MRefSR shows a significant improvement of 1.15 dB over the second best method on the LMR testing set.The above two results demonstrate the superiority of learning the interaction among multiple references, further manifesting the necessity of the LMR dataset that enables multi-reference RefSR training.On Sun80, SISR methods RRDB and RCAN get the best two results.The results gap of the top RefSR methods AMSA-rec, TDF-rec, C 2 -Matching-LMR and MRefSR are less than 0.04 dB, which further proves the reference image and its target image in Sun80 are not very similar.On the WR-SR benchmark, since there is only one reference image per LR, our results are very close to C 2 -matching-LMR.",
        "sentences": [
            {
                "text": "Training[40], RRDB[31]and ESRGAN[31].",
                "label": 0
            },
            {
                "text": "As for singlereference RefSR methods, Landmark[36], CrossNet[42], SRNTT[41], TTSR[35], MASA[20], C 2 -Matching[12], AMSA[34]and TDF[10]are included.",
                "label": 0
            },
            {
                "text": "For fair comparison, we retrain three high-performance SISR methods RCAN, RRDB and ESRGAN, and one open-sourced topperforming single-reference RefSR method C 2 -Matching on the training set of LMR.",
                "label": 0
            },
            {
                "text": "Quantitative evaluation.",
                "label": 0
            },
            {
                "text": "As shown in Table1, our MRefSR outperforms other methods by a large margin on two real multiple reference datasets, CUFED5 and LMR.",
                "label": 0
            },
            {
                "text": "On the most commonly used CUFED5 benchmark, MRefSR outperforms the retrained C 2 -Matching-LMR by 0.29dB.",
                "label": 0
            },
            {
                "text": "Models trained on LMR can achieve better performance on CUFED5, which also demonstrates the generalization ability and effectiveness of LMR.",
                "label": 0
            },
            {
                "text": "What's more, MRefSR shows a significant improvement of 1.15 dB over the second best method on the LMR testing set.",
                "label": 0
            },
            {
                "text": "The above two results demonstrate the superiority of learning the interaction among multiple references, further manifesting the necessity of the LMR dataset that enables multi-reference RefSR training.",
                "label": 0
            },
            {
                "text": "On Sun80, SISR methods RRDB and RCAN get the best two results.",
                "label": 0
            },
            {
                "text": "The results gap of the top RefSR methods AMSA-rec, TDF-rec, C 2 -Matching-LMR and MRefSR are less than 0.04 dB, which further proves the reference image and its target image in Sun80 are not very similar.",
                "label": 0
            },
            {
                "text": "On the WR-SR benchmark, since there is only one reference image per LR, our results are very close to C 2 -matching-LMR.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "LMR_ A Large_Scale Multi_Reference Dataset for Reference_based Super_Resolution",
        "section": 9,
        "paragraph_id": 9,
        "full_text": "Qualitative evaluation.As shown in Figure4, we compare the results of ESRGAN, MASA, C 2 -Matching, C 2 -Matching-LMR and our MRefSR.The top four examples are from CUFED5, and the models trained on LMR generalize well on the CUFED5 testing set, demonstrating the effectiveness of the proposed LMR.What's more, the results of our MRefSR trained with multiple references are much better than those trained with a single reference image.We also show four examples from the LMR testing set, and MRefSR can recover more texture details than other methods.",
        "sentences": [
            {
                "text": "Qualitative evaluation.",
                "label": 0
            },
            {
                "text": "As shown in Figure4, we compare the results of ESRGAN, MASA, C 2 -Matching, C 2 -Matching-LMR and our MRefSR.",
                "label": 0
            },
            {
                "text": "The top four examples are from CUFED5, and the models trained on LMR generalize well on the CUFED5 testing set, demonstrating the effectiveness of the proposed LMR.",
                "label": 0
            },
            {
                "text": "What's more, the results of our MRefSR trained with multiple references are much better than those trained with a single reference image.",
                "label": 0
            },
            {
                "text": "We also show four examples from the LMR testing set, and MRefSR can recover more texture details than other methods.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "LMR_ A Large_Scale Multi_Reference Dataset for Reference_based Super_Resolution",
        "section": 10,
        "paragraph_id": 10,
        "full_text": "Besides, we perform a user study to compare with some typical methods including ESRGAN, MASA and C 2 -Matching.Specifically, in each test, we present paired super-resolution results, one of which is generated by our MRefSR, and ask the users to choose the one with higher visual quality.As shown in Figure5, the users prefer our results over the others.",
        "sentences": [
            {
                "text": "Besides, we perform a user study to compare with some typical methods including ESRGAN, MASA and C 2 -Matching.",
                "label": 0
            },
            {
                "text": "Specifically, in each test, we present paired super-resolution results, one of which is generated by our MRefSR, and ask the users to choose the one with higher visual quality.",
                "label": 0
            },
            {
                "text": "As shown in Figure5, the users prefer our results over the others.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "LMR_ A Large_Scale Multi_Reference Dataset for Reference_based Super_Resolution",
        "section": 11,
        "paragraph_id": 11,
        "full_text": "In this section, we verify the effectiveness of Multireference Attention Module (MAM) and Spatial Aware Filtering Module (SAFM).Besides, we demonstrate the benefit of large-resolution training images of LMR.At last, we also investigate the impact of number of reference images.The effect of the number of reference images.To study the influence of number of reference images, we conduct experiments on the testing set of CUFED5, in which each LR input image has five reference images.As shown in Table3, as the number of reference images increases, although C 2 -Matching-LMR has a slight improvement with the stitching testing strategy, the gap is still smaller than the improvement of MRefSR.What's more, when the number of reference images is greater than 3, the results are worse than the case of 3 reference images, which indicates that the stitching testing strategy neglects the interaction among references, so the information from the fourth reference doesn't explore with that from the first three reference effectively in this case.In contrast, it can be seen that with the increase of reference images, MRefSR has a stable positive gain.Last but no least, MRefSR with five references has a PSNR increase of 0.272 dB than that with one reference, whereas C 2 -Matching-LMR only has a PSNR increase of 0.176 dB, which further demonstrates the superiority of modeling the relationship among multiple references.",
        "sentences": [
            {
                "text": "In this section, we verify the effectiveness of Multireference Attention Module (MAM) and Spatial Aware Filtering Module (SAFM).",
                "label": 0
            },
            {
                "text": "Besides, we demonstrate the benefit of large-resolution training images of LMR.",
                "label": 0
            },
            {
                "text": "At last, we also investigate the impact of number of reference images.",
                "label": 0
            },
            {
                "text": "The effect of the number of reference images.",
                "label": 0
            },
            {
                "text": "To study the influence of number of reference images, we conduct experiments on the testing set of CUFED5, in which each LR input image has five reference images.",
                "label": 0
            },
            {
                "text": "As shown in Table3, as the number of reference images increases, although C 2 -Matching-LMR has a slight improvement with the stitching testing strategy, the gap is still smaller than the improvement of MRefSR.",
                "label": 0
            },
            {
                "text": "What's more, when the number of reference images is greater than 3, the results are worse than the case of 3 reference images, which indicates that the stitching testing strategy neglects the interaction among references, so the information from the fourth reference doesn't explore with that from the first three reference effectively in this case.",
                "label": 0
            },
            {
                "text": "In contrast, it can be seen that with the increase of reference images, MRefSR has a stable positive gain.",
                "label": 0
            },
            {
                "text": "Last but no least, MRefSR with five references has a PSNR increase of 0.272 dB than that with one reference, whereas C 2 -Matching-LMR only has a PSNR increase of 0.176 dB, which further demonstrates the superiority of modeling the relationship among multiple references.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "LMR_ A Large_Scale Multi_Reference Dataset for Reference_based Super_Resolution",
        "section": 12,
        "paragraph_id": 12,
        "full_text": "Here, we present the computational cost comparisons between the proposed MRefSR and previous singlereference RefSR methods, including MASA[20]and C 2 -Matching[12].The computational cost is computed on CUFED5[41,32]using one NVIDIA V100 GPU.In specific, for the single-reference RefSR methods on CUFED5, we stitch five reference images into a 2500×500 image as the reference image for testing.Certainly, our MRefSR can directly utilize all the reference images for testing.Table4reports the GPU memory, runtime and performance for each method.Our MRefSR consumes the least GPU memory and achieves the best performance with acceptable runtime.",
        "sentences": [
            {
                "text": "Here, we present the computational cost comparisons between the proposed MRefSR and previous singlereference RefSR methods, including MASA[20]and C 2 -Matching[12].",
                "label": 0
            },
            {
                "text": "The computational cost is computed on CUFED5[41,32]using one NVIDIA V100 GPU.",
                "label": 0
            },
            {
                "text": "In specific, for the single-reference RefSR methods on CUFED5, we stitch five reference images into a 2500×500 image as the reference image for testing.",
                "label": 0
            },
            {
                "text": "Certainly, our MRefSR can directly utilize all the reference images for testing.",
                "label": 0
            },
            {
                "text": "Table4reports the GPU memory, runtime and performance for each method.",
                "label": 0
            },
            {
                "text": "Our MRefSR consumes the least GPU memory and achieves the best performance with acceptable runtime.",
                "label": 0
            }
        ]
      },
      {
        "paper_name": "MEETING ACTION ITEM DETECTION WITH REGULARIZED CONTEXT MODELING",
        "section": 0,
        "paragraph_id": 0,
        "full_text": "Abstract: Meetings are increasingly important for collaborations.Action items in meeting transcripts are crucial for managing post-meeting to-do tasks, which usually are summarized laboriously.The Action Item Detection task aims to automatically detect meeting content associated with action items.However, datasets manually annotated with action item detection labels are scarce and in small scale.We construct and release the first Chinese meeting corpus with manual action item annotations 1 .In addition, we propose a Context-Drop approach to utilize both local and global contexts by contrastive learning, and achieve better accuracy and robustness for action item detection.We also propose a Lightweight Model Ensemble method to exploit different pre-trained models 2 .Experimental results on our Chinese meeting corpus and the English AMI corpus demonstrate the effectiveness of the proposed approaches.",
        "sentences": [
            {
                "text": "Abstract: Meetings are increasingly important for collaborations.",
                "label": 0
            },
            {
                "text": "Action items in meeting transcripts are crucial for managing post-meeting to-do tasks, which usually are summarized laboriously.",
                "label": 0
            },
            {
                "text": "The Action Item Detection task aims to automatically detect meeting content associated with action items.",
                "label": 0
            },
            {
                "text": "However, datasets manually annotated with action item detection labels are scarce and in small scale.",
                "label": 0
            },
            {
                "text": "We construct and release the first Chinese meeting corpus with manual action item annotations 1 .",
                "label": 1
            },
            {
                "text": "In addition, we propose a Context-Drop approach to utilize both local and global contexts by contrastive learning, and achieve better accuracy and robustness for action item detection.",
                "label": 0
            },
            {
                "text": "We also propose a Lightweight Model Ensemble method to exploit different pre-trained models 2 .",
                "label": 0
            },
            {
                "text": "Experimental results on our Chinese meeting corpus and the English AMI corpus demonstrate the effectiveness of the proposed approaches.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "MEETING ACTION ITEM DETECTION WITH REGULARIZED CONTEXT MODELING",
        "section": 1,
        "paragraph_id": 1,
        "full_text": "Due to technological advances and the pandemic, online meetings become more and more common for collaboration and information sharing.Automatic Speech Recognition (ASR) systems can convert audio recordings of meetings into transcripts.Many Natural Language Processing (NLP) tasks are conducted on meeting transcripts to automatically extract or generate important information such as summaries, decisions, and action items.Action item refers to a task discussed in the meeting and assigned to participant(s) and expected to complete within a short time window after the meeting[1].The action item detection task aims to detect sentences containing information about actionable tasks in meeting transcripts.Action item detection could help users easily summarize meeting minutes, view and follow up on post-meeting to-do tasks.",
        "sentences": [
            {
                "text": "Due to technological advances and the pandemic, online meetings become more and more common for collaboration and information sharing.",
                "label": 0
            },
            {
                "text": "Automatic Speech Recognition (ASR) systems can convert audio recordings of meetings into transcripts.",
                "label": 0
            },
            {
                "text": "Many Natural Language Processing (NLP) tasks are conducted on meeting transcripts to automatically extract or generate important information such as summaries, decisions, and action items.",
                "label": 0
            },
            {
                "text": "Action item refers to a task discussed in the meeting and assigned to participant(s) and expected to complete within a short time window after the meeting[1].",
                "label": 0
            },
            {
                "text": "The action item detection task aims to detect sentences containing information about actionable tasks in meeting transcripts.",
                "label": 0
            },
            {
                "text": "Action item detection could help users easily summarize meeting minutes, view and follow up on post-meeting to-do tasks.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "MEETING ACTION ITEM DETECTION WITH REGULARIZED CONTEXT MODELING",
        "section": 2,
        "paragraph_id": 2,
        "full_text": "Action item detection is usually modeled as a sentence-level binary classification task, to determine whether a sentence contains action items or not.Many previous works[2]explore machine learning methods and feature engineering on publicly available meeting corpora such as ICSI[3]and AMI[4].Recently, with the success of the pretraining-finetuning paradigm and the revival of meeting-related research, approaches have been proposed based on pre-trained models[5], such as BERT[6]and ETC[7].In addition, some works[8]focus on detecting each element of action items independently, including task description, ownership, timeframe, and agreement.",
        "sentences": [
            {
                "text": "Action item detection is usually modeled as a sentence-level binary classification task, to determine whether a sentence contains action items or not.",
                "label": 0
            },
            {
                "text": "Many previous works[2]explore machine learning methods and feature engineering on publicly available meeting corpora such as ICSI[3]and AMI[4].",
                "label": 0
            },
            {
                "text": "Recently, with the success of the pretraining-finetuning paradigm and the revival of meeting-related research, approaches have been proposed based on pre-trained models[5], such as BERT[6]and ETC[7].",
                "label": 0
            },
            {
                "text": "In addition, some works[8]focus on detecting each element of action items independently, including task description, ownership, timeframe, and agreement.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "MEETING ACTION ITEM DETECTION WITH REGULARIZED CONTEXT MODELING",
        "section": 3,
        "paragraph_id": 3,
        "full_text": "For action item detection, existing public meeting corpora, such as the AMI meeting corpus and the ICSI meeting corpus, are far Fig.1.An example of action item detection.We show the speaker and sentence id, mark the action item, local context and global context.The local context provides the timeframe information.And the global context provides the task description information.from adequate to evaluate advanced deep learning models.We obtain 101 annotated AMI meetings with 381 action items following previous works[5].The ICSI meeting corpus comprises only 75 meetings without publicly available action item annotations.Therefore, we construct and make available a Chinese meeting corpus of 424 meetings with manual action item annotations on manual transcripts of meeting recordings (Table1), to prompt research on action item detection.",
        "sentences": [
            {
                "text": "For action item detection, existing public meeting corpora, such as the AMI meeting corpus and the ICSI meeting corpus, are far Fig.1.",
                "label": 0
            },
            {
                "text": "An example of action item detection.",
                "label": 0
            },
            {
                "text": "We show the speaker and sentence id, mark the action item, local context and global context.",
                "label": 0
            },
            {
                "text": "The local context provides the timeframe information.",
                "label": 0
            },
            {
                "text": "And the global context provides the task description information.from adequate to evaluate advanced deep learning models.",
                "label": 0
            },
            {
                "text": "We obtain 101 annotated AMI meetings with 381 action items following previous works[5].",
                "label": 0
            },
            {
                "text": "The ICSI meeting corpus comprises only 75 meetings without publicly available action item annotations.",
                "label": 0
            },
            {
                "text": "Therefore, we construct and make available a Chinese meeting corpus of 424 meetings with manual action item annotations on manual transcripts of meeting recordings (Table1), to prompt research on action item detection.",
                "label": 1
            }
        ]
    },
    {
        "paper_name": "MEETING ACTION ITEM DETECTION WITH REGULARIZED CONTEXT MODELING",
        "section": 4,
        "paragraph_id": 4,
        "full_text": "Context understanding plays a critical role in various tasks on meeting transcripts.Prior works[5,9]also explore context to improve action item detection performance.However, most methods concatenate the focus sentence with adjacent sentences (local context) and only achieve limited gains.As shown in Figure1, relevant but non-contiguous sentences (global context) also provide useful information for action items.On the other hand, both local and global contexts may contain irrelevant information, which may distract the classifier.We propose a novel Context-Drop approach to improve context modeling with regularization so that the model could focus more on the current sentence, to better exploit relevant information, and be less distracted by irrelevant information in context.",
        "sentences": [
            {
                "text": "Context understanding plays a critical role in various tasks on meeting transcripts.",
                "label": 0
            },
            {
                "text": "Prior works[5,9]also explore context to improve action item detection performance.",
                "label": 0
            },
            {
                "text": "However, most methods concatenate the focus sentence with adjacent sentences (local context) and only achieve limited gains.",
                "label": 0
            },
            {
                "text": "As shown in Figure1, relevant but non-contiguous sentences (global context) also provide useful information for action items.",
                "label": 0
            },
            {
                "text": "On the other hand, both local and global contexts may contain irrelevant information, which may distract the classifier.",
                "label": 0
            },
            {
                "text": "We propose a novel Context-Drop approach to improve context modeling with regularization so that the model could focus more on the current sentence, to better exploit relevant information, and be less distracted by irrelevant information in context.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "MEETING ACTION ITEM DETECTION WITH REGULARIZED CONTEXT MODELING",
        "section": 5,
        "paragraph_id": 5,
        "full_text": "In addition, we observe that the majority voting labels are usually correct during action item annotations.Inspired by this observation, we propose a Lightweight Model Ensemble method to improve performance by exploiting different pre-trained models while preserving inference latency.",
        "sentences": [
            {
                "text": "In addition, we observe that the majority voting labels are usually correct during action item annotations.",
                "label": 0
            },
            {
                "text": "Inspired by this observation, we propose a Lightweight Model Ensemble method to improve performance by exploiting different pre-trained models while preserving inference latency.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "MEETING ACTION ITEM DETECTION WITH REGULARIZED CONTEXT MODELING",
        "section": 6,
        "paragraph_id": 6,
        "full_text": "The contributions of our work are as follows: • We construct and make available a Chinese meeting corpus with action item annotations, to alleviate scarcity of resources and prompt related research.To the best of our knowledge, this is so far the largest meeting action item detection corpus.",
        "sentences": [
            {
                "text": "The contributions of our work are as follows: • We construct and make available a Chinese meeting corpus with action item annotations, to alleviate scarcity of resources and prompt related research.",
                "label": 1
            },
            {
                "text": "To the best of our knowledge, this is so far the largest meeting action item detection corpus.",
                "label": 1
            }
        ]
    },
    {
        "paper_name": "MEETING ACTION ITEM DETECTION WITH REGULARIZED CONTEXT MODELING",
        "section": 7,
        "paragraph_id": 7,
        "full_text": "The AMI meeting corpus[4]has played an essential role in various meeting-related research.It contains 171 meeting transcripts and various types of annotations.Among 171 meetings, 145 meetings are scenario-based meetings and 26 are naturally occurring meetings.The AMI meeting corpus is a common dataset for benchmarking action item detection systems.Although there are no direct annotations for action items for this corpus, indirect annotations can be generated based on annotations of the summary.Following previous works[5], we consider dialogue acts linked to the action-related abstractive summary as positive samples for action item detection and otherwise negative samples.In this way, we obtain 101 annotated meetings with 381 action items.",
        "sentences": [
            {
                "text": "The AMI meeting corpus[4]has played an essential role in various meeting-related research.",
                "label": 0
            },
            {
                "text": "It contains 171 meeting transcripts and various types of annotations.",
                "label": 0
            },
            {
                "text": "Among 171 meetings, 145 meetings are scenario-based meetings and 26 are naturally occurring meetings.",
                "label": 0
            },
            {
                "text": "The AMI meeting corpus is a common dataset for benchmarking action item detection systems.",
                "label": 0
            },
            {
                "text": "Although there are no direct annotations for action items for this corpus, indirect annotations can be generated based on annotations of the summary.",
                "label": 0
            },
            {
                "text": "Following previous works[5], we consider dialogue acts linked to the action-related abstractive summary as positive samples for action item detection and otherwise negative samples.",
                "label": 0
            },
            {
                "text": "In this way, we obtain 101 annotated meetings with 381 action items.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "MEETING ACTION ITEM DETECTION WITH REGULARIZED CONTEXT MODELING",
        "section": 8,
        "paragraph_id": 8,
        "full_text": "The two common datasets for action item detection, namely the AMI meeting corpus and ICSI meeting corpus, are both far from adequate for evaluating advanced deep learning models on action item detection.As described above, there are only 101 annotated meetings with 381 action items in the AMI meeting corpus.Another public meeting corpus, the ICSI meeting corpus, has action item annotations for 18 meetings[10]and is much smaller for action item detection research.Also, these annotations are no longer publicly available.Scarce and small-scale meeting datasets have hindered research on action item detection.To address this issue and prompt research on this topic, we construct and make available a Chinese meeting corpus, the AliMeeting-Action Corpus (denoted as AMC-A), with manual action item annotations on manual transcripts of meeting recordings.We extend 224 meetings previously published in[11]with additional 200 meetings.Each meeting session consists of a 15minute to 30-minute discussion by 2-4 participants covering certain topics from a diverse set, biased towards work meetings in various industries.All 424 meeting recordings are manually transcribed with punctuation inserted.Semantic units ended with a manually labeled period, question mark, and exclamation are treated as sentences for action item annotations and modeling.We formulate action item detection as a binary classification task and conduct sentence-level action item annotations, i.e., sentences containing action item information (task description, time frame, owner) as positive samples (labeled as 1) and otherwise negative samples (labeled as 0).As found in previous research and our experience, annotations of action items have high subjectivity and low consistency, e.g., only a Kappa coefficient of 0.36 on the ICSI corpus[10].To ease the task, we provide detailed annotation guidelines with sufficient examples.To reduce the annotation cost, we first select candidate sentences containing both temporal expressions (e.g., \"tomorrow\") and action-related verbs (e.g., \"finish\"), and highlight them in different colors.Candidate sentences are then annotated by three annotators independently.During annotation, candidate sentences are presented with their context so that annotators can easily exploit context information.With these quality control methods, the average Kappa coefficient on AMC-A between pairs of annotators is 0.47.For inconsistent labels from three annotators, an expert reviews the majority voting results and decides on final labels.Table1shows that AMC-A has much more meeting sessions, total utterances, and total action items than the AMI meeting corpus and comparable avg.action items per meeting.To the best of our knowledge, AMC-A is so far the first Chinese meeting corpus and the largest meeting corpus in any language labeled for action item detection.",
        "sentences": [
            {
                "text": "The two common datasets for action item detection, namely the AMI meeting corpus and ICSI meeting corpus, are both far from adequate for evaluating advanced deep learning models on action item detection.",
                "label": 0
            },
            {
                "text": "As described above, there are only 101 annotated meetings with 381 action items in the AMI meeting corpus.",
                "label": 0
            },
            {
                "text": "Another public meeting corpus, the ICSI meeting corpus, has action item annotations for 18 meetings[10]and is much smaller for action item detection research.",
                "label": 0
            },
            {
                "text": "Also, these annotations are no longer publicly available.",
                "label": 0
            },
            {
                "text": "Scarce and small-scale meeting datasets have hindered research on action item detection.",
                "label": 0
            },
            {
                "text": "To address this issue and prompt research on this topic, we construct and make available a Chinese meeting corpus, the AliMeeting-Action Corpus (denoted as AMC-A), with manual action item annotations on manual transcripts of meeting recordings.",
                "label": 1
            },
            {
                "text": "We extend 224 meetings previously published in[11]with additional 200 meetings.",
                "label": 1
            },
            {
                "text": "Each meeting session consists of a 15minute to 30-minute discussion by 2-4 participants covering certain topics from a diverse set, biased towards work meetings in various industries.",
                "label": 1
            },
            {
                "text": "All 424 meeting recordings are manually transcribed with punctuation inserted.",
                "label": 1
            },
            {
                "text": "Semantic units ended with a manually labeled period, question mark, and exclamation are treated as sentences for action item annotations and modeling.",
                "label": 1
            },
            {
                "text": "We formulate action item detection as a binary classification task and conduct sentence-level action item annotations, i.e., sentences containing action item information (task description, time frame, owner) as positive samples (labeled as 1) and otherwise negative samples (labeled as 0).",
                "label": 1
            },
            {
                "text": "As found in previous research and our experience, annotations of action items have high subjectivity and low consistency, e.g., only a Kappa coefficient of 0.36 on the ICSI corpus[10].",
                "label": 1
            },
            {
                "text": "To ease the task, we provide detailed annotation guidelines with sufficient examples.",
                "label": 1
            },
            {
                "text": "To reduce the annotation cost, we first select candidate sentences containing both temporal expressions (e.g., \"tomorrow\") and action-related verbs (e.g., \"finish\"), and highlight them in different colors.",
                "label": 1
            },
            {
                "text": "Candidate sentences are then annotated by three annotators independently.",
                "label": 1
            },
            {
                "text": "During annotation, candidate sentences are presented with their context so that annotators can easily exploit context information.",
                "label": 1
            },
            {
                "text": "With these quality control methods, the average Kappa coefficient on AMC-A between pairs of annotators is 0.47.",
                "label": 1
            },
            {
                "text": "For inconsistent labels from three annotators, an expert reviews the majority voting results and decides on final labels.",
                "label": 1
            },
            {
                "text": "Table1shows that AMC-A has much more meeting sessions, total utterances, and total action items than the AMI meeting corpus and comparable avg.action items per meeting.",
                "label": 1
            },
            {
                "text": "To the best of our knowledge, AMC-A is so far the first Chinese meeting corpus and the largest meeting corpus in any language labeled for action item detection.",
                "label": 1
            }
        ]
    },
    {
        "paper_name": "MEETING ACTION ITEM DETECTION WITH REGULARIZED CONTEXT MODELING",
        "section": 9,
        "paragraph_id": 9,
        "full_text": "We use both the AMI meeting corpus and our AMC-A corpus.We partition the AMI meeting corpus following the official scenarioonly dataset partitioning3.We partition AMC-A into train/dev/test sets with a ratio of 70:15:15, respectively.Considering the sparsity of positive samples, we report positive F1 as the evaluation metric.",
        "sentences": [
            {
                "text": "We use both the AMI meeting corpus and our AMC-A corpus.",
                "label": 0
            },
            {
                "text": "We partition the AMI meeting corpus following the official scenarioonly dataset partitioning3.",
                "label": 0
            },
            {
                "text": "We partition AMC-A into train/dev/test sets with a ratio of 70:15:15, respectively.",
                "label": 0
            },
            {
                "text": "Considering the sparsity of positive samples, we report positive F1 as the evaluation metric.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "MEETING ACTION ITEM DETECTION WITH REGULARIZED CONTEXT MODELING",
        "section": 10,
        "paragraph_id": 10,
        "full_text": "To evaluate our proposed methods on the AMC-A and AMI datasets, we use the following strong baseline pre-trained models, namely, BERT[6]4, RoBERTa[12], StructBERT[16]5, and Longformer[17]which provides efficient long-sequence modeling.For RoBERTa, We use the pre-trained Chinese RoBERTa-wwm-ext model[18]6.",
        "sentences": [
            {
                "text": "To evaluate our proposed methods on the AMC-A and AMI datasets, we use the following strong baseline pre-trained models, namely, BERT[6]4, RoBERTa[12], StructBERT[16]5, and Longformer[17]which provides efficient long-sequence modeling.",
                "label": 0
            },
            {
                "text": "For RoBERTa, We use the pre-trained Chinese RoBERTa-wwm-ext model[18]6.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "MEETING ACTION ITEM DETECTION WITH REGULARIZED CONTEXT MODELING",
        "section": 11,
        "paragraph_id": 11,
        "full_text": "For Longformer, we use the pre-trained Erlangshen-Longformer-110M[19]7to model action item detection as a sequence labeling task and use a fixed sliding window with size 4096 and allow one sentence overlap.The sentence labeling task takes multiple sentences as input and outputs the probabilities for every sentence.For BERT, StructBERT, and RoBERTa, we model action item detection as a sentence classification task and truncate input to 128 tokens.The sentence classification task takes a sentence as input and outputs the probabilities for the sentence.We compare our Context-Drop approach to R-Drop[15].R-Drop forces the predicted probability distribution of the same sample after two dropouts to be as close as possible.We compare the performance of R-Drop with sentencelevel inputs and context-level inputs (Figure2).We use TensorFlow and PyTorch to implement all models.All PLMs used are of BERT base size.The batch size is 32 and the dropout rate is 0.3.For each experiment in this paper, we run 5 times with different random seeds; for each run, we conduct a grid search among {1e -5, 2e -5} learning rate and {2, 3} epochs on the dev set.We then report the mean and standard deviation of the best results from 5 runs.The weight α of KL divergence loss is set to 4.0 for R-Drop and 1.0 for Context-Drop by optimizing positive F1 on the dev set.For each sentence, we use its preceding sentence and following sentence as local context, and select the top-2 most similar sentences to this sentence as global context (see Section 3.1 for details).The probability to keep contextual sentences is 50% for local or global contexts, and 70% for local & global contexts.Following setups in prior works, no sampling methods are applied.",
        "sentences": [
            {
                "text": "For Longformer, we use the pre-trained Erlangshen-Longformer-110M[19]7to model action item detection as a sequence labeling task and use a fixed sliding window with size 4096 and allow one sentence overlap.",
                "label": 0
            },
            {
                "text": "The sentence labeling task takes multiple sentences as input and outputs the probabilities for every sentence.",
                "label": 0
            },
            {
                "text": "For BERT, StructBERT, and RoBERTa, we model action item detection as a sentence classification task and truncate input to 128 tokens.",
                "label": 0
            },
            {
                "text": "The sentence classification task takes a sentence as input and outputs the probabilities for the sentence.",
                "label": 0
            },
            {
                "text": "We compare our Context-Drop approach to R-Drop[15].",
                "label": 0
            },
            {
                "text": "R-Drop forces the predicted probability distribution of the same sample after two dropouts to be as close as possible.",
                "label": 0
            },
            {
                "text": "We compare the performance of R-Drop with sentencelevel inputs and context-level inputs (Figure2).",
                "label": 0
            },
            {
                "text": "We use TensorFlow and PyTorch to implement all models.",
                "label": 0
            },
            {
                "text": "All PLMs used are of BERT base size.",
                "label": 0
            },
            {
                "text": "The batch size is 32 and the dropout rate is 0.3.",
                "label": 0
            },
            {
                "text": "For each experiment in this paper, we run 5 times with different random seeds; for each run, we conduct a grid search among {1e -5, 2e -5} learning rate and {2, 3} epochs on the dev set.",
                "label": 0
            },
            {
                "text": "We then report the mean and standard deviation of the best results from 5 runs.",
                "label": 0
            },
            {
                "text": "The weight α of KL divergence loss is set to 4.0 for R-Drop and 1.0 for Context-Drop by optimizing positive F1 on the dev set.",
                "label": 0
            },
            {
                "text": "For each sentence, we use its preceding sentence and following sentence as local context, and select the top-2 most similar sentences to this sentence as global context (see Section 3.1 for details).",
                "label": 0
            },
            {
                "text": "The probability to keep contextual sentences is 50% for local or global contexts, and 70% for local & global contexts.",
                "label": 0
            },
            {
                "text": "Following setups in prior works, no sampling methods are applied.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "MEETING ACTION ITEM DETECTION WITH REGULARIZED CONTEXT MODELING",
        "section": 12,
        "paragraph_id": 12,
        "full_text": "As shown in Table2, we compare different PLMs with different modeling tasks.When modeling action item detection as a sentence classification task, StructBERT outperforms BERT with a remarkable gain of +3.08 on positive F1.The word structural pretraining objective of StructBERT reconstructs tokens in the correct order from the shuffled trigrams.This could improve its robustness to disordered sentences, which is quite common in spoken languages, and in turn improve its performance of meeting action item detection.We formulate action item detection as a sequence labeling task to exploit the advantage of Longformer in long-sequence modeling.However, we only observe limited improvement from Longformer over BERT, 0.59 gain on positive F1.Therefore, we formulate action item detection as a sentence classification task and use StructBERT as the pre-trained model for evaluating Context-Drop.",
        "sentences": [
            {
                "text": "As shown in Table2, we compare different PLMs with different modeling tasks.",
                "label": 0
            },
            {
                "text": "When modeling action item detection as a sentence classification task, StructBERT outperforms BERT with a remarkable gain of +3.08 on positive F1.",
                "label": 0
            },
            {
                "text": "The word structural pretraining objective of StructBERT reconstructs tokens in the correct order from the shuffled trigrams.",
                "label": 0
            },
            {
                "text": "This could improve its robustness to disordered sentences, which is quite common in spoken languages, and in turn improve its performance of meeting action item detection.",
                "label": 0
            },
            {
                "text": "We formulate action item detection as a sequence labeling task to exploit the advantage of Longformer in long-sequence modeling.",
                "label": 0
            },
            {
                "text": "However, we only observe limited improvement from Longformer over BERT, 0.59 gain on positive F1.",
                "label": 0
            },
            {
                "text": "Therefore, we formulate action item detection as a sentence classification task and use StructBERT as the pre-trained model for evaluating Context-Drop.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "MEETING ACTION ITEM DETECTION WITH REGULARIZED CONTEXT MODELING",
        "section": 13,
        "paragraph_id": 13,
        "full_text": "As shown in Table3, based on the baseline StructBERT, we compare various contrastive learning methods using different contexts (Figure2).On AMC-A, when not using contrastive learning methods, i.e., no w/ R-Drop nor w/ Context-Drop, the baseline using both local and global context performs the best (69.09), followed by the baseline using the local context (68.50).We observe the same trend on AMI.This indicates that adjacent contextual sentences do provide useful information.Global context provides complementary information and a combination of global and local context achieves further improvement.On AMC-A, when using different contrastive learning methods, the configuration of using the focus sentence and local & global context as input with Context-Drop dynamic achieves the best performance (70.82), outperforming the baseline using the sentence as input without contrastive learning (67.84) by +2.98 absolute on positive F1, and also outperforming R-Drop (68.72) by +2.10 absolute gain.On AMI, sentence+local context w/ Context-Drop fixed (43.12) also outperforms the baseline sentence input (38.67) and R-Drop (42.72).These results confirm our hypothesis that Context-Drop could help the model to focus more on the current sentence, exploit relevant information in context and be less distracted by irrelevant information.Moreover, a reduction in the standard deviations shows that Context-Drop improves the stability and robustness of the model.For different contexts, Context-Drop dynamic outperforms Context-Drop fixed in most cases, which suggests that the flexible and dynamic contrastive learning method can achieve better performance.",
        "sentences": [
            {
                "text": "As shown in Table3, based on the baseline StructBERT, we compare various contrastive learning methods using different contexts (Figure2).",
                "label": 0
            },
            {
                "text": "On AMC-A, when not using contrastive learning methods, i.e., no w/ R-Drop nor w/ Context-Drop, the baseline using both local and global context performs the best (69.09), followed by the baseline using the local context (68.50).",
                "label": 0
            },
            {
                "text": "We observe the same trend on AMI.",
                "label": 0
            },
            {
                "text": "This indicates that adjacent contextual sentences do provide useful information.",
                "label": 0
            },
            {
                "text": "Global context provides complementary information and a combination of global and local context achieves further improvement.",
                "label": 0
            },
            {
                "text": "On AMC-A, when using different contrastive learning methods, the configuration of using the focus sentence and local & global context as input with Context-Drop dynamic achieves the best performance (70.82), outperforming the baseline using the sentence as input without contrastive learning (67.84) by +2.98 absolute on positive F1, and also outperforming R-Drop (68.72) by +2.10 absolute gain.",
                "label": 0
            },
            {
                "text": "On AMI, sentence+local context w/ Context-Drop fixed (43.12) also outperforms the baseline sentence input (38.67) and R-Drop (42.72).",
                "label": 0
            },
            {
                "text": "These results confirm our hypothesis that Context-Drop could help the model to focus more on the current sentence, exploit relevant information in context and be less distracted by irrelevant information.",
                "label": 0
            },
            {
                "text": "Moreover, a reduction in the standard deviations shows that Context-Drop improves the stability and robustness of the model.",
                "label": 0
            },
            {
                "text": "For different contexts, Context-Drop dynamic outperforms Context-Drop fixed in most cases, which suggests that the flexible and dynamic contrastive learning method can achieve better performance.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "MEETING ACTION ITEM DETECTION WITH REGULARIZED CONTEXT MODELING",
        "section": 14,
        "paragraph_id": 14,
        "full_text": "We also conduct ablation analysis on Context-Drop, as in the second group of Table3.Without the regularization loss of KL divergence (denoted KL loss), Context-Drop can be regarded as a data augmentation method using fixed or dynamically selected context.On AMC-A and AMI, for both Context-Drop fixed and Context-Drop dynamic , w/o KL loss degrades the performance, which indicates contrastive learning is important for gains.With the regularization loss, the model could better focus on the current sentence and be less distracted by irrelevant information in context.",
        "sentences": [
            {
                "text": "We also conduct ablation analysis on Context-Drop, as in the second group of Table3.",
                "label": 0
            },
            {
                "text": "Without the regularization loss of KL divergence (denoted KL loss), Context-Drop can be regarded as a data augmentation method using fixed or dynamically selected context.",
                "label": 0
            },
            {
                "text": "On AMC-A and AMI, for both Context-Drop fixed and Context-Drop dynamic , w/o KL loss degrades the performance, which indicates contrastive learning is important for gains.",
                "label": 0
            },
            {
                "text": "With the regularization loss, the model could better focus on the current sentence and be less distracted by irrelevant information in context.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "MEETING ACTION ITEM DETECTION WITH REGULARIZED CONTEXT MODELING",
        "section": 15,
        "paragraph_id": 15,
        "full_text": "As shown in Table4, we compare the performance of applying Lightweight Model Ensemble integrating various pre-trained models using the sentence input.StructBERT encoder with RoBERTa pooler layer parameters achieves +0.52 absolute gain and RoBERTa encoder with StructBERT pooler layer parameters achieves +0.38 absolute gain.These results show that Lightweight Model Ensemble could integrate knowledge from different models and achieve better performance without increasing the overall number of parameters.",
        "sentences": [
            {
                "text": "As shown in Table4, we compare the performance of applying Lightweight Model Ensemble integrating various pre-trained models using the sentence input.",
                "label": 0
            },
            {
                "text": "StructBERT encoder with RoBERTa pooler layer parameters achieves +0.52 absolute gain and RoBERTa encoder with StructBERT pooler layer parameters achieves +0.38 absolute gain.",
                "label": 0
            },
            {
                "text": "These results show that Lightweight Model Ensemble could integrate knowledge from different models and achieve better performance without increasing the overall number of parameters.",
                "label": 0
            }
        ]
      },
      {
        "paper_name": "Multi_Party Chat_ Conversational Agents in Group Settings with Humans and Models",
        "section": 0,
        "paragraph_id": 0,
        "full_text": "Abstract: Current dialogue research primarily studies pairwise (two-party) conversations, and does not address the everyday setting where more than two speakers converse together.In this work, we both collect and evaluate multi-party conversations to study this more general case.We use the LIGHT environment 1 to construct grounded conversations, where each participant has an assigned character to role-play.We thus evaluate the ability of language models to act as one or more characters in such conversations.Models require two skills that pairwise-trained models appear to lack: (1) being able to decide when to talk; (2) producing coherent utterances grounded on multiple characters.We compare models trained on our new dataset to existing pairwise-trained dialogue models, as well as large language models with few-shot prompting.We find that our new dataset, MultiLIGHT, which we publicly release 2 , can help bring significant improvements in the group setting.",
        "sentences": [
            {
                "text": "Abstract: Current dialogue research primarily studies pairwise (two-party) conversations, and does not address the everyday setting where more than two speakers converse together.",
                "label": 0
            },
            {
                "text": "In this work, we both collect and evaluate multi-party conversations to study this more general case.",
                "label": 1
            },
            {
                "text": "We use the LIGHT environment 1 to construct grounded conversations, where each participant has an assigned character to role-play.",
                "label": 1
            },
            {
                "text": "We thus evaluate the ability of language models to act as one or more characters in such conversations.",
                "label": 0
            },
            {
                "text": "Models require two skills that pairwise-trained models appear to lack: (1) being able to decide when to talk; (2) producing coherent utterances grounded on multiple characters.",
                "label": 0
            },
            {
                "text": "We compare models trained on our new dataset to existing pairwise-trained dialogue models, as well as large language models with few-shot prompting.",
                "label": 0
            },
            {
                "text": "We find that our new dataset, MultiLIGHT, which we publicly release 2 , can help bring significant improvements in the group setting.",
                "label": 1
            }
        ]
    },
    {
        "paper_name": "Multi_Party Chat_ Conversational Agents in Group Settings with Humans and Models",
        "section": 1,
        "paragraph_id": 1,
        "full_text": "Conversational AI in general and chatbots in particular have made remarkable breakthroughs in the last few years(Shuster et al., 2022b;Thoppilan et al., 2022;OpenAI, 2022).Despite having humanlike (and sometimes super-human) responses, the presupposed setting is constrained to a turn-based (round-robin) conversation.Further, almost all settings consider a single AI model and a single human user as the only participants in the conversation.Although a reasonable assumption for a rudimentary AI assistant, this restriction limits the social abilities of AI, such as cooperation and teamwork, which are essential for an intelligent agent(Seeber et al., 2020;Dafoe et al., 2021).* Work done during a Meta AI internship. † Work done while at Meta AI, now at DeepMind.",
        "sentences": [
            {
                "text": "Conversational AI in general and chatbots in particular have made remarkable breakthroughs in the last few years(Shuster et al., 2022b;Thoppilan et al., 2022;OpenAI, 2022).",
                "label": 0
            },
            {
                "text": "Despite having humanlike (and sometimes super-human) responses, the presupposed setting is constrained to a turn-based (round-robin) conversation.",
                "label": 0
            },
            {
                "text": "Further, almost all settings consider a single AI model and a single human user as the only participants in the conversation.",
                "label": 0
            },
            {
                "text": "* Work done during a Meta AI internship.",
                "label": 0
            },
            {
                "text": "† Work done while at Meta AI, now at DeepMind.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Multi_Party Chat_ Conversational Agents in Group Settings with Humans and Models",
        "section": 2,
        "paragraph_id": 2,
        "full_text": "1 https://parl.ai/projects/light/ 2https://github.com/facebookresearch/LIGHT/tree/main/light/modeling/tasks/multilight The aim of this work is to both evaluate how good current models are when extended to the multi-party dialogue setting, and to investigate how to improve such models for aspects where they fail.There are two main challenges that this study focuses on: 1. Turn-taking: deciding when to speak next is important for the flow of the conversation.Speaking out of turn or being silent when a response is expected negatively affects our assessment of a speaker. 2. Coherence of the utterances: generating appropriate responses requires taking into account the dialogue from multiple people in the conversation.The model must distinguish the states and characteristics of each of the participants to produce good responses.For example, even in pairwise (two-party) conversations, maintaining identity is known to be challenging for models(Shuster et al., 2021c).",
        "sentences": [
            {
                "text": "1 https://parl.ai/projects/light/ 2https://github.com/facebookresearch/LIGHT/tree/main/light/modeling/tasks/multilight The aim of this work is to both evaluate how good current models are when extended to the multi-party dialogue setting, and to investigate how to improve such models for aspects where they fail.",
                "label": 0
            },
            {
                "text": "There are two main challenges that this study focuses on: 1.Turn-taking: deciding when to speak next is important for the flow of the conversation.",
                "label": 0
            },
            {
                "text": "Speaking out of turn or being silent when a response is expected negatively affects our assessment of a speaker.",
                "label": 0
            },
            {
                "text": "2.Coherence of the utterances: generating appropriate responses requires taking into account the dialogue from multiple people in the conversation.",
                "label": 0
            },
            {
                "text": "The model must distinguish the states and characteristics of each of the participants to produce good responses.",
                "label": 0
            },
            {
                "text": "For example, even in pairwise (two-party) conversations, maintaining identity is known to be challenging for models(Shuster et al., 2021c).",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Multi_Party Chat_ Conversational Agents in Group Settings with Humans and Models",
        "section": 3,
        "paragraph_id": 3,
        "full_text": "We first collect a new dataset of group conversations, MultiLIGHT, each with three human participants.We assign a fantasy location setting to each conversation, and participants are given a character descriptions (personas) to role-play.The locations and characters are extracted from previous works on this game(Urbanek et al., 2019).Figure1shows a small snippet of an example from the new dataset.This setting allows us to control the conversational situations and have knowledge of the roles the agents should be playing in the conversation, making evaluation of systems more tractable compared to completely uncontrolled settings.",
        "sentences": [
            {
                "text": "We first collect a new dataset of group conversations, MultiLIGHT, each with three human participants.",
                "label": 1
            },
            {
                "text": "We assign a fantasy location setting to each conversation, and participants are given a character descriptions (personas) to role-play.",
                "label": 1
            },
            {
                "text": "The locations and characters are extracted from previous works on this game(Urbanek et al., 2019).",
                "label": 1
            },
            {
                "text": "Figure1shows a small snippet of an example from the new dataset.",
                "label": 1
            },
            {
                "text": "This setting allows us to control the conversational situations and have knowledge of the roles the agents should be playing in the conversation, making evaluation of systems more tractable compared to completely uncontrolled settings.",
                "label": 1
            }
        ]
    },
    {
        "paper_name": "Multi_Party Chat_ Conversational Agents in Group Settings with Humans and Models",
        "section": 4,
        "paragraph_id": 4,
        "full_text": "Using this setting, we then study the two aforementioned challenges via evaluating the use of language models in various settings.We use these findings, in addition to the training data collected from MultiLIGHT, to then build the best combined approach in each of these challenging dimensions.Finally, running interactive sessions between various models and human evaluators, we assess the quality of models along a set of different human judgments.We find that our selected model can outperform the existing state of the art in terms of consistency, engagingness, identity, (lack of) contradictions, and sensibility.",
        "sentences": [
            {
                "text": "Using this setting, we then study the two aforementioned challenges via evaluating the use of language models in various settings.",
                "label": 0
            },
            {
                "text": "We use these findings, in addition to the training data collected from MultiLIGHT, to then build the best combined approach in each of these challenging dimensions.",
                "label": 0
            },
            {
                "text": "Finally, running interactive sessions between various models and human evaluators, we assess the quality of models along a set of different human judgments.",
                "label": 0
            },
            {
                "text": "We find that our selected model can outperform the existing state of the art in terms of consistency, engagingness, identity, (lack of) contradictions, and sensibility.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Multi_Party Chat_ Conversational Agents in Group Settings with Humans and Models",
        "section": 5,
        "paragraph_id": 5,
        "full_text": "We first design a conversational setting which involves a group chat between three conversationalists.We extend the conversational setup fromUrbanek et al. (2019)(which otherwise previously focused on two-party conversations).We use the Mephisto framework(Urbanek and Ringshia, 2023)along with Amazon Mechanical Turk to conduct conversations between humans using a crowdsourcing task.We initially collect conversations between three humans in this setting to both supply training data and a gold evaluation setting.Later, we replace some of the humans with model agents instead.Importantly, players can send their message at any time and it will be shown to all the participants.For models to conduct dialogue in such a setting they will hence also have to choose both when to speak appropriately, in addition to producing coherent and relevant messages.",
        "sentences": [
            {
                "text": "We first design a conversational setting which involves a group chat between three conversationalists.",
                "label": 1
            },
            {
                "text": "We extend the conversational setup fromUrbanek et al. (2019)(which otherwise previously focused on two-party conversations).",
                "label": 1
            },
            {
                "text": "We use the Mephisto framework(Urbanek and Ringshia, 2023)along with Amazon Mechanical Turk to conduct conversations between humans using a crowdsourcing task.",
                "label": 1
            },
            {
                "text": "We initially collect conversations between three humans in this setting to both supply training data and a gold evaluation setting.",
                "label": 1
            },
            {
                "text": "Later, we replace some of the humans with model agents instead.",
                "label": 1
            },
            {
                "text": "Importantly, players can send their message at any time and it will be shown to all the participants.",
                "label": 1
            },
            {
                "text": "For models to conduct dialogue in such a setting they will hence also have to choose both when to speak appropriately, in addition to producing coherent and relevant messages.",
                "label": 1
            }
        ]
    },
    {
        "paper_name": "Multi_Party Chat_ Conversational Agents in Group Settings with Humans and Models",
        "section": 6,
        "paragraph_id": 6,
        "full_text": "Research on opendomain dialogue has found that taking into account character personas and interests makes the collected conversations more engaging(Dinan et al., 2020;Shuster et al., 2021b).We thus assign each conversationalist (which we also refer to as a \"player\") a role to play (referred to as their given persona).Roles are selected from a set of 955 characters from the LIGHT dataset(Urbanek et al., 2019), which range from tavern owner to boat captain to even talking animal characters such as a mouse.We also provide a description of the setting in which their interactions happen from a set of 579 locations from LIGHT, which is a diverse collection from bamboo hut to unicorn palace.We collect train, valid, and test conversations with randomized combinations of characters and settings.All crowdworkers are required to complete an onboarding in order to provide data, in addition to several checks after they complete their work to monitor data quality.More details on the data collection protocol are given in Appendix A.",
        "sentences": [
            {
                "text": "Research on opendomain dialogue has found that taking into account character personas and interests makes the collected conversations more engaging(Dinan et al., 2020;Shuster et al., 2021b).",
                "label": 1
            },
            {
                "text": "We thus assign each conversationalist (which we also refer to as a \"player\") a role to play (referred to as their given persona).",
                "label": 1
            },
            {
                "text": "Roles are selected from a set of 955 characters from the LIGHT dataset(Urbanek et al., 2019), which range from tavern owner to boat captain to even talking animal characters such as a mouse.",
                "label": 1
            },
            {
                "text": "We also provide a description of the setting in which their interactions happen from a set of 579 locations from LIGHT, which is a diverse collection from bamboo hut to unicorn palace.",
                "label": 1
            },
            {
                "text": "We collect train, valid, and test conversations with randomized combinations of characters and settings.",
                "label": 1
            },
            {
                "text": "All crowdworkers are required to complete an onboarding in order to provide data, in addition to several checks after they complete their work to monitor data quality.",
                "label": 1
            },
            {
                "text": "More details on the data collection protocol are given in Appendix A.",
                "label": 1
            }
        ]
    },
    {
        "paper_name": "Multi_Party Chat_ Conversational Agents in Group Settings with Humans and Models",
        "section": 7,
        "paragraph_id": 7,
        "full_text": "Figure1shows a snippet of a sample conversation from our training set and the information provided to the participants at that point during the conversation with the given setting and character dialogue.",
        "sentences": [
            {
                "text": "Figure1shows a snippet of a sample conversation from our training set and the information provided to the participants at that point during the conversation with the given setting and character dialogue.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Multi_Party Chat_ Conversational Agents in Group Settings with Humans and Models",
        "section": 8,
        "paragraph_id": 8,
        "full_text": "Overall Dataset Table1summarizes the statistics of our newly collected dataset, which we call MultiLIGHT.MultiLIGHT has 10,917 conversations and 313,433 utterances, collected from more than 300 crowdworkers.Of those, 390 conversations (11,005 utterances) are reserved for the validation set, and 323 conversations (9,164 utterances) for the test set.The dataset 2 and the crowdsourcing code3is open-sourced in the LIGHT codebase.",
        "sentences": [
            {
                "text": "Overall Dataset Table1summarizes the statistics of our newly collected dataset, which we call MultiLIGHT.",
                "label": 1
            },
            {
                "text": "MultiLIGHT has 10,917 conversations and 313,433 utterances, collected from more than 300 crowdworkers.",
                "label": 1
            },
            {
                "text": "Of those, 390 conversations (11,005 utterances) are reserved for the validation set, and 323 conversations (9,164 utterances) for the test set.",
                "label": 1
            },
            {
                "text": "The dataset 2 and the crowdsourcing code3is open-sourced in the LIGHT codebase.",
                "label": 1
            }
        ]
    },
    {
        "paper_name": "Multi_Party Chat_ Conversational Agents in Group Settings with Humans and Models",
        "section": 9,
        "paragraph_id": 9,
        "full_text": "We compare the approaches proposed in section 4 by utilizing the MultiLIGHT dataset for training and evaluating models.Our model input (prompts) include the description of the location, personas of the characters present, and the history of the conversation.The current speaker name may or may not be appended with that depending on the specific approach.",
        "sentences": [
            {
                "text": "We compare the approaches proposed in section 4 by utilizing the MultiLIGHT dataset for training and evaluating models.",
                "label": 0
            },
            {
                "text": "Our model input (prompts) include the description of the location, personas of the characters present, and the history of the conversation.",
                "label": 0
            },
            {
                "text": "The current speaker name may or may not be appended with that depending on the specific approach.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Multi_Party Chat_ Conversational Agents in Group Settings with Humans and Models",
        "section": 10,
        "paragraph_id": 10,
        "full_text": "We use the pre-trained 2.7B parameter Transformer model R2C2(Shuster et al., 2022a)as the base language model for most of our experiments, which was previously shown to compare well with other models of a similar size.For further details on the training of our models, please see Appendix B.",
        "sentences": [
            {
                "text": "We use the pre-trained 2.7B parameter Transformer model R2C2(Shuster et al., 2022a)as the base language model for most of our experiments, which was previously shown to compare well with other models of a similar size.",
                "label": 0
            },
            {
                "text": "For further details on the training of our models, please see Appendix B.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Multi_Party Chat_ Conversational Agents in Group Settings with Humans and Models",
        "section": 11,
        "paragraph_id": 11,
        "full_text": "We first measure the turn-taking ability of the various proposed models.For this we construct a new task with the MultiLIGHT validation set, wherein the goal is to predict the next speaker given previous dialogue turns.As our metric, we measure the accuracy of a model in predicting the right speaker for the next turn, over all turns.A random baseline in this task has an accuracy in predicting the next turn speaker of 33.3%.",
        "sentences": [
            {
                "text": "We first measure the turn-taking ability of the various proposed models.",
                "label": 0
            },
            {
                "text": "For this we construct a new task with the MultiLIGHT validation set, wherein the goal is to predict the next speaker given previous dialogue turns.",
                "label": 0
            },
            {
                "text": "As our metric, we measure the accuracy of a model in predicting the right speaker for the next turn, over all turns.",
                "label": 0
            },
            {
                "text": "A random baseline in this task has an accuracy in predicting the next turn speaker of 33.3%.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Multi_Party Chat_ Conversational Agents in Group Settings with Humans and Models",
        "section": 12,
        "paragraph_id": 12,
        "full_text": "This model is trained to either generate a special silence token (__SILENCE__ ) or the actual utterance from its respective character, if the model predicts it is the character's turn.Because of the over-presence of examples with the silence token as response (23 of the examples), the direct use of the dataset is prone to producing models that always output __SILENCE__ .Thus, we experimented with different values of \"silence drop out\" (referred to as SDO), where we drop a fraction of these silence examples.During the evaluation we generate with the model for each of the agents, then aggregate the results to determine the next speaker.Our approach breaks ties by selecting the agent with the lowest probability of generating __SILENCE__ , i.e. is most confident that it should speak (as we know in the evaluation someone does speak).",
        "sentences": [
            {
                "text": "This model is trained to either generate a special silence token (__SILENCE__ ) or the actual utterance from its respective character, if the model predicts it is the character's turn.",
                "label": 0
            },
            {
                "text": "Because of the over-presence of examples with the silence token as response (23 of the examples), the direct use of the dataset is prone to producing models that always output __SILENCE__ .",
                "label": 0
            },
            {
                "text": "Thus, we experimented with different values of \"silence drop out\" (referred to as SDO), where we drop a fraction of these silence examples.",
                "label": 0
            },
            {
                "text": "During the evaluation we generate with the model for each of the agents, then aggregate the results to determine the next speaker.",
                "label": 0
            },
            {
                "text": "Our approach breaks ties by selecting the agent with the lowest probability of generating __SILENCE__ , i.e. is most confident that it should speak (as we know in the evaluation someone does speak).",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Multi_Party Chat_ Conversational Agents in Group Settings with Humans and Models",
        "section": 13,
        "paragraph_id": 13,
        "full_text": "This approach was not able to achieve speaker prediction accuracy noticeably higher than the baseline (our best was 36.5%, achieved with SDO of 0.9).We believe that this is due to the accumulation of errors from all three generations, which is discussed in more detail in Appendix D.",
        "sentences": [
            {
                "text": "This approach was not able to achieve speaker prediction accuracy noticeably higher than the baseline (our best was 36.5%, achieved with SDO of 0.9).",
                "label": 0
            },
            {
                "text": "We believe that this is due to the accumulation of errors from all three generations, which is discussed in more detail in Appendix D.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Multi_Party Chat_ Conversational Agents in Group Settings with Humans and Models",
        "section": 14,
        "paragraph_id": 14,
        "full_text": "Training a model to predict the speaker name and the utterance, we can use only the first part (speaker name) to predict the actual speaker, and thus evaluate its performance in controlling the flow of the conversation.Using this approach, our best model trained on MultiLIGHT achieved an accuracy of 49.5%.",
        "sentences": [
            {
                "text": "Training a model to predict the speaker name and the utterance, we can use only the first part (speaker name) to predict the actual speaker, and thus evaluate its performance in controlling the flow of the conversation.",
                "label": 0
            },
            {
                "text": "Using this approach, our best model trained on MultiLIGHT achieved an accuracy of 49.5%.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Multi_Party Chat_ Conversational Agents in Group Settings with Humans and Models",
        "section": 15,
        "paragraph_id": 15,
        "full_text": "Finally, we explore models that generate the speaker name only.Here, as well as evaluating R2C2-based models, as before, we also include smaller models in our comparisons: BART (400m) and T5 (700m).We also ran experiments on predicting the speaker given not only the dialogue history, but also the current generated utterance as part of the context (knowing what was said, and asking the model who said it).The latter should be an easier task and is one way of measuring upper bounds / headroom for the more difficult task.Table3summarizes the accuracy of these models, as well as comparing them to the other approaches from subsubsection 5.1.1 and subsubsection 5.1.2.For models using the same underlying R2C2 language model we see performance that is similar between the Speaker only and Speaker AND Utterance models, with the Speaker AND Utterance being slightly better.This indicates the extra training targets of the latter model do not make the speaker identification task harder, and may actually bring a benefit.Changing the base language model, we also see superior performance of the BART model over T5 and R2C2 -where BART happens to be the smallest model in size (number of parameters).Comparing all of these numbers to including the current utterance (last column), we see a clear improvement on speaker prediction accuracy, with large gains.There may still be headroom for much better models.",
        "sentences": [
            {
                "text": "Finally, we explore models that generate the speaker name only.",
                "label": 0
            },
            {
                "text": "Here, as well as evaluating R2C2-based models, as before, we also include smaller models in our comparisons: BART (400m) and T5 (700m).",
                "label": 0
            },
            {
                "text": "We also ran experiments on predicting the speaker given not only the dialogue history, but also the current generated utterance as part of the context (knowing what was said, and asking the model who said it).",
                "label": 0
            },
            {
                "text": "The latter should be an easier task and is one way of measuring upper bounds / headroom for the more difficult task.",
                "label": 0
            },
            {
                "text": "2.For models using the same underlying R2C2 language model we see performance that is similar between the Speaker only and Speaker AND Utterance models, with the Speaker AND Utterance being slightly better.",
                "label": 0
            },
            {
                "text": "This indicates the extra training targets of the latter model do not make the speaker identification task harder, and may actually bring a benefit.",
                "label": 0
            },
            {
                "text": "Changing the base language model, we also see superior performance of the BART model over T5 and R2C2 -where BART happens to be the smallest model in size (number of parameters).",
                "label": 0
            },
            {
                "text": "Comparing all of these numbers to including the current utterance (last column), we see a clear improvement on speaker prediction accuracy, with large gains.",
                "label": 0
            },
            {
                "text": "There may still be headroom for much better models.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Multi_Party Chat_ Conversational Agents in Group Settings with Humans and Models",
        "section": 16,
        "paragraph_id": 16,
        "full_text": "The next step after investigating turntaking/conversational flow, is investigating the quality and the coherence of the utterances themselves.In these evaluations we assume the speaker is already known, following the choices given in the validation set annotations, and only MultiLIGHT (which is multi-party).We compare the performance of existing state of the art models (top 5 rows) to R2C2 based models (2.7B parameters) with three approaches to training: Utterance only: given the full context and the prompt for the next-speaker, it generates the utterance; Silence OR Utterance: given the full context and the prompt for the next-speaker, generates a silence token or the utterance; Speaker AND Utterance: given the full context, generates the next speaker name/ID along with the utterance from that speaker.The datasets that models are trained on is indicated in the brackets (L: LIGHT, W: LIGHT Wild, M: MultiLIGHT).PPL values are split into two columns based on the model dictionary sizes. evaluate the quality of various models at generating utterances given the speaker choice.",
        "sentences": [
            {
                "text": "The next step after investigating turntaking/conversational flow, is investigating the quality and the coherence of the utterances themselves.",
                "label": 0
            },
            {
                "text": "In these evaluations we assume the speaker is already known, following the choices given in the validation set annotations, and only MultiLIGHT (which is multi-party).",
                "label": 0
            },
            {
                "text": "We compare the performance of existing state of the art models (top 5 rows) to R2C2 based models (2.7B parameters) with three approaches to training: Utterance only: given the full context and the prompt for the next-speaker, it generates the utterance; Silence OR Utterance: given the full context and the prompt for the next-speaker, generates a silence token or the utterance; Speaker AND Utterance: given the full context, generates the next speaker name/ID along with the utterance from that speaker.",
                "label": 0
            },
            {
                "text": "The datasets that models are trained on is indicated in the brackets (L: LIGHT, W: LIGHT Wild, M: MultiLIGHT).",
                "label": 0
            },
            {
                "text": "PPL values are split into two columns based on the model dictionary sizes. evaluate the quality of various models at generating utterances given the speaker choice.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Multi_Party Chat_ Conversational Agents in Group Settings with Humans and Models",
        "section": 17,
        "paragraph_id": 17,
        "full_text": "We start with establishing baselines using the existing dialogue models from the literature that are comparable with our setup.We choose BlenderBot 1(Roller et al., 2020), BlenderBot 2 (FAIR, 2021), along with a prompted version of pre-trained-only OPT(Zhang et al., 2022)and its dialogue-finetuned variant BlenderBot 3(Shuster et al., 2022b). For OPT, we crafted custom prompts that included a few examples from the MultiLIGHT training set.",
        "sentences": [
            {
                "text": "We start with establishing baselines using the existing dialogue models from the literature that are comparable with our setup.",
                "label": 0
            },
            {
                "text": "We choose BlenderBot 1(Roller et al., 2020), BlenderBot 2 (FAIR, 2021), along with a prompted version of pre-trained-only OPT(Zhang et al., 2022)and its dialogue-finetuned variant BlenderBot 3(Shuster et al., 2022b).",
                "label": 0
            },
            {
                "text": "For OPT, we crafted custom prompts that included a few examples from the MultiLIGHT training set.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Multi_Party Chat_ Conversational Agents in Group Settings with Humans and Models",
        "section": 18,
        "paragraph_id": 18,
        "full_text": "For BlenderBot 2 and 3, we do not use their long term memory and internet search modules and use only their utterance generation module to output a given turn.This work is a multi-party extension of previous works on the text-based adventure game, LIGHT.",
        "sentences": [
            {
                "text": "For BlenderBot 2 and 3, we do not use their long term memory and internet search modules and use only their utterance generation module to output a given turn.",
                "label": 0
            },
            {
                "text": "This work is a multi-party extension of previous works on the text-based adventure game, LIGHT.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Multi_Party Chat_ Conversational Agents in Group Settings with Humans and Models",
        "section": 19,
        "paragraph_id": 19,
        "full_text": "As a result, we have access to two other in-domain datasets which, although not multi-party dialogue, provide more data for better fine-tuning.They also provide additional evaluation setups for our models, which allow us to check if they perform well in the two-party, as well as the multi-party case.Specifically we use the LIGHT(Urbanek et al., 2019)and LIGHT Wild(Shuster et al., 2021b)datasets, which are both two-party conversational setups in a crowdworker or organic user set, respectively.We thus also include in our baselines the current state of the art model for LIGHT from the best performing Expanded Attention model in(Shuster et al., 2021c)which was trained on the latter two datasets; we refer to it as LIGHT SotA for the rest of this paper.",
        "sentences": [
            {
                "text": "As a result, we have access to two other in-domain datasets which, although not multi-party dialogue, provide more data for better fine-tuning.",
                "label": 0
            },
            {
                "text": "They also provide additional evaluation setups for our models, which allow us to check if they perform well in the two-party, as well as the multi-party case.",
                "label": 0
            },
            {
                "text": "Specifically we use the LIGHT(Urbanek et al., 2019)and LIGHT Wild(Shuster et al., 2021b)datasets, which are both two-party conversational setups in a crowdworker or organic user set, respectively.",
                "label": 0
            },
            {
                "text": "We thus also include in our baselines the current state of the art model for LIGHT from the best performing Expanded Attention model in(Shuster et al., 2021c)which was trained on the latter two datasets; we refer to it as LIGHT SotA for the rest of this paper.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Multi_Party Chat_ Conversational Agents in Group Settings with Humans and Models",
        "section": 20,
        "paragraph_id": 20,
        "full_text": "Table4provides our main results.The first 5 rows show the performance of our baseline mod-els, in terms of perplexity and unigram F1, on the validation split of our three main datasets.The baseline models, except OPT, are fine-tuned on a large corpora of two-party turn-based dialogues.BlenderBot 3 and LIGHT SotA have LIGHT and LIGHT Wild in their fine-tuning data.BlenderBot 1 and 2, and the LIGHT SotA models are using the same dictionary with 8008 tokens; OPT, Blender-Bot 3, and R2C2 share a different dictionary with 50,264 tokens.Perplexities across different dictionaries are not comparable, but are comparable for models that share the same dictionary.With that in mind, we can see the improved performance of the LIGHT SotA model compared to BlenderBot 1 and 2, across all three datasets, which is clearly related to its in-domain fine-tuning.Comparing OPT to BlenderBot 3 shows better perplexity metrics for BlenderBot 3, but better F1 for the OPT model on MultiLIGHT.We attribute this to the observation that BlenderBot 3 is fine-tuned on dialogue data, including LIGHT and LIGHT Wild, but is biased towards two-party dialogues.",
        "sentences": [
            {
                "text": "Table4provides our main results.",
                "label": 0
            },
            {
                "text": "The first 5 rows show the performance of our baseline mod-els, in terms of perplexity and unigram F1, on the validation split of our three main datasets.",
                "label": 0
            },
            {
                "text": "The baseline models, except OPT, are fine-tuned on a large corpora of two-party turn-based dialogues.",
                "label": 0
            },
            {
                "text": "BlenderBot 3 and LIGHT SotA have LIGHT and LIGHT Wild in their fine-tuning data.",
                "label": 0
            },
            {
                "text": "BlenderBot 1 and 2, and the LIGHT SotA models are using the same dictionary with 8008 tokens; OPT, Blender-Bot 3, and R2C2 share a different dictionary with 50,264 tokens.",
                "label": 0
            },
            {
                "text": "Perplexities across different dictionaries are not comparable, but are comparable for models that share the same dictionary.",
                "label": 0
            },
            {
                "text": "With that in mind, we can see the improved performance of the LIGHT SotA model compared to BlenderBot 1 and 2, across all three datasets, which is clearly related to its in-domain fine-tuning.",
                "label": 0
            },
            {
                "text": "Comparing OPT to BlenderBot 3 shows better perplexity metrics for BlenderBot 3, but better F1 for the OPT model on MultiLIGHT.",
                "label": 0
            },
            {
                "text": "We attribute this to the observation that BlenderBot 3 is fine-tuned on dialogue data, including LIGHT and LIGHT Wild, but is biased towards two-party dialogues.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Multi_Party Chat_ Conversational Agents in Group Settings with Humans and Models",
        "section": 21,
        "paragraph_id": 21,
        "full_text": "Next, we study the improvements we can gain from training new models that take advantage of the new MultiLIGHT dataset.We use the three approaches from section 4 which are capable of response generation.",
        "sentences": [
            {
                "text": "Next, we study the improvements we can gain from training new models that take advantage of the new MultiLIGHT dataset.",
                "label": 0
            },
            {
                "text": "We use the three approaches from section 4 which are capable of response generation.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Multi_Party Chat_ Conversational Agents in Group Settings with Humans and Models",
        "section": 22,
        "paragraph_id": 22,
        "full_text": "Rows 6 to 9 in Table4summarize the results of using this approach when fine-tuning an R2C2 model.The first three results are vanilla training of the transformer models with different training sets, while the last row is from using the Cringe loss to also penalize negative examples(Adolphs et al., 2022).The details of generating positive and negative examples for the Cringe loss model is presented in Appendix E. Final results indicate that using the simple training (with no Cringe loss) that is multi-tasked with all the three datasets has the best performance.While using MultiLIGHT does not strongly impact the performance on the LIGHT and LIGHT Wild datasets, it makes a large difference in improving metrics for the multi-party case, e.g.validation PPL is reduced from 17.44 to 13.25.",
        "sentences": [
            {
                "text": "Rows 6 to 9 in Table4summarize the results of using this approach when fine-tuning an R2C2 model.",
                "label": 0
            },
            {
                "text": "The first three results are vanilla training of the transformer models with different training sets, while the last row is from using the Cringe loss to also penalize negative examples(Adolphs et al., 2022).",
                "label": 0
            },
            {
                "text": "The details of generating positive and negative examples for the Cringe loss model is presented in Appendix E.",
                "label": 0
            },
            {
                "text": "Final results indicate that using the simple training (with no Cringe loss) that is multi-tasked with all the three datasets has the best performance.",
                "label": 0
            },
            {
                "text": "While using MultiLIGHT does not strongly impact the performance on the LIGHT and LIGHT Wild datasets, it makes a large difference in improving metrics for the multi-party case, e.g.validation PPL is reduced from 17.44 to 13.25.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Multi_Party Chat_ Conversational Agents in Group Settings with Humans and Models",
        "section": 23,
        "paragraph_id": 23,
        "full_text": "Rows 10 to 12 in Table4shows the results for three selected values of silence drop out (SDO).Note here we are computing the utterance quality metrics only on turns that the speaker is not silent, and we force the model to generate an utterance in these cases for evaluation purposes.We trained this approach with only the multi-party MultiLIGHT dataset and evaluated it on MultiLIGHT.The performance drop from this approach, compared to other techniques, e.g.Speaker and Utterance models trained on MultiLIGHT only, is quite noticeable.",
        "sentences": [
            {
                "text": "Rows 10 to 12 in Table4shows the results for three selected values of silence drop out (SDO).",
                "label": 0
            },
            {
                "text": "Note here we are computing the utterance quality metrics only on turns that the speaker is not silent, and we force the model to generate an utterance in these cases for evaluation purposes.",
                "label": 0
            },
            {
                "text": "We trained this approach with only the multi-party MultiLIGHT dataset and evaluated it on MultiLIGHT.",
                "label": 0
            },
            {
                "text": "The performance drop from this approach, compared to other techniques, e.g.Speaker and Utterance models trained on MultiLIGHT only, is quite noticeable.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Multi_Party Chat_ Conversational Agents in Group Settings with Humans and Models",
        "section": 24,
        "paragraph_id": 24,
        "full_text": "The last four rows in Table4show the metrics on the quality of the utterances generated by the speaker AND utterance models.In order to only evaluate utterance coherence for this model, we provide the correct speaker label as a prefix (to avoid generating for the wrong speaker) and compute perplexity and unigram F1 only on the utterance part of the generated text, by masking the speaker name and its subsequent tokens (colon and trailing space).Again, we find that the multi-tasked model is outperforming the other models on Mul-tiLIGHT with a minimal deterioration on the twoparty LIGHT and LIGHT Wild tasks.Overall, it shows performance marginally, if at all, worse than the utterance-only approach, exceeding it in 2 out of 6 metrics in the table.",
        "sentences": [
            {
                "text": "The last four rows in Table4show the metrics on the quality of the utterances generated by the speaker AND utterance models.",
                "label": 0
            },
            {
                "text": "In order to only evaluate utterance coherence for this model, we provide the correct speaker label as a prefix (to avoid generating for the wrong speaker) and compute perplexity and unigram F1 only on the utterance part of the generated text, by masking the speaker name and its subsequent tokens (colon and trailing space).",
                "label": 0
            },
            {
                "text": "Again, we find that the multi-tasked model is outperforming the other models on Mul-tiLIGHT with a minimal deterioration on the twoparty LIGHT and LIGHT Wild tasks.",
                "label": 0
            },
            {
                "text": "Overall, it shows performance marginally, if at all, worse than the utterance-only approach, exceeding it in 2 out of 6 metrics in the table.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Multi_Party Chat_ Conversational Agents in Group Settings with Humans and Models",
        "section": 25,
        "paragraph_id": 25,
        "full_text": "In order to test our findings in practice, we run human evaluations(Smith et al., 2022).This is another round of crowdsourced tasks in which we ask crowdworkers to have an interactive conversation with our agents (models).We replicate a setup similar to the original crowdsourcing task for Mul-tiLIGHT: there are three characters, in a described location, all role-playing (either role-played by a human, or by a model) to their assigned personas.Here, the crowdworker takes the role of one of the characters, while the models play the other two roles.After each response from the bot, the human participant (i.e., the crowdworker) tags model responses for attributes such as consistency, contradiction, engagingness, sounding out of turn or nonsensical, and having mistaken identity.The crowdworkers are also asked to rate the overall quality of the conversation at the end of the chat.We provide further details about the task and evaluation in Appendix F.",
        "sentences": [
            {
                "text": "In order to test our findings in practice, we run human evaluations(Smith et al., 2022).",
                "label": 0
            },
            {
                "text": "This is another round of crowdsourced tasks in which we ask crowdworkers to have an interactive conversation with our agents (models).",
                "label": 0
            },
            {
                "text": "We replicate a setup similar to the original crowdsourcing task for Mul-tiLIGHT: there are three characters, in a described location, all role-playing (either role-played by a human, or by a model) to their assigned personas.",
                "label": 0
            },
            {
                "text": "Here, the crowdworker takes the role of one of the characters, while the models play the other two roles.",
                "label": 0
            },
            {
                "text": "After each response from the bot, the human participant (i.e., the crowdworker) tags model responses for attributes such as consistency, contradiction, engagingness, sounding out of turn or nonsensical, and having mistaken identity.",
                "label": 0
            },
            {
                "text": "The crowdworkers are also asked to rate the overall quality of the conversation at the end of the chat.",
                "label": 0
            },
            {
                "text": "We provide further details about the task and evaluation in Appendix F.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Multi_Party Chat_ Conversational Agents in Group Settings with Humans and Models",
        "section": 26,
        "paragraph_id": 26,
        "full_text": "We compare two different approaches for turntaking (a random baseline vs. a BART-based speaker only model) and two for utterance generation (LIGHT SotA as a baseline vs. Utterance only  The arrow on a metric shows the direction for its improvement (i.e., higher or lower). [LWM]).We thus devised four different agents for this experiment, combining these approaches.",
        "sentences": [
            {
                "text": "We compare two different approaches for turntaking (a random baseline vs. a BART-based speaker only model) and two for utterance generation (LIGHT SotA as a baseline vs. Utterance only  The arrow on a metric shows the direction for its improvement (i.e., higher or lower).[LWM]).",
                "label": 0
            },
            {
                "text": "We thus devised four different agents for this experiment, combining these approaches.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Multi_Party Chat_ Conversational Agents in Group Settings with Humans and Models",
        "section": 27,
        "paragraph_id": 27,
        "full_text": "Figure2shows the outcome of our human evaluation on the combinations of the turn-taking and the utterance generation models.There is significant improvement across the board on the quality of the utterance when comparing our best utterance generation model (solid bars) to the baseline (hollow bars).The same conclusion can not be made between the random baseline (blue bars) to the trained turn-taking model (yellow bars).We hypothesize that this is due to the nature of the conversations in this dataset: we are in an open-ended conversational setting, e.g.without any strict rules for accomplishing a task.One could argue that no matter who speaks next, if the utterance makes sense there is little if any harm to the flow of the conversation.We could even notice some utterances that can perhaps be reordered in the sample provided in Figure1.",
        "sentences": [
            {
                "text": "Figure2shows the outcome of our human evaluation on the combinations of the turn-taking and the utterance generation models.",
                "label": 0
            },
            {
                "text": "There is significant improvement across the board on the quality of the utterance when comparing our best utterance generation model (solid bars) to the baseline (hollow bars).",
                "label": 0
            },
            {
                "text": "The same conclusion can not be made between the random baseline (blue bars) to the trained turn-taking model (yellow bars).",
                "label": 0
            },
            {
                "text": "We hypothesize that this is due to the nature of the conversations in this dataset: we are in an open-ended conversational setting, e.g.without any strict rules for accomplishing a task.",
                "label": 0
            },
            {
                "text": "One could argue that no matter who speaks next, if the utterance makes sense there is little if any harm to the flow of the conversation.",
                "label": 0
            },
            {
                "text": "We could even notice some utterances that can perhaps be reordered in the sample provided in Figure1.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Multi_Party Chat_ Conversational Agents in Group Settings with Humans and Models",
        "section": 28,
        "paragraph_id": 28,
        "full_text": "Factoring out the turn-taking approach, which was concluded to have insignificant contribution to the quality, we compare the human evaluation re-sults from our two dialogue generations models in Table5.This provides the overall comparison between LIGHT SotA to our best utterance generation model (i.e., Utterance only[LWM]).Note that here we removed the \"out of turn\" criterion, because it was attributed to the flow control (turn-taking) models and not the utterance generation model.Results clearly shows the improvement gained from the MultiLIGHT dataset on all the attributes.These result show a statistically significant improvement across all the metrics (t-test, p-value of 0.05).",
        "sentences": [
            {
                "text": "Factoring out the turn-taking approach, which was concluded to have insignificant contribution to the quality, we compare the human evaluation re-sults from our two dialogue generations models in Table5.",
                "label": 0
            },
            {
                "text": "This provides the overall comparison between LIGHT SotA to our best utterance generation model (i.e., Utterance only[LWM]).",
                "label": 0
            },
            {
                "text": "Note that here we removed the \"out of turn\" criterion, because it was attributed to the flow control (turn-taking) models and not the utterance generation model.",
                "label": 0
            },
            {
                "text": "Results clearly shows the improvement gained from the MultiLIGHT dataset on all the attributes.",
                "label": 0
            },
            {
                "text": "These result show a statistically significant improvement across all the metrics (t-test, p-value of 0.05).",
                "label": 0
            }
        ]
      },
      {
        "paper_name": "Persian Typographical Error Type Detection Using Deep Neural Networks on Algorithmically_Generated Misspellings",
        "section": 0,
        "paragraph_id": 0,
        "full_text": "Abstract: Spelling correction is a remarkable challenge in the field of natural language processing.The objective of spelling correction tasks is to recognize and rectify spelling errors automatically.The development of applications that can effectually diagnose and correct Persian spelling and grammatical errors has become more important in order to improve the quality of Persian text.The Typographical Error Type Detection in Persian is a relatively understudied area.Therefore, this paper presents a compelling approach for detecting typographical errors in Persian texts.Our work includes the presentation of a publicly available dataset called FarsTypo, which comprises 3.4 million words arranged in chronological order and tagged with their corresponding part-of-speech.These words cover a wide range of topics and linguistic styles.We develop an algorithm designed to apply Persian-specific errors to a scalable portion of these words, resulting in a parallel dataset of correct and incorrect words.By leveraging FarsTypo, we establish a strong foundation and conduct a thorough comparison of various methodologies employing different architectures.Additionally, we introduce a groundbreaking Deep Sequential Neural Network that utilizes both word and character embeddings, along with bidirectional LSTM layers, for token classification aimed at detecting typographical errors across 51 distinct classes.Our approach is contrasted with highly advanced industrial systems that, unlike this study, have been developed using a diverse range of resources.The outcomes of our final method proved to be highly competitive, achieving an accuracy of 97.62%, precision of 98.83%, recall of 98.61%, and surpassing others in terms of speed.",
        "sentences": [
            {
                "text": "Abstract: Spelling correction is a remarkable challenge in the field of natural language processing.",
                "label": 0
            },
            {
                "text": "The objective of spelling correction tasks is to recognize and rectify spelling errors automatically.",
                "label": 0
            },
            {
                "text": "The development of applications that can effectually diagnose and correct Persian spelling and grammatical errors has become more important in order to improve the quality of Persian text.",
                "label": 0
            },
            {
                "text": "The Typographical Error Type Detection in Persian is a relatively understudied area.",
                "label": 0
            },
            {
                "text": "Therefore, this paper presents a compelling approach for detecting typographical errors in Persian texts.",
                "label": 0
            },
            {
                "text": "Our work includes the presentation of a publicly available dataset called FarsTypo, which comprises 3.4 million words arranged in chronological order and tagged with their corresponding part-of-speech.",
                "label": 1
            },
            {
                "text": "These words cover a wide range of topics and linguistic styles.",
                "label": 1
            },
            {
                "text": "We develop an algorithm designed to apply Persian-specific errors to a scalable portion of these words, resulting in a parallel dataset of correct and incorrect words.",
                "label": 0
            },
            {
                "text": "By leveraging FarsTypo, we establish a strong foundation and conduct a thorough comparison of various methodologies employing different architectures.",
                "label": 0
            },
            {
                "text": "Additionally, we introduce a groundbreaking Deep Sequential Neural Network that utilizes both word and character embeddings, along with bidirectional LSTM layers, for token classification aimed at detecting typographical errors across 51 distinct classes.",
                "label": 0
            },
            {
                "text": "Our approach is contrasted with highly advanced industrial systems that, unlike this study, have been developed using a diverse range of resources.",
                "label": 0
            },
            {
                "text": "The outcomes of our final method proved to be highly competitive, achieving an accuracy of 97.62%, precision of 98.83%, recall of 98.61%, and surpassing others in terms of speed.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Persian Typographical Error Type Detection Using Deep Neural Networks on Algorithmically_Generated Misspellings",
        "section": 1,
        "paragraph_id": 1,
        "full_text": "With the emersion of the computer era, the way we communicate has undergone a deep transformation, leading to a declined demand for traditional printed newspapers and handwritten letters.The rise of digitalization has presented humanity with manifold opportunities, but it has also introduced several challenges.As an example, in 2020, the global internet witnessed over 306 billion emails being sent worldwide(Johnson 2021), while in the United States alone, almost 2.2 trillion text messages were exchanged(CTIA 2021).Despite the fact that these statistics highlight the advantages of recent technological advancement in enhancing communication convenience and accessibility, there are certain issues that arise when natural language is employed as a communication medium.One such problem is misspelling a word, also known as making a Typographical Error (Typo).Although it seems minor, typos can have a significant and often negative impact, particularly when made by an organization or a company(Muller et al. 2019) such as customer attrition(Stiff 2012).This type of error is more prevalent in text messages and can lead to major unwanted misunderstandings(Boland and Queen 2016).In the field of Natural Language Processing (NLP), researchers have been actively working on the development of various proofreading tools to overcome this issue.",
        "sentences": [
            {
                "text": "With the emersion of the computer era, the way we communicate has undergone a deep transformation, leading to a declined demand for traditional printed newspapers and handwritten letters.",
                "label": 0
            },
            {
                "text": "The rise of digitalization has presented humanity with manifold opportunities, but it has also introduced several challenges.",
                "label": 0
            },
            {
                "text": "As an example, in 2020, the global internet witnessed over 306 billion emails being sent worldwide(Johnson 2021), while in the United States alone, almost 2.2 trillion text messages were exchanged(CTIA 2021).",
                "label": 0
            },
            {
                "text": "Despite the fact that these statistics highlight the advantages of recent technological advancement in enhancing communication convenience and accessibility, there are certain issues that arise when natural language is employed as a communication medium.",
                "label": 0
            },
            {
                "text": "One such problem is misspelling a word, also known as making a Typographical Error (Typo).",
                "label": 0
            },
            {
                "text": "Although it seems minor, typos can have a significant and often negative impact, particularly when made by an organization or a company(Muller et al. 2019) such as customer attrition(Stiff 2012).",
                "label": 0
            },
            {
                "text": "This type of error is more prevalent in text messages and can lead to major unwanted misunderstandings(Boland and Queen 2016).",
                "label": 0
            },
            {
                "text": "In the field of Natural Language Processing (NLP), researchers have been actively working on the development of various proofreading tools to overcome this issue.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Persian Typographical Error Type Detection Using Deep Neural Networks on Algorithmically_Generated Misspellings",
        "section": 2,
        "paragraph_id": 2,
        "full_text": "Typically, text-editing assistants are developed to focus on correcting spelling or grammar in input texts, known as spell-and grammar-checkers, respectively.The division of text-editing assistants is influenced byKukich's categorization (1992)of orthographical mistakes into non-word, real-word, and grammatical errors.Non-word errors happen when a word is not detected in the spell-checker's lexicon (e.g., writing 'hope' as 'hoope').Real-word errors, on the other hand, involve using a correctly spelled word from the vocabulary but in an incorrect context (e.g., writing \"I lust my keys\" instead of \"I lost my keys\").Lastly, grammatical errors, which unlike the previous two we do not focus on in this study, occur when a sentence does not adhere to the predefined rules of a language.Spell-checkers, which primarily focus on resolving non-word errors and real-word errors, generally comprise four consecutive subtasks.First, the input text is tokenized (tokenization), meaning it is divided into individual units such as words or characters.Next, spell-checker identify potential errors within the text (error detection).Afterwards, it aims to correct previously detected errors (error correction).Finally, the spell-checker ranks and provides a list of possible candidate corrections for each misspelt word (candidate ranking).These subtasks are part of a hierarchical module, and the overall effectiveness of a spell-checker relies on the success of individual components.",
        "sentences": [
            {
                "text": "Typically, text-editing assistants are developed to focus on correcting spelling or grammar in input texts, known as spell-and grammar-checkers, respectively.",
                "label": 0
            },
            {
                "text": "The division of text-editing assistants is influenced byKukich's categorization (1992)of orthographical mistakes into non-word, real-word, and grammatical errors.",
                "label": 0
            },
            {
                "text": "Non-word errors happen when a word is not detected in the spell-checker's lexicon (e.g., writing 'hope' as 'hoope').",
                "label": 0
            },
            {
                "text": "Real-word errors, on the other hand, involve using a correctly spelled word from the vocabulary but in an incorrect context (e.g., writing \"I lust my keys\" instead of \"I lost my keys\").",
                "label": 0
            },
            {
                "text": "Lastly, grammatical errors, which unlike the previous two we do not focus on in this study, occur when a sentence does not adhere to the predefined rules of a language.",
                "label": 0
            },
            {
                "text": "Spell-checkers, which primarily focus on resolving non-word errors and real-word errors, generally comprise four consecutive subtasks.",
                "label": 0
            },
            {
                "text": "First, the input text is tokenized (tokenization), meaning it is divided into individual units such as words or characters.",
                "label": 0
            },
            {
                "text": "Next, spell-checker identify potential errors within the text (error detection).",
                "label": 0
            },
            {
                "text": "Afterwards, it aims to correct previously detected errors (error correction).",
                "label": 0
            },
            {
                "text": "Finally, the spell-checker ranks and provides a list of possible candidate corrections for each misspelt word (candidate ranking).",
                "label": 0
            },
            {
                "text": "These subtasks are part of a hierarchical module, and the overall effectiveness of a spell-checker relies on the success of individual components.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Persian Typographical Error Type Detection Using Deep Neural Networks on Algorithmically_Generated Misspellings",
        "section": 3,
        "paragraph_id": 3,
        "full_text": "Applying test-editing tasks, such as spell checking, to low-resource natural languages like Persian presents additional challenges.Persian, spoken mainly in Iran, Afghanistan, and Tajikistan, has its own unique characteristics.While its linguistic structure has remained relatively unchanged over time(Bijankhan et al. 2011), it has incorporated numerous words from Arabic(Haghdadi and Azizi 2018).Furthermore, Persian is a free word order language, meaning that the order of words in a sentence can be flexible.Additionally, Persian includes letters that have both joiner and non-joiner forms(Ghayoomi et al. 2010).These linguistic features and complexities pose challenges when developing effective text-editing tools for Persian.Indeed, Persian poses additional challenges for text-editing tasks like spell-checking.One of the challenges is the presence of various letters that are written differently but have similar sounds.This can create difficulties when modeling the language, as there is no unified set of rules for writing(Rasooli et al. 2011).Additionally, distinguishing between white spaces and pseudo-spaces (ZWNJ1) can be problematic(Dastgheib et al. 2016).These factors contribute to the slow pace of computational advances in Persian language processing and its inherent ambiguity(QasemiZadeh et al. 2014).Nevertheless, several studies have focused on addressing these factors and have made progress in the field of spell-checking for Persian(Dastgheib and Fakhrahmad 2019;Samani et al. 2015).In general, these studies are focused on grammar(Ehsan and Faili 2010), a specific domain(Yazdani et al. 2020), or do not offer a comprehensive approach that is highly effective.Thus, with the advancement of neural network architectures, a more accurate method can be provided and more research needs to be conducted in order to address spell-checking in Persian.",
        "sentences": [
            {
                "text": "Applying test-editing tasks, such as spell checking, to low-resource natural languages like Persian presents additional challenges.",
                "label": 0
            },
            {
                "text": "Persian, spoken mainly in Iran, Afghanistan, and Tajikistan, has its own unique characteristics.",
                "label": 0
            },
            {
                "text": "While its linguistic structure has remained relatively unchanged over time(Bijankhan et al. 2011), it has incorporated numerous words from Arabic(Haghdadi and Azizi 2018).",
                "label": 0
            },
            {
                "text": "Furthermore, Persian is a free word order language, meaning that the order of words in a sentence can be flexible.",
                "label": 0
            },
            {
                "text": "Additionally, Persian includes letters that have both joiner and non-joiner forms(Ghayoomi et al. 2010).",
                "label": 0
            },
            {
                "text": "These linguistic features and complexities pose challenges when developing effective text-editing tools for Persian.",
                "label": 0
            },
            {
                "text": "Indeed, Persian poses additional challenges for text-editing tasks like spell-checking.",
                "label": 0
            },
            {
                "text": "One of the challenges is the presence of various letters that are written differently but have similar sounds.",
                "label": 0
            },
            {
                "text": "This can create difficulties when modeling the language, as there is no unified set of rules for writing(Rasooli et al. 2011).",
                "label": 0
            },
            {
                "text": "Additionally, distinguishing between white spaces and pseudo-spaces (ZWNJ1) can be problematic(Dastgheib et al. 2016).",
                "label": 0
            },
            {
                "text": "These factors contribute to the slow pace of computational advances in Persian language processing and its inherent ambiguity(QasemiZadeh et al. 2014).",
                "label": 0
            },
            {
                "text": "Nevertheless, several studies have focused on addressing these factors and have made progress in the field of spell-checking for Persian(Dastgheib and Fakhrahmad 2019;Samani et al. 2015).",
                "label": 0
            },
            {
                "text": "In general, these studies are focused on grammar(Ehsan and Faili 2010), a specific domain(Yazdani et al. 2020), or do not offer a comprehensive approach that is highly effective.",
                "label": 0
            },
            {
                "text": "Thus, with the advancement of neural network architectures, a more accurate method can be provided and more research needs to be conducted in order to address spell-checking in Persian.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Persian Typographical Error Type Detection Using Deep Neural Networks on Algorithmically_Generated Misspellings",
        "section": 4,
        "paragraph_id": 4,
        "full_text": "Our objective in this study is to enhance the development of more accurate Persian spell-checking systems by focusing on error detection.The effectiveness of spell-checking systems relies heavily on the accurate identification of errors, which can then be corrected.This is particularly evident in situations where Persian words contain Arabic letter, silent letters, or incorrect spacing.Once these errors are detected, they can be easily being corrected using practical techniques like predefined regular expressions patterns (e.g., correcting ‫'ي'‬ to ‫'ی'‬ using re.sub (r\", \"\", text) in Python).Hence, the detection part is crucial since the correction part can be performed quickly with only one line of code and better detection of errors results in a large number of Persian misspellings being effortlessly corrected.To achieve the goal of improving error detection and subsequently correcting Persian misspellings, this study introduces an algorithm that constructs a dataset comprising different Persian-specific typographical errors.This dataset consists of pairs of Persian words, their misspelled variants, and algorithmically assigned typographical error labels.Using this parallel dataset, the study train a Deep Sequential Neural Network to predict error types.The Deep Sequential Neural Network architecture model is designed to take input text and perform token classification, specifically classifying each word into one of 51 classes representing different types of typographical errors.By training the model on this dataset, it learns to accurately identify and classify the typographical errors associated with each word in Persian text.This work introduces several novel aspects that distinguish it from the existing literature.One key difference is the approach to error detection.Unlike previous methods that involved manual checking for every possible Persian typographical error of each word at each time step, the developed architecture in this study utilizes a trained neural network to detect any possible error for every word by propagating forward through the network.Additionally, while dictionary-based methods have been also employed in the past to address this problem, they are not effective in detecting real-word errors and struggle to adapt to various types of misspellings, especially when encountering outof-vocabulary (OOV) words(Dong et al. 2019).In contrast, the proposed approach in this study consider input text at both the sentence and character levels.This comprehensive approach allows the model to capture the necessary information to detect both real-and non-word errors, effectively.In previous studies, there has been a focus on developing better confusion sets to improve performance in detecting such errors.Confusion sets consists of small groups of words that are commonly confused with each other, such as \"site\", \"sight\", \"cite\".Constructing informative and well-investigated confusion sets is crucial for the success of spell-checking systems that utilize them.Furthermore, these spell-checking systems face the challenge of selecting the correct replacement from the confusion sets in the subsequent step.Choosing the most appropriate replacement from a set of similar words requires careful consideration and can be a complex task for the system.Therefore, in this study, addressing real-word error detection and the accurate selection of replacements from confusion sets are important aspects that need to be investigated and improved to enhance the overall performance of Persian spell-checking systems.During real-word error detection, our approach intelligently executes both steps simultaneously by propagating forward our developed neural network.This makes error detection much simpler and results in a reduction in the processing time of a spell-checker.In addition to the benefits mentioned earlier, when the error type is accurately identified, it enables more effective ordering of suggestions in the candidate ranking phase of spell-checkers (candidate ranking).Furthermore, the neural architecture developed in this study is designed to detect both non-word and real-word errors simultaneously.Our comprehensive approach allows the system to handle a wider range of errors and provides a more generalized alternative compared to previous methods.By leveraging the capabilities of the neural network, the approach reduces the need for extensive hard coding and manual rule-based interventions, making the system more adaptable and flexible.",
        "sentences": [
            {
                "text": "Our objective in this study is to enhance the development of more accurate Persian spell-checking systems by focusing on error detection.",
                "label": 0
            },
            {
                "text": "The effectiveness of spell-checking systems relies heavily on the accurate identification of errors, which can then be corrected.",
                "label": 0
            },
            {
                "text": "This is particularly evident in situations where Persian words contain Arabic letter, silent letters, or incorrect spacing.",
                "label": 0
            },
            {
                "text": "sub (r\", \"\", text) in Python).",
                "label": 0
            },
            {
                "text": "Hence, the detection part is crucial since the correction part can be performed quickly with only one line of code and better detection of errors results in a large number of Persian misspellings being effortlessly corrected.",
                "label": 0
            },
            {
                "text": "To achieve the goal of improving error detection and subsequently correcting Persian misspellings, this study introduces an algorithm that constructs a dataset comprising different Persian-specific typographical errors.",
                "label": 1
            },
            {
                "text": "This dataset consists of pairs of Persian words, their misspelled variants, and algorithmically assigned typographical error labels.",
                "label": 1
            },
            {
                "text": "Using this parallel dataset, the study train a Deep Sequential Neural Network to predict error types.",
                "label": 0
            },
            {
                "text": "The Deep Sequential Neural Network architecture model is designed to take input text and perform token classification, specifically classifying each word into one of 51 classes representing different types of typographical errors.",
                "label": 0
            },
            {
                "text": "By training the model on this dataset, it learns to accurately identify and classify the typographical errors associated with each word in Persian text.",
                "label": 0
            },
            {
                "text": "This work introduces several novel aspects that distinguish it from the existing literature.",
                "label": 0
            },
            {
                "text": "One key difference is the approach to error detection.",
                "label": 0
            },
            {
                "text": "Unlike previous methods that involved manual checking for every possible Persian typographical error of each word at each time step, the developed architecture in this study utilizes a trained neural network to detect any possible error for every word by propagating forward through the network.",
                "label": 0
            },
            {
                "text": "Additionally, while dictionary-based methods have been also employed in the past to address this problem, they are not effective in detecting real-word errors and struggle to adapt to various types of misspellings, especially when encountering outof-vocabulary (OOV) words(Dong et al. 2019).",
                "label": 0
            },
            {
                "text": "In contrast, the proposed approach in this study consider input text at both the sentence and character levels.",
                "label": 0
            },
            {
                "text": "This comprehensive approach allows the model to capture the necessary information to detect both real-and non-word errors, effectively.",
                "label": 0
            },
            {
                "text": "In previous studies, there has been a focus on developing better confusion sets to improve performance in detecting such errors.",
                "label": 0
            },
            {
                "text": "Confusion sets consists of small groups of words that are commonly confused with each other, such as \"site\", \"sight\", \"cite\".",
                "label": 0
            },
            {
                "text": "Constructing informative and well-investigated confusion sets is crucial for the success of spell-checking systems that utilize them.",
                "label": 0
            },
            {
                "text": "Furthermore, these spell-checking systems face the challenge of selecting the correct replacement from the confusion sets in the subsequent step.",
                "label": 0
            },
            {
                "text": "Choosing the most appropriate replacement from a set of similar words requires careful consideration and can be a complex task for the system.",
                "label": 0
            },
            {
                "text": "Therefore, in this study, addressing real-word error detection and the accurate selection of replacements from confusion sets are important aspects that need to be investigated and improved to enhance the overall performance of Persian spell-checking systems.",
                "label": 0
            },
            {
                "text": "During real-word error detection, our approach intelligently executes both steps simultaneously by propagating forward our developed neural network.",
                "label": 0
            },
            {
                "text": "This makes error detection much simpler and results in a reduction in the processing time of a spell-checker.",
                "label": 0
            },
            {
                "text": "In addition to the benefits mentioned earlier, when the error type is accurately identified, it enables more effective ordering of suggestions in the candidate ranking phase of spell-checkers (candidate ranking).",
                "label": 0
            },
            {
                "text": "Furthermore, the neural architecture developed in this study is designed to detect both non-word and real-word errors simultaneously.",
                "label": 0
            },
            {
                "text": "Our comprehensive approach allows the system to handle a wider range of errors and provides a more generalized alternative compared to previous methods.",
                "label": 0
            },
            {
                "text": "By leveraging the capabilities of the neural network, the approach reduces the need for extensive hard coding and manual rule-based interventions, making the system more adaptable and flexible.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Persian Typographical Error Type Detection Using Deep Neural Networks on Algorithmically_Generated Misspellings",
        "section": 5,
        "paragraph_id": 5,
        "full_text": "Overall, the contributions of this paper can be summarized as follows: • Introduction of FarsTypo: The paper introduces FarsTypo, one of the largest typographical error datasets specifically designed for the Persian language.This dataset is made publicly available, enabling researchers and practitioners to utilize it for further studies and developments2.The rest of this paper is organized as follows.In section 2, we review the existing literature on relevant spell-checking systems.Next, in section 3, we provide a detailed description of the algorithm used for error detection and introduce the dataset, FarsTypo, which is utilized for training and evaluating the system.In section 4, we describe the architecture used to perform error type detection.In section 5, we present the experimental setup conducted to evaluate the performance of the proposed approach.We also established a baseline for performance evaluation and compares the results obtained from the proposed approach with those of previous works in the field.In section 6, discussion of the research is conducted.Finally, in section 7, we conclude our work.",
        "sentences": [
            {
                "text": "Overall, the contributions of this paper can be summarized as follows: • Introduction of FarsTypo: The paper introduces FarsTypo, one of the largest typographical error datasets specifically designed for the Persian language.",
                "label": 1
            },
            {
                "text": "This dataset is made publicly available, enabling researchers and practitioners to utilize it for further studies and developments2.",
                "label": 1
            },
            {
                "text": "The rest of this paper is organized as follows.",
                "label": 0
            },
            {
                "text": "In section 2, we review the existing literature on relevant spell-checking systems.",
                "label": 0
            },
            {
                "text": "Next, in section 3, we provide a detailed description of the algorithm used for error detection and introduce the dataset, FarsTypo, which is utilized for training and evaluating the system.",
                "label": 0
            },
            {
                "text": "In section 4, we describe the architecture used to perform error type detection.",
                "label": 0
            },
            {
                "text": "In section 5, we present the experimental setup conducted to evaluate the performance of the proposed approach.",
                "label": 0
            },
            {
                "text": "We also established a baseline for performance evaluation and compares the results obtained from the proposed approach with those of previous works in the field.",
                "label": 0
            },
            {
                "text": "In section 6, discussion of the research is conducted.",
                "label": 0
            },
            {
                "text": "Finally, in section 7, we conclude our work.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Persian Typographical Error Type Detection Using Deep Neural Networks on Algorithmically_Generated Misspellings",
        "section": 6,
        "paragraph_id": 6,
        "full_text": "Several datasets for Persian spell-checking have been proposed.Similar to Aspell4, FAspell Although there are a number of Persian error datasets available, they do not consider all kinds of errors.Our objective was to develop a system that is capable of identifying any type of error (common or unusual).Therefore, we generate a dataset with a broad range of errors, which is an advantage of our method.",
        "sentences": [
            {
                "text": "Several datasets for Persian spell-checking have been proposed.",
                "label": 0
            },
            {
                "text": "Similar to Aspell4, FAspell Although there are a number of Persian error datasets available, they do not consider all kinds of errors.",
                "label": 0
            },
            {
                "text": "Our objective was to develop a system that is capable of identifying any type of error (common or unusual).",
                "label": 0
            },
            {
                "text": "Therefore, we generate a dataset with a broad range of errors, which is an advantage of our method.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Persian Typographical Error Type Detection Using Deep Neural Networks on Algorithmically_Generated Misspellings",
        "section": 7,
        "paragraph_id": 7,
        "full_text": "Using different layers and text representations, we will explore various neural architectures to establish a baseline for the task of Persian spelling error type detection.The dataset used in the study contains 3,450,918 parallel words and error types, which are divided into 60%, 20%, and 20% for training, validation, and testing.Table5lists the parameters used to train the model.The maximum length of a sequence is set at 30 words, and the model is trained in 30 epochs with a batch size of 256.The Adam optimizer is used to adjust the network's parameters and minimize the loss function, which is calculated using categorical cross entropy.The learning rate parameter has been set to 2e-5.Another factor to be compared between Tables6and7is how different layers have contributed either positively or actively towards the obtained results.The Vanilla RNN performed fastest in both training and testing, making the model light and more feasible to work with.However, this comes at the cost of an inefficient model.In contrast, the bidirectional layers served relatively more accurately using either word or character embeddings.Although the bidirectional mechanism takes longer to function, it can be negligible since it slightly differs.In this stage, we develop our final architecture by exploiting the effective components of previously-examined methods.Since their accumulation is speed-efficient in our case, both word and character embeddings are utilized simultaneously to gain insight from two different representations and their previously discussed benefits.The two embeddings are further connected to a deep architecture, comprised of LSTM layers that process text bidirectionally before connecting to a TimeDistributed output layer for the token classification purpose.In contrast to unidirectional layers, bidirectional LSTM layers account for textual information from the past and future of a time frame through both forward and backward processing.The effectiveness of this architecture is established by drawing analogies with modern methodologies.",
        "sentences": [
            {
                "text": "Using different layers and text representations, we will explore various neural architectures to establish a baseline for the task of Persian spelling error type detection.",
                "label": 0
            },
            {
                "text": "The dataset used in the study contains 3,450,918 parallel words and error types, which are divided into 60%, 20%, and 20% for training, validation, and testing.",
                "label": 1
            },
            {
                "text": "Table5lists the parameters used to train the model.",
                "label": 0
            },
            {
                "text": "The maximum length of a sequence is set at 30 words, and the model is trained in 30 epochs with a batch size of 256.",
                "label": 0
            },
            {
                "text": "The Adam optimizer is used to adjust the network's parameters and minimize the loss function, which is calculated using categorical cross entropy.",
                "label": 0
            },
            {
                "text": "The learning rate parameter has been set to 2e-5.",
                "label": 0
            },
            {
                "text": "Another factor to be compared between Tables6and7is how different layers have contributed either positively or actively towards the obtained results.",
                "label": 0
            },
            {
                "text": "The Vanilla RNN performed fastest in both training and testing, making the model light and more feasible to work with.",
                "label": 0
            },
            {
                "text": "However, this comes at the cost of an inefficient model.",
                "label": 0
            },
            {
                "text": "In contrast, the bidirectional layers served relatively more accurately using either word or character embeddings.",
                "label": 0
            },
            {
                "text": "Although the bidirectional mechanism takes longer to function, it can be negligible since it slightly differs.",
                "label": 0
            },
            {
                "text": "In this stage, we develop our final architecture by exploiting the effective components of previously-examined methods.",
                "label": 0
            },
            {
                "text": "Since their accumulation is speed-efficient in our case, both word and character embeddings are utilized simultaneously to gain insight from two different representations and their previously discussed benefits.",
                "label": 0
            },
            {
                "text": "The two embeddings are further connected to a deep architecture, comprised of LSTM layers that process text bidirectionally before connecting to a TimeDistributed output layer for the token classification purpose.",
                "label": 0
            },
            {
                "text": "In contrast to unidirectional layers, bidirectional LSTM layers account for textual information from the past and future of a time frame through both forward and backward processing.",
                "label": 0
            },
            {
                "text": "The effectiveness of this architecture is established by drawing analogies with modern methodologies.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Persian Typographical Error Type Detection Using Deep Neural Networks on Algorithmically_Generated Misspellings",
        "section": 8,
        "paragraph_id": 8,
        "full_text": "Over the years, several publicly-available Persian spell-checkers have been developed in the industry, but the same is not true in academia.Previous studies' implementations were either not released or not available at the time of writing this paper.This hindered comparisons with previous work but motivated us to establish a baseline for future studies.However, this came at the expense of competing with industrialized spell-checkers, which a plethora of technicians developed using significantly large data and computational resources.Nonetheless, we compared our final approach with four applications.Previous studies have compared themselves with Microsoft Word.Besides, Lilak 9 is another commonly used spell-checker among Persian writers which proofreads text after installing an extension in Mozilla Firefox or Google Chrome.Furthermore, we also compared our approach with two highly-developed applications called Virastman and Paknevis 10 .One dilemma when drawing comparisons is the non-identical set of error classes that these applications identify, including ours.This leaves comparisons to be made on binary classes based on whether a misspelt word is detected or not.We designated a test case consisting of 202 synthetically misspelt and 2,651 correct words (2,853 words in total) extracted from Wikipedia and news (namely, Wikipedia test case).After testing our method and prior work on our test case, we were able to achieve competitive accuracy, precision, and recall scores, while our approach outperformed others on test time (Table8).Meaning the knowledge of sentences, words, and characters that our architecture has acquired to make a token classification based on our dataset is an appropriate infrastructure for our task of interest.We also achieved competitive results when we tested our model on PerspellData and Shargh, two publicly available spell-checking test cases provided by Dadmatech11.Our approach scored higher precision (93.1%) and recall (94.7%) compared to Paknevis and Virastman on the PerspellData test case (other scores were not being reported).It is noteworthy that such results have been obtained using relatively much fewer training data.Although Algorithm 1 is adaptable so that it can be used to expand our dataset up to a much larger size so that results would consequently rise, the lack of computational power impeded us from following up the upswing trend of scores, which was witnessed by training our approach on datasets ranging from 500,000 to 3,450,918.",
        "sentences": [
            {
                "text": "Over the years, several publicly-available Persian spell-checkers have been developed in the industry, but the same is not true in academia.",
                "label": 0
            },
            {
                "text": "Previous studies' implementations were either not released or not available at the time of writing this paper.",
                "label": 0
            },
            {
                "text": "This hindered comparisons with previous work but motivated us to establish a baseline for future studies.",
                "label": 0
            },
            {
                "text": "However, this came at the expense of competing with industrialized spell-checkers, which a plethora of technicians developed using significantly large data and computational resources.",
                "label": 0
            },
            {
                "text": "Nonetheless, we compared our final approach with four applications.",
                "label": 0
            },
            {
                "text": "Previous studies have compared themselves with Microsoft Word.",
                "label": 0
            },
            {
                "text": "Besides, Lilak 9 is another commonly used spell-checker among Persian writers which proofreads text after installing an extension in Mozilla Firefox or Google Chrome.",
                "label": 0
            },
            {
                "text": "Furthermore, we also compared our approach with two highly-developed applications called Virastman and Paknevis 10 .",
                "label": 0
            },
            {
                "text": "One dilemma when drawing comparisons is the non-identical set of error classes that these applications identify, including ours.",
                "label": 0
            },
            {
                "text": "This leaves comparisons to be made on binary classes based on whether a misspelt word is detected or not.",
                "label": 0
            },
            {
                "text": "We designated a test case consisting of 202 synthetically misspelt and 2,651 correct words (2,853 words in total) extracted from Wikipedia and news (namely, Wikipedia test case).",
                "label": 0
            },
            {
                "text": "After testing our method and prior work on our test case, we were able to achieve competitive accuracy, precision, and recall scores, while our approach outperformed others on test time (Table8).",
                "label": 0
            },
            {
                "text": "Meaning the knowledge of sentences, words, and characters that our architecture has acquired to make a token classification based on our dataset is an appropriate infrastructure for our task of interest.",
                "label": 0
            },
            {
                "text": "We also achieved competitive results when we tested our model on PerspellData and Shargh, two publicly available spell-checking test cases provided by Dadmatech11.",
                "label": 0
            },
            {
                "text": "Our approach scored higher precision (93.1%) and recall (94.7%) compared to Paknevis and Virastman on the PerspellData test case (other scores were not being reported).",
                "label": 0
            },
            {
                "text": "It is noteworthy that such results have been obtained using relatively much fewer training data.",
                "label": 0
            },
            {
                "text": "Although Algorithm 1 is adaptable so that it can be used to expand our dataset up to a much larger size so that results would consequently rise, the lack of computational power impeded us from following up the upswing trend of scores, which was witnessed by training our approach on datasets ranging from 500,000 to 3,450,918.",
                "label": 0
            }
        ]
      },
      {
        "paper_name": "Re 3 Dial_ Retrieve_ Reorganize and Rescale Conversations for Long_Turn Open_Domain Dialogue Pre_training",
        "section": 0,
        "paragraph_id": 0,
        "full_text": "Abstract: Pre-training on large-scale open-domain dialogue data can substantially improve the performance of dialogue models.However, the pre-trained dialogue model's ability to utilize long-range context is limited due to the scarcity of long-turn dialogue sessions.Most dialogues in existing pre-training corpora contain fewer than three turns of dialogue.To alleviate this issue, we propose the Retrieve, Reorganize and Rescale framework (Re 3 Dial), which can automatically construct billion-scale long-turn dialogues by reorganizing existing short-turn ones.Given a short-turn session, Re 3 Dial first employs a session retriever to retrieve coherent consecutive sessions.To this end, we train the retriever to capture semantic and discourse relations within multi-turn dialogues through contrastive training.Next, Re 3 Dial samples a session from retrieved results following a diversity sampling strategy, which is designed to penalize repetitive or generic sessions.A longer session is then derived by concatenating the original session and the sampled session.By repeating the above process, Re 3 Dial can yield a coherent long-turn dialogue.Extensive experiments on multiple multi-turn dialogue benchmarks demonstrate that Re 3 Dial significantly improves the dialogue model's ability to utilize long-range context and thus generate more sensible and informative responses.Finally, we build a toolkit for efficiently rescaling conversations with Re 3 Dial, which enables us to construct a corpus containing 1B Chinese dialogue sessions with 11.3 turns on average (5× longer than the original corpus).Our retriever model, code, and data is publicly available at https://github.com/thu-coai/Re3Dial.",
        "sentences": [
            {
                "text": "Abstract: Pre-training on large-scale open-domain dialogue data can substantially improve the performance of dialogue models.",
                "label": 0
            },
            {
                "text": "However, the pre-trained dialogue model's ability to utilize long-range context is limited due to the scarcity of long-turn dialogue sessions.",
                "label": 0
            },
            {
                "text": "Most dialogues in existing pre-training corpora contain fewer than three turns of dialogue.",
                "label": 0
            },
            {
                "text": "To alleviate this issue, we propose the Retrieve, Reorganize and Rescale framework (Re 3 Dial), which can automatically construct billion-scale long-turn dialogues by reorganizing existing short-turn ones.",
                "label": 0
            },
            {
                "text": "Given a short-turn session, Re 3 Dial first employs a session retriever to retrieve coherent consecutive sessions.",
                "label": 0
            },
            {
                "text": "To this end, we train the retriever to capture semantic and discourse relations within multi-turn dialogues through contrastive training.",
                "label": 0
            },
            {
                "text": "Next, Re 3 Dial samples a session from retrieved results following a diversity sampling strategy, which is designed to penalize repetitive or generic sessions.",
                "label": 0
            },
            {
                "text": "A longer session is then derived by concatenating the original session and the sampled session.",
                "label": 0
            },
            {
                "text": "By repeating the above process, Re 3 Dial can yield a coherent long-turn dialogue.",
                "label": 0
            },
            {
                "text": "Extensive experiments on multiple multi-turn dialogue benchmarks demonstrate that Re 3 Dial significantly improves the dialogue model's ability to utilize long-range context and thus generate more sensible and informative responses.",
                "label": 0
            },
            {
                "text": "Finally, we build a toolkit for efficiently rescaling conversations with Re 3 Dial, which enables us to construct a corpus containing 1B Chinese dialogue sessions with 11.3 turns on average (5× longer than the original corpus).",
                "label": 1
            },
            {
                "text":"Our retriever model, code, and data is publicly available at https://github.com/thu-coai/Re3Dial.",
                "label": 1
            }
        ]
    },
    {
        "paper_name": "Re 3 Dial_ Retrieve_ Reorganize and Rescale Conversations for Long_Turn Open_Domain Dialogue Pre_training",
        "section": 1,
        "paragraph_id": 1,
        "full_text": "Building intelligent open-domain dialogue systems that can generate coherent and engaging multi-turn dialogues with humans has been one of the long-Parents really expect too much from their children in this society, and such children are 100% mentally unhealthy even if they achieve success in the future.",
        "sentences": [
            {
                "text": "Building intelligent open-domain dialogue systems that can generate coherent and engaging multi-turn dialogues with humans has been one of the long-Parents really expect too much from their children in this society, and such children are 100% mentally unhealthy even if they achieve success in the future.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Re 3 Dial_ Retrieve_ Reorganize and Rescale Conversations for Long_Turn Open_Domain Dialogue Pre_training",
        "section": 2,
        "paragraph_id": 2,
        "full_text": "There is no need for children to start working so hard from such a young age.standing goals in AI.Recently, a variety of largescale open-domain pre-trained dialogue models have dramatically promoted this progress(Roller et al., 2020;Zhou et al., 2021;Shuster et al., 2022b).And a critical ingredient to the success of these models is the pre-training dialogue corpus.However, while existing dialogue pre-training corpus collects millions to billions of dialogues from public social media, e.g., Reddit for English(Roller et al., 2020)and Weibo for Chinese(Zhou et al., 2021), long-turn dialogues are highly scarce.More specifically, based on the publicly reported data statistics shown in Figure1(a), most dialogues in existing pre-training corpora only have less than three turns.The lack of large-scale long-turn di-alogue data restricts dialogue models from deriving more advanced abilities to utilize long-range context for modeling multi-turn dialogues during pre-training(Xu et al., 2021(Xu et al., , 2022b)).",
        "sentences": [
            {
                "text": "There is no need for children to start working so hard from such a young age.standing goals in AI.",
                "label": 0
            },
            {
                "text": "Recently, a variety of largescale open-domain pre-trained dialogue models have dramatically promoted this progress(Roller et al., 2020;Zhou et al., 2021;Shuster et al., 2022b).",
                "label": 0
            },
            {
                "text": "And a critical ingredient to the success of these models is the pre-training dialogue corpus.",
                "label": 0
            },
            {
                "text": "However, while existing dialogue pre-training corpus collects millions to billions of dialogues from public social media, e.g., Reddit for English(Roller et al., 2020)and Weibo for Chinese(Zhou et al., 2021), long-turn dialogues are highly scarce.",
                "label": 0
            },
            {
                "text": "More specifically, based on the publicly reported data statistics shown in Figure1(a), most dialogues in existing pre-training corpora only have less than three turns.",
                "label": 0
            },
            {
                "text": "The lack of large-scale long-turn di-alogue data restricts dialogue models from deriving more advanced abilities to utilize long-range context for modeling multi-turn dialogues during pre-training(Xu et al., 2021(Xu et al., , 2022b)).",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Re 3 Dial_ Retrieve_ Reorganize and Rescale Conversations for Long_Turn Open_Domain Dialogue Pre_training",
        "section": 3,
        "paragraph_id": 3,
        "full_text": "In this paper, we focus on answering the following research question: Can we automatically build a billionscale long-turn dialogue corpus by reorganizing existing short-turn dialogues?",
        "sentences": [
            {
                "text": "In this paper, we focus on answering the following research question: Can we automatically build a billionscale long-turn dialogue corpus by reorganizing existing short-turn dialogues?",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Re 3 Dial_ Retrieve_ Reorganize and Rescale Conversations for Long_Turn Open_Domain Dialogue Pre_training",
        "section": 4,
        "paragraph_id": 4,
        "full_text": "Our basic idea is to construct a long-turn dialogue via recursively retrieving and selecting one consecutive session from the existing dialogue corpus.Despite the simplicity of this idea, we still face several challenges to make the constructed corpus effective in enhancing long-turn dialogue pre-training.First, the selected session should be coherent with the query session.Otherwise, it will introduce noisy utterances without long-range dependency or break the conversation flow(Liu et al., 2021), which may impact the performance of dialogue models.Second, our in-depth analysis reveals that the retrieved sessions tend to be biased to be relevant but semantically repetitive with the query or overly generic (e.g., \"A: Haha, it's so cute.B: Haha! LMAO.\") due to both the data bias in the dialogue corpus(Zhou et al., 2021;Lee et al., 2021;Li et al., 2015;Liu et al., 2018)and the model bias of the retriever(Thakur et al., 2021).These biases significantly lower the diversity and informativeness of the reorganized long-turn dialogues.",
        "sentences": [
            {
                "text": "Our basic idea is to construct a long-turn dialogue via recursively retrieving and selecting one consecutive session from the existing dialogue corpus.",
                "label": 0
            },
            {
                "text": "Despite the simplicity of this idea, we still face several challenges to make the constructed corpus effective in enhancing long-turn dialogue pre-training.",
                "label": 0
            },
            {
                "text": "First, the selected session should be coherent with the query session.",
                "label": 0
            },
            {
                "text": "Otherwise, it will introduce noisy utterances without long-range dependency or break the conversation flow(Liu et al., 2021), which may impact the performance of dialogue models.",
                "label": 0
            },
            {
                "text": "Second, our in-depth analysis reveals that the retrieved sessions tend to be biased to be relevant but semantically repetitive with the query or overly generic (e.g., \"A: Haha, it's so cute.B: Haha! LMAO.\") due to both the data bias in the dialogue corpus(Zhou et al., 2021;Lee et al., 2021;Li et al., 2015;Liu et al., 2018)and the model bias of the retriever(Thakur et al., 2021).",
                "label": 0
            },
            {
                "text": "These biases significantly lower the diversity and informativeness of the reorganized long-turn dialogues.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Re 3 Dial_ Retrieve_ Reorganize and Rescale Conversations for Long_Turn Open_Domain Dialogue Pre_training",
        "section": 5,
        "paragraph_id": 5,
        "full_text": "To tackle the above challenges, we propose the Retrieve, Reorganize and Rescale framework (Re 3 Dial), which employs an Unsupervised Dense Session Retriever (UDSR) to retrieve coherent short-turn dialogues and reorganize them into a long-turn one.We train UDSR through contrastive learning by taking consecutive dialogue segments from the same dialogue as positive pairs and those from different dialogues as negative pairs.To avoid overly retrieving semantically repetitive or generic sessions, we propose a diversity sampling strategy, effectively improving the diversity and informativeness of the reorganized long-turn dialogues.Figure1(b) shows an example of the automatically constructed long-turn dialogue using Re 3 Dial.",
        "sentences": [
            {
                "text": "To tackle the above challenges, we propose the Retrieve, Reorganize and Rescale framework (Re 3 Dial), which employs an Unsupervised Dense Session Retriever (UDSR) to retrieve coherent short-turn dialogues and reorganize them into a long-turn one.",
                "label": 0
            },
            {
                "text": "We train UDSR through contrastive learning by taking consecutive dialogue segments from the same dialogue as positive pairs and those from different dialogues as negative pairs.",
                "label": 0
            },
            {
                "text": "To avoid overly retrieving semantically repetitive or generic sessions, we propose a diversity sampling strategy, effectively improving the diversity and informativeness of the reorganized long-turn dialogues.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Re 3 Dial_ Retrieve_ Reorganize and Rescale Conversations for Long_Turn Open_Domain Dialogue Pre_training",
        "section": 6,
        "paragraph_id": 6,
        "full_text": "We verify the effectiveness of Re 3 Dial on three Chinese multi-turn open-domain dialogue benchmarks.Extensive experiments demonstrate that Re 3 Dial consistently and significantly enhances the dialogue model's ability to utilize long-range context, leading to more sensible and informative responses in multi-turn dialogue.Finally, we develop a toolkit for efficiently rescaling conversations with Re 3 Dial, which enables us to construct a corpus containing 1B Chinese dialogue sessions with 11.3 turns on average (5× longer than that of the original EVA corpus).We will make our retriever model, toolkit, and data public.We believe our work provides new opportunities in long-turn dialogue pre-training to the research community.",
        "sentences": [
            {
                "text": "We verify the effectiveness of Re 3 Dial on three Chinese multi-turn open-domain dialogue benchmarks.",
                "label": 0
            },
            {
                "text": "Extensive experiments demonstrate that Re 3 Dial consistently and significantly enhances the dialogue model's ability to utilize long-range context, leading to more sensible and informative responses in multi-turn dialogue.",
                "label": 0
            },
            {
                "text": "Finally, we develop a toolkit for efficiently rescaling conversations with Re 3 Dial, which enables us to construct a corpus containing 1B Chinese dialogue sessions with 11.3 turns on average (5× longer than that of the original EVA corpus).",
                "label": 1
            },
            {
                "text": "We will make our retriever model, toolkit, and data public.",
                "label": 1
            },
            {
                "text": "We believe our work provides new opportunities in long-turn dialogue pre-training to the research community.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Re 3 Dial_ Retrieve_ Reorganize and Rescale Conversations for Long_Turn Open_Domain Dialogue Pre_training",
        "section": 7,
        "paragraph_id": 7,
        "full_text": "Our contributions can be summarized as follows: • We introduce Re 3 Dial, which presents a novel perspective to alleviate the scarcity of longturn conversations by automatically building a billion-scale long-turn dialogue corpus via concatenating existing short-turn dialogue.• We propose to train a dense session retriever on massive unlabeled plain dialogue data with contrastive learning to capture the global semantic and discourse relations within multiturn dialogues.We also propose the diversity sampling strategy to improve the diversity and informativeness of the automatically constructed corpus.In the past few years, large-scale pre-training has greatly promoted the progress of the NLP community(Brown et al., 2020).Recently, large-scale pre-training has also become the mainstream approach to building open-domain dialogue models, both in English(Zhang et al., 2019;Roller et al., 2020;Thoppilan et al., 2022)and Chinese(Bao et al., 2020;Zhou et al., 2021;Gu et al., 2022;Wen et al., 2022).Through pre-training on massive dialogue data crawled from public social media, these models exhibit strong conversational ability, significantly outperforming traditional non-pre-trained dialogue models.However, the scarcity of longturn dialogues in the pre-training corpus hinders these models from deriving a better ability to utilize long-range context for modeling multi-turn dialogues during pre-training.To alleviate this issue, we study how to automatically and efficiently build a large-scale long-turn dialogue corpus based on the existing short-turn dialogue corpus.",
        "sentences": [
            {
                "text": "• We propose to train a dense session retriever on massive unlabeled plain dialogue data with contrastive learning to capture the global semantic and discourse relations within multiturn dialogues.",
                "label": 0
            },
            {
                "text": "We also propose the diversity sampling strategy to improve the diversity and informativeness of the automatically constructed corpus.",
                "label": 0
            },
            {
                "text": "In the past few years, large-scale pre-training has greatly promoted the progress of the NLP community(Brown et al., 2020).",
                "label": 0
            },
            {
                "text": "Recently, large-scale pre-training has also become the mainstream approach to building open-domain dialogue models, both in English(Zhang et al., 2019;Roller et al., 2020;Thoppilan et al., 2022)and Chinese(Bao et al., 2020;Zhou et al., 2021;Gu et al., 2022;Wen et al., 2022).",
                "label": 0
            },
            {
                "text": "Through pre-training on massive dialogue data crawled from public social media, these models exhibit strong conversational ability, significantly outperforming traditional non-pre-trained dialogue models.",
                "label": 0
            },
            {
                "text": "However, the scarcity of longturn dialogues in the pre-training corpus hinders these models from deriving a better ability to utilize long-range context for modeling multi-turn dialogues during pre-training.",
                "label": 0
            },
            {
                "text": "To alleviate this issue, we study how to automatically and efficiently build a large-scale long-turn dialogue corpus based on the existing short-turn dialogue corpus.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Re 3 Dial_ Retrieve_ Reorganize and Rescale Conversations for Long_Turn Open_Domain Dialogue Pre_training",
        "section": 8,
        "paragraph_id": 8,
        "full_text": "We train UDSR on a subset of the EVA pretraining corpus(Zhou et al., 2021), which contains 1,000,000/49,000/1,000 examples for the train/validation/test split.More details of data processing are provided in Appendix A.1.We adopt BERT-base(Devlin et al., 2018)as the encoder backbone.The parameters of E q and E c are not shared according to our preliminary experiments.",
        "sentences": [
            {
                "text": "We train UDSR on a subset of the EVA pretraining corpus(Zhou et al., 2021), which contains 1,000,000/49,000/1,000 examples for the train/validation/test split.",
                "label": 0
            },
            {
                "text": "More details of data processing are provided in Appendix A.1.",
                "label": 0
            },
            {
                "text": "We adopt BERT-base(Devlin et al., 2018)as the encoder backbone.",
                "label": 0
            },
            {
                "text": "The parameters of E q and E c are not shared according to our preliminary experiments.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Re 3 Dial_ Retrieve_ Reorganize and Rescale Conversations for Long_Turn Open_Domain Dialogue Pre_training",
        "section": 9,
        "paragraph_id": 9,
        "full_text": "Settings We consider three general scenarios where Re Benchmarks We conduct evaluations on three widely-adopted Chinese open-domain multi-turn dialogue benchmarks, including KdConv(Zhou et al., 2020), DuLeMon(Xu et al., 2022b), and NaturalConv(Wang et al., 2021), each has 16~20 turns on average.Data statistics are shown in Table9.",
        "sentences": [
            {
                "text": "Settings We consider three general scenarios where Re Benchmarks We conduct evaluations on three widely-adopted Chinese open-domain multi-turn dialogue benchmarks, including KdConv(Zhou et al., 2020), DuLeMon(Xu et al., 2022b), and NaturalConv(Wang et al., 2021), each has 16~20 turns on average.",
                "label": 0
            },
            {
                "text": "Data statistics are shown in Table9.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Re 3 Dial_ Retrieve_ Reorganize and Rescale Conversations for Long_Turn Open_Domain Dialogue Pre_training",
        "section": 10,
        "paragraph_id": 10,
        "full_text": "Metrics We adopt the following automatic metrics for evaluation.PPL zero-shot measures the perplexity on the test set without fine-tuning on the downstream training sets.PPL measures the perplexity on the test set after fine-tuning.BLEU-N measures the precision of the n-gram overlap between generated and ground-truth responses(Papineni et al., 2002)after fine-tuning.ROUGE-L measures the recall of the n-gram overlap between generated and ground-truth responses(Lin, 2004)after fine-tuning.Distinct-N measures the percentage of the unique n-grams over all the generated n-grams after fine-tuning(Li et al., 2015).",
        "sentences": [
            {
                "text": "Metrics We adopt the following automatic metrics for evaluation.",
                "label": 0
            },
            {
                "text": "PPL zero-shot measures the perplexity on the test set without fine-tuning on the downstream training sets.",
                "label": 0
            },
            {
                "text": "PPL measures the perplexity on the test set after fine-tuning.",
                "label": 0
            },
            {
                "text": "BLEU-N measures the precision of the n-gram overlap between generated and ground-truth responses(Papineni et al., 2002)after fine-tuning.",
                "label": 0
            },
            {
                "text": "ROUGE-L measures the recall of the n-gram overlap between generated and ground-truth responses(Lin, 2004)after fine-tuning.",
                "label": 0
            },
            {
                "text": "Distinct-N measures the percentage of the unique n-grams over all the generated n-grams after fine-tuning(Li et al., 2015).",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Re 3 Dial_ Retrieve_ Reorganize and Rescale Conversations for Long_Turn Open_Domain Dialogue Pre_training",
        "section": 11,
        "paragraph_id": 11,
        "full_text": "Table2shows the automatic evaluation results.In the zero-shot setting, Re  46.25 on DuLeMon, compared to the original baseline's performance of 48.79.This indicates a better ability in multi-turn dialogue modeling.Moreover, beyond benefiting zero-shot performance, Re 3 Dial can also significantly improve the model's performance after fine-tuning on sizable crowdsourcing high-quality long-turn datasets.Specifically, the Re 3 Dial-trained model achieves better perplexity, BLEU, and ROUGE scores, while showing an improved or comparable generation diversity.In summary, these results demonstrate that Re 3 Dial provides a well-generalized data foundation in the era of large-scale dialogue pre-training.",
        "sentences": [
            {
                "text": "Table2shows the automatic evaluation results.",
                "label": 0
            },
            {
                "text": "In the zero-shot setting, Re  46.25 on DuLeMon, compared to the original baseline's performance of 48.79.",
                "label": 0
            },
            {
                "text": "This indicates a better ability in multi-turn dialogue modeling.",
                "label": 0
            },
            {
                "text": "Moreover, beyond benefiting zero-shot performance, Re 3 Dial can also significantly improve the model's performance after fine-tuning on sizable crowdsourcing high-quality long-turn datasets.",
                "label": 0
            },
            {
                "text": "Specifically, the Re 3 Dial-trained model achieves better perplexity, BLEU, and ROUGE scores, while showing an improved or comparable generation diversity.",
                "label": 0
            },
            {
                "text": "In summary, these results demonstrate that Re 3 Dial provides a well-generalized data foundation in the era of large-scale dialogue pre-training.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Re 3 Dial_ Retrieve_ Reorganize and Rescale Conversations for Long_Turn Open_Domain Dialogue Pre_training",
        "section": 12,
        "paragraph_id": 12,
        "full_text": "We conduct a pair-wise human evaluation to study the models' performance when provided with dialogue contexts of different lengths.We first randomly sample 100 long-turn contexts (consisting of at least six turns) from DuLeMon as the Longturn test set.We then extract the last utterances from these contexts to form the Short-turn test set.We hence obtain 400 generated responses from the two models.For each pair of responses (one by the Re 3 Dial-trained model and the other by the Original-trained model), three annotators are hired to give a preference in sensibleness and informativeness, respectively.Sensibleness mea- sures whether the response is relevant and consistent with the context.Informativeness measures whether the response is informative given the context.We adopt majority voting to make final decisions among three annotators.",
        "sentences": [
            {
                "text": "We conduct a pair-wise human evaluation to study the models' performance when provided with dialogue contexts of different lengths.",
                "label": 0
            },
            {
                "text": "We first randomly sample 100 long-turn contexts (consisting of at least six turns) from DuLeMon as the Longturn test set.",
                "label": 0
            },
            {
                "text": "We then extract the last utterances from these contexts to form the Short-turn test set.",
                "label": 0
            },
            {
                "text": "We hence obtain 400 generated responses from the two models.",
                "label": 0
            },
            {
                "text": "For each pair of responses (one by the Re 3 Dial-trained model and the other by the Original-trained model), three annotators are hired to give a preference in sensibleness and informativeness, respectively.",
                "label": 0
            },
            {
                "text": "Sensibleness mea- sures whether the response is relevant and consistent with the context.",
                "label": 0
            },
            {
                "text": "Informativeness measures whether the response is informative given the context.",
                "label": 0
            },
            {
                "text": "We adopt majority voting to make final decisions among three annotators.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Re 3 Dial_ Retrieve_ Reorganize and Rescale Conversations for Long_Turn Open_Domain Dialogue Pre_training",
        "section": 13,
        "paragraph_id": 13,
        "full_text": "Effect of Retriever We compare different approaches to retrieve dialogue sessions and evaluate the final dialogue model performance.We try Random sampling, a term-based retriever BM25, and a state-of-the-art dense retriever Contriever.Table3presents the results.All baselines bring fewer improvements or even inversely hurt model performance, especially zero-shot performance in the further pre-training setting.In contrast, using the retriever in Re 3 Dial achieves consistent and significant improvements across different benchmarks and pre-training settings.",
        "sentences": [
            {
                "text": "Effect of Retriever We compare different approaches to retrieve dialogue sessions and evaluate the final dialogue model performance.",
                "label": 0
            },
            {
                "text": "We try Random sampling, a term-based retriever BM25, and a state-of-the-art dense retriever Contriever.",
                "label": 0
            },
            {
                "text": "Table3presents the results.",
                "label": 0
            },
            {
                "text": "All baselines bring fewer improvements or even inversely hurt model performance, especially zero-shot performance in the further pre-training setting.",
                "label": 0
            },
            {
                "text": "In contrast, using the retriever in Re 3 Dial achieves consistent and significant improvements across different benchmarks and pre-training settings.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Re 3 Dial_ Retrieve_ Reorganize and Rescale Conversations for Long_Turn Open_Domain Dialogue Pre_training",
        "section": 14,
        "paragraph_id": 14,
        "full_text": "To gain a deeper understanding of the effectiveness of different retrievers in capturing global semantic and discourse relations within multi-turn dialogues, we propose to evaluate the retriever using individual tests in different aspects(Ribeiro et al., 2020).To this end, we first construct positive pairs following the strategy illustrated in Section 3.1 and introduce perturbations to create negative pairs.We then compute the retriever's accuracy in discriminating between positive and negative pairs, expecting it assigns a higher score to positive pairs.Our evaluation focuses on three aspects: Irrelevance, Local Relevance, and Discourse Incoherence.For example, to create a locally relevant negative pair, we keep one utterance from the positive session unchanged while replacing the other utterances with a randomly sampled session.More details can be found in Appendix E. The results shown in Table4Overall, these results indicate that automatically building long-turn dialogues to enhance pretraining is non-trivial.Simply improving dialogue turns is insufficient.It is important to retrieve coherent sessions based on both global semantic relevance and discourse coherence within multi-turn dialogues rather than relying solely on word overlap or semantic similarity.Otherwise, it will introduce unexpected noise or biases and lead to slightly improved or even decreased model performance.",
        "sentences": [
            {
                "text": "To gain a deeper understanding of the effectiveness of different retrievers in capturing global semantic and discourse relations within multi-turn dialogues, we propose to evaluate the retriever using individual tests in different aspects(Ribeiro et al., 2020).",
                "label": 0
            },
            {
                "text": "To this end, we first construct positive pairs following the strategy illustrated in Section 3.1 and introduce perturbations to create negative pairs.",
                "label": 0
            },
            {
                "text": "We then compute the retriever's accuracy in discriminating between positive and negative pairs, expecting it assigns a higher score to positive pairs.",
                "label": 0
            },
            {
                "text": "Our evaluation focuses on three aspects: Irrelevance, Local Relevance, and Discourse Incoherence.",
                "label": 0
            },
            {
                "text": "For example, to create a locally relevant negative pair, we keep one utterance from the positive session unchanged while replacing the other utterances with a randomly sampled session.",
                "label": 0
            },
            {
                "text": "More details can be found in Appendix E.",
                "label": 0
            },
            {
                "text": "The results shown in Table4Overall, these results indicate that automatically building long-turn dialogues to enhance pretraining is non-trivial.Simply improving dialogue turns is insufficient.",
                "label": 0
            },
            {
                "text": "It is important to retrieve coherent sessions based on both global semantic relevance and discourse coherence within multi-turn dialogues rather than relying solely on word overlap or semantic similarity.",
                "label": 0
            },
            {
                "text": "Otherwise, it will introduce unexpected noise or biases and lead to slightly improved or even decreased model performance.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Re 3 Dial_ Retrieve_ Reorganize and Rescale Conversations for Long_Turn Open_Domain Dialogue Pre_training",
        "section": 15,
        "paragraph_id": 15,
        "full_text": "To further investigate the influence of the proposed diversity sampling strategy in Re 3 Dial, we conduct an ablation study.As shown in Table5, the dialogue-level and corpus-level weights reduce the bias towards repetitive and generic sessions and improve the diversity and the informativeness of the constructed corpus as expected.Finally, both of them contribute to the pre-trained dialogue model's performance.",
        "sentences": [
            {
                "text": "To further investigate the influence of the proposed diversity sampling strategy in Re 3 Dial, we conduct an ablation study.",
                "label": 0
            },
            {
                "text": "As shown in Table5, the dialogue-level and corpus-level weights reduce the bias towards repetitive and generic sessions and improve the diversity and the informativeness of the constructed corpus as expected.",
                "label": 0
            },
            {
                "text": "Finally, both of them contribute to the pre-trained dialogue model's performance.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Re 3 Dial_ Retrieve_ Reorganize and Rescale Conversations for Long_Turn Open_Domain Dialogue Pre_training",
        "section": 16,
        "paragraph_id": 16,
        "full_text": "To manifest the benefits of Re 3 Dial, we visualize the distribution of PPL zero-shot on samples with varying numbers of dialogue context turns.Specifically, we first (2) Although other retrieval baselines also exhibit a sharper decreasing trend in perplexity compared to the Original-trained model, they generally yield higher perplexity.This implies that while these baselines enhance the utilization of long-range context, they capture fewer long-range dependencies compared to Re 3 Dial and may even exhibit inferior performance when the local context is more effectively utilized.",
        "sentences": [
            {
                "text": "To manifest the benefits of Re 3 Dial, we visualize the distribution of PPL zero-shot on samples with varying numbers of dialogue context turns.",
                "label": 0
            },
            {
                "text": "Specifically, we first (2) Although other retrieval baselines also exhibit a sharper decreasing trend in perplexity compared to the Original-trained model, they generally yield higher perplexity.",
                "label": 0
            },
            {
                "text": "This implies that while these baselines enhance the utilization of long-range context, they capture fewer long-range dependencies compared to Re 3 Dial and may even exhibit inferior performance when the local context is more effectively utilized.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Re 3 Dial_ Retrieve_ Reorganize and Rescale Conversations for Long_Turn Open_Domain Dialogue Pre_training",
        "section": 17,
        "paragraph_id": 17,
        "full_text": "While Re 3 Dial aims to construct a long-turn dialogue pre-training corpus to enhance the utilization of long-range context, there is another line of work that focuses on compressing long contexts into short contexts.We hence additionally conduct experiments on a retrieval-based baseline and a summarization-based baseline for long-term context modeling and compare them with Re 3 Dial.",
        "sentences": [
            {
                "text": "While Re 3 Dial aims to construct a long-turn dialogue pre-training corpus to enhance the utilization of long-range context, there is another line of work that focuses on compressing long contexts into short contexts.",
                "label": 0
            },
            {
                "text":"We hence additionally conduct experiments on a retrieval-based baseline and a summarization-based baseline for long-term context modeling and compare them with Re 3 Dial.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Re 3 Dial_ Retrieve_ Reorganize and Rescale Conversations for Long_Turn Open_Domain Dialogue Pre_training",
        "section": 18,
        "paragraph_id": 18,
        "full_text": "We introduce an additional summarization model to summarize long-term context into short sentences.We try two summarization models: (1) Pegasus-523M(Zhang et al., 2020): It is a widely-adopted encoder-decoder model specifically pre-trained and fine-tuned for text summarization.",
        "sentences": [
            {
                "text": "We introduce an additional summarization model to summarize long-term context into short sentences.",
                "label": 0
            },
            {
                "text": "We try two summarization models: (1) Pegasus-523M(Zhang et al., 2020): It is a widely-adopted encoder-decoder model specifically pre-trained and fine-tuned for text summarization.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Re 3 Dial_ Retrieve_ Reorganize and Rescale Conversations for Long_Turn Open_Domain Dialogue Pre_training",
        "section": 19,
        "paragraph_id": 19,
        "full_text": "(2) ChatGLM-66B(Zeng et al., 2022): It is a widely-adopted instruction-tuned large language model.We report the average PPL zero-shot over three multi-turn dialogue benchmarks.From the results shown in Table6, we observe that Re 3 Dial significantly outperforms all baselines in long-turn dialogue benchmarks.Moreover, augmenting the dialogue model with a context summarization model or a retriever shows less improvement or inversely hurts model performance in several cases.",
        "sentences": [
            {
                "text": "(2) ChatGLM-66B(Zeng et al., 2022): It is a widely-adopted instruction-tuned large language model.",
                "label": 0
            },
            {
                "text": "We report the average PPL zero-shot over three multi-turn dialogue benchmarks.",
                "label": 0
            },
            {
                "text": "From the results shown in Table6, we observe that Re 3 Dial significantly outperforms all baselines in long-turn dialogue benchmarks.",
                "label": 0
            },
            {
                "text": "Moreover, augmenting the dialogue model with a context summarization model or a retriever shows less improvement or inversely hurts model performance in several cases.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Re 3 Dial_ Retrieve_ Reorganize and Rescale Conversations for Long_Turn Open_Domain Dialogue Pre_training",
        "section": 20,
        "paragraph_id": 20,
        "full_text": "On the one hand, the two-stage framework suffers from error propagation due to the introduced summarization model or the retriever.For example, both the summarization model and the retriever may lose important information in the original context.Moreover, the summarization model could also suffer from hallucination problems(Maynez et al., 2020), thereby introducing new noises.In contrast, Re 3 Dial keeps the original long-turn context unchanged and thus does not lead to information loss or introduce new noises.On the other hand, we conjecture that augmenting dialogue models with the context summarization model requires further training on summarizationbased dialogue datasets(Xu et al., 2022a).In contrast, Re 3 Dial does not require collecting additional training datasets and greatly improves the model performance.",
        "sentences": [
            {
                "text": "On the one hand, the two-stage framework suffers from error propagation due to the introduced summarization model or the retriever.",
                "label": 0
            },
            {
                "text": "For example, both the summarization model and the retriever may lose important information in the original context.",
                "label": 0
            },
            {
                "text": "Moreover, the summarization model could also suffer from hallucination problems(Maynez et al., 2020), thereby introducing new noises.",
                "label": 0
            },
            {
                "text": "In contrast, Re 3 Dial keeps the original long-turn context unchanged and thus does not lead to information loss or introduce new noises.",
                "label": 0
            },
            {
                "text": "On the other hand, we conjecture that augmenting dialogue models with the context summarization model requires further training on summarizationbased dialogue datasets(Xu et al., 2022a).",
                "label": 0
            },
            {
                "text": "In contrast, Re 3 Dial does not require collecting additional training datasets and greatly improves the model performance.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Re 3 Dial_ Retrieve_ Reorganize and Rescale Conversations for Long_Turn Open_Domain Dialogue Pre_training",
        "section": 21,
        "paragraph_id": 21,
        "full_text": "As shown in Table7, the Original-trained model mainly focuses on local context and tends to generate more generic responses (e.g., \"I think the same\" in responding to the preceding utterance, \"they thought it was too risky\").In contrast, the Re 3 Dial-trained dialogue model generates words related to the long-range context (e.g., \"fashion designer\" which has been mentioned nine turns prior), Original: Well, actually I think the same.",
        "sentences": [
            {
                "text": "As shown in Table7, the Original-trained model mainly focuses on local context and tends to generate more generic responses (e.g., \"I think the same\" in responding to the preceding utterance, \"they thought it was too risky\").",
                "label": 0
            },
            {
                "text": "In contrast, the Re 3 Dial-trained dialogue model generates words related to the long-range context (e.g., \"fashion designer\" which has been mentioned nine turns prior), Original: Well, actually I think the same.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Re 3 Dial_ Retrieve_ Reorganize and Rescale Conversations for Long_Turn Open_Domain Dialogue Pre_training",
        "section": 22,
        "paragraph_id": 22,
        "full_text": "Re 3 Dial: I think it would be a good idea to find another experienced fashion designer , which will help you to achieve your dream.",
        "sentences": [
            {
                "text": "Re 3 Dial: I think it would be a good idea to find another experienced fashion designer , which will help you to achieve your dream.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Re 3 Dial_ Retrieve_ Reorganize and Rescale Conversations for Long_Turn Open_Domain Dialogue Pre_training",
        "section": 23,
        "paragraph_id": 23,
        "full_text": "Table7: Generated responses from the model pretrained on Re 3 Dial and Original corpus (translated from Chinese to English).We highlight the generated spans that are related to long-range context. leading to a more sensible and specific response.",
        "sentences": [
            {
                "text": "Table7: Generated responses from the model pretrained on Re 3 Dial and Original corpus (translated from Chinese to English).",
                "label": 0
            },
            {
                "text": "We highlight the generated spans that are related to long-range context. leading to a more sensible and specific response.",
                "label": 0
            }
        ]
    },
    {
        "paper_name": "Re 3 Dial_ Retrieve_ Reorganize and Rescale Conversations for Long_Turn Open_Domain Dialogue Pre_training",
        "section": 24,
        "paragraph_id": 24,
        "full_text": "To show the efficiency of constructing large-scale long-turn dialogue data with Re 3 Dial and allow researchers to explore Re 3 Dial easily, we finally release Re 3 Dial-1B, an improved corpus based on the original EVA corpus that contains 1B sessions with 11.3 turns on average (5× longer than that of the original EVA corpus).The whole pipeline costs about five days with 32 V100 32G GPUs.",
        "sentences": [
            {
                "text": "To show the efficiency of constructing large-scale long-turn dialogue data with Re 3 Dial and allow researchers to explore Re 3 Dial easily, we finally release Re 3 Dial-1B, an improved corpus based on the original EVA corpus that contains 1B sessions with 11.3 turns on average (5× longer than that of the original EVA corpus).",
                "label": 1
            },
            {
                "text": "The whole pipeline costs about five days with 32 V100 32G GPUs.",
                "label": 1
            }
        ]
     },
     {
      "paper_name": "Regularizing Reasons for Outfit Evaluation with Gradient Penalty",
      "section": 0,
      "paragraph_id": 0,
      "full_text": "Abstract: In this paper, we build an outfit evaluation system which provides feedbacks consisting of a judgment with a convincing explanation.The system is trained in a supervised manner which faithfully follows the domain knowledge in fashion.We create the EVALUATION3 dataset which is annotated with judgment, the decisive reason for the judgment, and all corresponding attributes (e.g.print, silhouette, and material etc.).In the training process, features of all attributes in an outfit are first extracted and then concatenated as the input for the intra-factor compatibility net.Then, the inter-factor compatibility net is used to compute the loss for judgment.We penalize the gradient of judgment loss of so that our Grad-CAM-like reason is regularized to be consistent with the labeled reason.In inference, according to the obtained information of judgment, reason, and attributes, a user-friendly explanation sentence is generated by the predefined templates.The experimental results show that the obtained network combines the advantages of high precision and good interpretation.",
      "sentences": [
          {
              "text": "Abstract: In this paper, we build an outfit evaluation system which provides feedbacks consisting of a judgment with a convincing explanation.",
              "label": 0
          },
          {
              "text": "The system is trained in a supervised manner which faithfully follows the domain knowledge in fashion.",
              "label": 0
          },
          {
              "text": "We create the EVALUATION3 dataset which is annotated with judgment, the decisive reason for the judgment, and all corresponding attributes (e.g.print, silhouette, and material etc.).",
              "label": 1 
          },
          {
              "text": "In the training process, features of all attributes in an outfit are first extracted and then concatenated as the input for the intra-factor compatibility net.",
              "label": 0
          },
          {
              "text": "Then, the inter-factor compatibility net is used to compute the loss for judgment.",
              "label": 0
          },
          {
              "text": "We penalize the gradient of judgment loss of so that our Grad-CAM-like reason is regularized to be consistent with the labeled reason.",
              "label": 0
          },
          {
              "text": "In inference, according to the obtained information of judgment, reason, and attributes, a user-friendly explanation sentence is generated by the predefined templates.",
              "label": 0
          },
          {
              "text": "The experimental results show that the obtained network combines the advantages of high precision and good interpretation.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Regularizing Reasons for Outfit Evaluation with Gradient Penalty",
      "section": 1,
      "paragraph_id": 1,
      "full_text": "Fashion compatibility evaluation is closely related to our daily life (e.g.Echo Look[10]), and it has attracted increasing attention from researchers[7,6,35].Mainstream methods for fashion compatibility evaluation adopt metric learning: fashion items of the outfit are embedded into a common compatibility space, where items that appear in the dataset are closer in representation and otherwise, have a farther distance.They assume that the occurrence rate of an outfit has direct relevance with its compatibility, which effectively This outfit is bad.The floral top and the zebra print jeans make the outfit so dazzling. The outfit is normal.Nothing wrong but also nothing special. This outfit is good.The floral top make this outfit looks active.",
      "sentences": [
          {
              "text": "Fashion compatibility evaluation is closely related to our daily life (e.g.Echo Look[10]), and it has attracted increasing attention from researchers[7,6,35].",
              "label": 0
          },
          {
              "text": "Mainstream methods for fashion compatibility evaluation adopt metric learning: fashion items of the outfit are embedded into a common compatibility space, where items that appear in the dataset are closer in representation and otherwise, have a farther distance.",
              "label": 0
          },
          {
              "text": "They assume that the occurrence rate of an outfit has direct relevance with its compatibility, which effectively This outfit is bad.",
              "label": 0
          },
          {
              "text": "The floral top and the zebra print jeans make the outfit so dazzling.",
              "label": 0
          },
          {
              "text": "The outfit is normal.Nothing wrong but also nothing special. This outfit is good.",
              "label": 0
          },
          {
              "text": "The floral top make this outfit looks active.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Regularizing Reasons for Outfit Evaluation with Gradient Penalty",
      "section": 2,
      "paragraph_id": 2,
      "full_text": "Outfits need to be evaluated equates the concepts of common and uncommon to compatible and incompatible.However, a discrepancy between being common and being good exists in fashion,.A very common outfit is more likely to be normal rather than good.How to provide professional evaluations that give convincing judgments of good, normal and bad, is still open.",
      "sentences": [
          {
              "text": "Outfits need to be evaluated equates the concepts of common and uncommon to compatible and incompatible.",
              "label": 0
          },
          {
              "text": "However, a discrepancy between being common and being good exists in fashion,.",
              "label": 0
          },
          {
              "text": "A very common outfit is more likely to be normal rather than good.",
              "label": 0
          },
          {
              "text": "How to provide professional evaluations that give convincing judgments of good, normal and bad, is still open.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Regularizing Reasons for Outfit Evaluation with Gradient Penalty",
      "section": 3,
      "paragraph_id": 3,
      "full_text": "A few efforts further focused on giving explanations for the output judgments.[19,5]took user reviews as training data to generate textual explanations.[13]analyzed the outfit images and used heat maps as their explanation.[32]decomposed the item images into human-interpretable features, and identified the most influential feature that contributes to the output.However, the explanation generated by those methods is short on convincingness because of the following limitations: 1.Not relate to specific and concrete reasons.The textual explanation could be very vague, e.g.\"This dress is so beautiful, I like it\". 2. Lack of domain expertise.It might simply recognize fashion attributes rather than analyze their relation, e.g.\"This orange T-shirt and black pants\".3.Not aligned with human experience.Heatmaps may attend to image regions that are hard for the human to comprehend.",
      "sentences": [
          {
              "text": "[32]decomposed the item images into human-interpretable features, and identified the most influential feature that contributes to the output.",
              "label": 0
          },
          {
              "text": "However, the explanation generated by those methods is short on convincingness because of the following limitations: 1.",
              "label": 0
          },
          {
              "text": "Not relate to specific and concrete reasons.",
              "label": 0
          },
          {
              "text": "The textual explanation could be very vague, e.g.\"This dress is so beautiful, I like it\".",
              "label": 0
          },
          {
              "text": "2. Lack of domain expertise.",
              "label": 0
          },
          {
              "text": "3. Not aligned with human experience.",
              "label": 0
          },
          {
              "text": "Heatmaps may attend to image regions that are hard for the human to comprehend.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Regularizing Reasons for Outfit Evaluation with Gradient Penalty",
      "section": 4,
      "paragraph_id": 4,
      "full_text": "In this work, we build an outfit evaluation system that faithfully respects the domain knowledge in fashion.The judgment is summarized into three levels: the outfit is good, bad, or normal.For example, the first outfit in Figure1looks bad because the unmatched print types between the top and bottom make its appearance too dazzling.The mismatch in print is the logical reason to form the judgment.The normal is a common situation when evaluating outfits.For example the second outfit in Figure1, an ordinary black T-shirt with black jeans, does not reach the bar of a good mix and match, but not bad either.While it is hard to single out a concrete reason for being normal, we can give an explanation for the judgment of good and bad.In fashion, the major factors for evaluating outfits include color, print, material, etc.[8].The judgment is based on the overall visual expression of those interplaying factors.An outfit is regarded as bad as long as one factor is not well-matched.If all factors arrive at visual harmony, then the outfit at least can be put into normal.Moreover, a good one must possess certain special design to make it stand out from a normal one.In summary, normal is the intermediate level between good and bad; and for good and bad, we further expect a concrete reason for its deviation from normal.",
      "sentences": [
          {
              "text": "In this work, we build an outfit evaluation system that faithfully respects the domain knowledge in fashion.",
              "label": 0
          },
          {
              "text": "The judgment is summarized into three levels: the outfit is good, bad, or normal.For example, the first outfit in Figure1looks bad because the unmatched print types between the top and bottom make its appearance too dazzling.",
              "label": 0
          },
          {
              "text": "The mismatch in print is the logical reason to form the judgment.",
              "label": 0
          },
          {
              "text": "The normal is a common situation when evaluating outfits.",
              "label": 0
          },
          {
              "text": "For example the second outfit in Figure1, an ordinary black T-shirt with black jeans, does not reach the bar of a good mix and match, but not bad either.",
              "label": 0
          },
          {
              "text": "While it is hard to single out a concrete reason for being normal, we can give an explanation for the judgment of good and bad.",
              "label": 0
          },
          {
              "text": "In fashion, the major factors for evaluating outfits include color, print, material, etc.[8].",
              "label": 0
          },
          {
              "text": "The judgment is based on the overall visual expression of those interplaying factors.",
              "label": 0
          },
          {
              "text": "An outfit is regarded as bad as long as one factor is not well-matched.",
              "label": 0
          },
          {
              "text": "If all factors arrive at visual harmony, then the outfit at least can be put into normal.Moreover, a good one must possess certain special design to make it stand out from a normal one.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Regularizing Reasons for Outfit Evaluation with Gradient Penalty",
      "section": 5,
      "paragraph_id": 5,
      "full_text": "In correspondence to the above discussion, we prepared a dataset that consists of outfit images annotated with the judgment and its decisive reason.To realize the evaluation with reason task, we propose an explainable evaluation framework as follows.Given an outfit composed of several fashion items, feature vectors for color, print, material, silhouette, and design details are first extracted.For each of the factors, an independent net is used to produce an intra-factor compatibility feature.Then all intra-factor compatibility features are concatenated and input into the inter-factor compatibility net, which outputs the judgment.Reason for the judgment is traced back by computing gradients of the judgment w.r.t. the previous intra-factor compatibility features, in a way that resembles Grad-CAM[26].To increase reasonableness other than simply interpreting as-is, we enforce the traced reason to align with annotated reason by adding a regularization in the form of gradient penalty.Based on the results obtained by the network, a user-friendly explanation is generated based on the predesigned decision tree.",
      "sentences": [
          {
              "text": "In correspondence to the above discussion, we prepared a dataset that consists of outfit images annotated with the judgment and its decisive reason.",
              "label": 1
          },
          {
              "text": "To realize the evaluation with reason task, we propose an explainable evaluation framework as follows.",
              "label": 0
          },
          {
              "text": "Given an outfit composed of several fashion items, feature vectors for color, print, material, silhouette, and design details are first extracted.",
              "label": 0
          },
          {
              "text": "For each of the factors, an independent net is used to produce an intra-factor compatibility feature.",
              "label": 0
          },
          {
              "text": "Then all intra-factor compatibility features are concatenated and input into the inter-factor compatibility net, which outputs the judgment.",
              "label": 0
          },
          {
              "text": "Reason for the judgment is traced back by computing gradients of the judgment w.r.t. the previous intra-factor compatibility features, in a way that resembles Grad-CAM[26].",
              "label": 0
          },
          {
              "text": "To increase reasonableness other than simply interpreting as-is, we enforce the traced reason to align with annotated reason by adding a regularization in the form of gradient penalty.",
              "label": 0
          },
          {
              "text": "Based on the results obtained by the network, a user-friendly explanation is generated based on the predesigned decision tree.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Regularizing Reasons for Outfit Evaluation with Gradient Penalty",
      "section": 6,
      "paragraph_id": 6,
      "full_text": "Our main contributions include:(1)We formulate the fashion compatibility evaluation with reason task in a new framework, which respects the domain knowledge in fashion.(2)We annotate an outfit evaluation dataset EVALU-ATION3 which has 18,108 pairs of outfit with judgments, reasons and attributes.(3)We use gradient penalty to align the explanation of the network decisions to that of expert's.Dataset and source code will be released shortly.",
      "sentences": [
          {
              "text": "Our main contributions include:(1)We formulate the fashion compatibility evaluation with reason task in a new framework, which respects the domain knowledge in fashion.",
              "label": 0
          },
          {
              "text":"(2)We annotate an outfit evaluation dataset EVALU-ATION3 which has 18,108 pairs of outfit with judgments, reasons and attributes.",
              "label": 1
          },
          {
              "text": "(3)We use gradient penalty to align the explanation of the network decisions to that of expert's.",
              "label": 0
          },
          {
              "text": "Dataset and source code will be released shortly.",
              "label": 1
          }
      ]
  },
  {
      "paper_name": "Regularizing Reasons for Outfit Evaluation with Gradient Penalty",
      "section": 7,
      "paragraph_id": 7,
      "full_text": "Based on the perspective of fashion, we grade an outfit into three progressive levels: bad, normal, and good.The bad level is defined as the outfit having something wrong, e.g. a wrong color matching or a dazzling print.If the outfit does not make any mistake that breaks the visual balance, it comes up to the normal level.Further, if the outfit has a special design, e.g.attractive color matching, special print, or good cutting, it reaches the good level.Note the logical connection among the three levels.If the outfit has something wrong, i.e. some factors not well-matched, it will be regarded as bad, whether it has some special design or not.",
      "sentences": [
          {
              "text": "Based on the perspective of fashion, we grade an outfit into three progressive levels: bad, normal, and good.",
              "label": 0
          },
          {
              "text": "The bad level is defined as the outfit having something wrong, e.g. a wrong color matching or a dazzling print.",
              "label": 0
          },
          {
              "text": "If the outfit does not make any mistake that breaks the visual balance, it comes up to the normal level.",
              "label": 0
          },
          {
              "text": "Further, if the outfit has a special design, e.g.attractive color matching, special print, or good cutting, it reaches the good level.",
              "label": 0
          },
          {
              "text": "Note the logical connection among the three levels.",
              "label": 0
          },
          {
              "text": "If the outfit has something wrong, i.e. some factors not well-matched, it will be regarded as bad, whether it has some special design or not.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Regularizing Reasons for Outfit Evaluation with Gradient Penalty",
      "section": 8,
      "paragraph_id": 8,
      "full_text": "Our evaluation system needs three-level judgment with the corresponding reason.Detailed attribute annotations are also needed to train factor feature extractors.However, existing public fashion datasets do not have labeled judgments and their decisive reasons.Thus we introduce a new dataset, namely EVALUATION3.The image source is a subset of the Polyvore dataset[11].The dataset contains four sets including train set, validation set, test set, and test-random set.All labels are manually annotated by 9 independent annotators (all major in fashion).Voting mechanism was adopted to get the final label.Furthermore, to mitigate the influence of cultural backgrounds, the 9 annotators (4 male and 5 female) were selected from different fashion regions including Asia-Pacific, Europe, North America, etc.",
      "sentences": [
          {
              "text": "Our evaluation system needs three-level judgment with the corresponding reason.",
              "label": 0
          },
          {
              "text": "Detailed attribute annotations are also needed to train factor feature extractors.",
              "label": 0
          },
          {
              "text": "However, existing public fashion datasets do not have labeled judgments and their decisive reasons.",
              "label": 0
          },
          {
              "text": "Thus we introduce a new dataset, namely EVALUATION3.",
              "label": 1
          },
          {
              "text": "The image source is a subset of the Polyvore dataset[11].",
              "label": 1
          },
          {
              "text": "The dataset contains four sets including train set, validation set, test set, and test-random set.",
              "label": 1
          },
          {
              "text": "All labels are manually annotated by 9 independent annotators (all major in fashion).",
              "label": 1
          },
          {
              "text": "Voting mechanism was adopted to get the final label.",
              "label": 1
          }
      ]
  },
  {
      "paper_name": "Regularizing Reasons for Outfit Evaluation with Gradient Penalty",
      "section": 9,
      "paragraph_id": 9,
      "full_text": "Each outfit contains a top image and a bottom image.We annotated 18,108 outfits in total, including 2,861 (15.8%) in good, 13,587 (75.0%) in normal, and 1,660 (9.2%) in bad.Among them, 12,608 outfits (15.7% in good, 75.4% in normal, and 8.8% in bad) belong to train set, 2,500 outfits (17.0% in good, 74.4% in normal, and 8.6% in bad) are put into validation set, and the rest 3,000 outfits (15.6% in good, 73.8% in normal, and 10.6% in bad) are used for testing.In particular, we added a test-random set that randomly pairs tops and bottoms in the test set.It contains 3,000 outfits with 22.8% in good, 61.2% in normal, and 15.4% in bad.",
      "sentences": [
          {
              "text": "Each outfit contains a top image and a bottom image.",
              "label": 1
          },
          {
              "text": "We annotated 18,108 outfits in total, including 2,861 (15.8%) in good, 13,587 (75.0%) in normal, and 1,660 (9.2%) in bad.",
              "label": 1
          },
          {
              "text": "Among them, 12,608 outfits (15.7% in good, 75.4% in normal, and 8.8% in bad) belong to train set, 2,500 outfits (17.0% in good, 74.4% in normal, and 8.6% in bad) are put into validation set, and the rest 3,000 outfits (15.6% in good, 73.8% in normal, and 10.6% in bad) are used for testing.",
              "label": 1
          },
          {
              "text": "In particular, we added a test-random set that randomly pairs tops and bottoms in the test set.",
              "label": 1
          },
          {
              "text": "It contains 3,000 outfits with 22.8% in good, 61.2% in normal, and 15.4% in bad.",
              "label": 1
          }
      ]
  },
  {
      "paper_name": "Regularizing Reasons for Outfit Evaluation with Gradient Penalty",
      "section": 10,
      "paragraph_id": 10,
      "full_text": "This set provides a different data distribution because outfits in the original dataset are all paired by Internet users.",
      "sentences": [
          {
              "text": "This set provides a different data distribution because outfits in the original dataset are all paired by Internet users.",
              "label": 1
          }
      ]
  },
  {
      "paper_name": "Regularizing Reasons for Outfit Evaluation with Gradient Penalty",
      "section": 11,
      "paragraph_id": 11,
      "full_text": "As our outfit evaluation is based on features of the design factors, the accuracy of attribute recognition directly affects the evaluation result.To reduce domain gap, we labeled the corresponding attributes on our dataset instead of adopting different fashion attribute datasets.We summarized 14 print types, including abstract, allover, animal print, etc. Besides, there are 5, 8 and 10 attributes in silhouettes (A-line, H-line, Peg-top, etc.), design details (tiered, wrap, ruffle etc.), and materials (knit, lace, leather, etc.), respectively.Figure2shows some examples of the dataset.",
      "sentences": [
          {
              "text": "As our outfit evaluation is based on features of the design factors, the accuracy of attribute recognition directly affects the evaluation result.",
              "label": 1
          },
          {
              "text": "To reduce domain gap, we labeled the corresponding attributes on our dataset instead of adopting different fashion attribute datasets.",
              "label": 1
          },
          {
              "text": "We summarized 14 print types, including abstract, allover, animal print, etc. Besides, there are 5, 8 and 10 attributes in silhouettes (A-line, H-line, Peg-top, etc.), design details (tiered, wrap, ruffle etc.), and materials (knit, lace, leather, etc.), respectively.",
              "label": 1
          },
          {
              "text": "Figure2shows some examples of the dataset.",
              "label": 1
          }
      ]
  },
  {
      "paper_name": "Regularizing Reasons for Outfit Evaluation with Gradient Penalty",
      "section": 12,
      "paragraph_id": 12,
      "full_text": "In this section, we first conduct experiments on EVAL-UATION3 to show the advantages of the proposed evaluation system quantitatively and qualitatively.The quantitative analysis focuses on both the judgment accuracy and the reason accuracy.We show the superiority of our approach on the test set and the test-random set.Recall that the testrandom set is the random composition of fashion items in the test set, which could be regarded as an indicator of the model's generalization ability.Regarding qualitative analysis, we visualize the detailed contributions in varied situations, e.g.judgment: bad, reason: print, to verify the advantage of our method with gradient penalty.Then, we provide ablation study on the choice of contribution formulations and regularizers.",
      "sentences": [
          {
              "text": "In this section, we first conduct experiments on EVAL-UATION3 to show the advantages of the proposed evaluation system quantitatively and qualitatively.",
              "label": 0
          },
          {
              "text": "The quantitative analysis focuses on both the judgment accuracy and the reason accuracy.",
              "label": 0
          },
          {
              "text": "We show the superiority of our approach on the test set and the test-random set.",
              "label": 0
          },
          {
              "text": "Recall that the testrandom set is the random composition of fashion items in the test set, which could be regarded as an indicator of the model's generalization ability.",
              "label": 0
          },
          {
              "text": "Regarding qualitative analysis, we visualize the detailed contributions in varied situations, e.g.judgment: bad, reason: print, to verify the advantage of our method with gradient penalty.",
              "label": 0
          },
          {
              "text": "Then, we provide ablation study on the choice of contribution formulations and regularizers.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Regularizing Reasons for Outfit Evaluation with Gradient Penalty",
      "section": 13,
      "paragraph_id": 13,
      "full_text": "Metric.We measure both the judgment accuracy and the reason accuracy.The judgment accuracy is computed as the number of correct predicted judgment (compared with the labeled ground-truth of judgment) divided by the number of total predicted samples.It is worthwhile to note that, for an outfit, if the judgment is wrong, then the reason for this judgment may not make sense.Thus, the premise of the correctly predicted reason is that the judgment is correct.Under the condition of the right predicted judgment, we count the number of correctly predicted reason.Then divided it by the number of total predicted outfits.Then divide it by the number of correct predicted judgments.",
      "sentences": [
          {
              "text": "Metric.",
              "label": 0
          },
          {
              "text": "We measure both the judgment accuracy and the reason accuracy.",
              "label": 0
          },
          {
              "text": "The judgment accuracy is computed as the number of correct predicted judgment (compared with the labeled ground-truth of judgment) divided by the number of total predicted samples.",
              "label": 0
          },
          {
              "text": "It is worthwhile to note that, for an outfit, if the judgment is wrong, then the reason for this judgment may not make sense.",
              "label": 0
          },
          {
              "text": "Thus, the premise of the correctly predicted reason is that the judgment is correct.",
              "label": 0
          },
          {
              "text": "Under the condition of the right predicted judgment, we count the number of correctly predicted reason.",
              "label": 0
          },
          {
              "text": "Then divided it by the number of total predicted outfits.",
              "label": 0
          },
          {
              "text": "Then divide it by the number of correct predicted judgments.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Regularizing Reasons for Outfit Evaluation with Gradient Penalty",
      "section": 14,
      "paragraph_id": 14,
      "full_text": "Training details.We train the pipeline (Figure3) in two steps.In the first step, we train four CNNs to learn the representations of the print, the material, the silhouette and the design details separately.The CNN structure is ResNet18.Each factor has its corresponding labels, with examples shown in Figure2. All the networks are trained using RM-SProp with initial learning rate 0.0001 and weight decay 0.00005, the learning rate is divided by 10 every 30 epochs.Parameters of the pre-trained model are fixed for the first 20 epochs.We use the feature of the second last fc-layer from the pre-trained model as the input to the second stage.The color feature is directly extracted by the histogram.In the second step, we train the intra-factor compatibility networks, the inter-factor compatibility network, and the reason regularization (Modern DL frameworks such as Py-Torch have built-in support for computing the gradient of gradients) jointly.In particular, we use SGD with initial learning rate 0.01, and weight decay 0.0005 for 70 epochs and the learning rate is also divided by 10 every 30 epochs.Since the dataset is unbalanced (the ratio of normal in the train set reaches 75.4%), we adjust the sampling rate for each class (good, normal, and bad) to obtain the balanced sampled data.",
      "sentences": [
          {
              "text": "Training details.",
              "label": 0
          },
          {
              "text": "We train the pipeline (Figure3) in two steps.",
              "label": 0
          },
          {
              "text": "In the first step, we train four CNNs to learn the representations of the print, the material, the silhouette and the design details separately.",
              "label": 0
          },
          {
              "text": "The CNN structure is ResNet18.",
              "label": 0
          },
          {
              "text": "Each factor has its corresponding labels, with examples shown in Figure2.",
              "label": 0
          },
          {
              "text": "All the networks are trained using RM-SProp with initial learning rate 0.0001 and weight decay 0.00005, the learning rate is divided by 10 every 30 epochs.",
              "label": 0
          },
          {
              "text": "Parameters of the pre-trained model are fixed for the first 20 epochs.",
              "label": 0
          },
          {
              "text": "We use the feature of the second last fc-layer from the pre-trained model as the input to the second stage.",
              "label": 0
          },
          {
              "text": "The color feature is directly extracted by the histogram.",
              "label": 0
          },
          {
              "text": "In the second step, we train the intra-factor compatibility networks, the inter-factor compatibility network, and the reason regularization (Modern DL frameworks such as Py-Torch have built-in support for computing the gradient of gradients) jointly.",
              "label": 0
          },
          {
              "text": "In particular, we use SGD with initial learning rate 0.01, and weight decay 0.0005 for 70 epochs and the learning rate is also divided by 10 every 30 epochs.",
              "label": 0
          },
          {
              "text": "Since the dataset is unbalanced (the ratio of normal in the train set reaches 75.4%), we adjust the sampling rate for each class (good, normal, and bad) to obtain the balanced sampled data.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Regularizing Reasons for Outfit Evaluation with Gradient Penalty",
      "section": 15,
      "paragraph_id": 15,
      "full_text": "Quantitative analysis.Table1shows the comparison on the accuracy with related methods.IFIV [32, Equation10] computes the item feature influence value, which has the same maximum factor as our contrib j .We also test the performance without regularization (i.e., set α = 0) as shown in Reason-NoReg.Meanwhile, we evaluate the performance with three regularizations, linear, square, and cross-entropy, respectively.We can see that (1) High judgment accuracy: Our method reaches a high classification accuracy on evaluation without increasing parameters.As shown in the first line in  even surpasses to pure-classification models w.r.t.regarding the judgment and reason as two independent classification problems and jointly train via multitasking.Meanwhile, under the setting of cross-entropy loss, the judgment accuracy is larger than Reason-NoReg which has no regularization.This demonstrates the effectiveness of the gradient penalty.(2) High reason accuracy: For methods like IFIV and Reason-NoReg, they can bring us some explanations or feedback.However, their results are less likely to be fully aligned to expert thoughts.(3) Good generalization: From the good performance on test-random sets, we see that our result still achieves the highest accuracy among the compared methods, which means our approach enjoys good generalizability.",
      "sentences": [
          {
              "text": "Quantitative analysis.",
              "label": 0
          },
          {
              "text": "Table1shows the comparison on the accuracy with related methods.",
              "label": 0
          },
          {
              "text": "IFIV [32, Equation10] computes the item feature influence value, which has the same maximum factor as our contrib j .",
              "label": 0
          },
          {
              "text": "We also test the performance without regularization (i.e., set α = 0) as shown in Reason-NoReg.",
              "label": 0
          },
          {
              "text": "Meanwhile, we evaluate the performance with three regularizations, linear, square, and cross-entropy, respectively.",
              "label": 0
          },
          {
              "text": "We can see that (1) High judgment accuracy: Our method reaches a high classification accuracy on evaluation without increasing parameters.",
              "label": 0
          },
          {
              "text": "regarding the judgment and reason as two independent classification problems and jointly train via multitasking.",
              "label": 0
          },
          {
              "text": "Meanwhile, under the setting of cross-entropy loss, the judgment accuracy is larger than Reason-NoReg which has no regularization.",
              "label": 0
          },
          {
              "text": "(2) High reason accuracy: For methods like IFIV and Reason-NoReg, they can bring us some explanations or feedback.",
              "label": 0
          },
          {
              "text": "(3) Good generalization: From the good performance on test-random sets, we see that our result still achieves the highest accuracy among the compared methods, which means our approach enjoys good generalizability.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Regularizing Reasons for Outfit Evaluation with Gradient Penalty",
      "section": 16,
      "paragraph_id": 16,
      "full_text": "As discussed in the Section 2, the embedding methods are mainly adopted in previous works.We conducted a study to show that embedding may fail to distinguish good, normal, and bad, which makes it not suitable for our task.There are two basic ways to represent evaluation: absolute rating and relative embedding.Embedding methods as-sume that there is a common compatibility space for fashion items.The idea is to pull the representations of positive pairs closer and push the negative pairs farther (positive pairs are those that appear in dataset e.g.Polyvore[11]and FashionVC[30], and negative pairs are random combinations).We built an embedding model consisting of two ResNet18 networks which respectively embeds top and bottom to the compatibility space.The compatibility is measured by the cosine distance, and the model is trained using the BPR loss.After training, the average distance between top and bottom of bad is 0.464, far less than that of normal (0.562) and good (0.574).This result is against with the assumption that more compatible items have a smaller distance.",
      "sentences": [
          {
              "text": "As discussed in the Section 2, the embedding methods are mainly adopted in previous works.",
              "label": 0
          },
          {
              "text": "We conducted a study to show that embedding may fail to distinguish good, normal, and bad, which makes it not suitable for our task.",
              "label": 0
          },
          {
              "text": "There are two basic ways to represent evaluation: absolute rating and relative embedding.",
              "label": 0
          },
          {
              "text": "Embedding methods as-sume that there is a common compatibility space for fashion items.",
              "label": 0
          },
          {
              "text": "The idea is to pull the representations of positive pairs closer and push the negative pairs farther (positive pairs are those that appear in dataset e.g.Polyvore[11]and FashionVC[30], and negative pairs are random combinations).",
              "label": 0
          },
          {
              "text": "We built an embedding model consisting of two ResNet18 networks which respectively embeds top and bottom to the compatibility space.",
              "label": 0
          },
          {
              "text": "The compatibility is measured by the cosine distance, and the model is trained using the BPR loss.",
              "label": 0
          },
          {
              "text": "After training, the average distance between top and bottom of bad is 0.464, far less than that of normal (0.562) and good (0.574).",
              "label": 0
          },
          {
              "text": "This result is against with the assumption that more compatible items have a smaller distance.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Regularizing Reasons for Outfit Evaluation with Gradient Penalty",
      "section": 17,
      "paragraph_id": 17,
      "full_text": "Qualitative analysis.We show some examples with more details in Figure5.Take the first mini-table as an example.The ground-truth and predicted judgment for this outfit are both good while the ground-truth reason is color.The scores computed by Equation (7) (the second row with label G)  Additionally, we present the final performance of our outfit evaluation system in Figure6.Given an outfit, an absolute judgment with convincing explanation can be provided to users.Take the outfit at the fifth line in Figure6for example.This outfit is bad, because the pink yarrow color in top and cadmium green color in the bottom is wrong color matching.Meanwhile, our method could also be applied in outfit recommendation scene by simply scaling the logit or the probability of the judgment as of the relative score for recommendation sort.This kind of explanation framework took the suggestions of fashion experts as reference.",
      "sentences": [
          {
              "text": "Qualitative analysis.",
              "label": 0
          },
          {
              "text": "We show some examples with more details in Figure5.",
              "label": 0
          },
          {
              "text": "Take the first mini-table as an example.",
              "label": 0
          },
          {
              "text": "The ground-truth and predicted judgment for this outfit are both good while the ground-truth reason is color.",
              "label": 0
          },
          {
              "text": "The scores computed by Equation (7) (the second row with label G)  Additionally, we present the final performance of our outfit evaluation system in Figure6.",
              "label": 0
          },
          {
              "text": "Given an outfit, an absolute judgment with convincing explanation can be provided to users.",
              "label": 0
          },
          {
              "text": "Take the outfit at the fifth line in Figure6for example.",
              "label": 0
          },
          {
              "text": "This outfit is bad, because the pink yarrow color in top and cadmium green color in the bottom is wrong color matching.",
              "label": 0
          },
          {
              "text": "Meanwhile, our method could also be applied in outfit recommendation scene by simply scaling the logit or the probability of the judgment as of the relative score for recommendation sort.",
              "label": 0
          },
          {
              "text": "This kind of explanation framework took the suggestions of fashion experts as reference.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Regularizing Reasons for Outfit Evaluation with Gradient Penalty",
      "section": 18,
      "paragraph_id": 18,
      "full_text": "Contribution formulations.We explore other candidate formulations for the reasons in fashion compatibility evaluation.The network in Figure3was trained without any reason regularizations, i.e. using an unsupervised treatment for reasons.After the training, we analyze the reason accuracy for different reason formulations in Figure7.The first one is  Here the notation \"good\" represents contrib good and \"good + \" is shorthand for contrib + good , etc.We can see that the 6-th formulation positive contribution difference has the greatest accuracy on average.to directly use the contribution defined in Equation (3) and we use it as the reference performance.Especially, Grad-CAM is equivalent to the first one when we take the maximum for the major reason.We see that the contribution difference (the 2nd formulation) and the positive contribution difference (the 6th formulation) are slightly better than the baseline.Particularly, the positive contribution difference has lower variance and thus we adopt the 6th one as the formulation for reason.",
      "sentences": [
          {
              "text": "Contribution formulations.",
              "label": 0
          },
          {
              "text": "We explore other candidate formulations for the reasons in fashion compatibility evaluation.",
              "label": 0
          },
          {
              "text": "The network in Figure3was trained without any reason regularizations, i.e. using an unsupervised treatment for reasons.",
              "label": 0
          },
          {
              "text": "After the training, we analyze the reason accuracy for different reason formulations in Figure7.",
              "label": 0
          },
          {
              "text": "The first one is  Here the notation \"good\" represents contrib good and \"good + \" is shorthand for contrib + good , etc.We can see that the 6-th formulation positive contribution difference has the greatest accuracy on average.to directly use the contribution defined in Equation (3) and we use it as the reference performance.",
              "label": 0
          },
          {
              "text": "Especially, Grad-CAM is equivalent to the first one when we take the maximum for the major reason.",
              "label": 0
          },
          {
              "text": "We see that the contribution difference (the 2nd formulation) and the positive contribution difference (the 6th formulation) are slightly better than the baseline.",
              "label": 0
          },
          {
              "text": "Particularly, the positive contribution difference has lower variance and thus we adopt the 6th one as the formulation for reason.",
              "label": 0
          }
      ]
    },
    {
      "paper_name": "Rethinking Image_based Table Recognition Using Weakly Supervised Methods",
      "section": 0,
      "paragraph_id": 0,
      "full_text": "Abstract: Most of the previous methods for table recognition rely on training datasets containing many richly annotated table images.Detailed table image annotation, e.g., cell or text bounding box annotation, however, is costly and often subjective.In this paper, we propose a weakly supervised model named WSTabNet for table recognition that relies only on HTML (or LaTeX) code-level annotations of table images.The proposed model consists of three main parts: an encoder for feature extraction, a structure decoder for generating table structure, and a cell decoder for predicting the content of each cell in the table.Our system is trained end-to-end by stochastic gradient descent algorithms, requiring only table images and their ground-truth HTML (or LaTeX) representations.To facilitate table recognition with deep learning, we create and release WikiTableSet, the largest publicly available image-based table recognition dataset built from Wikipedia.WikiTableSet contains nearly 4 million English table images, 590K Japanese table images, and 640k French table images with corresponding HTML representation and cell bounding boxes . The extensive experiments on WikiTableSet and two large-scale datasets: FinTabNet and PubTabNet demonstrate that the proposed weakly supervised model achieves better, or similar accuracies compared to the state-of-the-art models on all benchmark datasets. a",
      "sentences": [
          {
              "text": "Abstract: Most of the previous methods for table recognition rely on training datasets containing many richly annotated table images.",
              "label": 0
          },
          {
              "text": "Detailed table image annotation, e.g., cell or text bounding box annotation, however, is costly and often subjective.",
              "label": 0
          },
          {
              "text": "In this paper, we propose a weakly supervised model named WSTabNet for table recognition that relies only on HTML (or LaTeX) code-level annotations of table images.",
              "label": 0
          },
          {
              "text": "The proposed model consists of three main parts: an encoder for feature extraction, a structure decoder for generating table structure, and a cell decoder for predicting the content of each cell in the table.",
              "label": 0
          },
          {
              "text": "Our system is trained end-to-end by stochastic gradient descent algorithms, requiring only table images and their ground-truth HTML (or LaTeX) representations.",
              "label": 0
          },
          {
              "text": "To facilitate table recognition with deep learning, we create and release WikiTableSet, the largest publicly available image-based table recognition dataset built from Wikipedia.",
              "label": 1
          },
          {
              "text": "WikiTableSet contains nearly 4 million English table images, 590K Japanese table images, and 640k French table images with corresponding HTML representation and cell bounding boxes .",
              "label": 1
          },
          {
              "text": "The extensive experiments on WikiTableSet and two large-scale datasets: FinTabNet and PubTabNet demonstrate that the proposed weakly supervised model achieves better, or similar accuracies compared to the state-of-the-art models on all benchmark datasets.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Rethinking Image_based Table Recognition Using Weakly Supervised Methods",
      "section": 1,
      "paragraph_id": 1,
      "full_text": "Table recognition has been receiving much attention from numerous researchers.It plays an important role in many document analysis systems, in which tabular data presented by PDF or document image often contains rich and essential information in a structured format.Table recognition aims to recognize the table structure and the content of each table cell from a table image, and to represent them in a machinereadable format, which can be HTML code(Jimeno Yepes et al., 2021;Li et al., 2019;Zhong et al., 2020), or LaTeX code(Deng et al., 2019;Kayal et al., 2021).However, this task is still a big challenging problem, due to the diversity of table styles and the complexity of table structures.",
      "sentences": [
          {
              "text": "Table recognition has been receiving much attention from numerous researchers.",
              "label": 0
          },
          {
              "text": "It plays an important role in many document analysis systems, in which tabular data presented by PDF or document image often contains rich and essential information in a structured format.",
              "label": 0
          },
          {
              "text": "Table recognition aims to recognize the table structure and the content of each table cell from a table image, and to represent them in a machinereadable format, which can be HTML code(Jimeno Yepes et al., 2021;Li et al., 2019;Zhong et al., 2020), or LaTeX code(Deng et al., 2019;Kayal et al., 2021).",
              "label": 0
          },
          {
              "text": "However, this task is still a big challenging problem, due to the diversity of table styles and the complexity of table structures.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Rethinking Image_based Table Recognition Using Weakly Supervised Methods",
      "section": 2,
      "paragraph_id": 2,
      "full_text": "In recent years, many table recognition methods have been proposed and proven to be powerful models for both PDF and image-based table recognition.However, most previous studies(Nassar et al., 2022;Qiao et al., 2021;Ye et al., 2021;Zhang et al., 2022)are based on fully supervised learning approaches.They rely on training datasets that contain lots of richly annotated table images.Detailed table image annotation, e.g., cell or text bounding box annotation, however, is costly and often subjective.Due to the rapid development of deep learning, some works(Deng et al., 2019;Zhong et al., 2020)focus on weakly supervised approaches that rely only on HTML (or LaTeX) code-level labels of table images.However, their performance is still mediocre compared to the fully supervised methods.",
      "sentences": [
          {
              "text": "In recent years, many table recognition methods have been proposed and proven to be powerful models for both PDF and image-based table recognition.",
              "label": 0
          },
          {
              "text": "However, most previous studies(Nassar et al., 2022;Qiao et al., 2021;Ye et al., 2021;Zhang et al., 2022)are based on fully supervised learning approaches.",
              "label": 0
          },
          {
              "text": "They rely on training datasets that contain lots of richly annotated table images.",
              "label": 0
          },
          {
              "text": "Detailed table image annotation, e.g., cell or text bounding box annotation, however, is costly and often subjective.",
              "label": 0
          },
          {
              "text": "Due to the rapid development of deep learning, some works(Deng et al., 2019;Zhong et al., 2020)focus on weakly supervised approaches that rely only on HTML (or LaTeX) code-level labels of table images.",
              "label": 0
          },
          {
              "text": "However, their performance is still mediocre compared to the fully supervised methods.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Rethinking Image_based Table Recognition Using Weakly Supervised Methods",
      "section": 3,
      "paragraph_id": 3,
      "full_text": "This paper proposes a weakly supervised model named WSTabNet for image-based table recognition.The proposed model consists of three main parts: an encoder for feature extraction, a structure decoder for generating table structure, and a cell decoder for predicting the content of each cell in the table.The system is trained in an end-to-end manner by stochastic gradient descent algorithms, requiring only table images and their ground-truth HTML (or LaTeX) representations.To facilitate table recognition with deep learning, we create and release WikiTableSet5, the largest publicly available imagebased table recognition dataset built from Wikipedia.",
      "sentences": [
          {
              "text": "This paper proposes a weakly supervised model named WSTabNet for image-based table recognition.",
              "label": 0
          },
          {
              "text": "The proposed model consists of three main parts: an encoder for feature extraction, a structure decoder for generating table structure, and a cell decoder for predicting the content of each cell in the table.",
              "label": 0
          },
          {
              "text": "The system is trained in an end-to-end manner by stochastic gradient descent algorithms, requiring only table images and their ground-truth HTML (or LaTeX) representations.",
              "label": 0
          },
          {
              "text": "To facilitate table recognition with deep learning, we create and release WikiTableSet5, the largest publicly available imagebased table recognition dataset built from Wikipedia.",
              "label": 1
          }
      ]
  },
  {
      "paper_name": "Rethinking Image_based Table Recognition Using Weakly Supervised Methods",
      "section": 4,
      "paragraph_id": 4,
      "full_text": "The experimental results on WikiTableSet and two large-scale datasets: FinTabNet(Zheng et al., 2021)and PubTabNet(Zhong et al., 2020)show that our model achieves better or similar accuracies compared to the state-of-the-art models on all benchmark datasets.We also evaluated the proposed model on the final evaluation set of the ICDAR2021 competition on scientific literature parsing(Jimeno Yepes et al., 2021), demonstrating that the proposed weakly supervised model outperforms the 5th ranking solution and achieves competitive results when compared to the top four solutions.The code and WikiTableSet dataset will be publicly released to GitHub.",
      "sentences": [
          {
              "text": "The experimental results on WikiTableSet and two large-scale datasets: FinTabNet(Zheng et al., 2021)and PubTabNet(Zhong et al., 2020)show that our model achieves better or similar accuracies compared to the state-of-the-art models on all benchmark datasets.",
              "label": 0
          },
          {
              "text": "We also evaluated the proposed model on the final evaluation set of the ICDAR2021 competition on scientific literature parsing(Jimeno Yepes et al., 2021), demonstrating that the proposed weakly supervised model outperforms the 5th ranking solution and achieves competitive results when compared to the top four solutions.",
              "label": 0
          },
          {
              "text": "The code and WikiTableSet dataset will be publicly released to GitHub.",
              "label": 1
          }
      ]
  },
  {
      "paper_name": "Rethinking Image_based Table Recognition Using Weakly Supervised Methods",
      "section": 5,
      "paragraph_id": 5,
      "full_text": "In summary, the main contributions of this paper are as follows: • The rest of this paper is organized as follows.In Sec.2, we give a brief overview of the related works.We introduce the overview of the proposed model in Sec. 3. Sec. 4 describes the overview of the WikiTableSet dataset.In Sec. 5, we report the experimental details and results.Finally, we draw conclusions in Sec. 6.García et al., 2020;Huang et al., 2019)and table recognition(Deng et al., 2019;Ye et al., 2021;Zhong et al., 2020), which we will briefly survey here.Most of the previous methods for table recognition are based on two-step approaches dividing the problem into two steps.The first step is table structure recognition, which recognizes the table structure (including cell location) from a table image.The second step is cell content recognition that predicts the text content of each table cell from its location.Due to the simplicity of the cell content recognition, which can be easily solved by the standard OCR model(Lu et al., 2021;Ly et al., 2021;Shi et al., 2017), many researchers only focused on the table structure recognition problem(Prasad et al., 2020;Raja et al., 2020a;Schreiber et al., 2017).Early works of table structure recognition are based on handcrafted features and heuristic rules and mainly applied to simple table structure or pre-defined table structure formats(Itonori, 1993;Kieninger, 1998;Wang et al., 2004).In recent years, motivated by the success of deep learning, especially in object detection and semantic segmentation, many deep learning-based methods(Prasad et al., 2020;Raja et al., 2020a;Schreiber et al., 2017)have been proposed and proven to be powerful models for table structure recognition.S. Schreiber et al.(Schreiber et al., 2017)proposed a two-fold system named DeepDeSRT that applies Faster RCNN(Ren et al., 2015)and FCN(Long et al., 2015)for both table detection and row/column segmentation.Sachin et al.(Raja et al., 2020a)proposed a table structure recognizer named TabStruct-Net that predicts the aligned cell regions and the localized cell relations in a joint manner.",
      "sentences": [
          {
              "text": "In summary, the main contributions of this paper are as follows: • The rest of this paper is organized as follows.",
              "label": 0
          },
          {
              "text": "In Sec.2, we give a brief overview of the related works.",
              "label": 0
          },
          {
              "text": "We introduce the overview of the proposed model in Sec. 3. Sec. 4 describes the overview of the WikiTableSet dataset.",
              "label": 0
          },
          {
              "text": "In Sec. 5, we report the experimental details and results.",
              "label": 0
          },
          {
              "text": "Finally, we draw conclusions in Sec. 6.García et al., 2020;Huang et al., 2019)and table recognition(Deng et al., 2019;Ye et al., 2021;Zhong et al., 2020), which we will briefly survey here.",
              "label": 0
          },
          {
              "text": "Most of the previous methods for table recognition are based on two-step approaches dividing the problem into two steps.",
              "label": 0
          },
          {
              "text": "The first step is table structure recognition, which recognizes the table structure (including cell location) from a table image.The second step is cell content recognition that predicts the text content of each table cell from its location.",
              "label": 0
          },
          {
              "text": "Due to the simplicity of the cell content recognition, which can be easily solved by the standard OCR model(Lu et al., 2021;Ly et al., 2021;Shi et al., 2017), many researchers only focused on the table structure recognition problem(Prasad et al., 2020;Raja et al., 2020a;Schreiber et al., 2017).",
              "label": 0
          },
          {
              "text": "Early works of table structure recognition are based on handcrafted features and heuristic rules and mainly applied to simple table structure or pre-defined table structure formats(Itonori, 1993;Kieninger, 1998;Wang et al., 2004).",
              "label": 0
          },
          {
              "text": "In recent years, motivated by the success of deep learning, especially in object detection and semantic segmentation, many deep learning-based methods(Prasad et al., 2020;Raja et al., 2020a;Schreiber et al., 2017)have been proposed and proven to be powerful models for table structure recognition.S. Schreiber et al.(Schreiber et al., 2017)proposed a two-fold system named DeepDeSRT that applies Faster RCNN(Ren et al., 2015)and FCN(Long et al., 2015)for both table detection and row/column segmentation.",
              "label": 0
          },
          {
              "text": "Sachin et al.(Raja et al., 2020a)proposed a table structure recognizer named TabStruct-Net that predicts the aligned cell regions and the localized cell relations in a joint manner.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Rethinking Image_based Table Recognition Using Weakly Supervised Methods",
      "section": 6,
      "paragraph_id": 6,
      "full_text": "To evaluate the performance of the proposed model, we employ the Tree-Edit-Distance-Based Similarity (TEDS) metric(Zhong et al., 2020).It represents the prediction and the ground-truth as a tree structure of HTML tags, and is calculated as follows: where   and   represent tables in a tree structured HTML format, EditDist denotes the tree-edit distance, and || represents the number of nodes in T.",
      "sentences": [
          {
              "text": "To evaluate the performance of the proposed model, we employ the Tree-Edit-Distance-Based Similarity (TEDS) metric(Zhong et al., 2020).",
              "label": 0
          },
          {
              "text": "It represents the prediction and the ground-truth as a tree structure of HTML tags, and is calculated as follows: where   and   represent tables in a tree structured HTML format, EditDist denotes the tree-edit distance, and || represents the number of nodes in T.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Rethinking Image_based Table Recognition Using Weakly Supervised Methods",
      "section": 7,
      "paragraph_id": 7,
      "full_text": "We denote TEDS-struc.as the TEDS score between two tables when considering only the table structure information.",
      "sentences": [
          {
              "text": "as the TEDS score between two tables when considering only the table structure information.",
              "label": 0
          }
      ]
    },
    {
      "paper_name": "Simple Dataset for Proof Method Recommendation in Isabelle_HOL _",
      "section": 0,
      "paragraph_id": 0,
      "full_text": "Abstract: Recently, a growing number of researchers have applied machine learning to assist users of interactive theorem provers.However, the expressive nature of underlying logics and esoteric structures of proof documents impede machine learning practitioners, who often do not have much expertise in formal logic, let alone Isabelle/HOL, from achieving a large scale success in this field.In this data description, we present a simple dataset that contains data on over 400k proof method applications along with over 100 extracted features for each in a format that can be processed easily without any knowledge about formal logic.Our simple data format allows machine learning practitioners to try machine learning tools to predict proof methods in Isabelle/HOL without requiring domain expertise in logic.",
      "sentences": [
          {
              "text": "Abstract: Recently, a growing number of researchers have applied machine learning to assist users of interactive theorem provers.",
              "label": 0
          },
          {
              "text": "However, the expressive nature of underlying logics and esoteric structures of proof documents impede machine learning practitioners, who often do not have much expertise in formal logic, let alone Isabelle/HOL, from achieving a large scale success in this field.",
              "label": 0
          },
          {
              "text": "In this data description, we present a simple dataset that contains data on over 400k proof method applications along with over 100 extracted features for each in a format that can be processed easily without any knowledge about formal logic.",
              "label": 1
          },
          {
              "text": "Our simple data format allows machine learning practitioners to try machine learning tools to predict proof methods in Isabelle/HOL without requiring domain expertise in logic.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Simple Dataset for Proof Method Recommendation in Isabelle_HOL _",
      "section": 1,
      "paragraph_id": 1,
      "full_text": "As our society relies heavily on software systems, it has become essential to ensure that our software systems are trustworthy.Interactive theorem provers (ITPs), such as Isabelle/HOL[20], allow users to specify desirable functionalities of a system and prove that the corresponding implementation is correct in terms of the specification.",
      "sentences": [
          {
              "text": "As our society relies heavily on software systems, it has become essential to ensure that our software systems are trustworthy.",
              "label": 0
          },
          {
              "text": "Interactive theorem provers (ITPs), such as Isabelle/HOL[20], allow users to specify desirable functionalities of a system and prove that the corresponding implementation is correct in terms of the specification.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Simple Dataset for Proof Method Recommendation in Isabelle_HOL _",
      "section": 2,
      "paragraph_id": 2,
      "full_text": "A crucial step in developing proof documents in ITPs is to choose the right tool for a proof goal at hand.Isabelle/HOL, for example, comes with more than 100 proof methods.Proof methods are sub-tools inside Isabelle/HOL.Some of these are general purpose methods, such as auto and simp.Others are special purpose methods, such as intro_classes and intro_locales.The Isabelle community provides various documentations[20]and on-line supports to help new Isabelle users learn when to use which proof methods.",
      "sentences": [
          {
              "text": "A crucial step in developing proof documents in ITPs is to choose the right tool for a proof goal at hand.",
              "label": 0
          },
          {
              "text": "Isabelle/HOL, for example, comes with more than 100 proof methods.",
              "label": 0
          },
          {
              "text": "Proof methods are sub-tools inside Isabelle/HOL.",
              "label": 0
          },
          {
              "text": "Some of these are general purpose methods, such as auto and simp.",
              "label": 0
          },
          {
              "text": "Others are special purpose methods, such as intro_classes and intro_locales.",
              "label": 0
          },
          {
              "text": "The Isabelle community provides various documentations[20]and on-line supports to help new Isabelle users learn when to use which proof methods.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Simple Dataset for Proof Method Recommendation in Isabelle_HOL _",
      "section": 3,
      "paragraph_id": 3,
      "full_text": "Previously, we developed PaMpeR[17], a proof method recommendation tool for Isabelle/HOL.Given a proof goal specified in a proof context, PaMpeR recommends a list of proof methods likely to be suitable for the goal.PaMpeR learns which proof method to recommend to what kind of proof goal from proof documents in Isabelle's standard library and the Archive of Formal Proofs[10].",
      "sentences": [
          {
              "text": "Previously, we developed PaMpeR[17], a proof method recommendation tool for Isabelle/HOL.",
              "label": 0
          },
          {
              "text": "Given a proof goal specified in a proof context, PaMpeR recommends a list of proof methods likely to be suitable for the goal.PaMpeR learns which proof method to recommend to what kind of proof goal from proof documents in Isabelle's standard library and the Archive of Formal Proofs[10].",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Simple Dataset for Proof Method Recommendation in Isabelle_HOL _",
      "section": 4,
      "paragraph_id": 4,
      "full_text": "The key component of PaMpeR is its elaborate feature extractor.Instead of applying machine learning algorithms to Isabelle's proof documents directly, PaMpeR first applies 113 assertions to the pair of a proof goal and its underlying context.Each assertion checks a certain property about the pair and returns a boolean value.Some assertions check if a proof goal involves certain constants or types defined in the standard library.Others check the meta-data of constants and types appearing in a goal.For example, one assertion checks if the goal has a term of a type defined with the codatatype keyword.",
      "sentences": [
          {
              "text": "The key component of PaMpeR is its elaborate feature extractor.",
              "label": 0
          },
          {
              "text": "Instead of applying machine learning algorithms to Isabelle's proof documents directly, PaMpeR first applies 113 assertions to the pair of a proof goal and its underlying context.",
              "label": 0
          },
          {
              "text": "Each assertion checks a certain property about the pair and returns a boolean value.",
              "label": 0
          },
          {
              "text": "Some assertions check if a proof goal involves certain constants or types defined in the standard library.",
              "label": 0
          },
          {
              "text": "Others check the meta-data of constants and types appearing in a goal.For example, one assertion checks if the goal has a term of a type defined with the codatatype keyword.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Simple Dataset for Proof Method Recommendation in Isabelle_HOL _",
      "section": 5,
      "paragraph_id": 5,
      "full_text": "When developing PaMpeR, we applied these 113 assertions to the proof method invocations appearing in the proof documents and constructed a dataset consisting of 425,334 unique data points.",
      "sentences": [
          {
              "text": "When developing PaMpeR, we applied these 113 assertions to the proof method invocations appearing in the proof documents and constructed a dataset consisting of 425,334 unique data points.",
              "label": 1
          }
      ]
  },
  {
      "paper_name": "Simple Dataset for Proof Method Recommendation in Isabelle_HOL _",
      "section": 6,
      "paragraph_id": 6,
      "full_text": "Note that this number is strictly smaller than all the available proof method invocations in Isabelle2020 and the Archive of Formal Proofs in May 2020, from which we can find more than 900k proof method invocations.One obvious reason for this gap is the ever growing size of the available proof documents.The other reason is that we are intentionally ignoring compound proof methods while producing data points.We decided to ignore them because they may pollute the database by introducing proof method invocations that are eventually backtracked by Isabelle.Such backtracking compound methods may reduce the size of proof documents at the cost of introducing backtracked proof steps, which are not necessary to complete proofs.Since we are trying to recommend proof methods appropriate to complete a proof search, we should not include data points produced by such backtracked steps.",
      "sentences": [
          {
              "text": "Note that this number is strictly smaller than all the available proof method invocations in Isabelle2020 and the Archive of Formal Proofs in May 2020, from which we can find more than 900k proof method invocations.",
              "label": 0
          },
          {
              "text": "One obvious reason for this gap is the ever growing size of the available proof documents.",
              "label": 0
          },
          {
              "text": "The other reason is that we are intentionally ignoring compound proof methods while producing data points.",
              "label": 0
          },
          {
              "text": "We decided to ignore them because they may pollute the database by introducing proof method invocations that are eventually backtracked by Isabelle.",
              "label": 0
          },
          {
              "text": "Such backtracking compound methods may reduce the size of proof documents at the cost of introducing backtracked proof steps, which are not necessary to complete proofs.",
              "label": 0
          },
          {
              "text": "Since we are trying to recommend proof methods appropriate to complete a proof search, we should not include data points produced by such backtracked steps.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Simple Dataset for Proof Method Recommendation in Isabelle_HOL _",
      "section": 7,
      "paragraph_id": 7,
      "full_text": "We trained PaMpeR by constructing regression trees[3]from this dataset.Even though our tree construction is based on a fixed height and we did not take advantage of modern development of machine learning research, our cross evaluation showed PaMpeR can correctly predict experts' choice of proof methods for many cases.However, decision tree construction based on a fixed height is an old technique that tends to cause overfitting and underfitting.We expect that one can achieve better performance by applying other algorithms to this dataset.",
      "sentences": [
          {
              "text": "We trained PaMpeR by constructing regression trees[3]from this dataset.",
              "label": 0
          },
          {
              "text": "Even though our tree construction is based on a fixed height and we did not take advantage of modern development of machine learning research, our cross evaluation showed PaMpeR can correctly predict experts' choice of proof methods for many cases.",
              "label": 0
          },
          {
              "text": "However, decision tree construction based on a fixed height is an old technique that tends to cause overfitting and underfitting.",
              "label": 0
          },
          {
              "text": "We expect that one can achieve better performance by applying other algorithms to this dataset.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Simple Dataset for Proof Method Recommendation in Isabelle_HOL _",
      "section": 8,
      "paragraph_id": 8,
      "full_text": "In the following we present the simple dataset we used to train PaMpeR.Our aim is to provide a dataset that is publicly available at Zenodo[15]and easily usable for machine learning practitioners without backgrounds in theorem proving, so that they can exploit the latest development of machine learning research without being hampered by technicalities of theorem proving.",
      "sentences": [
          {
              "text": "In the following we present the simple dataset we used to train PaMpeR.",
              "label": 1
          },
          {
              "text": "Our aim is to provide a dataset that is publicly available at Zenodo[15]and easily usable for machine learning practitioners without backgrounds in theorem proving, so that they can exploit the latest development of machine learning research without being hampered by technicalities of theorem proving.",
              "label": 1
          }
      ]
  },
  {
      "paper_name": "Simple Dataset for Proof Method Recommendation in Isabelle_HOL _",
      "section": 9,
      "paragraph_id": 9,
      "full_text": "Each data point in the dataset consists of the following three entries: the location of a proof method invocation, -the name of the proof method used there, -an array of 0s and 1s expressing the proof goal and its context.The following is an example data point:",
      "sentences": [
          {
              "text": "Each data point in the dataset consists of the following three entries: the location of a proof method invocation, -the name of the proof method used there, -an array of 0s and 1s expressing the proof goal and its context.",
              "label": 1
          }
      ]
  },
  {
      "paper_name": "Simple Dataset for Proof Method Recommendation in Isabelle_HOL _",
      "section": 10,
      "paragraph_id": 10,
      "full_text": "Functors.thy119simp 1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,...This data point describes that in the theory file named Functors.thy,a proof author applied the simp method in line 119 to a proof goal represented by the sequence of 1s and 0s where 1 indicates the corresponding assertion returns true while 0 indicates the otherwise.",
      "sentences": [
          {
              "text": "Functors.thy119simp 1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,...This data point describes that in the theory file named Functors.thy,a proof author applied the simp method in line 119 to a proof goal represented by the sequence of 1s and 0s where 1 indicates the corresponding assertion returns true while 0 indicates the otherwise.",
              "label": 1
          }
      ]
  },
  {
      "paper_name": "Simple Dataset for Proof Method Recommendation in Isabelle_HOL _",
      "section": 11,
      "paragraph_id": 11,
      "full_text": "This dataset has important characteristics worth mentioning.Firstly, this dataset is heavily imbalanced in terms of occurrences of proof methods.Some general purpose methods, such as auto and simp, appear far more often than other lesser known methods: each of auto and simp accounts more than 25% of all proof method invocations in the dataset, whereas no proof methods account for more than 1% of invocations except for the 15 most popular methods.",
      "sentences": [
          {
              "text": "This dataset has important characteristics worth mentioning.",
              "label": 1
          },
          {
              "text": "Firstly, this dataset is heavily imbalanced in terms of occurrences of proof methods.",
              "label": 1
          },
          {
              "text": "Some general purpose methods, such as auto and simp, appear far more often than other lesser known methods: each of auto and simp accounts more than 25% of all proof method invocations in the dataset, whereas no proof methods account for more than 1% of invocations except for the 15 most popular methods.",
              "label": 1
          }
      ]
  },
  {
      "paper_name": "Simple Dataset for Proof Method Recommendation in Isabelle_HOL _",
      "section": 12,
      "paragraph_id": 12,
      "full_text": "Secondly, this dataset only serves to learn what proof methods to apply, but it does not describe how to apply a proof method.None of our 113 assertions examines arguments passed to proof methods.For some proof methods, notably the induct method, the choice of arguments is the hardest problem to tackle, whereas some methods rarely take arguments at all.We hope that users can learn what arguments to pass to proof methods from the use case of these methods in existing proof documents once they learn which methods to apply to their goal.",
      "sentences": [
          {
              "text": "Secondly, this dataset only serves to learn what proof methods to apply, but it does not describe how to apply a proof method.",
              "label": 1
          },
          {
              "text": "None of our 113 assertions examines arguments passed to proof methods.",
              "label": 1
          },
          {
              "text": "For some proof methods, notably the induct method, the choice of arguments is the hardest problem to tackle, whereas some methods rarely take arguments at all.",
              "label": 1
          }
      ]
  },
  {
      "paper_name": "Simple Dataset for Proof Method Recommendation in Isabelle_HOL _",
      "section": 13,
      "paragraph_id": 13,
      "full_text": "Thirdly, it is certainly possible that PaMpeR's feature extractor misses out certain information essential to accurately recommend some methods.This dataset was not built to preserve the information in the original proof documents: we built the dataset, so that we can effectively apply machine learning algorithms to produce recommendations.",
      "sentences": [
          {
              "text": "Thirdly, it is certainly possible that PaMpeR's feature extractor misses out certain information essential to accurately recommend some methods.",
              "label": 1
          },
          {
              "text": "This dataset was not built to preserve the information in the original proof documents: we built the dataset, so that we can effectively apply machine learning algorithms to produce recommendations.",
              "label": 1
          }
      ]
  },
  {
      "paper_name": "Simple Dataset for Proof Method Recommendation in Isabelle_HOL _",
      "section": 14,
      "paragraph_id": 14,
      "full_text": "Finally, this dataset shows only one way to prove a given goal, ignoring alternative possible approaches to prove the same goal.Consider the following goal: \"True ∨ False\".Both auto or simp can prove this goal equally well; however, if this goal appeared in our dataset our dataset would show only the choice of the proof author, say auto, ignoring alternative proofs, say simp.",
      "sentences": [
          {
              "text": "Finally, this dataset shows only one way to prove a given goal, ignoring alternative possible approaches to prove the same goal.Consider the following goal: \"True ∨ False\".",
              "label": 1
          },
          {
              "text": "Both auto or simp can prove this goal equally well; however, if this goal appeared in our dataset our dataset would show only the choice of the proof author, say auto, ignoring alternative proofs, say simp.",
              "label": 1
          }
      ]
  },
  {
      "paper_name": "Simple Dataset for Proof Method Recommendation in Isabelle_HOL _",
      "section": 15,
      "paragraph_id": 15,
      "full_text": "One might guess that we could build a larger dataset that also includes alternative proofs by trying to complete a proof using various methods, thus converting this problem into a multi-label problem.That approach would suffer from two problems.Firstly, there are infinitely many ways to apply methods since we often have to apply multiple proof methods in a sequence to prove a conjecture.Secondly, some combinations of methods are not appropriate even though they can finish a proof in Isabelle.For example, the following is an alternative proof for the aforementioned proposition: lemma \"True ∨ False\" apply(rule disjI1) apply auto done This is a valid proof script, with which Isabelle can check the correctness of the conjecture; however, the application of the rule method is hardly appropriate since the subsequent application of the auto method can discharge the proof without the preceding rule.For these reasons we take the proof methods chosen by human proof authors as the correct choice while ignoring other possibilities.",
      "sentences": [
          {
              "text": "One might guess that we could build a larger dataset that also includes alternative proofs by trying to complete a proof using various methods, thus converting this problem into a multi-label problem.",
              "label": 0
          },
          {
              "text": "That approach would suffer from two problems.",
              "label": 0
          },
          {
              "text": "Firstly, there are infinitely many ways to apply methods since we often have to apply multiple proof methods in a sequence to prove a conjecture.",
              "label": 0
          },
          {
              "text": "Secondly, some combinations of methods are not appropriate even though they can finish a proof in Isabelle.",
              "label": 0
          },
          {
              "text": "For example, the following is an alternative proof for the aforementioned proposition: lemma \"True ∨ False\" apply(rule disjI1) apply auto done This is a valid proof script, with which Isabelle can check the correctness of the conjecture; however, the application of the rule method is hardly appropriate since the subsequent application of the auto method can discharge the proof without the preceding rule.",
              "label": 0
          },
          {
              "text": "For these reasons we take the proof methods chosen by human proof authors as the correct choice while ignoring other possibilities.",
              "label": 0
          }
      ]
     },
     {
      "paper_name": "Syn2Real_ Forgery Classification via Unsupervised Domain Adaptation",
      "section": 0,
      "paragraph_id": 0,
      "full_text": "Abstract: In recent years, image manipulation is becoming increasingly more accessible, yielding more natural-looking images, owing to the modern tools in image processing and computer vision techniques.The task of the identification of forged images has become very challenging.Amongst different types of forgeries, the cases of Copy-Move forgery are increasing manifold, due to the difficulties involved to detect this tampering.To tackle such problems, publicly available datasets are insufficient.In this paper, we propose to create a synthetic forged dataset using deep semantic image inpainting and copy-move forgery algorithm.However, models trained on these datasets have a significant drop in performance when tested on more realistic data.To alleviate this problem, we use unsupervised domain adaptation networks to detect copy-move forgery in new domains by mapping the feature space from our synthetically generated dataset.Furthermore, we improvised the F1 score on CA-SIA and CoMoFoD dataset to 80.3% and 78.8%, respectively.Our approach can be helpful in those cases where the classification of data is unavailable.",
      "sentences": [
          {
              "text": "Abstract: In recent years, image manipulation is becoming increasingly more accessible, yielding more natural-looking images, owing to the modern tools in image processing and computer vision techniques.",
              "label": 0
          },
          {
              "text": "The task of the identification of forged images has become very challenging.",
              "label": 0
          },
          {
              "text": "Amongst different types of forgeries, the cases of Copy-Move forgery are increasing manifold, due to the difficulties involved to detect this tampering.",
              "label": 0
          },
          {
              "text": "To tackle such problems, publicly available datasets are insufficient.",
              "label": 0
          },
          {
              "text": "In this paper, we propose to create a synthetic forged dataset using deep semantic image inpainting and copy-move forgery algorithm.",
              "label": 1
          },
          {
              "text": "However, models trained on these datasets have a significant drop in performance when tested on more realistic data.",
              "label": 0
          },
          {
              "text": "To alleviate this problem, we use unsupervised domain adaptation networks to detect copy-move forgery in new domains by mapping the feature space from our synthetically generated dataset.",
              "label": 0
          },
          {
              "text": "Furthermore, we improvised the F1 score on CA-SIA and CoMoFoD dataset to 80.3% and 78.8%, respectively.",
              "label": 0
          },
          {
              "text": "Our approach can be helpful in those cases where the classification of data is unavailable.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Syn2Real_ Forgery Classification via Unsupervised Domain Adaptation",
      "section": 1,
      "paragraph_id": 1,
      "full_text": "With the advancement of new image editing technologies, there is a sharp increase in the number of forgery cases.While sophisticated image editing tools are meant to enhance the quality of images, they are misused to create forged images for nefarious purposes.These images look so natural that it is difficult to tell with naked eyes whether they have been tampered or are they authentic.It has led to a rise in the cases of image forgery in several fields -medical imaging, industrial photography, surveillance system, criminal, and forensic investigation.[10]There are diverse ways of forging images, of which Copy-Move, Splicing, Retouching, and Resampling forgeries are the most common ones.Copy-Move Forgery (CMF) is a type of passive image forgery technique in which a section of an image is copied and pasted within the same image.Many post-image processing operations such as rescaling, affine transformations, resizing, and blurring are applied to the copied region.As the source and target image remains the same, the photometric characteristics of the image remain mostly invariable.Thus, the detection becomes even more difficult.For instance, in contrast to CMF, splicing forgery is a composition of two images.A section is cut from an image and pasted on another image.As a result, there is an edge discrepancy that makes the detection of splicing forgery relatively easier.",
      "sentences": [
          {
              "text": "With the advancement of new image editing technologies, there is a sharp increase in the number of forgery cases.",
              "label": 0
          },
          {
              "text": "While sophisticated image editing tools are meant to enhance the quality of images, they are misused to create forged images for nefarious purposes.",
              "label": 0
          },
          {
              "text": "These images look so natural that it is difficult to tell with naked eyes whether they have been tampered or are they authentic.",
              "label": 0
          },
          {
              "text": "[10]There are diverse ways of forging images, of which Copy-Move, Splicing, Retouching, and Resampling forgeries are the most common ones.",
              "label": 0
          },
          {
              "text": "Copy-Move Forgery (CMF) is a type of passive image forgery technique in which a section of an image is copied and pasted within the same image.",
              "label": 0
          },
          {
              "text": "Many post-image processing operations such as rescaling, affine transformations, resizing, and blurring are applied to the copied region.",
              "label": 0
          },
          {
              "text": "As the source and target image remains the same, the photometric characteristics of the image remain mostly invariable.",
              "label": 0
          },
          {
              "text": "Thus, the detection becomes even more difficult.",
              "label": 0
          },
          {
              "text": "For instance, in contrast to CMF, splicing forgery is a composition of two images.",
              "label": 0
          },
          {
              "text": "A section is cut from an image and pasted on another image.",
              "label": 0
          },
          {
              "text": "As a result, there is an edge discrepancy that makes the detection of splicing forgery relatively easier.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Syn2Real_ Forgery Classification via Unsupervised Domain Adaptation",
      "section": 2,
      "paragraph_id": 2,
      "full_text": "Image tampering can have significant effects in various domains.For instance, in medical imaging, the images are procured with the utmost sensitivity and is a tiresome process.There can be ulterior economical motives for tampering these confidential and sophisticated images.Consequently, it could misguide the patients about their illnesses and injuries.In the field of education, students can tamper their documents with online available software tools.The significant impact of image tampering can happen in the socio-political area, as manipulated images can affect the perception of a large group of people.Many magazines and newspaper editors tamper the images in such a way that they can change the semantic meaning of the image.",
      "sentences": [
          {
              "text": "Image tampering can have significant effects in various domains.",
              "label": 0
          },
          {
              "text": "For instance, in medical imaging, the images are procured with the utmost sensitivity and is a tiresome process.",
              "label": 0
          },
          {
              "text": "There can be ulterior economical motives for tampering these confidential and sophisticated images.",
              "label": 0
          },
          {
              "text": "Consequently, it could misguide the patients about their illnesses and injuries.",
              "label": 0
          },
          {
              "text": "In the field of education, students can tamper their documents with online available software tools.",
              "label": 0
          },
          {
              "text": "The significant impact of image tampering can happen in the socio-political area, as manipulated images can affect the perception of a large group of people.",
              "label": 0
          },
          {
              "text": "Many magazines and newspaper editors tamper the images in such a way that they can change the semantic meaning of the image.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Syn2Real_ Forgery Classification via Unsupervised Domain Adaptation",
      "section": 3,
      "paragraph_id": 3,
      "full_text": "There have been several traditional approaches for forgery detection that include mostly block-based and keypoint feature extraction[7,16,21]and matching procedures.Nowadays, deep learning approaches[20,1,17]have been proposed to counterattack the problem of image forgery.However, most of the approaches are based on supervised learning.When there are a lot of labeled examples, then it is easy to train the model via supervised learning.To counter the problems of training data, we generally surrogate the training data by including the dataset from adjacent modality or use synthetic imagery.When the same model is evaluated on these datasets, it results in a significant drop in the performance.It happens due to the shift in style, content, or appearance distribution between various datasets.In these cases, domain adaptation is needed to learn the distribution shift.",
      "sentences": [
          {
              "text": "There have been several traditional approaches for forgery detection that include mostly block-based and keypoint feature extraction[7,16,21]and matching procedures.",
              "label": 0
          },
          {
              "text": "Nowadays, deep learning approaches[20,1,17]have been proposed to counterattack the problem of image forgery.",
              "label": 0
          },
          {
              "text": "However, most of the approaches are based on supervised learning.",
              "label": 0
          },
          {
              "text": "When there are a lot of labeled examples, then it is easy to train the model via supervised learning.",
              "label": 0
          },
          {
              "text": "To counter the problems of training data, we generally surrogate the training data by including the dataset from adjacent modality or use synthetic imagery.",
              "label": 0
          },
          {
              "text": "When the same model is evaluated on these datasets, it results in a significant drop in the performance.",
              "label": 0
          },
          {
              "text": "It happens due to the shift in style, content, or appearance distribution between various datasets.",
              "label": 0
          },
          {
              "text": "In these cases, domain adaptation is needed to learn the distribution shift.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Syn2Real_ Forgery Classification via Unsupervised Domain Adaptation",
      "section": 4,
      "paragraph_id": 4,
      "full_text": "In this work, we show that manipulations in images across different domains can be detected via domain adaptation.We leverage the power of Convolutional Neural Figure1.The first and second column shows the example of target domain dataset (CASIA and CoMoFoD respectively).Subsequent column shows the generated synthetic data from COCO dataset using semantic inpainting and copy-move forgery algorithm.First row is authentic image of each category and second row is forged image.networks (CNNs) to perceive the distinguishable features of authentic and tampered images.We tackle the problem of performance drop by incorporating the feature space alignment between our synthetic generated datasets and datasets that are publicly available.We generate the synthetic dataset using Edge-connect semantic inpainting and CMF algorithm.",
      "sentences": [
          {
              "text": "In this work, we show that manipulations in images across different domains can be detected via domain adaptation.",
              "label": 0
          },
          {
              "text": "We leverage the power of Convolutional Neural Figure1.",
              "label": 0
          },
          {
              "text": "The first and second column shows the example of target domain dataset (CASIA and CoMoFoD respectively).",
              "label": 0
          },
          {
              "text": "Subsequent column shows the generated synthetic data from COCO dataset using semantic inpainting and copy-move forgery algorithm.",
              "label": 1
          },
          {
              "text": "First row is authentic image of each category and second row is forged image.networks (CNNs) to perceive the distinguishable features of authentic and tampered images.",
              "label": 1
          },
          {
              "text": "We tackle the problem of performance drop by incorporating the feature space alignment between our synthetic generated datasets and datasets that are publicly available.",
              "label": 1
          },
          {
              "text": "We generate the synthetic dataset using Edge-connect semantic inpainting and CMF algorithm.",
              "label": 1
          }
      ]
  },
  {
      "paper_name": "Syn2Real_ Forgery Classification via Unsupervised Domain Adaptation",
      "section": 5,
      "paragraph_id": 5,
      "full_text": "Contributions Our main contributions in the paper are summarized as follows: 1) The primary task is to classify images as forged or authentic, for which we employ Unsupervised Domain Adaptation (DA), due to the difference in content and style between our source and target dataset, 2) As the publicly available datasets are small, we generate a new dataset comprising of 80,000 images using deep semantic inpainting and copy-move forgery algorithms on COCO[6]dataset, and 3) We explore two Unsupervised DA methods to adapt the features from source dataset to target dataset, such that the variation between the domains is minimized.",
      "sentences": [
          {
              "text": "Contributions Our main contributions in the paper are summarized as follows: 1) The primary task is to classify images as forged or authentic, for which we employ Unsupervised Domain Adaptation (DA), due to the difference in content and style between our source and target dataset, 2) As the publicly available datasets are small, we generate a new dataset comprising of 80,000 images using deep semantic inpainting and copy-move forgery algorithms on COCO[6]dataset, and 3) We explore two Unsupervised DA methods to adapt the features from source dataset to target dataset, such that the variation between the domains is minimized.",
              "label": 1
          }
      ]
  },
  {
      "paper_name": "Syn2Real_ Forgery Classification via Unsupervised Domain Adaptation",
      "section": 6,
      "paragraph_id": 6,
      "full_text": "The paper is organized as follows: Section II describes the traditional and deep learning solutions that evolved over the years for forgery classification and a review of domain adaptation methods.Section III describes our methodology in detail that involves dataset generation, Unsupervised DA, and final architectures used for training.After that, in Section IV, we evaluate the performance of our architecture on CASIA[2]and CoMoFoD[13]dataset.Section V discusses the conclusion and future directions of our work.",
      "sentences": [
          {
              "text": "The paper is organized as follows: Section II describes the traditional and deep learning solutions that evolved over the years for forgery classification and a review of domain adaptation methods.",
              "label": 0
          },
          {
              "text": "Section III describes our methodology in detail that involves dataset generation, Unsupervised DA, and final architectures used for training.",
              "label": 1
          },
          {
              "text": "After that, in Section IV, we evaluate the performance of our architecture on CASIA[2]and CoMoFoD[13]dataset.",
              "label": 0
          },
          {
              "text": "Section V discusses the conclusion and future directions of our work.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Syn2Real_ Forgery Classification via Unsupervised Domain Adaptation",
      "section": 7,
      "paragraph_id": 7,
      "full_text": "We applied two methods to generate the dataset.The inclusion of any one of them shows an increase in performance.Semantic Inpainting helps the model to learn edge discrepancies when the objects are removed.Copy-Move tampered images improve the focus of the network to recognize similar patches.",
      "sentences": [
          {
              "text": "We applied two methods to generate the dataset.",
              "label": 1
          },
          {
              "text": "The inclusion of any one of them shows an increase in performance.",
              "label": 1
          },
          {
              "text": "Semantic Inpainting helps the model to learn edge discrepancies when the objects are removed.",
              "label": 1
          },
          {
              "text": "Copy-Move tampered images improve the focus of the network to recognize similar patches.",
              "label": 1
          }
      ]
  },
  {
      "paper_name": "Syn2Real_ Forgery Classification via Unsupervised Domain Adaptation",
      "section": 8,
      "paragraph_id": 8,
      "full_text": "We evaluated our architecture on CASIA V2 and CoMo-FoD datasets.In our case, the source domain constitutes of COCO CMF and semantic inpainted images, and, target domain comprises CASIA V2 and CoMoFoD datasets.Exhaustive experiments were done using AlexNet[5]and VGG-7[11]for feature extraction.These datasets are explained briefly in the following sections: It contains 12,614 images in total, of which 7,497 are authentic, and 5,123 are forged images.The resolution of images ranges from 240 x 160 to 900 x 600.The tampered images have been applied to post-processing operations and saved in JPEG and TIFF formats.Out of these 5,123 tampered images, 3,274 images are copy-move, and 1,788 are splicing.The number of authentic images presents, respectively, for forged images, are 1,701.Henceforth, our total dataset size comes out to be of 4,975 images.",
      "sentences": [
          {
              "text": "We evaluated our architecture on CASIA V2 and CoMo-FoD datasets.",
              "label": 1
          },
          {
              "text": "In our case, the source domain constitutes of COCO CMF and semantic inpainted images, and, target domain comprises CASIA V2 and CoMoFoD datasets.",
              "label": 1
          },
          {
              "text": "Exhaustive experiments were done using AlexNet[5]and VGG-7[11]for feature extraction.",
              "label": 1
          },
          {
              "text": "These datasets are explained briefly in the following sections: It contains 12,614 images in total, of which 7,497 are authentic, and 5,123 are forged images.",
              "label": 1
          },
          {
              "text": "The resolution of images ranges from 240 x 160 to 900 x 600.",
              "label": 1
          },
          {
              "text": "The tampered images have been applied to post-processing operations and saved in JPEG and TIFF formats.",
              "label": 1
          },
          {
              "text": "Out of these 5,123 tampered images, 3,274 images are copy-move, and 1,788 are splicing.",
              "label": 1
          },
          {
              "text": "The number of authentic images presents, respectively, for forged images, are 1,701.",
              "label": 1
          },
          {
              "text": "Henceforth, our total dataset size comes out to be of 4,975 images.",
              "label": 1
          }
      ]
  },
  {
      "paper_name": "Syn2Real_ Forgery Classification via Unsupervised Domain Adaptation",
      "section": 9,
      "paragraph_id": 9,
      "full_text": "This dataset contains 400 images, 200 authentic, and 200 forged.It contains only copy-move forgery cases in PNG format.The dimension of images in this dataset is 512 x 512.Various distortions such as translation, rotation, and scaling are applied to tampered images.",
      "sentences": [
          {
              "text": "This dataset contains 400 images, 200 authentic, and 200 forged.",
              "label": 1
          },
          {
              "text": "It contains only copy-move forgery cases in PNG format.",
              "label": 1
          },
          {
              "text": "The dimension of images in this dataset is 512 x 512.",
              "label": 1
          },
          {
              "text": "Various distortions such as translation, rotation, and scaling are applied to tampered images.",
              "label": 1
          }
      ]
  },
  {
      "paper_name": "Syn2Real_ Forgery Classification via Unsupervised Domain Adaptation",
      "section": 10,
      "paragraph_id": 10,
      "full_text": "We explored diverse color spaces to get a sense of the behavior of CMF images in different color spaces.Using Alexnet for feature extraction and DANN for domain adaptation, we varied the number of CMF images across RGB and YCrCb color space for the CASIA dataset.Chrominance component of YCrCb illuminates the identical regions in images with the same luminosity.It helps the deep networks to visualize copy-pasted regions in images.",
      "sentences": [
          {
              "text": "We explored diverse color spaces to get a sense of the behavior of CMF images in different color spaces.",
              "label": 0
          },
          {
              "text": "Using Alexnet for feature extraction and DANN for domain adaptation, we varied the number of CMF images across RGB and YCrCb color space for the CASIA dataset.",
              "label": 0
          },
          {
              "text": "Chrominance component of YCrCb illuminates the identical regions in images with the same luminosity.",
              "label": 0
          },
          {
              "text": "It helps the deep networks to visualize copy-pasted regions in images.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Syn2Real_ Forgery Classification via Unsupervised Domain Adaptation",
      "section": 11,
      "paragraph_id": 11,
      "full_text": "In DANN, we used categorical cross-entropy as loss function and Adam optimizer with learning rate 0.001.The DDC network is trained using Stochastic Gradient Descent optimizer, with a momentum value of 0.9, and learning rate value of 0.0001.At the time of training, we initially used only CMF images for unsupervised domain adaptation.Then, we included semantic inpainted images to study the effects of edge discrepancy in recognizing forged images.There are no labels used at the time of training.For the target domain, images are passed with a domain label attached to it, and the source domain has a class label also assigned to it.The source model adapts the weights to classify target images with the same features into a particular category.",
      "sentences": [
          {
              "text": "In DANN, we used categorical cross-entropy as loss function and Adam optimizer with learning rate 0.001.",
              "label": 0
          },
          {
              "text": "The DDC network is trained using Stochastic Gradient Descent optimizer, with a momentum value of 0.9, and learning rate value of 0.0001.",
              "label": 0
          },
          {
              "text": "At the time of training, we initially used only CMF images for unsupervised domain adaptation.",
              "label": 0
          },
          {
              "text": "Then, we included semantic inpainted images to study the effects of edge discrepancy in recognizing forged images.",
              "label": 0
          },
          {
              "text": "There are no labels used at the time of training.",
              "label": 0
          },
          {
              "text": "For the target domain, images are passed with a domain label attached to it, and the source domain has a class label also assigned to it.",
              "label": 0
          },
          {
              "text": "The source model adapts the weights to classify target images with the same features into a particular category.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Syn2Real_ Forgery Classification via Unsupervised Domain Adaptation",
      "section": 12,
      "paragraph_id": 12,
      "full_text": "During testing, the target images are passed through the source classifier model, whose weights are now adapted to features specific to target data.We divided the CASIA dataset into an 80:20 ratio.80% of the data used for training, and then, 20% used for testing.As CoMoFoD contains only 200 images, all the images were used to learn the discriminative features, as well as for evaluation.We used classification accuracy, precision, recall, and F1-score as performance metrics to evaluate our architectures.Precision is expressed as the number of true positives divided by the sum of true and false positives.The recall is defined as the ratio of true positives by true positives and false negatives.F1-score is the harmonic mean of recall and precision score.",
      "sentences": [
          {
              "text": "During testing, the target images are passed through the source classifier model, whose weights are now adapted to features specific to target data.",
              "label": 0
          },
          {
              "text": "80% of the data used for training, and then, 20% used for testing.",
              "label": 0
          },
          {
              "text": "As CoMoFoD contains only 200 images, all the images were used to learn the discriminative features, as well as for evaluation.",
              "label": 0
          },
          {
              "text": "We used classification accuracy, precision, recall, and F1-score as performance metrics to evaluate our architectures.",
              "label": 0
          },
          {
              "text": "Precision is expressed as the number of true positives divided by the sum of true and false positives.",
              "label": 0
          },
          {
              "text": "The recall is defined as the ratio of true positives by true positives and false negatives.",
              "label": 0
          },
          {
              "text": "F1-score is the harmonic mean of recall and precision score.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Syn2Real_ Forgery Classification via Unsupervised Domain Adaptation",
      "section": 13,
      "paragraph_id": 13,
      "full_text": "We will now discuss the results summarized in Table1, 2 and 3. We trained our architecture on source dataset and evaluated it on target dataset.From Table As the number of images increased, the results improved for domain adaptation.Due to complex post-processing operations, YCrCb space was unable to localize same tampered regions.As RGB color space performed better, therefore, for our future training of domain adaptation algorithms, we chose RGB images for source and target domains.",
      "sentences": [
          {
              "text": "We will now discuss the results summarized in Table1, 2 and 3.",
              "label": 0
          },
          {
              "text": "We trained our architecture on source dataset and evaluated it on target dataset.",
              "label": 0
          },
          {
              "text": "From Table As the number of images increased, the results improved for domain adaptation.",
              "label": 0
          },
          {
              "text": "Due to complex post-processing operations, YCrCb space was unable to localize same tampered regions.",
              "label": 0
          },
          {
              "text": "As RGB color space performed better, therefore, for our future training of domain adaptation algorithms, we chose RGB images for source and target domains.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Syn2Real_ Forgery Classification via Unsupervised Domain Adaptation",
      "section": 14,
      "paragraph_id": 14,
      "full_text": "In starting, we only used CMF images for unsupervised DA.DANN and DDC were able to minimize the distance between the two datasets distributions, but using only CMF images makes the network biased towards objects resembling the same feature characteristics.To analyze the contribution of the amount of CMF images for domain adaptation, we examined each time with an increment of 10,000 images.We saw that just by increasing CMF images, there are no noteworthy changes in the accuracy and F1-score on the target domain.In cases where the copied and background region are the same, e.g., grass, then the model is unable to distinguish the image as authentic or tampered.To alleviate this problem, we incorporated semantic inpainted images to learn the edge discriminative features.Itmodel to learn the dissimilarities near the edges of the images are copy-pasted.As the target domain contains CMF images, increasing the distribution of semantic images beyond 10,000 images leads to drop in performance.Table2shows the effect of utilizing both semantic inpainted and copy-move tampered images.In contrast to contemporary networks such as Inception[12]and ResNet[4], we used AlexNet and VGG-7 as our base models, because, these networks have a huge number of parameters and due to limited amount of target domain images, the model doesn't generalize well.COCO → CASIA: In CASIA, there are 3979 images used for training purposes and 996 images for evaluation.With DDC, we saw a sudden jump by including semantic inpainted images.Using DANN, we achieved the best score, when the highest number of images were used.As there is a large number of images available at the time of training, we can see from Table3that DANN+VGG-7 achieves the highest recall and F1-score.COCO → CoMoFoD: CoMoFoD dataset is very small.Due to the presence of 200 images only, we trained and evaluated on the whole dataset.With the increase in the number of images in the source domain, accuracy and F1 score decreased in DDC, and, insignificant increase using DANN.As the dataset was small, we can see from Table3that DDC+ MMD with Alexnet as base model performed better compared to VGG-7.VGG-7 has a huge number of parameters that can't be optimized; hence, they performed poorly at the test time.",
      "sentences": [
          {
              "text": "In starting, we only used CMF images for unsupervised DA.",
              "label": 0
          },
          {
              "text": "DANN and DDC were able to minimize the distance between the two datasets distributions, but using only CMF images makes the network biased towards objects resembling the same feature characteristics.",
              "label": 0
          },
          {
              "text": "To analyze the contribution of the amount of CMF images for domain adaptation, we examined each time with an increment of 10,000 images.",
              "label": 0
          },
          {
              "text": "We saw that just by increasing CMF images, there are no noteworthy changes in the accuracy and F1-score on the target domain.",
              "label": 0
          },
          {
              "text": "In cases where the copied and background region are the same, e.g., grass, then the model is unable to distinguish the image as authentic or tampered.",
              "label": 0
          },
          {
              "text": "To alleviate this problem, we incorporated semantic inpainted images to learn the edge discriminative features.",
              "label": 0
          },
          {
              "text": "Itmodel to learn the dissimilarities near the edges of the images are copy-pasted.",
              "label": 0
          },
          {
              "text": "As the target domain contains CMF images, increasing the distribution of semantic images beyond 10,000 images leads to drop in performance.",
              "label": 0
          },
          {
              "text": "Table2shows the effect of utilizing both semantic inpainted and copy-move tampered images.",
              "label": 0
          },
          {
              "text": "In contrast to contemporary networks such as Inception[12]and ResNet[4], we used AlexNet and VGG-7 as our base models, because, these networks have a huge number of parameters and due to limited amount of target domain images, the model doesn't generalize well.",
              "label": 0
          },
          {
              "text": "COCO → CASIA: In CASIA, there are 3979 images used for training purposes and 996 images for evaluation.",
              "label": 0
          },
          {
              "text": "With DDC, we saw a sudden jump by including semantic inpainted images.",
              "label": 0
          },
          {
              "text": "Using DANN, we achieved the best score, when the highest number of images were used.",
              "label": 0
          },
          {
              "text": "As there is a large number of images available at the time of training, we can see from Table3that DANN+VGG-7 achieves the highest recall and F1-score.",
              "label": 0
          },
          {
              "text": "COCO → CoMoFoD: CoMoFoD dataset is very small.",
              "label": 0
          },
          {
              "text": "Due to the presence of 200 images only, we trained and evaluated on the whole dataset.",
              "label": 0
          },
          {
              "text": "With the increase in the number of images in the source domain, accuracy and F1 score decreased in DDC, and, insignificant increase using DANN.",
              "label": 0
          },
          {
              "text": "As the dataset was small, we can see from Table3that DDC+ MMD with Alexnet as base model performed better compared to VGG-7.",
              "label": 0
          },
          {
              "text": "VGG-7 has a huge number of parameters that can't be optimized; hence, they performed poorly at the test time.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Syn2Real_ Forgery Classification via Unsupervised Domain Adaptation",
      "section": 15,
      "paragraph_id": 15,
      "full_text": "To compare with previous work, we analyzed our results with BusterNet architecture.They mainly took into account of CASIA CMF and CoMoFoD dataset.Other works, mainly used all images of CASIA dataset, not explicitly for CMF images.In BusterNet, they created and trained on 1 lakh images for supervised training, and then evaluated on these datasets.In CoMoFoD, they used 200 images as ours, but, in CASIA, they took only 1356 CMFD images into account compared to 4975 of ours.Our approach improves the accuracy by 5-6% in the case of CASIA and 27-28% in the case of CoMoFoD.Table3shows the performance comparison between ours and BusterNet.Whereas Buster-Net has used pixel-wise annotations to learn the class of images, we have not used any label at the time of training.In our case, as the data distribution is too much imbalanced, precision and recall score plays a significant role.We can see that our precision score is not in the comparable range of recall scores.It is due to the reason, as we have less num-ber of positive class images in contrast to the negative class.As we look into the denominator of precision and recall, in the first case, the denominator is the sum of true plus false positives.Now, we have too many images in a false class.It attributes to a large number of false positives.Whereas in the recall, the denominator is the sum of true positives plus false negatives.The false-negative number is less as the number of images in the correct class is fewer.",
      "sentences": [
          {
              "text": "To compare with previous work, we analyzed our results with BusterNet architecture.",
              "label": 0
          },
          {
              "text": "They mainly took into account of CASIA CMF and CoMoFoD dataset.",
              "label": 0
          },
          {
              "text": "Other works, mainly used all images of CASIA dataset, not explicitly for CMF images.",
              "label": 0
          },
          {
              "text": "In BusterNet, they created and trained on 1 lakh images for supervised training, and then evaluated on these datasets.",
              "label": 0
          },
          {
              "text": "In CoMoFoD, they used 200 images as ours, but, in CASIA, they took only 1356 CMFD images into account compared to 4975 of ours.",
              "label": 0
          },
          {
              "text": "Our approach improves the accuracy by 5-6% in the case of CASIA and 27-28% in the case of CoMoFoD.",
              "label": 0
          },
          {
              "text": "Table3shows the performance comparison between ours and BusterNet.",
              "label": 0
          },
          {
              "text": "Whereas Buster-Net has used pixel-wise annotations to learn the class of images, we have not used any label at the time of training.",
              "label": 0
          },
          {
              "text": "In our case, as the data distribution is too much imbalanced, precision and recall score plays a significant role.",
              "label": 0
          },
          {
              "text": "We can see that our precision score is not in the comparable range of recall scores.",
              "label": 0
          },
          {
              "text": "It is due to the reason, as we have less num-ber of positive class images in contrast to the negative class.",
              "label": 0
          },
          {
              "text": "As we look into the denominator of precision and recall, in the first case, the denominator is the sum of true plus false positives.",
              "label": 0
          },
          {
              "text": "Now, we have too many images in a false class.",
              "label": 0
          },
          {
              "text": "It attributes to a large number of false positives.",
              "label": 0
          },
          {
              "text": "Whereas in the recall, the denominator is the sum of true positives plus false negatives.",
              "label": 0
          },
          {
              "text": "The false-negative number is less as the number of images in the correct class is fewer.",
              "label": 0
          }
      ]
     },
     {
      "paper_name": "Topic Shift Detection in Chinese Dialogues_ Corpus and Benchmark",
      "section": 0,
      "paragraph_id": 0,
      "full_text": "Abstract: Dialogue topic shift detection is to detect whether an ongoing topic has shifted or should shift in a dialogue, which can be divided into two categories, i.e., response-known task and response-unknown task.Currently, only a few investigated the latter, because it is still a challenge to predict the topic shift without the response information.In this paper, we first annotate a Chinese Natural Topic Dialogue (CNTD) corpus consisting of 1308 dialogues to fill the gap in the Chinese natural conversation topic corpus.And then we focus on the response-unknown task and propose a teacher-student framework based on hierarchical contrastive learning to predict the topic shift without the response.Specifically, the response at high-level teacher-student is introduced to build the contrastive learning between the response and the context, while the label contrastive learning is constructed at low-level student.The experimental results on our Chinese CNTD and English TIAGE show the effectiveness of our proposed model.",
      "sentences": [
          {
              "text": "Abstract: Dialogue topic shift detection is to detect whether an ongoing topic has shifted or should shift in a dialogue, which can be divided into two categories, i.e., response-known task and response-unknown task.",
              "label": 0
          },
          {
              "text": "Currently, only a few investigated the latter, because it is still a challenge to predict the topic shift without the response information.",
              "label": 0
          },
          {
              "text": "In this paper, we first annotate a Chinese Natural Topic Dialogue (CNTD) corpus consisting of 1308 dialogues to fill the gap in the Chinese natural conversation topic corpus.",
              "label": 1
          },
          {
              "text": "And then we focus on the response-unknown task and propose a teacher-student framework based on hierarchical contrastive learning to predict the topic shift without the response.",
              "label": 0
          },
          {
              "text": "Specifically, the response at high-level teacher-student is introduced to build the contrastive learning between the response and the context, while the label contrastive learning is constructed at low-level student.",
              "label": 0
          },
          {
              "text": "The experimental results on our Chinese CNTD and English TIAGE show the effectiveness of our proposed model.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Topic Shift Detection in Chinese Dialogues_ Corpus and Benchmark",
      "section": 1,
      "paragraph_id": 1,
      "full_text": "Dialogue topic shift detection is to detect whether a dialogue's utterance has shifted in the topic, which can help the dialog system to change the topic and guide the dialogue actively.Although dialog topic shift detection is a new task, it has become a hotspot due to its remarkable benefit to many downstream tasks, such as response generation[1]and reading comprehension[2,3], and can help those real-time applications produce on-topic or topic-shift responses which perform well in dialogue scenarios[4,5,6].",
      "sentences": [
          {
              "text": "Dialogue topic shift detection is to detect whether a dialogue's utterance has shifted in the topic, which can help the dialog system to change the topic and guide the dialogue actively.",
              "label": 0
          },
          {
              "text": "Although dialog topic shift detection is a new task, it has become a hotspot due to its remarkable benefit to many downstream tasks, such as response generation[1]and reading comprehension[2,3], and can help those real-time applications produce on-topic or topic-shift responses which perform well in dialogue scenarios[4,5,6].",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Topic Shift Detection in Chinese Dialogues_ Corpus and Benchmark",
      "section": 2,
      "paragraph_id": 2,
      "full_text": "The task of dialogue topic shift detection can be divided into two lines, i.e., response-known task and response-unknown task, as shown in Fig.1.The former can gain the response information and obtain a better result, while the latter is the opposite.Moreover, both of them are not accessible to future information.This is the biggest difference from the task of text topic segmentation, in which all the basic utterances are visible to each other.That is, those existing topic segmentation models cannot be applied to dialogue topic shift detection since it depends on the response and its subsequent utterances heavily.Therefore, it is more difficult to discern differences between utterances in the task of dialogue topic shift detection.Due to the absence of future utterances, dialogue topic shift detection is still a challenging task.",
      "sentences": [
          {
              "text": "The task of dialogue topic shift detection can be divided into two lines, i.e., response-known task and response-unknown task, as shown in Fig.1.",
              "label": 0
          },
          {
              "text": "The former can gain the response information and obtain a better result, while the latter is the opposite.",
              "label": 0
          },
          {
              "text": "Moreover, both of them are not accessible to future information.",
              "label": 0
          },
          {
              "text": "This is the biggest difference from the task of text topic segmentation, in which all the basic utterances are visible to each other.",
              "label": 0
          },
          {
              "text": "That is, those existing topic segmentation models cannot be applied to dialogue topic shift detection since it depends on the response and its subsequent utterances heavily.",
              "label": 0
          },
          {
              "text": "Therefore, it is more difficult to discern differences between utterances in the task of dialogue topic shift detection.",
              "label": 0
          },
          {
              "text": "Due to the absence of future utterances, dialogue topic shift detection is still a challenging task.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Topic Shift Detection in Chinese Dialogues_ Corpus and Benchmark",
      "section": 3,
      "paragraph_id": 3,
      "full_text": "In this paper, we focus on the response-unknown task of topic shift detection in Chinese dialogues.There are two issues in the response-unknown task of topic shift detection in Chinese dialogues, i.e., lack of annotated corpus in Chinese and how to predict the response.",
      "sentences": [
          {
              "text": "In this paper, we focus on the response-unknown task of topic shift detection in Chinese dialogues.",
              "label": 0
          },
          {
              "text": "There are two issues in the response-unknown task of topic shift detection in Chinese dialogues, i.e., lack of annotated corpus in Chinese and how to predict the response.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Topic Shift Detection in Chinese Dialogues_ Corpus and Benchmark",
      "section": 4,
      "paragraph_id": 4,
      "full_text": "Fig.1.Two lines of dialogue topic shift detection tasks to detect whether it exists topic shift between the utterances ui-1 and ui, where the response-known task (left) can use the response ui, while the response-unknown task (right) can be regarded as topic shift prediction without the response ui.",
      "sentences": [
          {
              "text": "Fig.1.",
              "label": 0
          },
          {
              "text": "Two lines of dialogue topic shift detection tasks to detect whether it exists topic shift between the utterances ui-1 and ui, where the response-known task (left) can use the response ui, while the response-unknown task (right) can be regarded as topic shift prediction without the response ui.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Topic Shift Detection in Chinese Dialogues_ Corpus and Benchmark",
      "section": 5,
      "paragraph_id": 5,
      "full_text": "There are only a few publicly dialogue topic shift corpus available and most of them are provided for the segmentation task, which does not satisfy natural conversation.Xie et al.[7]provided a detailed definition of the dialogue topic shift detection task, and annotated an English dialogue topics corpus TIAGE.Although it can fill the gap in the corpus of English conversation topics, its scale is still too small.In Chinese, Xu et al.[8]annotated a Chinese dialogue topic corpus.However, due to its small size and poor quality, this is detrimental to the further research and development of Chinese dialogue topic shift tasks.To fill the gap in the Chinese natural dialogue topic corpus, we first annotated a Chinese Natural Topic Dialogue (CNTD) corpus which consists of 1308 dialogues with high quality.",
      "sentences": [
          {
              "text": "There are only a few publicly dialogue topic shift corpus available and most of them are provided for the segmentation task, which does not satisfy natural conversation.",
              "label": 0
          },
          {
              "text": "Xie et al.[7]provided a detailed definition of the dialogue topic shift detection task, and annotated an English dialogue topics corpus TIAGE.",
              "label": 0
          },
          {
              "text": "Although it can fill the gap in the corpus of English conversation topics, its scale is still too small.",
              "label": 0
          },
          {
              "text": "In Chinese, Xu et al.[8]annotated a Chinese dialogue topic corpus.",
              "label": 0
          },
          {
              "text": "However, due to its small size and poor quality, this is detrimental to the further research and development of Chinese dialogue topic shift tasks.",
              "label": 0
          },
          {
              "text": "To fill the gap in the Chinese natural dialogue topic corpus, we first annotated a Chinese Natural Topic Dialogue (CNTD) corpus which consists of 1308 dialogues with high quality.",
              "label": 1
          }
      ]
  },
  {
      "paper_name": "Topic Shift Detection in Chinese Dialogues_ Corpus and Benchmark",
      "section": 6,
      "paragraph_id": 6,
      "full_text": "Xie et al.[7]also established a benchmark for this response-unknown task based on the T5 model[9]and this benchmark only used the context to predict topic shift and performed poorly due to the lack of the response information.Thus, it is more challenging to predict the topic shift in natural dialogue without useful response information.",
      "sentences": [
          {
              "text": "Xie et al.[7]also established a benchmark for this response-unknown task based on the T5 model[9]and this benchmark only used the context to predict topic shift and performed poorly due to the lack of the response information.",
              "label": 0
          },
          {
              "text": "Thus, it is more challenging to predict the topic shift in natural dialogue without useful response information.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Topic Shift Detection in Chinese Dialogues_ Corpus and Benchmark",
      "section": 7,
      "paragraph_id": 7,
      "full_text": "The teacher-student framework has been used widely to obtain information that is not available to the model[1].To solve the issue of the lack of response information, we propose a teacher-student framework to introduce the response information.The teacher can obtain the response information, and the student can learn the response information from the teacher through knowledge distillation.To facilitate knowledge transfer, the student mimics the teacher on every layer instead of just the top layer, which alleviates the delayed supervised signal problem using hierarchical semantic information in the teacher[10].",
      "sentences": [
          {
              "text": "The teacher-student framework has been used widely to obtain information that is not available to the model[1].",
              "label": 0
          },
          {
              "text": "To solve the issue of the lack of response information, we propose a teacher-student framework to introduce the response information.",
              "label": 0
          },
          {
              "text": "The teacher can obtain the response information, and the student can learn the response information from the teacher through knowledge distillation.",
              "label": 0
          },
          {
              "text": "To facilitate knowledge transfer, the student mimics the teacher on every layer instead of just the top layer, which alleviates the delayed supervised signal problem using hierarchical semantic information in the teacher[10].",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Topic Shift Detection in Chinese Dialogues_ Corpus and Benchmark",
      "section": 8,
      "paragraph_id": 8,
      "full_text": "Besides, we construct hierarchical contrastive learning in which we consider the teacher-student as high-level and the student as low-level.At high-level, we build an information simulation loss between the context and the response to improve the semantic information of the student model with more reliable predictive information.At low-level, we design a semantic coherence-aware loss to better distinguish the different shift cases and produce more reliable prediction results.",
      "sentences": [
          {
              "text": "Besides, we construct hierarchical contrastive learning in which we consider the teacher-student as high-level and the student as low-level.",
              "label": 0
          },
          {
              "text": "At high-level, we build an information simulation loss between the context and the response to improve the semantic information of the student model with more reliable predictive information.",
              "label": 0
          },
          {
              "text": "At low-level, we design a semantic coherence-aware loss to better distinguish the different shift cases and produce more reliable prediction results.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Topic Shift Detection in Chinese Dialogues_ Corpus and Benchmark",
      "section": 9,
      "paragraph_id": 9,
      "full_text": "Finally, the experimental results on our Chinese CNTD and the English TIAGE show that our proposed model outperforms the baselines.The contributions of this paper are as follows.",
      "sentences": [
          {
              "text": "Finally, the experimental results on our Chinese CNTD and the English TIAGE show that our proposed model outperforms the baselines.",
              "label": 0
          },
          {
              "text": "The contributions of this paper are as follows.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Topic Shift Detection in Chinese Dialogues_ Corpus and Benchmark",
      "section": 10,
      "paragraph_id": 10,
      "full_text": "-We manually annotate a corpus with 1308 dialogues based on NaturalConv to fill the gaps in the Chinese natural dialogues topic corpus.-We propose a teacher-student framework to learn the response information for the topic shift detection task.-We introduce hierarchical contrastive learning to further improve performance.",
      "sentences": [
          {
              "text": "-We introduce hierarchical contrastive learning to further improve performance.",
              "label": 1
          }
      ]
  },
  {
      "paper_name": "Topic Shift Detection in Chinese Dialogues_ Corpus and Benchmark",
      "section": 11,
      "paragraph_id": 11,
      "full_text": "-The experimental results both on the CNTD and TIAGE datasets show that our model outperforms the baselines. 2 Related Work",
      "sentences": [
          {
              "text": "-The experimental results both on the CNTD and TIAGE datasets show that our model outperforms the baselines.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Topic Shift Detection in Chinese Dialogues_ Corpus and Benchmark",
      "section": 12,
      "paragraph_id": 12,
      "full_text": "Previous studies explored the dialogue topic tasks and published the annotated topic dialogue corpus.For English, Xie et al.[7]annotated the TIAGE consisting of 500 dialogues with 7861 turns based on PersonaChat[11].Xu et al.[8]built a dataset including 711 dialogues by joining dialogues from existing multi-turn dialogue datasets: MultiWOZ Corpus[12], and Stanford Dialog Dataset[13].",
      "sentences": [
          {
              "text": "Previous studies explored the dialogue topic tasks and published the annotated topic dialogue corpus.",
              "label": 0
          },
          {
              "text": "For English, Xie et al.[7]annotated the TIAGE consisting of 500 dialogues with 7861 turns based on PersonaChat[11].",
              "label": 0
          },
          {
              "text": "Xu et al.[8]built a dataset including 711 dialogues by joining dialogues from existing multi-turn dialogue datasets: MultiWOZ Corpus[12], and Stanford Dialog Dataset[13].",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Topic Shift Detection in Chinese Dialogues_ Corpus and Benchmark",
      "section": 13,
      "paragraph_id": 13,
      "full_text": "Both corpora are either small or limited to a particular domain, and neither applies to the study of the natural dialogue domain.",
      "sentences": [
          {
              "text": "Both corpora are either small or limited to a particular domain, and neither applies to the study of the natural dialogue domain.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Topic Shift Detection in Chinese Dialogues_ Corpus and Benchmark",
      "section": 14,
      "paragraph_id": 14,
      "full_text": "For Chinese, Xu et al.[8]annotated a dataset including 505 phone records of customer service on banking consultation.However, this corpus is likewise restricted to a few specialized domains while natural dialogues are more complicated.Natural dialogues have a range of topic shift scenarios, unrestricted topics, and more free colloquialisms in the utterances.The above corpus is insufficient to fill the gap in the Chinese natural dialogue topic corpus.",
      "sentences": [
          {
              "text": "For Chinese, Xu et al.[8]annotated a dataset including 505 phone records of customer service on banking consultation.",
              "label": 0
          },
          {
              "text": "However, this corpus is likewise restricted to a few specialized domains while natural dialogues are more complicated.",
              "label": 0
          },
          {
              "text": "Natural dialogues have a range of topic shift scenarios, unrestricted topics, and more free colloquialisms in the utterances.",
              "label": 0
          },
          {
              "text": "The above corpus is insufficient to fill the gap in the Chinese natural dialogue topic corpus.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Topic Shift Detection in Chinese Dialogues_ Corpus and Benchmark",
      "section": 15,
      "paragraph_id": 15,
      "full_text": "The existing corpus of Chinese dialogue topic detection[8]is small and does not satisfy natural conversation.Although the English dialogue topic corpora can be converted into Chinese by machine translation, they lack natural conversation colloquiality and are small in size.Therefore, we annotate a Chinese dialogue topic detection corpus CNTD based on NaturalConv dataset[20].",
      "sentences": [
          {
              "text": "The existing corpus of Chinese dialogue topic detection[8]is small and does not satisfy natural conversation.",
              "label": 1
          },
          {
              "text": "Although the English dialogue topic corpora can be converted into Chinese by machine translation, they lack natural conversation colloquiality and are small in size.",
              "label": 1
          },
          {
              "text": "Therefore, we annotate a Chinese dialogue topic detection corpus CNTD based on NaturalConv dataset[20].",
              "label": 1
          }
      ]
  },
  {
      "paper_name": "Topic Shift Detection in Chinese Dialogues_ Corpus and Benchmark",
      "section": 16,
      "paragraph_id": 16,
      "full_text": "In this section, we show our annotation guidelines and outline the reasons for our selection of corpus sources, as well as the manual annotation procedure and data statistics.We also analyze the topic shift distribution in CNTD.",
      "sentences": [
          {
              "text": "In this section, we show our annotation guidelines and outline the reasons for our selection of corpus sources, as well as the manual annotation procedure and data statistics.",
              "label": 1
          },
          {
              "text": "We also analyze the topic shift distribution in CNTD.",
              "label": 1
          }
      ]
  },
  {
      "paper_name": "Topic Shift Detection in Chinese Dialogues_ Corpus and Benchmark",
      "section": 17,
      "paragraph_id": 17,
      "full_text": "Each dialogue in our corpus has a piece of news as a base document, which is not available in other corpus and can be used as additional information for further research and expansion.The news is from six domains, which brings our conversations closer to natural dialogue.Besides, the speakers in our corpus are not restricted in any way, which also makes it closer to natural dialogues.In addition, we annotated the fine-grained dialogues topics, refer to Section 3.2.Fine-grained labels are beneficial to promote further research on dialogue topics.",
      "sentences": [
          {
              "text": "Each dialogue in our corpus has a piece of news as a base document, which is not available in other corpus and can be used as additional information for further research and expansion.",
              "label": 1
          },
          {
              "text": "The news is from six domains, which brings our conversations closer to natural dialogue.",
              "label": 1
          },
          {
              "text": "Besides, the speakers in our corpus are not restricted in any way, which also makes it closer to natural dialogues.",
              "label": 1
          },
          {
              "text": "In addition, we annotated the fine-grained dialogues topics, refer to Section 3.2.",
              "label": 1
          },
          {
              "text": "Fine-grained labels are beneficial to promote further research on dialogue topics.",
              "label": 1
          }
      ]
  },
  {
      "paper_name": "Topic Shift Detection in Chinese Dialogues_ Corpus and Benchmark",
      "section": 18,
      "paragraph_id": 18,
      "full_text": "Compared with the existing Chinese topic corpus annotated by Xu et al.[8], the dialogues in our corpus do not have meaningless and repetitive turns.Also, the corpus is more than twice the size of the other corpus.In addition, the news in the corpus can be studied as additional information for the dialogues.",
      "sentences": [
          {
              "text": "Compared with the existing Chinese topic corpus annotated by Xu et al.[8], the dialogues in our corpus do not have meaningless and repetitive turns.",
              "label": 1
          },
          {
              "text": "Also, the corpus is more than twice the size of the other corpus.",
              "label": 1
          },
          {
              "text": "In addition, the news in the corpus can be studied as additional information for the dialogues.",
              "label": 1
          }
      ]
  },
  {
      "paper_name": "Topic Shift Detection in Chinese Dialogues_ Corpus and Benchmark",
      "section": 19,
      "paragraph_id": 19,
      "full_text": "Following the annotation guidelines in TIAGE[7], we distinguish each dialogue turns whether changed the topic compared with the context.The response of a speaker to the dialogue context usually falls into one of the following cases in dialogues where the examples can be found in Table1.-Commenting on the previous context: The response is a comment on what is said by the speaker previously; -Question answering: The response is an answer to the question that comes from the speaker previously; -Developing the dialogue to sub-topics: The response develops to a sub-topic compared to the context; -Introducing a relevant but different topic: The response introduces a relevant but different topic compared to the context; -Completely changing the topic: The response completely changes the topic compared to the context.",
      "sentences": [
          {
              "text": "Following the annotation guidelines in TIAGE[7], we distinguish each dialogue turns whether changed the topic compared with the context.",
              "label": 1
          },
          {
              "text": "-Commenting on the previous context: The response is a comment on what is said by the speaker previously; -Question answering: The response is an answer to the question that comes from the speaker previously; -Developing the dialogue to sub-topics: The response develops to a sub-topic compared to the context; -Introducing a relevant but different topic: The response introduces a relevant but different topic compared to the context; -Completely changing the topic: The response completely changes the topic compared to the context.",
              "label": 1
          }
      ]
  },
  {
      "paper_name": "Topic Shift Detection in Chinese Dialogues_ Corpus and Benchmark",
      "section": 20,
      "paragraph_id": 20,
      "full_text": "Among them, we uniformly identify the two cases of greeting and farewell specific to CNTD as the topic shift.",
      "sentences": [
          {
              "text": "Among them, we uniformly identify the two cases of greeting and farewell specific to CNTD as the topic shift.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Topic Shift Detection in Chinese Dialogues_ Corpus and Benchmark",
      "section": 21,
      "paragraph_id": 21,
      "full_text": "We chose the NaturalConv dataset[20]as the source corpus, which contains about 400K utterances and 19.9K dialogues in multiple domains.It is designed to collect a multi-turn document grounded dialogue dataset with scenario and naturalness properties of dialogue.",
      "sentences": [
          {
              "text": "We chose the NaturalConv dataset[20]as the source corpus, which contains about 400K utterances and 19.9K dialogues in multiple domains.",
              "label": 1
          },
          {
              "text": "It is designed to collect a multi-turn document grounded dialogue dataset with scenario and naturalness properties of dialogue.",
              "label": 1
          }
      ]
  },
  {
      "paper_name": "Topic Shift Detection in Chinese Dialogues_ Corpus and Benchmark",
      "section": 22,
      "paragraph_id": 22,
      "full_text": "We consider NaturalConv as a promising dataset for dialogue topic detection for the following reasons: 1) NaturalConv is much closer to human-like dialogue with the natural property, including a full and natural setting such as scenario assumption, free topic extension, greetings, etc.; 2) NaturalConv contains about 400K utterances and 19.9K dialogues in multiple domains; 3) The average turn number of this corpus is 20, and longer dialogue contexts tend to exhibit a flow with more topics; 4) The corpus has almost no restrictions or assumptions about the speakers, e.g., no explicit goal is proposed[21].",
      "sentences": [
          {
              "text": "We consider NaturalConv as a promising dataset for dialogue topic detection for the following reasons: 1) NaturalConv is much closer to human-like dialogue with the natural property, including a full and natural setting such as scenario assumption, free topic extension, greetings, etc.; 2) NaturalConv contains about 400K utterances and 19.9K dialogues in multiple domains; 3) The average turn number of this corpus is 20, and longer dialogue contexts tend to exhibit a flow with more topics; 4) The corpus has almost no restrictions or assumptions about the speakers, e.g., no explicit goal is proposed[21].",
              "label": 1
          }
      ]
  },
  {
      "paper_name": "Topic Shift Detection in Chinese Dialogues_ Corpus and Benchmark",
      "section": 23,
      "paragraph_id": 23,
      "full_text": "We have three annotators for coarse-grained annotations and two for fine-grained annotations.Both annotations are divided into three stages as follows.",
      "sentences": [
          {
              "text": "We have three annotators for coarse-grained annotations and two for fine-grained annotations.",
              "label": 1
          },
          {
              "text": "Both annotations are divided into three stages as follows.",
              "label": 1
          }
      ]
  },
  {
      "paper_name": "Topic Shift Detection in Chinese Dialogues_ Corpus and Benchmark",
      "section": 24,
      "paragraph_id": 24,
      "full_text": "Co-annotation Stage First, for coarse-grained annotations, we draw a total of 100 dialogues from each domain of the NaturalConv dataset proportionally for a total of 2014 dialogue turns.In this stage, three annotators are asked to discuss every 20 dialogues they annotated, and each annotator is asked to give a reason for the annotation during the discussion.Finally, the Kappa value of all annotators for coarse-grained annotations at this stage is 0.7426.In addition, we annotated the fine-grained information based on the results of the complete coarse-grained annotations.Two annotators annotated the same 150 dialogues and discussed them several times for consistency.Finally, the kappa value of all annotators for fine-grained annotations at this stage is 0.9032.These kappa values confirm that our annotators already have sufficient annotation capabilities for independent annotation, as well as the high quality of our corpus.",
      "sentences": [
          {
              "text": "Co-annotation Stage First, for coarse-grained annotations, we draw a total of 100 dialogues from each domain of the NaturalConv dataset proportionally for a total of 2014 dialogue turns.",
              "label": 1
          },
          {
              "text": "In this stage, three annotators are asked to discuss every 20 dialogues they annotated, and each annotator is asked to give a reason for the annotation during the discussion.",
              "label": 1
          },
          {
              "text": "Finally, the Kappa value of all annotators for coarse-grained annotations at this stage is 0.7426.",
              "label": 1
          },
          {
              "text": "In addition, we annotated the fine-grained information based on the results of the complete coarse-grained annotations.",
              "label": 1
          },
          {
              "text": "Two annotators annotated the same 150 dialogues and discussed them several times for consistency.",
              "label": 1
          },
          {
              "text": "Finally, the kappa value of all annotators for fine-grained annotations at this stage is 0.9032.",
              "label": 1
          },
          {
              "text": "These kappa values confirm that our annotators already have sufficient annotation capabilities for independent annotation, as well as the high quality of our corpus.",
              "label": 1
          }
      ]
  },
  {
      "paper_name": "Topic Shift Detection in Chinese Dialogues_ Corpus and Benchmark",
      "section": 25,
      "paragraph_id": 25,
      "full_text": "Independent-annotation Stage We ensured the quality of each annotator's annotation and judging criteria before starting the second phase of annotation.For both granularity annotations, we randomly assign the dialogues drawn from each domain to each annotator for independent annotation.At this stage, we annotate 1208 dialogues for coarse-grained annotations and 1158 dialogues for fine-grained annotations.",
      "sentences": [
          {
              "text": "Independent-annotation Stage We ensured the quality of each annotator's annotation and judging criteria before starting the second phase of annotation.",
              "label": 1
          },
          {
              "text": "For both granularity annotations, we randomly assign the dialogues drawn from each domain to each annotator for independent annotation.",
              "label": 1
          },
          {
              "text": "At this stage, we annotate 1208 dialogues for coarse-grained annotations and 1158 dialogues for fine-grained annotations.",
              "label": 1
          }
      ]
  },
  {
      "paper_name": "Topic Shift Detection in Chinese Dialogues_ Corpus and Benchmark",
      "section": 26,
      "paragraph_id": 26,
      "full_text": "Semi-automatic Rechecking Stage Finally, we use a semi-automatic rechecking process to ensure that the corpus is still of high quality.On the one hand, we automatically format the dialogues with annotations to detect formatting problems caused by manual annotation.On the other hand, we automatically match the related news to each dialogue and check that the topic attributes are consistent with the dialogue to rule out any possible errors.",
      "sentences": [
          {
              "text": "Semi-automatic Rechecking Stage Finally, we use a semi-automatic rechecking process to ensure that the corpus is still of high quality.",
              "label": 1
          },
          {
              "text": "On the one hand, we automatically format the dialogues with annotations to detect formatting problems caused by manual annotation.",
              "label": 1
          },
          {
              "text": "On the other hand, we automatically match the related news to each dialogue and check that the topic attributes are consistent with the dialogue to rule out any possible errors.",
              "label": 1
          }
      ]
  },
  {
      "paper_name": "Topic Shift Detection in Chinese Dialogues_ Corpus and Benchmark",
      "section": 27,
      "paragraph_id": 27,
      "full_text": "Due to the limited time, we randomly select 1308 dialogues from the Natural-Conv dataset and annotate them with four annotators.Finally, we construct a Chinese natural topic dialogues corpus containing 26K dialogue turns.",
      "sentences": [
          {
              "text": "Due to the limited time, we randomly select 1308 dialogues from the Natural-Conv dataset and annotate them with four annotators.",
              "label": 1
          },
          {
              "text": "Finally, we construct a Chinese natural topic dialogues corpus containing 26K dialogue turns.",
              "label": 1
          }
      ]
  },
  {
      "paper_name": "Topic Shift Detection in Chinese Dialogues_ Corpus and Benchmark",
      "section": 28,
      "paragraph_id": 28,
      "full_text": "As shown in Table2, we randomly split them into 1041 train, 134 validation, and 133 test dialogues respectively, according to the percentage of different categories.In addition, we show the details of CNTD in Table3, which shows that our corpus has enough topics and long turns which is suitable for dialogue topic detection.Finally, there are the statistics of our fine-grained labels, as shown in Table4.",
      "sentences": [
          {
              "text": "As shown in Table2, we randomly split them into 1041 train, 134 validation, and 133 test dialogues respectively, according to the percentage of different categories.",
              "label": 0
          },
          {
              "text": "In addition, we show the details of CNTD in Table3, which shows that our corpus has enough topics and long turns which is suitable for dialogue topic detection.",
              "label": 1
          },
          {
              "text": "Finally, there are the statistics of our fine-grained labels, as shown in Table4.",
              "label": 1
          }
      ]
  },
  {
      "paper_name": "Topic Shift Detection in Chinese Dialogues_ Corpus and Benchmark",
      "section": 29,
      "paragraph_id": 29,
      "full_text": "We count the number of dialogues with different numbers of topics, as shown in Fig.2. On another side, we count the distribution of topic shift signals in dialogues, shown in Fig.3.We can see there are a total of 21 turns and three peaks of topic shift signals, which occur in 2 nd , 4 th , and 18 th turns, respectively.The reason is that the dialogue in our corpus usually starts with a greeting and   ends with a farewell, which leads to more topic shifts at the beginning and end of the dialogues.In addition, the NaturalConv corpus gives a piece of news as the base document of the dialogue, so there are more frequent transitions from news to derived topics, leading to the third highest peak in 4 th turn.However, we think this is consistent with a natural dialogue scenario because people often talk about recent news after daily greetings.",
      "sentences": [
          {
              "text": "We count the number of dialogues with different numbers of topics, as shown in Fig.2.",
              "label": 1
          },
          {
              "text": "On another side, we count the distribution of topic shift signals in dialogues, shown in Fig.3.",
              "label": 1
          },
          {
              "text": "We can see there are a total of 21 turns and three peaks of topic shift signals, which occur in 2 nd , 4 th , and 18 th turns, respectively.",
              "label": 1
          },
          {
              "text": "The reason is that the dialogue in our corpus usually starts with a greeting and   ends with a farewell, which leads to more topic shifts at the beginning and end of the dialogues.",
              "label": 1
          },
          {
              "text": "In addition, the NaturalConv corpus gives a piece of news as the base document of the dialogue, so there are more frequent transitions from news to derived topics, leading to the third highest peak in 4 th turn.",
              "label": 1
          },
          {
              "text": "However, we think this is consistent with a natural dialogue scenario because people often talk about recent news after daily greetings.",
              "label": 1
          }
      ]
  },
  {
      "paper_name": "Topic Shift Detection in Chinese Dialogues_ Corpus and Benchmark",
      "section": 30,
      "paragraph_id": 30,
      "full_text": "Based on the train/validation/test dataset of CNTD we partitioned in Table2and previous work on TIAGE[7], we extract (context, response) pairs from each dialogue as input and the label of response as a target for the responseunknown task.In our experiments, every utterance except the first utterance of the dialogue can be considered as a response.As for evaluation, we report Precision (P), Recall (R), and Micro-F1 scores.",
      "sentences": [
          {
              "text": "Based on the train/validation/test dataset of CNTD we partitioned in Table2and previous work on TIAGE[7], we extract (context, response) pairs from each dialogue as input and the label of response as a target for the responseunknown task.",
              "label": 0
          },
          {
              "text": "In our experiments, every utterance except the first utterance of the dialogue can be considered as a response.",
              "label": 0
          },
          {
              "text": "As for evaluation, we report Precision (P), Recall (R), and Micro-F1 scores.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Topic Shift Detection in Chinese Dialogues_ Corpus and Benchmark",
      "section": 31,
      "paragraph_id": 31,
      "full_text": "We use BERT as an encoder and fine-tune it during training.For both the TIAGE and CNTD corpus, all pre-trained model parameters are set to default values.We conduct our experiments on NVIDIA GeForce GTX 1080 Ti and NVIDIA GeForce GTX 3090 with batch sizes of 2 and 6 for both CNTD and TIAGE, with the initial learning rates of 2e-5.And we set the epochs of training to 20, and the dropout to 0.5.",
      "sentences": [
          {
              "text": "We use BERT as an encoder and fine-tune it during training.",
              "label": 0
          },
          {
              "text": "For both the TIAGE and CNTD corpus, all pre-trained model parameters are set to default values.",
              "label": 0
          },
          {
              "text": "We conduct our experiments on NVIDIA GeForce GTX 1080 Ti and NVIDIA GeForce GTX 3090 with batch sizes of 2 and 6 for both CNTD and TIAGE, with the initial learning rates of 2e-5.",
              "label": 0
          },
          {
              "text": "And we set the epochs of training to 20, and the dropout to 0.5.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Topic Shift Detection in Chinese Dialogues_ Corpus and Benchmark",
      "section": 32,
      "paragraph_id": 32,
      "full_text": "For the pre-trained models in the experiment, we apply BERT-base-Chinese and MT5-base to obtain the semantic representation of the dialogues in CNTD, and we apply BERT-base-uncased and T5-base to obtain the semantic representation of the dialogues in TIAGE.",
      "sentences": [
          {
              "text": "For the pre-trained models in the experiment, we apply BERT-base-Chinese and MT5-base to obtain the semantic representation of the dialogues in CNTD, and we apply BERT-base-uncased and T5-base to obtain the semantic representation of the dialogues in TIAGE.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Topic Shift Detection in Chinese Dialogues_ Corpus and Benchmark",
      "section": 33,
      "paragraph_id": 33,
      "full_text": "Dialogue topic shift detection is a new task and there is no complex model available, besides a simple T5[7]that can be considered as the SOTA model.Since we employ BERT as our encoder and the T5 model is used in TIAGE, we use the pre-trained models of T5[9]and BERT[27]as baselines.For BERT, we For T5, we also connect utterances in the context and classify the undecidable predicted results to the 'not a topic shift' category.",
      "sentences": [
          {
              "text": "Dialogue topic shift detection is a new task and there is no complex model available, besides a simple T5[7]that can be considered as the SOTA model.",
              "label": 0
          },
          {
              "text": "Since we employ BERT as our encoder and the T5 model is used in TIAGE, we use the pre-trained models of T5[9]and BERT[27]as baselines.",
              "label": 0
          },
          {
              "text": "For BERT, we For T5, we also connect utterances in the context and classify the undecidable predicted results to the 'not a topic shift' category.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Topic Shift Detection in Chinese Dialogues_ Corpus and Benchmark",
      "section": 34,
      "paragraph_id": 34,
      "full_text": "Table5shows the performance comparison between our model and the baselines, in which TS denotes our teacher-student model without the hierarchical comparative learning (HCL) and Ours denotes our final model, i.e., the addition of SCL on the student side based on the addition of ISL on both the teacher and student sides.",
      "sentences": [
          {
              "text": "Table5shows the performance comparison between our model and the baselines, in which TS denotes our teacher-student model without the hierarchical comparative learning (HCL) and Ours denotes our final model, i.e., the addition of SCL on the student side based on the addition of ISL on both the teacher and student sides.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Topic Shift Detection in Chinese Dialogues_ Corpus and Benchmark",
      "section": 35,
      "paragraph_id": 35,
      "full_text": "It can be found that on CNTD, our model achieves a good improvement and improves both precision and recall in comparison with the baselines.Although T5 does not perform poorly on recall, its precision is inadequate in comparison with BERT, and it is clear that T5 is not effective in predicting topics.In contrast, TS improved by 1.0 in Micro-F1 in comparison with BERT, which confirms that the teacher-student framework is effective in introducing response information.As well, Ours improved by 4.0 in micro-F1 in comparison with TS, and also showed significant improvement in P and R, which fully demonstrates that our HCL can improve the model's ability to discriminate between different topic situations.In particular, our model improves on CNTD by 5.0 in comparison with the best baseline BERT, which shows the effectiveness of our proposed model.",
      "sentences": [
          {
              "text": "It can be found that on CNTD, our model achieves a good improvement and improves both precision and recall in comparison with the baselines.",
              "label": 0
          },
          {
              "text": "Although T5 does not perform poorly on recall, its precision is inadequate in comparison with BERT, and it is clear that T5 is not effective in predicting topics.",
              "label": 0
          },
          {
              "text": "In contrast, TS improved by 1.0 in Micro-F1 in comparison with BERT, which confirms that the teacher-student framework is effective in introducing response information.",
              "label": 0
          },
          {
              "text": "As well, Ours improved by 4.0 in micro-F1 in comparison with TS, and also showed significant improvement in P and R, which fully demonstrates that our HCL can improve the model's ability to discriminate between different topic situations.",
              "label": 0
          },
          {
              "text": "In particular, our model improves on CNTD by 5.0 in comparison with the best baseline BERT, which shows the effectiveness of our proposed model.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Topic Shift Detection in Chinese Dialogues_ Corpus and Benchmark",
      "section": 36,
      "paragraph_id": 36,
      "full_text": "To verify the effectiveness of the components used in our model, we conduct ablation studies on CTND, and the experimental results are shown in Table6.",
      "sentences": [
          {
              "text": "To verify the effectiveness of the components used in our model, we conduct ablation studies on CTND, and the experimental results are shown in Table6.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Topic Shift Detection in Chinese Dialogues_ Corpus and Benchmark",
      "section": 37,
      "paragraph_id": 37,
      "full_text": "If we remove ISL on the teacher side (-ISL S ) or the student side (-ISL T ), the performance of the model decreased by 1.5 and 1.3 on the Micro-F1 value, respectively, with the largest decrease after removing the ISL on the student side.Although -ISL T has the highest precision in predicting topics and lower error probability than Ours and -ISL S .However, it can be seen that adding ISL at both the teacher and student sides can better improve the correct prediction rate.Moreover, if we remove ISL both on the teacher and student side (-ISL T S ), it achieves a similar performance on Micro-F1, in comparison with -ISL S and -ISL T .However, it achieves the highest precision (58.8%).",
      "sentences": [
          {
              "text": "If we remove ISL on the teacher side (-ISL S ) or the student side (-ISL T ), the performance of the model decreased by 1.5 and 1.3 on the Micro-F1 value, respectively, with the largest decrease after removing the ISL on the student side.",
              "label": 0
          },
          {
              "text": "Although -ISL T has the highest precision in predicting topics and lower error probability than Ours and -ISL S .",
              "label": 0
          },
          {
              "text": "However, it can be seen that adding ISL at both the teacher and student sides can better improve the correct prediction rate.",
              "label": 0
          },
          {
              "text": "Moreover, if we remove ISL both on the teacher and student side (-ISL T S ), it achieves a similar performance on Micro-F1, in comparison with -ISL S and -ISL T .",
              "label": 0
          },
          {
              "text": "However, it achieves the highest precision (58.8%).",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Topic Shift Detection in Chinese Dialogues_ Corpus and Benchmark",
      "section": 38,
      "paragraph_id": 38,
      "full_text": "If we remove SCL (-SCL) or HCL (-HCL) from our model, the Micro-F1 value of the models -SCL and -HCL drop from 53.9 to 52.4 (-1.5) and 49.9 (-4.0), respectively.These results show that our Semantic Conherent-aware Loss(SCL), and Hierarchical Contrastive Learning(HCL) are effective for this task, especially HCL.",
      "sentences": [
          {
              "text": "If we remove SCL (-SCL) or HCL (-HCL) from our model, the Micro-F1 value of the models -SCL and -HCL drop from 53.9 to 52.4 (-1.5) and 49.9 (-4.0), respectively.",
              "label": 0
          },
          {
              "text": "These results show that our Semantic Conherent-aware Loss(SCL), and Hierarchical Contrastive Learning(HCL) are effective for this task, especially HCL.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Topic Shift Detection in Chinese Dialogues_ Corpus and Benchmark",
      "section": 39,
      "paragraph_id": 39,
      "full_text": "In addition, we explore the performance of the dialogues with different numbers of topics to analyze our model in comparison with BERT, as shown in Table7.It can be found that our model has a better performance than BERT on dialogues with fewer topics.Our model gets at least a 6% improvement in topic shift prediction on dialogues with 2 to 5 topics and obtains above-average performance.",
      "sentences": [
          {
              "text": "In addition, we explore the performance of the dialogues with different numbers of topics to analyze our model in comparison with BERT, as shown in Table7.",
              "label": 0
          },
          {
              "text": "It can be found that our model has a better performance than BERT on dialogues with fewer topics.",
              "label": 0
          },
          {
              "text": "Our model gets at least a 6% improvement in topic shift prediction on dialogues with 2 to 5 topics and obtains above-average performance.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Topic Shift Detection in Chinese Dialogues_ Corpus and Benchmark",
      "section": 40,
      "paragraph_id": 40,
      "full_text": "And when the number of topics increases to 9, the performance improves because the conversation length is still about 20 and the topics shift more significantly.",
      "sentences": [
          {
              "text": "And when the number of topics increases to 9, the performance improves because the conversation length is still about 20 and the topics shift more significantly.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Topic Shift Detection in Chinese Dialogues_ Corpus and Benchmark",
      "section": 41,
      "paragraph_id": 41,
      "full_text": "In Table8, we also investigate the recall of the topic shift detection for various topic turns.Our model is improved for varying degrees across topic turns, with the most significant improvements in turns 7-9.Even in long topic shift cases, our model can obtain an effective boost.However, the performance of our model inevitably decreases compared to short topic shift cases.When there are fewer topic turns, the topic shift situation is simpler, so it is easier to determine.When the length of turns becomes longer and the situation becomes complicated, the topic of long turns has more information so it is easier to identify.",
      "sentences": [
          {
              "text": "In Table8, we also investigate the recall of the topic shift detection for various topic turns.",
              "label": 0
          },
          {
              "text": "Our model is improved for varying degrees across topic turns, with the most significant improvements in turns 7-9.",
              "label": 0
          },
          {
              "text": "Even in long topic shift cases, our model can obtain an effective boost.",
              "label": 0
          },
          {
              "text": "However, the performance of our model inevitably decreases compared to short topic shift cases.",
              "label": 0
          },
          {
              "text": "When there are fewer topic turns, the topic shift situation is simpler, so it is easier to determine.",
              "label": 0
          },
          {
              "text": "When the length of turns becomes longer and the situation becomes complicated, the topic of long turns has more information so it is easier to identify.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Topic Shift Detection in Chinese Dialogues_ Corpus and Benchmark",
      "section": 42,
      "paragraph_id": 42,
      "full_text": "As shown in Table9, it can be found that our model also achieves a good improvement on English TIAGE.Although our model is not the best on precision, Lin.et al. we obtain the best performance on both recall and Micro-F1 values, especially on micro-F1 with a 5.8% improvement over T5.This proves that our model achieves the best performance both in English and Chinese.",
      "sentences": [
          {
              "text": "As shown in Table9, it can be found that our model also achieves a good improvement on English TIAGE.",
              "label": 0
          },
          {
              "text": "et al. we obtain the best performance on both recall and Micro-F1 values, especially on micro-F1 with a 5.8% improvement over T5.",
              "label": 0
          },
          {
              "text": "This proves that our model achieves the best performance both in English and Chinese.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Topic Shift Detection in Chinese Dialogues_ Corpus and Benchmark",
      "section": 43,
      "paragraph_id": 43,
      "full_text": "We also conducted a case study.The prediction made by our model, the BERT model on the instance, and the manual labels are shown in Table10.Compared with the BERT model, it is obvious that our model can accurately anticipate the change of topic in the instances corresponding to the utterances \"Yes, that's right.\",\"Itis, indeed, should pay attention to it.\"etc., belonging to the questionanswering scenario.However, if you respond \"Well, the policy has been implemented in place this time.\"and \"And now we are promoting the development of children's creative and practical skills.\"etc. belonging to the commenting on the previous context scenario, our model or BERT cannot accurately predict the topic shift in this scenario.This shows that detecting the topic shifts in natural dialogue is still challenging.",
      "sentences": [
          {
              "text": "We also conducted a case study.",
              "label": 0
          },
          {
              "text": "The prediction made by our model, the BERT model on the instance, and the manual labels are shown in Table10.",
              "label": 0
          },
          {
              "text": "\"etc., belonging to the questionanswering scenario.",
              "label": 0
          },
          {
              "text": "\"etc. belonging to the commenting on the previous context scenario, our model or BERT cannot accurately predict the topic shift in this scenario.",
              "label": 0
          },
          {
              "text": "This shows that detecting the topic shifts in natural dialogue is still challenging.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Topic Shift Detection in Chinese Dialogues_ Corpus and Benchmark",
      "section": 44,
      "paragraph_id": 44,
      "full_text": "We further analyze the errors of the prediction produced in our experiments.Specifically, we analyzed the example to explore whether the error in the results of this example is prevalent in other dialogues.From Table10, we can find that the wrong predictions at 14 th and 18 th turn.We predict \"The teaching equipment must be updated, right?\" as 'not a topic shift' and \"Well, thanks to the government!\"as 'topic shift'.",
      "sentences": [
          {
              "text": "We further analyze the errors of the prediction produced in our experiments.",
              "label": 0
          },
          {
              "text": "Specifically, we analyzed the example to explore whether the error in the results of this example is prevalent in other dialogues.",
              "label": 0
          },
          {
              "text": "From Table10, we can find that the wrong predictions at 14 th and 18 th turn.",
              "label": 0
          },
          {
              "text": "\"as 'topic shift'.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Topic Shift Detection in Chinese Dialogues_ Corpus and Benchmark",
      "section": 45,
      "paragraph_id": 45,
      "full_text": "We counted the appearance of many errors, and the errors are mainly divided into two categories.One is for the \"Introducing a relevant but different topic\" type of utterance.It was predicted that no topic shift occurred due to the lack of information about the future of the conversation.The other is the \"commenting on the previous context\" category.Since this type of response does not affect the integrity of the previous topic, it is mostly predicted to be a topic shift.",
      "sentences": [
          {
              "text": "We counted the appearance of many errors, and the errors are mainly divided into two categories.",
              "label": 0
          },
          {
              "text": "One is for the \"Introducing a relevant but different topic\" type of utterance.",
              "label": 0
          },
          {
              "text": "It was predicted that no topic shift occurred due to the lack of information about the future of the conversation.",
              "label": 0
          },
          {
              "text": "The other is the \"commenting on the previous context\" category.",
              "label": 0
          },
          {
              "text": "Since this type of response does not affect the integrity of the previous topic, it is mostly predicted to be a topic shift.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Topic Shift Detection in Chinese Dialogues_ Corpus and Benchmark",
      "section": 46,
      "paragraph_id": 46,
      "full_text": "Table10.The results of BERT, Ours, and Human of different turns where \"1\" indicates that a topic shift has occurred and \"0\" indicates the opposite.We omit the lines with all 0.",
      "sentences": [
          {
              "text": "Table10.",
              "label": 0
          },
          {
              "text": "The results of BERT, Ours, and Human of different turns where \"1\" indicates that a topic shift has occurred and \"0\" indicates the opposite.",
              "label": 0
          },
          {
              "text": "We omit the lines with all 0.",
              "label": 0
          }
      ]
    },
    {
      "paper_name": "Towards Equitable Representation in Text_to_Image Synthesis Models with the Cross_Cultural Understanding Benchmark _CCUB_ Dataset",
      "section": 0,
      "paragraph_id": 0,
      "full_text": "Abstract: It has been shown that accurate representation in media improves the well-being of the people who consume it.By contrast, inaccurate representations can negatively affect viewers and lead to harmful perceptions of other cultures.To achieve inclusive representation in generated images, we propose a culturally-aware priming approach for text-to-image synthesis using a small but culturally curated dataset that we collected, known here as Cross-Cultural Understanding Benchmark (CCUB) Dataset, to fight the bias prevalent in giant datasets.Our proposed approach is comprised of two fine-tuning techniques: (1) Adding visual context via fine-tuning a pre-trained text-to-image synthesis model, Stable Diffusion, on the CCUB text-image pairs, and (2) Adding semantic context via automated prompt engineering using the finetuned large language model, GPT-3, trained on our CCUB culturally-aware text data.CCUB dataset is curated and our approach is evaluated by people who have a personal relationship with that particular culture.Our experiments indicate that priming using both text and image is effective in improving the cultural relevance and decreasing the offensiveness of generated images while maintaining quality.Our CCUB dataset and codes 1 are publicly available.* indicates equal contribution.† indicates corresponding authors.",
      "sentences": [
          {
              "text": "Abstract: It has been shown that accurate representation in media improves the well-being of the people who consume it.",
              "label": 0
          },
          {
              "text": "By contrast, inaccurate representations can negatively affect viewers and lead to harmful perceptions of other cultures.",
              "label": 0
          },
          {
              "text": "To achieve inclusive representation in generated images, we propose a culturally-aware priming approach for text-to-image synthesis using a small but culturally curated dataset that we collected, known here as Cross-Cultural Understanding Benchmark (CCUB) Dataset, to fight the bias prevalent in giant datasets.",
              "label": 1
          },
          {
              "text": "Our proposed approach is comprised of two fine-tuning techniques: (1) Adding visual context via fine-tuning a pre-trained text-to-image synthesis model, Stable Diffusion, on the CCUB text-image pairs, and (2) Adding semantic context via automated prompt engineering using the finetuned large language model, GPT-3, trained on our CCUB culturally-aware text data.",
              "label": 0
          },
          {
              "text": "CCUB dataset is curated and our approach is evaluated by people who have a personal relationship with that particular culture.",
              "label": 1
          },
          {
              "text": "Our experiments indicate that priming using both text and image is effective in improving the cultural relevance and decreasing the offensiveness of generated images while maintaining quality.",
              "label": 1
          },
          {
              "text": "† indicates corresponding authors.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Towards Equitable Representation in Text_to_Image Synthesis Models with the Cross_Cultural Understanding Benchmark _CCUB_ Dataset",
      "section": 1,
      "paragraph_id": 1,
      "full_text": "In media, studies repeatedly show that representation affects the well-being of its viewers[Shaw, 2010;Caswell et al., 2017;Elbaba, 2019].Representation can positively affect viewers by providing them with role models that they identify with, but it can also negatively affect viewers by creating harmful, stereotypical understandings of people and culture[Castañeda, 2018].When people are accurately represented in media, it allows people to properly understand cultures without harmful stereo- types forming[Dixon and Linz, 2000;Mastro and Greenberg, 2000].Despite the benefits of representation, many media generating Artificial Intelligence (AI) models show poor representation in their results[Ntoutsi et al., 2020].Many of these issues stem from their large training datasets which are gathered by crawling the Internet without filtering supervision and contain malign stereotypes and ethnic slurs among other problematic content[Birhane et al., 2021].",
      "sentences": [
          {
              "text": "In media, studies repeatedly show that representation affects the well-being of its viewers[Shaw, 2010;Caswell et al., 2017;Elbaba, 2019].",
              "label": 0
          },
          {
              "text": "Representation can positively affect viewers by providing them with role models that they identify with, but it can also negatively affect viewers by creating harmful, stereotypical understandings of people and culture[Castañeda, 2018].",
              "label": 0
          },
          {
              "text": "When people are accurately represented in media, it allows people to properly understand cultures without harmful stereo- types forming[Dixon and Linz, 2000;Mastro and Greenberg, 2000].",
              "label": 0
          },
          {
              "text": "Despite the benefits of representation, many media generating Artificial Intelligence (AI) models show poor representation in their results[Ntoutsi et al., 2020].",
              "label": 0
          },
          {
              "text": "Many of these issues stem from their large training datasets which are gathered by crawling the Internet without filtering supervision and contain malign stereotypes and ethnic slurs among other problematic content[Birhane et al., 2021].",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Towards Equitable Representation in Text_to_Image Synthesis Models with the Cross_Cultural Understanding Benchmark _CCUB_ Dataset",
      "section": 2,
      "paragraph_id": 2,
      "full_text": "As AI models are increasingly used to create and aid in the production of visual content, it is important that the models have a true understanding of culture such that it can give accurate and proper representation leading to well-being rewards for its consumers.In this paper, we aim to address such a representation issue in image generation and introduce a new task of culturally-aware image synthesis: generating visual content within a cultural context that is both accurate and inoffensive.Our overarching goal is to improve the well-being of consumers of the AI generated images with particular attention to those consumers from underrepresented groups.Specifically, we formulate the culturally-aware text-to-image synthesis task to take an additional input of a country name to specify a cultural context in addition to language description.",
      "sentences": [
          {
              "text": "As AI models are increasingly used to create and aid in the production of visual content, it is important that the models have a true understanding of culture such that it can give accurate and proper representation leading to well-being rewards for its consumers.",
              "label": 0
          },
          {
              "text": "In this paper, we aim to address such a representation issue in image generation and introduce a new task of culturally-aware image synthesis: generating visual content within a cultural context that is both accurate and inoffensive.",
              "label": 0
          },
          {
              "text": "Our overarching goal is to improve the well-being of consumers of the AI generated images with particular attention to those consumers from underrepresented groups.",
              "label": 0
          },
          {
              "text": "Specifically, we formulate the culturally-aware text-to-image synthesis task to take an additional input of a country name to specify a cultural context in addition to language description.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Towards Equitable Representation in Text_to_Image Synthesis Models with the Cross_Cultural Understanding Benchmark _CCUB_ Dataset",
      "section": 3,
      "paragraph_id": 3,
      "full_text": "It was found that large datasets such as the LAION-5B[Schuhmann et al., 2021]used to train many text-toimage synthesis models such as Stable Diffusion[Rombach et al., 2021]are Anglo-centric and Euro-centric[Birhane et al., 2021]as shown in the top row of Figure1.As a consequence, these powerful models may generate culturally offensive images due to misrepresentation during training.Our research question is, how can effective existing text-to-image models be improved to become more culturally representative and thus less offensive?It may be infeasible to vet billions of training examples for accurate cultural content.",
      "sentences": [
          {
              "text": "It was found that large datasets such as the LAION-5B[Schuhmann et al., 2021]used to train many text-toimage synthesis models such as Stable Diffusion[Rombach et al., 2021]are Anglo-centric and Euro-centric[Birhane et al., 2021]as shown in the top row of Figure1.",
              "label": 0
          },
          {
              "text": "As a consequence, these powerful models may generate culturally offensive images due to misrepresentation during training.",
              "label": 0
          },
          {
              "text": "Our research question is, how can effective existing text-to-image models be improved to become more culturally representative and thus less offensive?",
              "label": 0
          },
          {
              "text": "It may be infeasible to vet billions of training examples for accurate cultural content.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Towards Equitable Representation in Text_to_Image Synthesis Models with the Cross_Cultural Understanding Benchmark _CCUB_ Dataset",
      "section": 4,
      "paragraph_id": 4,
      "full_text": "We hypothesize that a small dataset that is veritably representative of a culture can be used to prime pre-trained textto-image models to guide the model towards more culturally accurate content creation.To verify the hypothesis, we collected a dataset of image and caption pairs for 8 cultures.For each culture, data was collected by a few people who are native of that culture as they are the people who properly understand it and are most affected by its misrepresentations.We call this the Cross-Cultural Understanding Benchmark (CCUB) dataset which comprises of 100-200 images each with a manually written caption as shown in Figure2.",
      "sentences": [
          {
              "text": "We hypothesize that a small dataset that is veritably representative of a culture can be used to prime pre-trained textto-image models to guide the model towards more culturally accurate content creation.",
              "label": 1
          },
          {
              "text": "To verify the hypothesis, we collected a dataset of image and caption pairs for 8 cultures.",
              "label": 1
          },
          {
              "text": "For each culture, data was collected by a few people who are native of that culture as they are the people who properly understand it and are most affected by its misrepresentations.",
              "label": 1
          },
          {
              "text": "We call this the Cross-Cultural Understanding Benchmark (CCUB) dataset which comprises of 100-200 images each with a manually written caption as shown in Figure2.",
              "label": 1
          }
      ]
  },
  {
      "paper_name": "Towards Equitable Representation in Text_to_Image Synthesis Models with the Cross_Cultural Understanding Benchmark _CCUB_ Dataset",
      "section": 5,
      "paragraph_id": 5,
      "full_text": "We propose two techniques for enhancing the text-toimage pipelines using CCUB.First, we fine-tune a text-toimage synthesis model, Stable Diffusion, on the CCUB textimage pairs to generate images tailored for a given cultural context.Second, we create an automatic prompt augmenting approach usingGPT-3 [Brown et al., 2020]fine-tuned on CCUB to include culturally relevant details, e.g., \"Two people walking down a street\" can be augmented with \"using WeChat Pay to pay a bus ticket, in Shenzhen, China.\"",
      "sentences": [
          {
              "text": "We propose two techniques for enhancing the text-toimage pipelines using CCUB.",
              "label": 0
          },
          {
              "text": "First, we fine-tune a text-toimage synthesis model, Stable Diffusion, on the CCUB textimage pairs to generate images tailored for a given cultural context.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Towards Equitable Representation in Text_to_Image Synthesis Models with the Cross_Cultural Understanding Benchmark _CCUB_ Dataset",
      "section": 6,
      "paragraph_id": 6,
      "full_text": "We evaluate our approach's two components individually as well as combined against the baseline of simply specifying the culture in the text prompt.Our evaluation was performed by native people of each country.Our survey results based on 2,244 image comparisions conducted by 72 participants from 5 countries indicate that our proposed approach is both less offensive and more cultural relevant than simply adding the country name as a suffix to the prompt.Our contributions are as follows: 1.The introduction of culturally-aware text-to-image synthesis as a valuable task within text-to-image synthesis;",
      "sentences": [
          {
              "text": "We evaluate our approach's two components individually as well as combined against the baseline of simply specifying the culture in the text prompt.",
              "label": 0
          },
          {
              "text": "Our evaluation was performed by native people of each country.",
              "label": 0
          },
          {
              "text": "Our survey results based on 2,244 image comparisions conducted by 72 participants from 5 countries indicate that our proposed approach is both less offensive and more cultural relevant than simply adding the country name as a suffix to the prompt.",
              "label": 0
          },
          {
              "text": "Our contributions are as follows: 1.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Towards Equitable Representation in Text_to_Image Synthesis Models with the Cross_Cultural Understanding Benchmark _CCUB_ Dataset",
      "section": 7,
      "paragraph_id": 7,
      "full_text": "Following the definition of culture in[Halpern, 1955]and [Key and Comrie, 2021], nine categories are used to represent cultural elements in our dataset: food & drink, clothing, artwork, dance and music, religion, architecture, people, city and nature.The categories are further divided into traditional and modern to reflect a characteristic of the culture that culture changes over time.",
      "sentences": [
          {
              "text": "Following the definition of culture in[Halpern, 1955]and [Key and Comrie, 2021], nine categories are used to represent cultural elements in our dataset: food & drink, clothing, artwork, dance and music, religion, architecture, people, city and nature.",
              "label": 0
          },
          {
              "text": "The categories are further divided into traditional and modern to reflect a characteristic of the culture that culture changes over time.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Towards Equitable Representation in Text_to_Image Synthesis Models with the Cross_Cultural Understanding Benchmark _CCUB_ Dataset",
      "section": 8,
      "paragraph_id": 8,
      "full_text": "Our CCUB image are collected based on the nine cultural categories.For collection, we recruited cultural experts who confidently know this culture well or belong to it.Cultural experts are asked to collect 10-20 relevant images containing different objects for each cultural category.The images were collected either from Creative Commons licensed images from Google searches or the collectors own photographs.Cultural experts were also asked to select images with common or culturally representative items.",
      "sentences": [
          {
              "text": "Our CCUB image are collected based on the nine cultural categories.",
              "label": 1
          },
          {
              "text": "For collection, we recruited cultural experts who confidently know this culture well or belong to it.",
              "label": 1
          },
          {
              "text": "Cultural experts are asked to collect 10-20 relevant images containing different objects for each cultural category.",
              "label": 1
          },
          {
              "text": "The images were collected either from Creative Commons licensed images from Google searches or the collectors own photographs.",
              "label": 1
          },
          {
              "text": "Cultural experts were also asked to select images with common or culturally representative items.",
              "label": 1
          }
      ]
  },
  {
      "paper_name": "Towards Equitable Representation in Text_to_Image Synthesis Models with the Cross_Cultural Understanding Benchmark _CCUB_ Dataset",
      "section": 9,
      "paragraph_id": 9,
      "full_text": "Each image in the CCUB dataset is also captioned by cultural experts forming paired image-text data.Cultural experts were asked to focus on the general and specific items in each cultural image, rather than adding captions to subtle components of the image.The captions accurately express cultural contents in English as opposed to large datasets such asLAION [Schuhmann et al., 2021]which are scraped from the internet and not vetted for cultural accuracy.",
      "sentences": [
          {
              "text": "Each image in the CCUB dataset is also captioned by cultural experts forming paired image-text data.",
              "label": 1
          },
          {
              "text": "Cultural experts were asked to focus on the general and specific items in each cultural image, rather than adding captions to subtle components of the image.",
              "label": 1
          },
          {
              "text": "The captions accurately express cultural contents in English as opposed to large datasets such asLAION [Schuhmann et al., 2021]which are scraped from the internet and not vetted for cultural accuracy.",
              "label": 1
          }
      ]
  },
  {
      "paper_name": "Towards Equitable Representation in Text_to_Image Synthesis Models with the Cross_Cultural Understanding Benchmark _CCUB_ Dataset",
      "section": 10,
      "paragraph_id": 10,
      "full_text": "We produced surveys to evaluate the effectiveness of our two proposed techniques for culturally-aware text-to-image synthesis and compare them to a baseline of simply appending the culture to the prompt, e.g., \"A family eating dinner , China.\" and using an existing text-to-image model.",
      "sentences": [
          {
              "text": "We produced surveys to evaluate the effectiveness of our two proposed techniques for culturally-aware text-to-image synthesis and compare them to a baseline of simply appending the culture to the prompt, e.g., \"A family eating dinner , China.\" and using an existing text-to-image model.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Towards Equitable Representation in Text_to_Image Synthesis Models with the Cross_Cultural Understanding Benchmark _CCUB_ Dataset",
      "section": 11,
      "paragraph_id": 11,
      "full_text": "In setting up our study, we consider a comparative structure between images: the baseline image versus another image from our results.The setup of a single question in our survey was as follows: Given two images, the participant selects which image best fits three given comparative properties.The properties analyzed were: (1) Text and Image Alignment: Participants are given a text prompt and consider which of the two images is more similar to the prompt; (2) Cultural Alignment: Participants decide which of the two images is a better representation of the country's culture; and (3) Offensiveness: Participants consider which of the two images is more offensive to them.",
      "sentences": [
          {
              "text": "In setting up our study, we consider a comparative structure between images: the baseline image versus another image from our results.",
              "label": 0
          },
          {
              "text": "The setup of a single question in our survey was as follows: Given two images, the participant selects which image best fits three given comparative properties.",
              "label": 0
          },
          {
              "text": "The properties analyzed were: (1) Text and Image Alignment: Participants are given a text prompt and consider which of the two images is more similar to the prompt; (2) Cultural Alignment: Participants decide which of the two images is a better representation of the country's culture; and (3) Offensiveness: Participants consider which of the two images is more offensive to them.",
              "label": 0
          }
      ]
  },
  {
      "paper_name": "Towards Equitable Representation in Text_to_Image Synthesis Models with the Cross_Cultural Understanding Benchmark _CCUB_ Dataset",
      "section": 12,
      "paragraph_id": 12,
      "full_text": "The participants for the study were selected based on whether they had a personal understanding of the culture for which the images in the survey were generated.Participants were recruited among university students, friends, and family members of the authors.It was ensured that the participants would not be able to discern the approaches used to generate the compared images by randomizing the order of questions and images in the survey.",
      "sentences": [
          {
              "text": "The participants for the study were selected based on whether they had a personal understanding of the culture for which the images in the survey were generated.",
              "label": 0
          },
          {
              "text": "Participants were recruited among university students, friends, and family members of the authors.",
              "label": 0
          },
          {
              "text": "It was ensured that the participants would not be able to discern the approaches used to generate the compared images by randomizing the order of questions and images in the survey.",
              "label": 0
          }
      ]
    }                                                                                                                                                                                                            
   ]
}