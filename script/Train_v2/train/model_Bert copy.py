# train.py
import os
import torch
from utils.utils_train import (
    set_seed, get_device, load_json_papers,
    build_tokenizer, build_backbone_model, SentenceClassifier,
    make_dataloader, train_one_epoch, evaluate,
    save_predictions_to_json, plot_history, make_experiment_dir
)
from sklearn.model_selection import train_test_split
import json



def main():
    # ---------------------------- é…ç½®åŒº ----------------------------
    MODEL_PATH = "/home/kzlab/muse/Savvy/Data_collection/models/bert-base-cased"          # âœ… ä¾‹å¦‚ "bert-base-cased" æˆ–æœ¬åœ°è·¯å¾„
    TRAIN_JSON = "/home/kzlab/muse/Savvy/Data_collection/script/reconstruct_dataset/train_val_revise.json"         # âœ… è®­ç»ƒé›† JSON æ–‡ä»¶è·¯å¾„
    TEST_JSON   = None        # âœ… éªŒè¯é›† JSON æ–‡ä»¶è·¯å¾„
    VAL_JSON   = None        # ç”¨ None è¡¨ç¤ºåªæœ‰ä¸€ä¸ªæ–‡ä»¶
    VAL_RATIO  = 0.1         # éªŒè¯é›†å æ¯”
    SEED       = 42
    OUT_ROOT   = "/home/kzlab/muse/Savvy/Data_collection/script/Train_v2/train/outputs"                    # âœ… è¾“å‡ºä¿å­˜ç›®å½•
    TAG        = "exp1"                         # âœ… å¯é€‰å®žéªŒæ ‡è¯†

    BATCH_SIZE = 1
    MAX_LENGTH = 512                            # è‹¥ä¸º Noneï¼Œè‡ªåŠ¨å– tokenizer.model_max_length
    MIN_SENT   = 3
    CONTEXT    = 2
    EPOCHS     = 10
    LR         = 2e-5
    WEIGHT_DECAY = 0.01
    GRAD_ACCUM   = 4
    USE_AMP      = False
    # -------------------------------------------------------------

    set_seed(SEED)
    device = get_device()

    # 1) æ•°æ®åŠ è½½
    print("Loading data ...")
    all_papers = load_json_papers(TRAIN_JSON)
    print("Splitting data into train and validation sets...")
    train_papers, val_papers = train_test_split(all_papers, test_size=VAL_RATIO, random_state=SEED)
    print(f"Train set size: {len(train_papers)}, Validation set size: {len(val_papers)}")
    
    # 2) æ¨¡åž‹ä¸Ž tokenizer åˆå§‹åŒ–
    tokenizer = build_tokenizer(MODEL_PATH)
    backbone  = build_backbone_model(MODEL_PATH)
    model     = SentenceClassifier(backbone.config, backbone, tokenizer, num_labels=2).to(device)

    # 3) æž„å»º DataLoader
    train_loader = make_dataloader(train_papers, tokenizer, batch_size=BATCH_SIZE,
                                   max_length=MAX_LENGTH, min_sentences=MIN_SENT,
                                   context_size=CONTEXT, shuffle=True)
    val_loader   = make_dataloader(val_papers, tokenizer, batch_size=BATCH_SIZE,
                                   max_length=MAX_LENGTH, min_sentences=MIN_SENT,
                                   context_size=CONTEXT, shuffle=False)

    # 4) ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)

    # 5) è¾“å‡ºç›®å½•
    save_dir = make_experiment_dir(OUT_ROOT, MODEL_PATH, dataset_tag=os.path.basename(TRAIN_JSON), tag=TAG)

    # 6) è®­ç»ƒå¾ªçŽ¯
    best_recall = 0.0
    history = {k: [] for k in ["train_acc","train_prec","train_recall","train_f1","val_acc","val_prec","val_recall","val_f1"]}

    for epoch in range(EPOCHS):
        print(f"\n===== Epoch {epoch+1}/{EPOCHS} =====")
        # 1 è®­ç»ƒ
        tr = train_one_epoch(model, train_loader, optimizer, device, grad_accum_steps=GRAD_ACCUM, use_amp=USE_AMP)
        # 2 è¯„ä¼°
        va, preds = evaluate(model, val_loader, device)

        for k in ["acc","prec","recall","f1"]:
            history[f"train_{k}"].append(tr[k])
            history[f"val_{k}"].append(va[k])

        if va["recall"] > best_recall: # æŠŠè¿™é‡Œæ”¹æˆäº†recall
            best_recall = va["recall"]
            
            # torch.save(model.state_dict(), os.path.join(save_dir, "best_model.pth"))
            # ä»¥ Hugging Face ç›®å½•å½¢å¼ä¿å­˜ï¼ˆbackbone+tokenizerï¼‰ï¼Œåˆ†ç±»å¤´å•ç‹¬å­˜
            hf_dir = os.path.join(save_dir, "best_hf")
            os.makedirs(hf_dir, exist_ok=True)

            # 1) ä¿å­˜ backboneï¼ˆä¼šå†™ config.json + pytorch_model.bin ç­‰ï¼‰
            backbone.save_pretrained(hf_dir)
            # 2) ä¿å­˜ tokenizerï¼ˆä¼šå†™ tokenizer.json/vocab ç­‰ï¼‰
            tokenizer.save_pretrained(hf_dir)
            # 3) ä¿å­˜å¥çº§åˆ†ç±»å¤´ï¼ˆå•ç‹¬ä¸€ä¸ª binï¼‰
            torch.save(model.classifier.state_dict(), os.path.join(hf_dir, "classifier_head.bin"))
            # 4) å†™ä¸€ä»½åˆ†ç±»å¤´å…ƒä¿¡æ¯ï¼Œä¾¿äºŽå¤çŽ°ä¸ŽåŠ è½½
            head_cfg = {
                "num_labels": 2,
                "hidden_size": backbone.config.hidden_size,
                "sep_token_id": tokenizer.sep_token_id,
                "sep_token": tokenizer.sep_token,
                "select_metric": "recall",
                "best_recall": best_recall,
                "epoch": epoch + 1,
                "window_hparams": {
                    "max_length": MAX_LENGTH,
                    "min_sentences": MIN_SENT,
                    "context_size": CONTEXT
                },
                "model_name_or_path": MODEL_PATH,
                "train_json": TRAIN_JSON
            }
            with open(os.path.join(hf_dir, "sentence_head_config.json"), "w", encoding="utf-8") as f:
                json.dump(head_cfg, f, ensure_ascii=False, indent=2)
            
            
            
            
            print(f"[epoch {epoch+1}] ðŸŽ¯ new best Recall={best_recall:.4f} -> saved")
            
            
        # 3 ç»˜å›¾
        plot_history(history, os.path.join(save_dir, "plots"), epoch+1)
        save_predictions_to_json(preds, os.path.join(save_dir, "predictions", f"preds_epoch_{epoch+1}.json"))
        print(f"[epoch {epoch+1}] train={tr}  val={va}")

if __name__ == "__main__":
    main()
