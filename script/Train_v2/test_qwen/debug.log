2025-10-14 19:47:28,064 - INFO - Initializing ResearchDatasetEvaluator
2025-10-14 19:48:28,144 - INFO - Initializing ResearchDatasetEvaluator
2025-10-14 19:48:28,163 - INFO - Starting data loading process
2025-10-14 19:48:28,164 - DEBUG - Processing paper: A Multilingual Multi_Target Dataset for Stance Detection
2025-10-14 19:48:28,165 - DEBUG - Processed 139 sentences for paper A Multilingual Multi_Target Dataset for Stance Detection
2025-10-14 19:48:28,165 - DEBUG - Processing paper: Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation
2025-10-14 19:48:28,165 - DEBUG - Processed 256 sentences for paper Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation
2025-10-14 19:48:28,165 - DEBUG - Processing paper: Amazon_M2_ A Multilingual Multi_locale Shopping Session Dataset for Recommendation and Text Generation
2025-10-14 19:48:28,165 - DEBUG - Processed 121 sentences for paper Amazon_M2_ A Multilingual Multi_locale Shopping Session Dataset for Recommendation and Text Generation
2025-10-14 19:48:28,165 - DEBUG - Processing paper: AntM 2 C_ A Large Scale Dataset For Multi_Scenario Multi_Modal CTR Prediction
2025-10-14 19:48:28,165 - DEBUG - Processed 182 sentences for paper AntM 2 C_ A Large Scale Dataset For Multi_Scenario Multi_Modal CTR Prediction
2025-10-14 19:48:28,165 - DEBUG - Processing paper: AU_AIR_ A Multi_modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance
2025-10-14 19:48:28,165 - DEBUG - Processed 170 sentences for paper AU_AIR_ A Multi_modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance
2025-10-14 19:48:28,165 - DEBUG - Processing paper: AV_NeRF_ Learning Neural Fields for Real_World Audio_Visual Scene Synthesis
2025-10-14 19:48:28,165 - DEBUG - Processed 67 sentences for paper AV_NeRF_ Learning Neural Fields for Real_World Audio_Visual Scene Synthesis
2025-10-14 19:48:28,165 - DEBUG - Processing paper: BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment
2025-10-14 19:48:28,165 - DEBUG - Processed 286 sentences for paper BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment
2025-10-14 19:48:28,165 - DEBUG - Processing paper: Comprehensive Named Entity Recognition on CORD_19 with Distant or Weak Supervision
2025-10-14 19:48:28,165 - DEBUG - Processed 25 sentences for paper Comprehensive Named Entity Recognition on CORD_19 with Distant or Weak Supervision
2025-10-14 19:48:28,165 - DEBUG - Processing paper: DCQA_ DOCUMENT_LEVEL CHART QUESTION ANSWERING TOWARDS COMPLEX REASONING AND COMMON_SENSE UNDERSTANDING
2025-10-14 19:48:28,165 - DEBUG - Processed 179 sentences for paper DCQA_ DOCUMENT_LEVEL CHART QUESTION ANSWERING TOWARDS COMPLEX REASONING AND COMMON_SENSE UNDERSTANDING
2025-10-14 19:48:28,165 - DEBUG - Processing paper: Debate Helps Supervise Unreliable Experts
2025-10-14 19:48:28,165 - DEBUG - Processed 75 sentences for paper Debate Helps Supervise Unreliable Experts
2025-10-14 19:48:28,165 - DEBUG - Processing paper: Distilling Large Language Models for Matching Patients to Clinical Trials
2025-10-14 19:48:28,165 - DEBUG - Processed 91 sentences for paper Distilling Large Language Models for Matching Patients to Clinical Trials
2025-10-14 19:48:28,165 - DEBUG - Processing paper: ECMD_ An Event_Centric Multisensory Driving Dataset for SLAM
2025-10-14 19:48:28,165 - DEBUG - Processed 170 sentences for paper ECMD_ An Event_Centric Multisensory Driving Dataset for SLAM
2025-10-14 19:48:28,165 - DEBUG - Processing paper: Inline Citation Classification using Peripheral Context and Time_evolving Augmentation
2025-10-14 19:48:28,165 - DEBUG - Processed 35 sentences for paper Inline Citation Classification using Peripheral Context and Time_evolving Augmentation
2025-10-14 19:48:28,165 - DEBUG - Processing paper: llm_japanese_dataset v0_ Construction of Japanese Chat Dataset for Large Language Models and its Methodology
2025-10-14 19:48:28,165 - DEBUG - Processed 67 sentences for paper llm_japanese_dataset v0_ Construction of Japanese Chat Dataset for Large Language Models and its Methodology
2025-10-14 19:48:28,165 - DEBUG - Processing paper: Re 3 Dial_ Retrieve_ Reorganize and Rescale Conversations for Long_Turn Open_Domain Dialogue Pre_training
2025-10-14 19:48:28,165 - DEBUG - Processed 108 sentences for paper Re 3 Dial_ Retrieve_ Reorganize and Rescale Conversations for Long_Turn Open_Domain Dialogue Pre_training
2025-10-14 19:48:28,165 - DEBUG - Processing paper: Syn2Real_ Forgery Classification via Unsupervised Domain Adaptation
2025-10-14 19:48:28,165 - DEBUG - Processed 122 sentences for paper Syn2Real_ Forgery Classification via Unsupervised Domain Adaptation
2025-10-14 19:48:28,165 - DEBUG - Processing paper: Topic Shift Detection in Chinese Dialogues_ Corpus and Benchmark
2025-10-14 19:48:28,165 - DEBUG - Processed 146 sentences for paper Topic Shift Detection in Chinese Dialogues_ Corpus and Benchmark
2025-10-14 19:48:28,165 - DEBUG - Processing paper: Towards Equitable Representation in Text_to_Image Synthesis Models with the Cross_Cultural Understanding Benchmark _CCUB_ Dataset
2025-10-14 19:48:28,165 - DEBUG - Processed 47 sentences for paper Towards Equitable Representation in Text_to_Image Synthesis Models with the Cross_Cultural Understanding Benchmark _CCUB_ Dataset
2025-10-14 19:48:28,165 - INFO - Starting evaluation of 18 papers
2025-10-14 19:48:28,165 - INFO - Evaluating paper 1/18: A Multilingual Multi_Target Dataset for Stance Detection
2025-10-14 19:48:28,165 - INFO - Starting model prediction
2025-10-14 19:48:28,165 - INFO - Attempt 1 of 5
2025-10-14 19:48:28,243 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-576b4344-c139-4523-af0b-76dbd53d0db0', 'json_data': {'messages': [{'role': 'system', 'content': '你是一个专门用于识别学术论文中数据集描述的助手。你需要判断每个输入的句子是否描述了研究中使用的数据集。请确保：\n    1. 只输出JSON格式的结果\n    2. 结果格式必须为 {"labels": [0,1,0,...]}\n    3. 不要输出任何其他文字或解释'}, {'role': 'user', 'content': '你是一位专业的学术论文数据集描述识别专家。请仔细分析下面这篇论文中的每个句子，判断它们是否描述了本研究特定构建的数据集信息。\n\n    论文标题: A Multilingual Multi_Target Dataset for Stance Detection\n\n    判断标准:\n    判断为1(数据集描述)的标准：\n    句子必须直接描述本研究特定构建的数据集,包括:\n    1. 该数据集的构建过程和方法\n    2. 该数据集的具体构成和规模\n    3. 该数据的来源和收集方式\n    4. 该数据的预处理步骤\n    5. 该数据集的可获取方式(如发布地址)\n    6. 或者其他直接或者间接描述了该数据集的句子\n\n    所有其他类型的句子均标记为0。\n    \n    请对每个句子进行分析，返回一个JSON格式的标签数组。格式要求：\n    1. 必须是有效的JSON格式\n    2. 只包含labels字段，值为0和1组成的数组\n    3. 数组长度必须与句子数量相同\n    4. 示例格式：{"labels": [0,1,0,1,...]}\n\n    需要判断的句子：\n\n    1. Abstract: We extract a large-scale stance detection dataset from comments written by candidates of elections in Switzerland.\n2. The dataset consists of German, French and Italian text, allowing for a cross-lingual evaluation of stance detection.\n3. It contains 67 000 comments on more than 150 political issues (targets).\n4. Unlike stance detection models that have specific target issues, we use the dataset to train a single model on all the issues.\n5. To make learning across targets possible, we prepend to each instance a natural question that represents the target (e.g."Do you support X?").\n6. Baseline results from multilingual BERT show that zero-shot crosslingual and cross-target transfer of stance detection is moderately successful with this approach.\n7. In recent years many datasets have been created for the task of automated stance detection, advancing natural language understanding systems for political science, opinion research and other application areas.\n8. Typically, such benchmarks(Mohammad et al., 2016a)are composed of short pieces of text commenting on politicians or public issues and are manually annotated with their stance towards a target entity (e.g.Climate Change, or Trump).\n9. However, they are limited in scope on multiple levels(Küçük and Can, 2020).\n10. First of all, it is questionable how well current stance detection methods perform in a crosslingual setting, as the multilingual datasets avail-able today are relatively small, and specific to a single target(Taulé et al., 2017(Taulé et al., , 2018)).\n11. Furthermore, specific models tend to be developed for each single target or pair of targets(Sobhani et al., 2017).\n12. Concerns have been raised that cross-target performance is often considerably lower than fully supervised performance(Küçük and Can, 2020).\n13. In this paper we propose a much larger dataset that combines multilinguality and a multitude of topics and targets.\n14. X-stance comprises more than 150 questions about Swiss politics and more than 67k answers given by candidates running for political office in Switzerland.\n15. Questions are available in four languages: English, Swiss Standard German, French, and Italian.\n16. The language of a comment depends on the candidate\'s region of origin.\n17. We have extracted the data from the voting advice application Smartvote.\n18. Candidates respond to questions mainly in categorical form (yes / rather yes / rather no / no).\n19. They can also submit a freetext comment to justify or explain their categorical answer.\n20. An example is given in Figure1.\n21. We transform the dataset into a stance detection task by interpreting the question as a naturallanguage representation of the target, and the commentary as the input to be classified.\n22. The dataset is split into a multilingual training set and into several test sets to evaluate zeroshot cross-lingual and cross-target transfer.\n23. To provide a baseline, we fine-tune a multilingual BERT model(Devlin et al., 2019)on X-stance.\n24. We show that the baseline accuracy is comparable to previous stance detection benchmarks while leaving ample room for improvement.\n25. In addition, the model can generalize to a degree both crosslingually and in a cross-target setting.\n26. We have made the dataset and the code for reproducing the baseline models publicly available.\n27. Figure1: Example of a question and two answers in the X-stance dataset.\n28. The answers were submitted by electoral candidates on a voting advice website.\n29. The author of the German comment was in favor of the issue; the author of the French comment against.\n30. Both authors use comments to explain their respective stance.\n31. Provenance We downloaded the questions and answers via the Smartvote API 2 .\n32. The downloaded data cover 175 communal, cantonal and national elections between 2011 and 2020.\n33. All candidates in an election who participate in Smartvote are asked the same set of questions, but 2 https://smartvote.chdepending on the locale they see translated versions of the questions.\n34. They can answer each question with either \'yes\', \'rather yes\', \'rather no\', or \'no\'.\n35. They can supplement each answer with a comment of at most 500 characters.\n36. The questions asked on Smartvote have been edited by a team of political scientists.\n37. They are intended to cover a broad range of political issues relevant at the time of the election.\n38. A detailed documentation of the design of Smartvote and the editing process of the questions is provided byThurman and Gasser (2009).\n39. Preprocessing We merged the two labels on each pole into a single label: \'yes\' and \'rather yes\' were combined into \'favor\'; \'rather no\', or \'no\' into \'against\'.\n40. This improves the consistency of the data and the comparability to previous stance detection datasets.\n41. We did not further preprocess the text of the comments.\n42. Language Identification As the API does not provide the language of comments, we employed a language identifier to automatically annotate this information.\n43. We used the langdetect library(Shuyo, 2010).\n44. For each responder we classified all the comments jointly, assuming that responders did not switch code during the answering of the questionnaire.\n45. We applied the identifier in a two-step approach.\n46. In the first run we allowed the identifier to output all 55 languages that it supports out of the box, plus Romansh, the fourth official language in Switzerland3.\n47. We found that no Romansh comments were detected and that all unexpected outputs were misclassifications of German, French or Italian comments.\n48. We further concluded that little or no Swiss German comments are in the dataset; otherwise, some of them would have manifested themselves via misclassifications (e.g. as Dutch).\n49. In the second run, drawing from these conclusions, we restricted the identifier\'s set of choices to English, French, German and Italian.\n50. Filtering We pre-filtered the questions and answers to improve the quality of the dataset.\n51. In the right column the model encounters unseen answers to unseen questions within an unseen topic.\n52. The two test sets in parentheses are too small for a significant evaluation. questions and corresponding answers pertaining to national elections were included.\n53. In the context of communal and cantonal elections, candidates have answered both local questions and a subset of the national questions.\n54. Of those elections, we only considered answers to the questions that also had been asked in a national election.\n55. They were only used to augment the training set while the validation and test sets were restricted to answers from national elections.\n56. We discarded the fewer than 20 comments classified as English.\n57. Furthermore, we discarded instances that met any of the following conditions: • Question is not a closed question or does not address a clearly defined political issue.\n58. • No comment was submitted by the candidate or the comment is shorter than 50 characters.\n59. • Comment starts with "but" or a similar indicator that the comment is not self-contained.\n60. • Comment contains a URL.\n61. In total, a fifth of the comments were filtered out.\n62. Topics The questions have been organized by the Smartvote editors into categories (such as "Economy").\n63. We further consolidated the predefined categories into 12 broad topics (Table1).\n64. Compliance The dataset is shared under a CC BY-NC 4.0 license.\n65. Copyright remains with www.smartvote.ch.\n66. Given the sensitive nature of the data, we increase the anonymity of the data by hashing the respondents\' IDs.\n67. No personal attributes of the respondents are included in the dataset.\n68. We provide a data statement(Bender and Friedman, 2018)in Appendix B.\n69. We held out the topics "Healthcare" and "Political System" from the training data and created a separate cross-topic test set that contains the questions and answers related to those topics.\n70. Furthermore, in order to test cross-question generalization performance within previously seen topics, we manually selected 16 held-out questions that are distributed over the remaining 10 topics.\n71. We selected the held-out questions manually because we wanted to make sure that they are truly unseen and that no paraphrases of the questions are found in the training set.\n72. We designated Italian as a test-only language, since relatively few comments have been written in Italian.\n73. From the remaining German and French data we randomly selected a percentage of respondents as validation or as test respondents.\n74. As a result we received one training set, one validation set and four test sets.\n75. The sizes of the sets are listed in Table 2.\n76. We did not consider test sets that are cross-lingual and cross-target at the same time, as they would have been too small to yield significant results.\n77. We evaluate four baselines to obtain an impression of the difficulty of the task.\n78. The first pair of baselines uses the most frequent class in the training set for prediction.\n79. Specifically, the global majority class baseline predicts the most frequent class across all training targets while the target-wise majority class baseline predicts the class that is most frequent for a given target question.\n80. The latter can only be applied to the intra-target test sets.\n81. As a second baseline, we train a fastText bag-ofwords linear classifier(Joulin et al., 2017).\n82. For each comment, we select the translation of the question that matches its language, and concatenate it to the comment.\n83. We tokenize the text using the Europarl preprocessing tools(Koehn, 2005).\n84. The \'against\' class was slightly upsampled in the training data so that the classes are balanced when summing over all questions and topics.\n85. We use the standard settings provided by the fastText library.\n86. The word vectors were set to a size of 300.\n87. We do not initialize them with pre-trained multilingual embeddings since preliminary experiments did not show a beneficial effect.\n88. As our main baseline model we fine-tune multilingual BERT (M-BERT) on the task(Devlin et al., 2019)which has been pre-trained jointly in 104 languages 5 and has established itself as a state of the art for various multilingual tasks(Wu and Dredze, 2019;Pires et al., 2019).\n89. Within the field of stance detection, BERT can outperform both feature-based and other neural approaches in a monolingual English setting(Ghosh et al., 2019).\n90. Architecture In the context of BERT we interpret the X-stance task as sequence pair classification inspired by natural language inference tasks(Bowman et al., 2015).\n91. We follow the procedure outlined byDevlin et al. (2019)for such tasks.\n92. We designate the question as segment A and the comment as segment B.\n93. The two segments are separated with the special token [SEP], and the special token [CLS] is prepended to the sequence.\n94. The final hidden state corresponding to [CLS] is then classified by a linear layer.\n95. We fine-tune the full model with a cross-entropy loss, using the AllenNLP library(Gardner et al., 2018)as a basis for our implementation.\n96. Training As above, we balanced out the number of classes in the training set.\n97. We use a batch size of 16 and a maximum sequence length of 512 subwords, and performed a grid search over the following hyperparameters based on the validation accuracy: • Learning rate: 5e-5, 3e-5, 2e-5 No Italian samples were seen during training, making this a case of zero-shot cross-lingual transfer.\n98. The scores are reported as the macro-average of the F1scores for \'favor\' and for \'against\'.\n99. The grid search was repeated independently for every variant that we test in the following subsections.\n100. Furthermore, the standard recommendations for fine-tuning BERT were used: Adam with β 1 = 0.9 and β 2 = 0.999; an L2 weight decay of 0.01; a learning rate warmup over the first 10% of the steps; and a linear decay of the learning rate.\n101. A dropout probability of 0.1 was set on all layers.\n102. Results Table3shows the results for the crosslingual setting.\n103. M-BERT performs consistently better than the previous baselines.\n104. Even the zeroshot performance in Italian, while significantly lower than the supervised scores, is much better than the target-wise majority class baseline.\n105. Results for the cross-target setting are given in Table4.\n106. Similar to the cross-lingual setting, model performance drops in the cross-target setting, but M-BERT remains the strongest baseline and easily surpasses the majority class baselines.\n107. Furthermore, the cross-question score of M-BERT is slightly lower than the cross-topic score.\n108. The default setup preserves horizontal language consistency in that the language of the questions always corresponds to the language of the comments.\n109. For example, the Italian test instances are combined with the Italian version of the questions, even though during training the model has only ever seen the German and French version of them.\n110. An alternative concept is vertical language consistency, whereby the questions are consistently presented in one language, regardless of the comment.\n111. To test whether horizontal or vertical consistency is more helpful, we train and evaluate M-BERT on a dataset variant where all questions are in their English version.\n112. We chose English as a lingua franca because it had the largest share of data during the pre-training of M-BERT.\n113. Cross-topic Results are shown in Table5.\n114. While the effect is negligible in most settings, cross-lingual performance increases when all questions are in English.\n115. In order to rule out that only the questions or only the comments are necessary to optimally solve the task, we conduct some additional experiments: • Only use a single segment containing the comment, removing the questions from the training and test data (missing questions).\n116. • Only use the question and remove the comment (missing comments).\n117. In both cases the performance decreases across all evaluation settings (Table5).\n118. The loss in performance is much higher when comments are missing, indicating that the comments contain the most important information about stance.\n119. As can be expected, the score achieved without comments is only slightly different from the target-wise majority class baseline.\n120. But there is also a loss in performance when the questions are missing, which underlines the importance of pairing both pieces of text.\n121. The effect of missing questions is especially strong in the supervised and cross-lingual settings.\n122. To illustrate this, we provide in TableA8some examples of comments that occur with multiple different targets in the training set.\n123. Those examples can explain why the target can be essential for disambiguating a stance detection problem.\n124. On the other hand, the effect of omitting the questions is less pronounced in the cross-target settings.\n125. The above single-segment experiments tell us that both the comment and the question provide crucial information.\n126. But it is possible that the M-BERT model, even though trained on both segments, mainly looks at a single segment at test time.\n127. To rule this out, we probe the model with randomized data at test time: • Test the model on versions of the test sets where the comments remain in place but the questions are shuffled randomly (random questions).\n128. We make sure that the random questions come from the same test set and language as the original questions.\n129. • Keep the questions in place and randomize the comments (random comments).\n130. Again we shuffle the comments only within test set boundaries.\n131. The results in Table5show that the performance of the model decreases in both cases, confirming that it learns to take into account both segments.\n132. 4.6 How Important are Spelled-Out Targets?\n133. Finally we test whether the target really needs to be represented by natural language (e.g."Do you support X?").\n134. An alternative is to represent the target with a trainable embedding instead.\n135. In order to fit target embeddings smoothly into our architecture, we represent each target type with a different reserved symbol from the M-BERT vocabulary.\n136. Segment A is then set to this symbol instead of a natural language question.\n137. The results for this experiment are listed in the bottom row of Table5.\n138. An M-BERT model that learns target embeddings instead of encoding a question performs clearly worse in the supervised and cross-lingual settings.\n139. From this we conclude that spelled-out natural language questions provide important linguistic detail that can help in stance detection.\n\n请严格按照JSON格式返回：{"labels": [0,1,0,...]}，不要包含任何其他文字。'}], 'model': 'qwen-plus', 'response_format': {'type': 'json_object'}}}
2025-10-14 19:48:28,248 - DEBUG - Sending HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
2025-10-14 19:48:28,248 - DEBUG - connect_tcp.started host='dashscope.aliyuncs.com' port=443 local_address=None timeout=5.0 socket_options=None
2025-10-14 19:48:28,251 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7fb27ca6e340>
2025-10-14 19:48:28,251 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x7fb27d3d5640> server_hostname='dashscope.aliyuncs.com' timeout=5.0
2025-10-14 19:48:28,340 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7fb27ca6e400>
2025-10-14 19:48:28,340 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-10-14 19:48:28,341 - DEBUG - send_request_headers.complete
2025-10-14 19:48:28,341 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-10-14 19:48:28,341 - DEBUG - send_request_body.complete
2025-10-14 19:48:28,341 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-10-14 19:48:33,641 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'vary', b'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding'), (b'x-request-id', b'2cd5ffd9-2909-4507-842b-a3077f522ce0'), (b'x-dashscope-call-gateway', b'true'), (b'content-type', b'application/json'), (b'req-cost-time', b'5266'), (b'req-arrive-time', b'1760442499669'), (b'resp-start-time', b'1760442504935'), (b'x-envoy-upstream-service-time', b'5230'), (b'set-cookie', b'acw_tc=2cd5ffd9-2909-4507-842b-a3077f522ce08c2e8d99dd77e0e83ef8f37a7fde1cd8;path=/;HttpOnly;Max-Age=1800'), (b'content-encoding', b'gzip'), (b'date', b'Tue, 14 Oct 2025 11:48:24 GMT'), (b'server', b'istio-envoy'), (b'transfer-encoding', b'chunked')])
2025-10-14 19:48:33,642 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-14 19:48:33,642 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-10-14 19:48:33,643 - DEBUG - receive_response_body.complete
2025-10-14 19:48:33,643 - DEBUG - response_closed.started
2025-10-14 19:48:33,643 - DEBUG - response_closed.complete
2025-10-14 19:48:33,643 - DEBUG - HTTP Response: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "200 OK" Headers({'vary': 'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding', 'x-request-id': '2cd5ffd9-2909-4507-842b-a3077f522ce0', 'x-dashscope-call-gateway': 'true', 'content-type': 'application/json', 'req-cost-time': '5266', 'req-arrive-time': '1760442499669', 'resp-start-time': '1760442504935', 'x-envoy-upstream-service-time': '5230', 'set-cookie': 'acw_tc=2cd5ffd9-2909-4507-842b-a3077f522ce08c2e8d99dd77e0e83ef8f37a7fde1cd8;path=/;HttpOnly;Max-Age=1800', 'content-encoding': 'gzip', 'date': 'Tue, 14 Oct 2025 11:48:24 GMT', 'server': 'istio-envoy', 'transfer-encoding': 'chunked'})
2025-10-14 19:48:33,643 - DEBUG - request_id: 2cd5ffd9-2909-4507-842b-a3077f522ce0
2025-10-14 19:48:33,647 - DEBUG - API request completed in 5.48 seconds
2025-10-14 19:48:33,647 - DEBUG - Raw model response: {"labels": [1,1,1,0,1,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]}
2025-10-14 19:48:33,647 - INFO - Successfully processed 100 labels
2025-10-14 19:48:33,647 - ERROR - Label count mismatch for A Multilingual Multi_Target Dataset for Stance Detection
2025-10-14 19:48:33,647 - INFO - Evaluating paper 2/18: Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation
2025-10-14 19:48:33,647 - INFO - Starting model prediction
2025-10-14 19:48:33,647 - INFO - Attempt 1 of 5
2025-10-14 19:48:33,648 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-ec4ffff3-91b1-4518-ba32-18ce7e010b62', 'json_data': {'messages': [{'role': 'system', 'content': '你是一个专门用于识别学术论文中数据集描述的助手。你需要判断每个输入的句子是否描述了研究中使用的数据集。请确保：\n    1. 只输出JSON格式的结果\n    2. 结果格式必须为 {"labels": [0,1,0,...]}\n    3. 不要输出任何其他文字或解释'}, {'role': 'user', 'content': '你是一位专业的学术论文数据集描述识别专家。请仔细分析下面这篇论文中的每个句子，判断它们是否描述了本研究特定构建的数据集信息。\n\n    论文标题: Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation\n\n    判断标准:\n    判断为1(数据集描述)的标准：\n    句子必须直接描述本研究特定构建的数据集,包括:\n    1. 该数据集的构建过程和方法\n    2. 该数据集的具体构成和规模\n    3. 该数据的来源和收集方式\n    4. 该数据的预处理步骤\n    5. 该数据集的可获取方式(如发布地址)\n    6. 或者其他直接或者间接描述了该数据集的句子\n\n    所有其他类型的句子均标记为0。\n    \n    请对每个句子进行分析，返回一个JSON格式的标签数组。格式要求：\n    1. 必须是有效的JSON格式\n    2. 只包含labels字段，值为0和1组成的数组\n    3. 数组长度必须与句子数量相同\n    4. 示例格式：{"labels": [0,1,0,1,...]}\n\n    需要判断的句子：\n\n    1. Abstract: Digital humans have witnessed extensive applications in various domains, necessitating related quality assessment studies.\n2. However, there is a lack of comprehensive digital human quality assessment (DHQA) databases.\n3. To address this gap, we propose SJTU-H3D, a subjective quality assessment database specifically designed for full-body digital humans.\n4. It comprises 40 high-quality reference digital humans and 1,120 labeled distorted counterparts generated with seven types of distortions.\n5. The SJTU-H3D database can serve as a benchmark for DHQA research, allowing evaluation and refinement of processing algorithms.\n6. Further, we propose a zero-shot DHQA approach that focuses on no-reference (NR) scenarios to ensure generalization capabilities while mitigating database bias.\n7. Our method leverages semantic and distortion features extracted from projections, as well as geometry features derived from the mesh structure of digital humans.\n8. Specifically, we employ the Contrastive Language-Image Pre-training (CLIP) model to measure semantic affinity and incorporate the Naturalness Image Quality Evaluator (NIQE) model to capture low-level distortion information.\n9. Additionally, we utilize dihedral angles as geometry descriptors to extract mesh features.\n10. By aggregating these measures, we introduce the Digital Human Quality Index (DHQI), which demonstrates significant improvements in zeroshot performance.\n11. The DHQI can also serve as a robust baseline for DHQA tasks, facilitating advancements in the field.\n12. The database and the code are available at https://github.com/zzc-1998/SJTU-H3D.\n13. Fig.1.Motovation of our works.\n14. Unlike 2D images/videos, collection of 3D digital humans is more difficult and expensive.\n15. Therefore there is a lack of subjective databases for 3D digital humans currently.\n16. To tackle this issue, we propose the first perceptual quality assessment database for full-body digital humans (SJTU-H3D).\n17. Furthermore, in contrast to the numerous large-scale image/video quality assessment (I/VQA) databases that facilitate data-driven methodologies, supervised methods can be easily bothered by the bias of the limited number of DHQA databases, affecting generalization ability.\n18. Thus we propose a zero-shot no-reference quality assessment method to address this concern. humans.\n19. Regrettably, acquisition of digital human models is a laborious and costly process compared with 2D media such as images and videos, requiring specialized three-dimensional (3D) scanning devices and professional post-production, which makes it quite difficult to carry out digital human quality assessment (DHQA) databases.\n20. Therefore, few works about subjective DHQA have been carried out in the literature.\n21. Then the absence of large-scale subjective experiments for assessing the visual quality of digital humans further hinders progress in this domain.\n22. Therefore, in this paper, we propose a comprehensive subjective quality assessment database (SJTU-H3D) targeted at digital humans, aiming to address this research gap and contribute to the advancement of DHQA.\n23. The SJTU-H3D database introduced in this study comprises 40 high-quality reference digital humans, represented by textured meshes in full-body format, and the database includes 1,120 distorted digital humans that have been generated using seven different types of distortions.\n24. The perceptual mean opinion scores (MOSs) of these distorted digital humans are collected through a meticulously controlled subjective experiment.\n25. Notably, the SJTU-H3D database is the first large-scale database specifically designed for digital human quality assessment (DHQA) that focuses on full-body representations.\n26. The primary objective of this database is to advance the research and development of DHQA within the scientific community.\n27. Furthermore, it serves as an ideal platform for evaluating and refining various processing algorithms, including but not being limited to denoising and compression techniques.\n28. By providing a comprehensive database consisting of high-quality reference models and distorted counterparts, the proposed SJTU-H3D database offers researchers and practitioners an opportunity to explore and enhance their DHQA methodologies.\n29. The availability of such a resource is expected to significantly contribute to the growth and advancement of the DHQA research community.\n30. During recent years, data-driven image and video quality assessment (I/VQA) approaches[2],[3],[4],[5]have garnered significant attention and have demonstrated remarkable performance in various application domains.\n31. The success of these approaches can be partly attributed to the availability of largescale I/VQA databases such as the SPAQ database (containing 11,125 labeled images)[6]and the LSVQ database (comprising up to 38,811 annotated videos)[7].\n32. These databases have also contributed to ensuring the generalization capability and robustness of data-driven methods.\n33. However, in the realm of DHQA research, the availability of suitable perceptual quality assessment databases is limited.\n34. With the exception of the proposed SJTU-H3D database, only one perceptual quality assessment database, DHHQA[8], focusing solely on digital human heads rather than full-body representations, exists.\n35. This scarcity of databases makes it challenging to develop datadriven DHQA methods and ensure their generalization ability in practical scenarios.\n36. Hence, this challenge serves as a motivation for us to devise a zero-shot DHQA method that does not necessitate training on labeled DHQA databases.\n37. To cater to most practical applications where pristine references may not be readily available, our focus is only on no-reference (NR) methods.\n38. To extract both semantic and distortion features for evaluating the visual quality of digital humans, we employ projection rendering techniques.\n39. From a semantic perspective, we utilize the Contrastive Language-Image Pre-training (CLIP) model[9]to measure the correlation between the input projections and quality-related texts.\n40. Our hypothesis is that high-quality digital human projections should exhibit a strong correlation with positive quality-related texts and a weak correlation with negative ones.\n41. To determine the quality levels of the input projections, we design several positive-negative text pairs.\n42. The semantic affinity quality measure is then derived by computing the difference in affinity between positive and negative texts.\n43. However, CLIP operates on low-resolution images, which limits its ability to capture low-level distortion information.\n44. To address this limitation, we incorporate the completely blind Naturalness Image Quality Evaluator (NIQE)[10]to extract low-level quality representations from the raw resolution.\n45. To further enhance the accuracy of quality prediction, we also extract features from the mesh modality.\n46. For robustness and effectiveness, we choose the dihedral angle as the geometry descriptor, as it has been widely recognized for effectively capturing geometric features relevant to visual quality[11],[12],[13],[14]and its values are confined within the range of [0, π].\n47. By analyzing the changing tendency of dihedral angles corresponding to geometry compression and simplification levels, we average-pool the dihedral angles to derive the geometry loss quality measure.\n48. Finally, all three quality measures (semantic affinity quality measure, spatial naturalness quality measure, and geometry loss quality measure) are aggregated using a sum function to form the proposed Digital Human Quality Index (DHQI).\n49. Experimental results demonstrate that DHQI significantly improves zero-shot performance and even achieves competitiveness with supervised methods.\n50. In summary, our contributions are as follows: • We propose the first large-scale full-body DHQA database, SJTU-H3D, which consists of 40 high-quality digital humans represented by textured meshes and 1,120 distorted digital humans generated by 7 types of distortions.\n51. 40 human subjects are invited and a total of 44,800 ratings are collected to gather the mean opinion scores (MOSs) for 1,120 distorted digital humans.\n52. In this section, we give a brief introduction to the development of 3D model quality assessment (3DQA) and noreference image quality assessment (NR-IQA) methods.\n53. A. 3DQA Development 1) 3DQA Databases: Early subjective 3D quality assessment (3DQA) databases primarily employ colorless point clouds and are relatively small in scale[20],[21],[22].\n54. However, recent efforts have been directed towards addressing the challenge of assessing visual quality in colored 3D models, resulting in the development of substantial 3DQA databases[20],[21],[22],[15],[16],[17],[18],[19].\n55. A detailed comparison between these databases and the proposed database is presented in TableI.\n56. From the table, it is evident that the recent 3DQA databases, with the exception of DHHQA, encompass general 3D objects and do not specifically focus on 3D digital humans.\n57. Although the DHHQA database comprises real human heads, it neglects the consideration of the body part.\n58. This highlights the significance of the proposed SJTU-H3D database.\n59. 2) 3DQA Methods: In the field of 3D quality assessment (3DQA), metrics can be broadly categorized into model-based and projection-based methods.\n60. Model-based methods[23],[24],[11],[25],[26],[27],[28],[29],[30],[31]involve extracting features directly from the 3D model, which offers the advantage of being viewpoint-invariant and relatively straightforward.\n61. However, due to the inherent complexity of 3D models, these methods can be computationally expensive and time-consuming.\n62. On the other hand, projection-based methods[15],[32],[14],[33],[34]infer the visual quality of a 3D model based on its corresponding projections.\n63. These methods leverage mature and successful 2D media analysis tools, which often lead to excellent performance.\n64. However, projection-based methods are highly dependent on the selection of viewpoints and can be susceptible to instability when subjected to various rendering setups.\n65. More recently,[36], which utilizes natural scene statistics (NSS) in the spatial domain to analyze image quality.\n66. CPBD[37]estimates blur levels by computing the cumulative probability of blur detection.\n67. BMPRI[38]predicts image quality by generating multiple pseudo-reference images obtained through further degradation of the distorted image and comparing their similarities.\n68. NFERM[39]investigates image quality using the free energy principle.\n69. Deep learning-based IQA methods have gained momentum with the advancement of deep neural networks.\n70. DBCNN[40]consists of two streams of deep neural networks to address both synthetic and authentic distortions.\n71. HyperIQA[41]employs a self-adaptive hyper network to handle challenges arising from distortion diversity and content variation in IQA tasks.\n72. MUSIQ[42]utilizes a multi-scale image quality transformer to represent image quality at different levels of granularity.\n73. StairIQA[43]hierarchically integrates features extracted from intermediate layers to leverage low-level and high-level visual information.\n74. 2) Zero-shot NR-IQA: Zero-shot IQA methods, also known as opinion-unaware methods, have emerged, which do not rely on training on subjective-rated quality assessment databases and can operate on unseen images.\n75. The earliest zero-shot NR-IQA methods are NIQE[10]and IL-NIQE[44].\n76. NIQE extracts handcrafted natural scene statistics (NSS) features from raw-resolution images and quantifies naturalness quality by computing the Multivariate Gaussian (MVG) distance to high-quality images.\n77. IL-NIQE enhances the feature set by incorporating additional quality-aware features, including gradient features, log Gabor filter responses, and color statistics.\n78. In this section, we mainly present the construction details of the proposed SJTU-H3D database, which includes reference collection, reference characterization, distortion generation, and subjective experiment.\n79. In order to ensure the visual quality and content diversity of the reference 3D digital humans, a manual selection process is conducted to choose all reference digital humans from the HumanAlloy1, a wonderful platform that provides high-quality 3D humans.\n80. A total of 40 digital humans are purchased and collected for this study.\n81. These digital humans are represented as textured meshes, with texture resolutions of 2048×2048.\n82. Fig.2illustrates the rendered projections of the selected digital humans, and Table II provides detailed information regarding the number of vertices and faces for each model.\n83. The primary objective of our study is to curate a database that exhibits high diversity and generality while minimizing biases associated with the selection of source models.\n84. 1) Geometry Information: In the domain of image quality assessment (IQA), the analysis of spatial information often involves computing the standard deviation of the Sobel-filtered image.\n85. Motivated by this concept, we propose a novel approach to quantify the geometry information by utilizing the standard deviation of the dihedral angles in a mesh.\n86. The dihedral angle is a fundamental metric employed in computer graphics and geometric modeling to characterize the shape and curvature of meshes[11],[12], thus drawing a parallel to the Sobel-filtering process in image analysis.\n87. It denotes the angle between two neighboring faces that share an edge within the mesh, providing valuable insights into the smoothness or sharpness of the surface.\n88. Specifically, the geometry information can be obtained as: where GI represents the geometry information and std(•) stands for the standard deviation function.\n89. By leveraging the standard deviation of dihedral angles, we aim to capture and assess the geometric characteristics of the mesh, enabling a more comprehensive evaluation of its structure and shape.\n90. 2) Colorfulness: To evaluate the color characteristics, we focus solely on the texture map.\n91. Following the common color calculation process[45],[46], we first convert the texture from RGB channels to LAB channels and combine the standard deviation of A and B channels, which can be mathematically expressed as: where CF represents the colorfulness measure, A and B denote the corresponding color channels of the texture.\n92. 3) Characterization Visualization: We apply the extracted geometry information and colorfulness measure to the collection of 40 reference digital humans.\n93. The results are visualized in Fig.3.\n94. The analysis demonstrates that the selected reference 3D digital humans exhibit a wide spectrum of geometry information and colorfulness.\n95. Notably, model #24 positioned in the top-right corner showcases intricate geometry details and vibrant colorfulness.\n96. In contrast, model #15 portrays simpler geometry information and relatively subdued colorfulness.\n97. The proposed measures thoroughly capture the distinctiveness of 3D digital humans concerning their geometry and color characteristics.\n98. It is important to emphasize that these measures are directly computed from the underlying model files, thereby ensuring their stability and viewpoint invariance.\n99. To account for the common sources of distortion, we incorporate distortions arising from both the generation process and the transmission process.\n100. During the generation process, we consider geometry noise resulting from erroneous scanning procedures, as well as color noise introduced by cameras.\n101. Furthermore, compression and simplification techniques are widely employed during the transmission process.\n102. Hence, these factors are also taken into consideration in our assessment.\n103. By considering the full range of distortion sources, we aim to provide a comprehensive evaluation of the quality of 3D digital humans.\n104. Therefore, to degrade the quality of the reference 3D digital humans, we apply seven types of distortions and the specific settings for each distortion type are listed in TableIII.\n105. We manually select the distortion parameters to cover most visual quality range and the details are illustrated as follows: • Geometry Noise (GN): Gaussian noise with standard deviations σ g of 0.05, 0.1, 0.15, and 0.2 is added to the vertices\' geometry coordinates of the digital humans.\n106. In accordance with the recommended procedure outlined in[15],[16], passive watching is chosen over interactive watching for the subjective experiment to mitigate potential viewing bias.\n107. The 3D digital humans are rendered into video sequences for exhibition purposes.\n108. The open3d library is utilized to generate the projections[52].\n109. The rendering window is configured with a resolution of 1080 × 1920.\n110. To capture the video frames, a horizontal and a vertical circle are employed as the predefined camera paths.\n111. Each 3D digital human is captured at one frame every 3 degree, resulting in a total of 240 frames (360 × 2 ÷ 3).\n112. These frames are then compiled into an 8-second video with a framerate of 30 frames per second.\n113. This approach ensures that the viewers can effectively perceive the significant quality information.\n114. The rendering process is depicted in Fig.5.\n115. 2) Experiment Process: A total of 40 human subjects, comprising 20 males and 20 females, are recruited to participate in the subjective experiment.\n116. Prior to the experiment, a training session is conducted, wherein additional videos generated using the same aforementioned process are presented to familiarize the subjects with the tasks.\n117. The rating process takes place within a well-controlled laboratory environment, maintaining a normal level of illumination.\n118. The viewers are seated at a distance of twice the screen height.\n119. The videos are displayed on an iMac monitor capable of supporting resolutions up to 4096×2304.\n120. The order of video presentations is randomized.\n121. To facilitate the evaluation process, a double stimuli strategy is employed, where the reference and distorted videos are simultaneously displayed on the screen.\n122. The rating interface is excited in Fig.5and the quality score ranges from 0 to 5.\n123. In order to mitigate viewer fatigue, the entire experiment is divided into 20 sessions, with each session featuring 56 digital humans.\n124. Ultimately, a total of 44,800 subjective ratings (1, 120 × 40) are collected.\n125. 3) Subjective Data Analysis: After the subjective experiment, we calculate the z-scores from the raw ratings as follows: where and N i is the number of digital humans judged by subject i.\n126. 500-13[53]standard, ratings from unreliable subjects are excluded from the analysis.\n127. The corresponding z-scores are linearly rescaled to the range of [0, 5].\n128. Finally, the mean opinion scores (MOSs) are computed by averaging the rescaled z-scores.\n129. Fig.6illustrates the distribution of MOSs and the corresponding probability distributions for different distortion types.\n130. Interestingly, the probability distributions reveal that visual quality is less sensitive to varying levels of FS distortions compared to other distortion types.\n131. Even when reducing the face numbers to a ratio of 0.05 (only about 2k faces are preserved), the visual quality score remains higher than other distortions with similar levels.\n132. This observation indicates that visual quality is relatively resilient to FS distortions, implying that the reduction in face complexity may not significantly impact the perceived quality.\n133. In this section, we introduce the three indexes that make up the whole proposed digital human quality index (DHQI), which includes the text-prompted semantic affinity quality measure, spatial naturalness quality measure, and geometry loss quality measure.\n134. These three indexes are then aligned and aggregated into the proposed DHQI quality index.\n135. The framework is exhibited in Fig.7.\n136. We acquire the cube-like projection set of the given digital human as follows: P = ψ(DH), where P represents the set of the 6 rendered projections and ψ(•) stands for the rendering process.\n137. Such rendering process has been employed in the popular point cloud compression standard MPEG VPCC[54]and many other 3DQA works[15],[34].\n138. The projections are utilized as the input information for the text-prompted semantic affinity and spatial naturalness measure.\n139. To assess the perception of quality related to semantic content, specifically evaluating the quality of contents and the ability to discern semantic distortions, we design the text-prompted semantic affinity quality measure.\n140. Inspired by CLIP[9]-based quality assessment tasks[55],[56], we hold the hypothesis that the projections of the high-quality digital humans should have higher affinity with positive qualityrelated descriptions (e.g.good, perfect) and lower affinity with negative quality-related descriptions (e.g.bad, distorted).\n141. 1) Text Prompt Format: In accordance with the official recommendation provided by CLIP[9]and drawing from established practices, our text prompts are designed as a concatenation of three components: a prefix, a description, and a suffix.\n142. To be more precise, the text prompt T corresponding to the raw description D is defined as: T = "a" + D + "projection of 3d human model",(5)where the suffix "projection of 3d human model" is specifically designed to fit the task of DHQA.\n143. This carefully chosen suffix can encourage the CLIP model to prioritize and focus its attention on the detection and evaluation of content-aware distortions that may arise in the context of 3D digital humans.\n144. 2) Description Selection: We have identified descriptions pertaining to quality assessment that encompass broad evaluation aspects to ensure robustness.\n145. In this study, the general quality-related descriptions employed comprise the contrasting pairs of high quality ↔ low quality, good ↔ bad, and perfect ↔ distorted.\n146. The utilization of the high quality ↔ low quality as well as the good ↔ bad text pair assists in directing the attention of the CLIP model towards general subjective impressions.\n147. Conversely, the perfect ↔ distorted pair compels the CLIP model to prioritize the existence of distortions.\n148. 3) Affinity Difference Computation: Given the input image I and text T , the senmantic affinity can be calculated with the assistance of CLIP as: where E I and E T stand for the image and text encoders of CLIP, F I and F T represent the CLIP-encoded features, and A(I, T ) indicates the affinity between the input image and text.\n149. Afterward, the computation of zero-shot quality affinity can be derived from the aforementioned selected descriptions by calculating the disparity between the probabilities assigned to positive and negative textual inputs:\n150. A(P k , T ), where the averaged affinity to the given text T , denoted by A(P, T ), is calculated by CLIP across the six projections P.\n151. In this context, T i + and T i -refer to the positive and negative text descriptions, respectively, from the i-th text pair.\n152. The variable N T represents the total number of text pairs.\n153. Furthermore, A dif f signifies the cumulative difference between the averaged positive and negative affinity.\n154. The sigmoid remapping technique is then used to map the raw difference scores A dif f obtained from perceptual quality evaluation into a range of [0, 1].\n155. This remapping is done based on the guidance provided by the Video Quality Experts Group (VQEG)[57].\n156. Apart from evaluating semantic affinity, we incorporate the use of NIQE (Naturalness Image Quality Evaluator[10]) as a blind quality evaluator to assess the spatial naturalness of the digital humans.\n157. The purpose of employing NIQE is to identify and quantify common low-level distortions encountered in practical digital humans, including Gaussian noise, blur, and JPEG compression artifacts.\n158. By incorporating NIQE alongside semantic affinity evaluation, we aim to complement the assessment of high-level information with an evaluation of low-level technical quality.\n159. The NIQE index operates by quantifying the disparity between the characteristics of the input image features and the anticipated distribution of features observed in "high-quality" images, which are derived from a diverse set of pristine natural images.\n160. Since the raw NIQE scores and the raw affinity difference scores are on different scales, it is necessary to normalize the NIQE scores to facilitate meaningful comparison.\n161. To achieve this, we divide the NIQE scores by a constant value, denoted as c 1 , which effectively restricts the majority of NIQE scores to the range of [0,1].\n162. Consequently, the spatial naturalness quality measure can be computed as follows: where N (P k ) denotes the NIQE value for the k-th projection, N (P) represents the average NIQE value across the 6 projections, and Q N stands for the spatial naturalness quality measure.\n163. It\'s worth noting that the NIQE scores are inversely correlated with quality and the negative sign is incorporated into the sigmoid function, allowing for a consistent interpretation and alignment of the NIQE scores with the quality evaluation framework.\n164. The aforementioned measures are applied to projections, specifically the image modality.\n165. In order to enhance the model\'s understanding of digital humans, it is proposed to directly extract features from the mesh modality to capture the loss in geometry with respect to visual quality.\n166. 1) Descriptor Selection: Various geometry attributes have been utilized to describe the quality-related geometric characteristics of meshes[14], including curvature, dihedral angle, face angle, face area, etc.For the purpose of preserving stability and improving the robustness of the proposed zeroshot method, the dihedral angle is selected as the geometry descriptor for the following reasons: a) Extensive evidence supports the effectiveness of the dihedral angle in describing geometric features relevant to visual quality[11],[12],[13],[14].b).\n167. b) Unlike other geometry attributes, the dihedral angle is invariant to scale.\n168. Its values are confined within the range of [0, π], thereby contributing to its robustness.\n169. The dihedral angle is the angle between two adjacent faces, which can be calculated as the dot product of corresponding normal vectors: where θj π indicates the scaled dihedral angle corresponding to the j-th edge of the mesh, Θ indicates the set of the scaled dihedral angle values, n j1 and n j2 stand for the normal vectors of the two adjacent faces whose co-edge is the j-th edge.\n170. 2) Quality Correlation with Dihedral Angle: Lossy mesh compression and simplification techniques can potentially diminish a mesh\'s structural details, resulting in a smoother and simpler surface representation.\n171. In such cases, the faces comprising the smoother and simpler surface tend to exhibit dihedral angles that approach π, leading to an inherent inclination for larger dihedral angles.\n172. To substantiate this observation, we present the tendencies of the mean values of the dihedral angles in Fig.8, from which we can find a consistent upward trend in dihedral angle means as compression/simplification levels increase.\n173. Therefore, the mean values of the dihedral angle can be generally taken as an indicator of geometry detail loss caused by compression/simplification.\n174. Then geometry loss quality measure can be calculated as: where Q G represents the geometry loss quality measure, Θ indicates the mean value of the dihedral angles, and the negative sign is added to the sigmoid function due to the positive correlation between the dihedral angles\' mean values and compression/simplification levels.\n175. In order to develop a reliable zero-shot perceptual quality index, we adopt a direct aggregation approach wherein we sum up the scale-aligned scores of various indices without performing any fine-tuning processes.\n176. Considering that the Q A , Q N , and Q G have undergone sigmoid rescaling, all three measures are bounded within the range of [0, 1].\n177. Consequently, we define the comprehensive unified DHQI (digital human quality index) as follows: where Q DHQI indicates the final quality values for the digital humans.\n178. V. EXPERIMENT A.\n179. Validation Setup 1) Benchmark Databases: In addition to the proposed SJTU-H3D database, we have incorporated the digital human quality assessment (DHHQA) database[8]as an additional resource for benchmark validation.\n180. The DHHQA database comprises a total of 55 scanned digital human heads that serve as reference samples, along with 1,540 labeled distorted digital human heads.\n181. These distorted samples have been intentionally degraded through the introduction of noise and compression/simplification.\n182. 2) k-fold Cross-Validation: To ensure robust evaluation, we adopt a k-fold cross-validation strategy.\n183. This approach involves dividing the database into k equally sized folds.\n184. The model is then trained on k-1 of these folds and subsequently tested on the remaining fold.\n185. This process is repeated k times, with each fold being used as the test set once.\n186. By averaging the performance across these k iterations, we obtain a more reliable estimate of the model\'s effectiveness, minimizing the impact of random variations.\n187. For both the SJTU-H3D and DHHQA databases, we have selected a value of k = 5 to conduct the k-fold cross-validation, ensuring a balanced evaluation across multiple subsets.\n188. It\'s worth mentioning that there is no content overlap between the training and testing folds.\n189. To facilitate a direct and fair comparison between zeroshot and supervised methods, we validate their performance in the following way.\n190. Zero-shot methods are directly applied to the testing folds, as they do not require any training.\n191. The performance is then averaged across the testing folds and reported as the final performance.\n192. On the other hand, supervised methods undergo training on the training folds and are subsequently tested on the testing folds.\n193. Similar to zero-shot methods, the average performance is calculated and reported as the final performance.\n194. Adopting this methodology enables a direct and unbiased comparison of the performance between zero-shot and supervised methods, insights into their respective strengths and limitations.\n195. 3) Implemetation Details: The cube-like projection process described in Section IV-A is conducted with the assistance of open3d[52]library with a resolution of 1080P.\n196. The white background is cropped out.\n197. The projections are downsampled to 224×224 as the input of the CLIP[9]image encoder.\n198. The ViT-B-32[64]backbone with LAION-2B[65]pretrained weights is utilized as the CLIP model.\n199. To fit the DHHQA database, we replace the suffix "projection of 3d human model" as described in Equation5with "projection of 3d human face".\n200. The scale parameter c 1 constant described in Section IV-C is set as 100.\n201. The supervised training of the proposed DHQA method is conducted with the Support Vector Regression (SVR) model with RBF kernel.\n202. The official source code is used for the competitors and default parameters are maintained.\n203. The default 5-fold crossvalidation is strictly followed for the competitors to make the comparison fair.\n204. In addition, the predicted scores of all the methods are followed by a five-parameter logistic regression to map the scores to the MOS scale.\n205. The competitors\' selection is conducted to ensure high diversity, which includes the zero-shot FR methods, zero-shot NR methods, and the supervised NR methods.\n206. 1) Zero-shot FR Methods: We consider several classical projection-based FR methods: PSNR, SSIM[58], MS-SSIM[59], and GMSD[60].\n207. These methods are applied to the six perpendicular projections, and the resulting scores are averaged and recorded.\n208. Additionally, we incorporate three popular point-based FR metrics proposed by MPEG: PSNR p2po[61], PSNR p2pl[62], and PSNR yuv[63].\n209. For the purpose of validation, we convert the digital human models into point clouds.\n210. Furthermore, we utilize G-LPIPS*[19], which is a projection-based FR metric modified from LPIPS[66]and is designed for textured meshes.\n211. The official pretrained weights are employed for this metric.\n212. 2) Zero-shot NR Methods: These methods comprise CPBD[37], pretrained BRISQUE*[36], NIQE[10], and IL-NIQE[44].\n213. 3) Supervised NR Methods: These methods encompass handcrafted approaches such as BRISQUE[36], NFERM[39], and BMPRI[38], which are supervised using the Support Vector Regression (SVR) model.\n214. Additionally, we include deep learning-based methods, namely DBCNN[40], Hyper-IQA[41], MUSIQ[42], and StaiIQA[43], which have been retrained for our evaluation.\n215. The overall performance on the SJTU-H3D and DHHQA databases are exhibited in TableIV, from which we can draw several conclusions.\n216. 1) Zero-shot Performance: a) Among all the zero-shot methods compared on the SJTU-H3D database, the DHQI method demonstrates superior performance and outperforms them all.\n217. b) Nevertheless, the FR metrics that exhibit the highest performance on the DHHQA database, namely MS-SSIM & GMSD, suffer significant performance degradation when applied to the SJTU-H3D database.\n218. c) In contrast, all the competing zero-shot NR methods consistently exhibit lower performance compared to the proposed DHQI method.\n219. The reason for this disparity lies in the focus of these methods on addressing low-level distortions, which restricts their ability to effectively capture and model high-level semantic quality representations.\n220. By leveraging the semantic affinity quality measure, the DHQI method can enhance the performance of zero-shot NR approaches even further.\n221. 2) Supervised Performance: Due to the significant advancements achieved by deep neural networks, deep learning-based methods such as HyperIQA and MUSIQ have demonstrated superior performance compared to traditional handcrafted methods.\n222. Despite this, the proposed DHQI method, which is solely supervised by Support Vector Regression (SVR) model, achieves the top-ranking performance on the SJTU-H3D database.\n223. One notable advantage of the proposed supervised DHQI index is its cost-effectiveness in terms of time and computational resources.\n224. b) The point-based methods proposed by MPEG exhibit high sensitivity to noise-related distortions.\n225. This can be attributed to the direct impact of geometry and color noise on the point-level quality characteristics.\n226. Additionally, the PSNR yuv metric demonstrates a strong discriminative ability in distinguishing quality differences within CN, PC, UMC, and TD distortions.\n227. c) The zeroshot NR methods NIQE and IL-NIQE show competitive performance for UMC, TD, and TC distortions.\n228. This can be attributed to the fact that UMC and TD distortions introduce blurring effects to digital human projections, which aligns with the strengths of these methods.\n229. d) FS distortion proves to be the most challenging distortion to evaluate.\n230. This is due to the fact that the MOS distribution for FS distortion tends to be more centered, as shown in Fig.6, indicating a more fine-grained quality level that is less distinctive.\n231. FS distortion primarily causes digital humans to exhibit more geometric characteristics, which may lead to small differences in NSS reflected by the projections and result in the poor performance of NIQE and IL-NIQE.\n232. In this section, we present an analysis of the effects of different quality measures: Q A , Q N , and Q G , on the experimental performance.\n233. The combinations of these quality measures are tested, and the results are summarized in TableVI.\n234. Throughout the experiments, we maintain the default experimental setup.\n235. Table VI clearly demonstrates that among the single quality measures, Q A achieves the highest performance.\n236. This finding indicates a strong correlation between quality-aware semantic affinity and the visual quality of digital humans.\n237. It suggests that considering the quality of semantic representations is crucial for accurately assessing the visual fidelity of digital human models.\n238. Furthermore, excluding any of the three quality measures leads to a drop in performance compared to utilizing all quality measures together.\n239. This observation implies that each quality measure contributes significantly to the final results.\n240. The effectiveness of the proposed framework is thereby validated by the consistent performance improvements achieved when all quality measures are incorporated.\n241. To further analyze the performance of the proposed method, we conduct the statistical test in this section.\n242. We follow the same experiment setup as in[67]and compare the difference between the predicted quality scores with the subjective ratings.\n243. All possible pairs of models are tested and the results are listed in Fig.9.\n244. Our method demonstrates remarkable superiority over 12 zero-shot methods and 5 supervised methods when compared on the SJTU-H3D database.\n245. On the DHHQA database, our method exhibits substantial outperformance compared to 9 zero-shot methods and 3 supervised methods.\n246. The increasing applications of digital humans across various domains have highlighted the need for comprehensive A black/white block means the row method is statistically worse/better than the column one.\n247. A gray block means the row method and the column method are statistically indistinguishable.\n248. The methods are denoted by the same index as in TableIV. quality assessment studies.\n249. However, the limited availability of comprehensive digital human quality assessment (DHQA) databases has posed challenges in this area.\n250. To address this gap, we have introduced the SJTU-H3D subjective quality assessment database, specifically designed for full-body digital humans.\n251. This database consists of 40 high-quality reference digital humans and 1,120 labeled distorted counterparts created with seven types of distortions.\n252. Nonetheless, the scarcity of suitable DHQA databases remains a hindrance to the development of data-driven methods.\n253. To overcome this limitation and enhance generalization capabilities, we propose a zero-shot DHQA approach that focuses on no-reference (NR) scenarios.\n254. Our approach leverages semantic and distortion features obtained from projections, as well as geometry features derived from the mesh structure of digital humans.\n255. The proposed DHQI not only serves as a robust baseline for DHQA tasks but also facilitates advancements in the field.\n256. We hope our work can contribute to the establishment of effective evaluation frameworks and methodologies for digital humans, enabling their widespread application in diverse domains.\n\n请严格按照JSON格式返回：{"labels": [0,1,0,...]}，不要包含任何其他文字。'}], 'model': 'qwen-plus', 'response_format': {'type': 'json_object'}}}
2025-10-14 19:48:33,649 - DEBUG - Sending HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
2025-10-14 19:48:33,649 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-10-14 19:48:33,649 - DEBUG - send_request_headers.complete
2025-10-14 19:48:33,649 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-10-14 19:48:33,649 - DEBUG - send_request_body.complete
2025-10-14 19:48:33,649 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-10-14 19:48:45,574 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'vary', b'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding'), (b'x-request-id', b'163ebf27-6797-49ec-868d-ebd9a9ebeccc'), (b'x-dashscope-call-gateway', b'true'), (b'content-type', b'application/json'), (b'req-cost-time', b'11879'), (b'req-arrive-time', b'1760442504976'), (b'resp-start-time', b'1760442516856'), (b'x-envoy-upstream-service-time', b'11837'), (b'content-encoding', b'gzip'), (b'date', b'Tue, 14 Oct 2025 11:48:36 GMT'), (b'server', b'istio-envoy'), (b'transfer-encoding', b'chunked')])
2025-10-14 19:48:45,574 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-14 19:48:45,574 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-10-14 19:48:45,574 - DEBUG - receive_response_body.complete
2025-10-14 19:48:45,574 - DEBUG - response_closed.started
2025-10-14 19:48:45,574 - DEBUG - response_closed.complete
2025-10-14 19:48:45,574 - DEBUG - HTTP Response: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "200 OK" Headers({'vary': 'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding', 'x-request-id': '163ebf27-6797-49ec-868d-ebd9a9ebeccc', 'x-dashscope-call-gateway': 'true', 'content-type': 'application/json', 'req-cost-time': '11879', 'req-arrive-time': '1760442504976', 'resp-start-time': '1760442516856', 'x-envoy-upstream-service-time': '11837', 'content-encoding': 'gzip', 'date': 'Tue, 14 Oct 2025 11:48:36 GMT', 'server': 'istio-envoy', 'transfer-encoding': 'chunked'})
2025-10-14 19:48:45,575 - DEBUG - request_id: 163ebf27-6797-49ec-868d-ebd9a9ebeccc
2025-10-14 19:48:45,575 - DEBUG - API request completed in 11.93 seconds
2025-10-14 19:48:45,575 - DEBUG - Raw model response: {"labels": [1,1,1,1,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,1,1,1,1,1,1,1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}
2025-10-14 19:48:45,575 - INFO - Successfully processed 141 labels
2025-10-14 19:48:45,575 - ERROR - Label count mismatch for Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation
2025-10-14 19:48:45,575 - INFO - Evaluating paper 3/18: Amazon_M2_ A Multilingual Multi_locale Shopping Session Dataset for Recommendation and Text Generation
2025-10-14 19:48:45,575 - INFO - Starting model prediction
2025-10-14 19:48:45,575 - INFO - Attempt 1 of 5
2025-10-14 19:48:45,575 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-8de95ecf-4775-4e78-9525-1d70c352d984', 'json_data': {'messages': [{'role': 'system', 'content': '你是一个专门用于识别学术论文中数据集描述的助手。你需要判断每个输入的句子是否描述了研究中使用的数据集。请确保：\n    1. 只输出JSON格式的结果\n    2. 结果格式必须为 {"labels": [0,1,0,...]}\n    3. 不要输出任何其他文字或解释'}, {'role': 'user', 'content': '你是一位专业的学术论文数据集描述识别专家。请仔细分析下面这篇论文中的每个句子，判断它们是否描述了本研究特定构建的数据集信息。\n\n    论文标题: Amazon_M2_ A Multilingual Multi_locale Shopping Session Dataset for Recommendation and Text Generation\n\n    判断标准:\n    判断为1(数据集描述)的标准：\n    句子必须直接描述本研究特定构建的数据集,包括:\n    1. 该数据集的构建过程和方法\n    2. 该数据集的具体构成和规模\n    3. 该数据的来源和收集方式\n    4. 该数据的预处理步骤\n    5. 该数据集的可获取方式(如发布地址)\n    6. 或者其他直接或者间接描述了该数据集的句子\n\n    所有其他类型的句子均标记为0。\n    \n    请对每个句子进行分析，返回一个JSON格式的标签数组。格式要求：\n    1. 必须是有效的JSON格式\n    2. 只包含labels字段，值为0和1组成的数组\n    3. 数组长度必须与句子数量相同\n    4. 示例格式：{"labels": [0,1,0,1,...]}\n\n    需要判断的句子：\n\n    1. Abstract: Modeling customer shopping intentions is a crucial task for e-commerce, as it directly impacts user experience and engagement.\n2. Thus, accurately understanding customer preferences is essential for providing personalized recommendations.\n3. Session-based recommendation, which utilizes customer session data to predict their next interaction, has become increasingly popular.\n4. However, existing session datasets have limitations in terms of item attributes, user diversity, and dataset scale.\n5. As a result, they cannot comprehensively capture the spectrum of user behaviors and preferences.\n6. To bridge this gap, we present the Amazon Multilingual Multilocale Shopping Session Dataset, namely Amazon-M2.\n7. It is the first multilingual dataset consisting of millions of user sessions from six different locales, where the major languages of products are English, German, Japanese, French, Italian, and Spanish.\n8. Remarkably, the dataset can help us enhance personalization and understanding of user preferences, which can benefit various existing tasks as well as enable new tasks.\n9. To test the potential of the dataset, we introduce three tasks in this work: (1) next-product recommendation, (2) next-product recommendation with domain shifts, and (3) next-product title generation.\n10. With the above tasks, we benchmark a range of algorithms on our proposed dataset, drawing new insights for further research and practice.\n11. In addition, based on the proposed dataset and tasks, we hosted a competition in the KDD CUP 2023 2 and have attracted thousands of users and submissions.\n12. The winning solutions and the associated workshop can be accessed at our website https://kddcup23.github.io/.* Equal contribution. 2https://www.aicrowd.com/challenges/amazon-kdd-cup-23-multilingual-recommendationchallenge37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks.\n13. In the era of information explosion, recommender systems have become a prevalent tool for understanding user preferences and reducing information overload[1,2,3,4,5,6].\n14. Traditionally, the majority of recommendation algorithms focus on understanding long-term user interests through utilizing user-profiles and behavioral records.\n15. However, they tend to overlook the user\'s current purpose which often has a dominant impact on user\'s next behavior.\n16. Besides, many recommendation algorithms require access to user profiles[7,8,9], which can be incomplete or even missing in real-world situations especially when users are browsing in an incognito mode.\n17. In these cases, only the most recent user interactions in the current session can be utilized for understanding their preferences.\n18. Consequently, the session-based recommendation has emerged as an effective solution for modeling user\'s short-term interest, focusing on a user\'s most recent interactions within the current session to predict the next product.\n19. Over the past few years, the session-based recommendation has gained significant attention and has prompted the development of numerous models[10,11,12,13,14,15,16,17].\n20. A critical ingredient for evaluating the efficacy of these methods is the session dataset.\n21. While numerous session datasets[18,19,20,11,21]have been carefully curated to meet the requirements of modeling user intent and are extensively employed for evaluating session-based recommender systems, they have several drawbacks.\n22. First, existing datasets only provide limited product attributes, resulting in incomplete product information and obscuring studies that leverage attribute information to advance the recommendation.\n23. Second, the user diversity within these datasets is limited and may not adequately represent the diversity of user-profiles and behaviors.\n24. Consequently, it can result in biased or less accurate recommendations, as the models may not capture the full range of customer preferences.\n25. Third, the dataset scale, particularly in terms of the product set, is limited, which falls short of reflecting real-world recommendation scenarios with vast product and user bases.\n26. To break the aforementioned limitations, we introduce the Amazon Multilingual Multi-Locale Shopping Session Dataset, namely Amazon-M2, a large dataset of anonymized user sessions with their interacted products collected from multiple language sources at Amazon.\n27. Specifically, the dataset contains samples constructed from real user session data, where each sample contains a list of user-engaged products in chronological order.\n28. In addition, we provide a table of product attributes, which contains all the interacted products with their associated attributes such as title, brand, color, etc. Modeling such session data can help us better understand customers\' shopping intentions, which is also the main focus of e-commerce.\n29. Particularly, the proposed dataset exhibits the following characteristics that make it unique from existing session datasets.\n30. (a) Rich semantic attributes: Amazon-M2 includes rich product attributes (categorical, textual, and numerical attributes) as product features including title, price, brand, description, etc.These attributes provide a great opportunity to accurately comprehend the user\'s interests.\n31. To our best knowledge, it is the first session dataset to provide textual features.\n32. (b) Large scale: Amazon-M2 is a large-scale dataset with millions of user sessions and products, while existing datasets only contain tens of thousands of products.\n33. (c) Multiple locales: Amazon-M2 collected data from diverse sources, i.e., six different locales including the United Kingdom, Japan, Italian, Spanish, French, and Germany.\n34. (d) Multiple languages: Given the included locales, Amazon-M2 is special for its multilingual property.\n35. Particularly, six different languages (English, Japanese, Italian, Spanish, French, and German) are provided.\n36. It enables us to leverage recent advances such as language models[22,23,24]to model different languages in user sessions.\n37. By utilizing this dataset, we can perform diverse downstream tasks for evaluating relevant algorithms in recommendation and text generation.\n38. Here, we focus on three different tasks, consisting of (1) next-product recommendation, (2) next-product recommendation with domain shifts, and (3) nextproduct title generation.\n39. The first task is the classic session-based recommendation which requires models to predict the ID of the next product, where the training dataset and test dataset are from the same domain.\n40. The second task is similar to the first task but requires the models to pre-train on the large dataset from large locales and transfer the knowledge to make predictions on downstream datasets from different domains (i.e., underrepresented locales).\n41. The third task is a novel task proposed by us, which asks models to predict the title of the next product which has never been shown in the training set.\n42. Based on these tasks, we benchmark representative baselines along with simple heuristic methods.\n43. Our empirical observations suggest that the representative baselines fail to outperform simple heuristic methods in certain evaluation metrics in these new settings.\n44. Therefore, we believe that Amazon-M2 can inspire novel solutions for session-based recommendation and enable new opportunities for tasks that revolve around large language models and recommender systems.\n45. denote a dictionary of unique products that appeared in the sessions, and each product is associated with some attributes.\n46. Designed for session-based recommendation, Amazon-M2 is a large-scale dataset composed of customer shopping sessions with interacted products.\n47. Specifically, the dataset consists of two components: (1) user sessions where each session is a list of product IDs interacted by the current user (Figure1a), and (2) a table of products with each row representing the attributes of one product (Figure1b).\n48. Particularly, the user sessions come from six different locales, i.e., the United Kingdom (UK), Japan (JP), German (DE), Spain (ES), Italian (IT), and France (FR).\n49. Given its multi-locale nature, the dataset is also multilingual: the textual attributes (e.g., title and description) of the products in the user sessions are in multiple languages, namely, English, Italian, French, Germany, and Spanish.\n50. Based on this dataset, we construct the training/test dataset for each task.\n51. A summary of our session dataset is given in Table2.\n52. It includes the number of sessions, the number of interactions, the number of products, and the average session length for six different locales.\n53. We can find that UK, DE, and JP have approximately 10 times the number of sessions/products compared to ES, FR, and IT.\n54. More details about the collection process of the dataset can be found in Appendix B.\n55. Comparison with Existing Datasets.\n56. We summarize the differences between existing session datasets (especially from session-based recommendation) and Amazon-M2 in Table1.\n57. First of all, Amazon-M2 is the first dataset to provide textural information while other datasets majorly focus on the product ID information or categorical attributes.\n58. Without comprehensive product attributes, the recommendation models may struggle to capture the nuanced preferences of customers.\n59. Second, existing datasets only provide sessions from a single locale (or country) which limits their user diversity.\n60. Consequently, it may lead to biased or less accurate recommendations, as the models may not capture the full range of customer preferences.\n61. By contrast, our proposed Amazon-M2 is collected from multiple locales and is multilingual in nature.\n62. Third, our proposed Amazon-M2 provides a large number of user sessions and is on a much larger product scale, which can better reflect real-world recommendation scenarios with huge product bases.\n63. In this section, we offer a comprehensive analysis of the Amazon-M2 dataset to uncover valuable insights.\n64. Our analysis covers several essential perspectives: long-tail phenomenon, product overlap between locales, session lengths, repeat pattern, and collaborative filtering pattern.\n65. Corresponding codes can be found here.\n66. Long-tail phenomenon[26,27]is a significant challenge in the session recommendation domain.\n67. It refers to the situation where only a handful of products enjoy high popularity, while the majority of products receive only a limited number of interactions.\n68. To investigate the presence of the long-tail phenomenon in Amazon-M2 dataset, we analyze the distribution of product frequencies, as depicted  in Figure2a.\n69. The results clearly demonstrate the existence of a long-tail distribution, where the head of the distribution represents popular items and the tail consists of less popular ones.\n70. Furthermore, we observe that the long-tail phenomenon is also evident within each individual locale.\n71. For detailed experimental results regarding this phenomenon in each locale, please refer to Appendix B.2.\n72. The long-tail distribution makes it difficult to effectively recommend less popular products, as a small number of popular items dominate the recommendations.\n73. Product overlap ratio between locales is the proportion of the same products shared by different locales.\n74. A large number of overlapping products indicates a better transferability potential when transferring the knowledge from one locale to the other.\n75. For example, cross-domain recommendation algorithms like[28]can then be successfully applied, which directly transfers the learned embedding of the overlapping products from popular locales to the underrepresented locales.\n76. We then examine product overlap between locales in Amazon-M2 with the product overlap ratio.\n77. |Na| , where N a and N b correspond to the products set of locale a and b, respectively.\n78. In Figure2bwe use a heatmap to show the overlap ratio, where x and y axes stand for locale a and b, respectively.\n79. (2) Considering the product overlap ratio between large locales and underrepresented locales, i.e., ES, FR, and IT, we can see a large product overlapping, indicating products in the underrepresented domain also appear in the large locales.\n80. Particularly, the overlap ratio between small locales and DE can reach around 0.4.\n81. Thus, it has the potential to facilitate knowledge transfer from large locales and areas to underrepresented regions.\n82. Notably, despite the existence of overlapping products between different locales, there still remains a large proportion of distinguished products in each locale, indicating the difficulty of transferability with distribution shift.\n83. Moreover, the multilingual property, where the product textual description from different locales is in different languages, also induces to distribution shift issue.\n84. Such a multilingual issue is a long-established topic in the NLP domain.\n85. For instance,[29,30,31]point out morphology disparity, tokenization differences, and negative transfer in the multilingual scenario, leading to distribution shift.\n86. Session length is an important factor in the session recommendation domain.\n87. Typically, a longer session length may lead to the interest shift challenge problem[15]with difficulties in capturing multiple user interests in one single session.\n88. Most existing algorithms[13,16]show a better performance on the shorter sessions while failing down on the longer ones.\n89. As shown in Figure2c.\n90. We can observe that the session length also exhibits a long-tail distribution: most sessions are short while only few sessions are with a length larger than 100.\n91. Repeat pattern[32,33,34,35]is also an important user pattern, which refers to the phenomenon that a user repeatedly engages the same products multiple times in a single session.\n92. The presence of repeat patterns in recommender systems can potentially result in the system offering more familiar products that match users\' existing preferences, which may lead to a less diverse and potentially less satisfying user experience.\n93. On the other hand, the repeat pattern is also an important property utilized in the graph-based session recommendation algorithms[13,17,36,37].\n94. Typically, those graph-based algorithms construct a session graph where each node represents a product and each edge indicates two products interacted by the user consecutively.\n95. Complicated session graphs with different structure patterns can be built when sessions exhibit evident repeat patterns.\n96. In Figure2d, we report the proportion of sessions with repeat patterns for the six locales and we can observe that there are around 35% sessions with repeat patterns across different locales.\n97. Furthermore, we examine the number of repeat products in those sessions with repeat patterns and report results on the distribution of repeated products in Figure2e.\n98. We make two observations: (1) the number of repeated products varies on different sessions; and (2) the number of repeated products in a session also follows the long-tail distribution where most sessions only appear with a few repeated products.\n99. Collaborative filtering pattern.\n100. Collaborative filtering is a widely used technique that generates recommended products based on the behavior of other similar users.\n101. It is generally utilized as an important data argumentation technique to alleviate the data sparsity issue, especially for short sessions[38,39].\n102. Since Amazon-M2 encompasses a much larger product set than existing datasets, we investigate whether collaborative filtering techniques can potentially operate in this challenging data environment.\n103. Specifically, we utilize the session collaborative filtering algorithm, Session-KNN (SKNN)[11], to identify sessions that are similar to the target user\'s current session.\n104. The similarity score of SKNN can be calculated in the following steps.\n105. First, for a particular session s, we first determines a set of its most similar sessions N (s) ⊆ S with the cosine similarity sim(s, s j ) = |s ∩ s j |/ |s||s j |.\n106. • SRGNN[13]is the first to employ GNN layer to capture user interest in the current session.\n107. • CORE[41]ensures that sessions and items are in the same representation space via encoding the session embedding as a linear combination of item embeddings.\n108. • MGS[42]incorporates product attribute information to construct a mirror graph, aiming to learn better preference understanding via combining session graph and mirror graph.\n109. Notably, MGS can only adapt categorical attributes.\n110. Therefore, we discretize the price attribute as the input feature.\n111. In addition, we include a simple yet effective method, Popularity, by simply recommending all users the most popular products.\n112. We utilize Mean Reciprocal Rank@K (MRR@100) and Recall@100 to evaluate various recommendation algorithms.\n113. More results on NDCG@100 metric can be found in Appendix C.\n114. Corresponding codes can be found here.\n115. Results & Observations.\n116. The experiment results across different locales can be found in Table3.\n117. We can observe that the popularity heuristic generally outperforms all the deep models with respect to both MRR and Recall.\n118. The only exception is that CORE achieves better performance on Recall.\n119. On one hand, the success of the popularity heuristic indicates that the product popularity is a strong bias for this dataset.\n120. On the other hand, it indicates that the large product set in Amazon-M2 poses great challenges in developing effective recommendation algorithms.\n121. Thus, more advanced recommendation strategies are needed for handling the challenging Amazon-M2 dataset.\n\n请严格按照JSON格式返回：{"labels": [0,1,0,...]}，不要包含任何其他文字。'}], 'model': 'qwen-plus', 'response_format': {'type': 'json_object'}}}
2025-10-14 19:48:45,576 - DEBUG - Sending HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
2025-10-14 19:48:45,576 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-10-14 19:48:45,576 - DEBUG - send_request_headers.complete
2025-10-14 19:48:45,576 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-10-14 19:48:45,576 - DEBUG - send_request_body.complete
2025-10-14 19:48:45,576 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-10-14 19:48:50,800 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'vary', b'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding'), (b'x-request-id', b'ce0b8006-45a9-40b7-b548-da6d39d93cf9'), (b'x-dashscope-call-gateway', b'true'), (b'content-type', b'application/json'), (b'req-cost-time', b'5187'), (b'req-arrive-time', b'1760442516897'), (b'resp-start-time', b'1760442522085'), (b'x-envoy-upstream-service-time', b'5152'), (b'content-encoding', b'gzip'), (b'date', b'Tue, 14 Oct 2025 11:48:41 GMT'), (b'server', b'istio-envoy'), (b'transfer-encoding', b'chunked')])
2025-10-14 19:48:50,801 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-14 19:48:50,801 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-10-14 19:48:50,801 - DEBUG - receive_response_body.complete
2025-10-14 19:48:50,801 - DEBUG - response_closed.started
2025-10-14 19:48:50,801 - DEBUG - response_closed.complete
2025-10-14 19:48:50,801 - DEBUG - HTTP Response: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "200 OK" Headers({'vary': 'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding', 'x-request-id': 'ce0b8006-45a9-40b7-b548-da6d39d93cf9', 'x-dashscope-call-gateway': 'true', 'content-type': 'application/json', 'req-cost-time': '5187', 'req-arrive-time': '1760442516897', 'resp-start-time': '1760442522085', 'x-envoy-upstream-service-time': '5152', 'content-encoding': 'gzip', 'date': 'Tue, 14 Oct 2025 11:48:41 GMT', 'server': 'istio-envoy', 'transfer-encoding': 'chunked'})
2025-10-14 19:48:50,801 - DEBUG - request_id: ce0b8006-45a9-40b7-b548-da6d39d93cf9
2025-10-14 19:48:50,801 - DEBUG - API request completed in 5.23 seconds
2025-10-14 19:48:50,801 - DEBUG - Raw model response: {"labels": [0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}
2025-10-14 19:48:50,801 - INFO - Successfully processed 103 labels
2025-10-14 19:48:50,801 - ERROR - Label count mismatch for Amazon_M2_ A Multilingual Multi_locale Shopping Session Dataset for Recommendation and Text Generation
2025-10-14 19:48:50,801 - INFO - Evaluating paper 4/18: AntM 2 C_ A Large Scale Dataset For Multi_Scenario Multi_Modal CTR Prediction
2025-10-14 19:48:50,801 - INFO - Starting model prediction
2025-10-14 19:48:50,801 - INFO - Attempt 1 of 5
2025-10-14 19:48:50,802 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-984a3040-55e9-495b-b51a-179f80acbad8', 'json_data': {'messages': [{'role': 'system', 'content': '你是一个专门用于识别学术论文中数据集描述的助手。你需要判断每个输入的句子是否描述了研究中使用的数据集。请确保：\n    1. 只输出JSON格式的结果\n    2. 结果格式必须为 {"labels": [0,1,0,...]}\n    3. 不要输出任何其他文字或解释'}, {'role': 'user', 'content': '你是一位专业的学术论文数据集描述识别专家。请仔细分析下面这篇论文中的每个句子，判断它们是否描述了本研究特定构建的数据集信息。\n\n    论文标题: AntM 2 C_ A Large Scale Dataset For Multi_Scenario Multi_Modal CTR Prediction\n\n    判断标准:\n    判断为1(数据集描述)的标准：\n    句子必须直接描述本研究特定构建的数据集,包括:\n    1. 该数据集的构建过程和方法\n    2. 该数据集的具体构成和规模\n    3. 该数据的来源和收集方式\n    4. 该数据的预处理步骤\n    5. 该数据集的可获取方式(如发布地址)\n    6. 或者其他直接或者间接描述了该数据集的句子\n\n    所有其他类型的句子均标记为0。\n    \n    请对每个句子进行分析，返回一个JSON格式的标签数组。格式要求：\n    1. 必须是有效的JSON格式\n    2. 只包含labels字段，值为0和1组成的数组\n    3. 数组长度必须与句子数量相同\n    4. 示例格式：{"labels": [0,1,0,1,...]}\n\n    需要判断的句子：\n\n    1. Abstract: Click-through rate (CTR) prediction is a crucial issue in recommendation systems, directly impacting user experience and platform revenue.\n2. In recent years, CTR has garnered attention from both industry and academia, leading to the emergence of various public CTR datasets.\n3. However, existing CTR datasets primarily suffer from the following limitations.\n4. Firstly, users generally click different types of items from multiple scenarios, and modeling the CTR from multiple scenarios can provide a more comprehensive understanding of users and share knowledge between different scenarios.\n5. Existing datasets only include CTR data for the same type of items from a single scenario.\n6. Secondly, multi-modal features are essential in multi-scenario CTR prediction as they effectively address the issue of inconsistent ID encoding between different scenarios.\n7. The existing datasets are based on ID features and lack multi-modal features.\n8. Third, a large-scale CTR dataset can provide a more reliable and comprehensive evaluation of complex models, fully reflecting the performance differences between models.\n9. While the scale of existing datasets is around 100 million, which is relatively small compared to the real-world industrial CTR prediction.\n10. To address these limitations, we propose AntM 2 C, a Multi-Scenario Multi-Modal CTR dataset based on real industrial data from the Alipay platform.\n11. Specifically, AntM 2 C possesses the following characteristics: 1) It covers CTR data of 5 different types of items from Alipay, providing insights into the preferences of users for different items, including advertisements, vouchers, mini-programs, contents, and videos.\n12. 2) Apart from ID-based features, AntM 2 C also provides 2 multi-modal features, raw text and image features, which can effectively establish connections between items with different IDs.\n13. 3) AntM 2 C provides 1 billion CTR data with 200 features, including:\n14. Click-through rate (CTR) prediction plays a significant role in various domains, including online advertising, search engines, and recommendation systems.\n15. CTR prediction refers to the task of estimating the probability that a user will click on a given item.\n16. It is essential for optimizing ad revenue, enhancing user experience, and improving engagement.\n17. One of the challenging issues in CTR prediction lies in the faithful evaluation of the model.\n18. Public CTR datasets provide a standardized and benchmarked environment for evaluating the performance of different CTR models.\n19. This enables researchers to compare the effectiveness of different models and identify the most suitable ones for specific applications.\n20. However, in order to meet the constantly growing demands of users, the current CTR scenarios and items are becoming increasingly diverse, and the amount of CTR data is also increasing.\n21. For example, in Alipay, CTR occurs in the consumer coupons at marketing campaigns, videos on the tab3 page, and mini-programs after a search.\n22. As a result, the existing CTR datasets suffer from the following limitations.\n23. Firstly, in real-world industrial CTR prediction, users generally click various types of items from different business scenarios, reflecting their preferences for different items.\n24. For example, on Alipay, a user may browse a video about coffee on the Tab3 page, then click on a coffee coupon during a marketing campaign, and finally use the Alipay search to click a coffee ordering mini-program to place an order.\n25. Jointly modeling this multi-scenario CTR data can provide a more comprehensive understanding of user preferences, and the knowledge across scenarios can be shared to improve the CTR performance in each scenario.\n26. However, existing CTR datasets have a limited range of item types and generally originate from the same business scenario, which fails to capture the multi-scenario preferences of users.\n27. For example, Criteo1and Avazu2only involve CTR data for advertisements.\n28. As e-commerce platforms, both Amazon3and AliExpress4provide CTR data for their e-commerce items.\n29. Tenrec[14]focuses more on video and article recommendations.\n30. Secondly, multi-modal features can address the issue of inconsistent IDs for similar items in different business scenarios and effectively establish a bridge between different scenarios.\n31. For example, a video about coffee and a coffee coupon have different IDs in different business scenarios.\n32. Directly using ID features cannot perceive the relationship between these two items.\n33. Multi-modal features inherently carry semantic meaning and can better compensate for the inconsistency of ID features across different domains.\n34. Additionally, with the rise of large language models (LLMs), combining LLMs with CTR prediction has become an emerging research field.\n35. Existing CTR datasets are based on ID features and lack abundant multi-modal features, resulting in the CTR model being unable to test the performance in multi-scenarios and multi-modal settings.\n36. Furthermore, large-scale datasets can reliably and comprehensively reflect the performance of CTR models, while also highlighting the differences between CTR models.\n37. The existing datasets are typically at the scale of 100 million, which is insufficient to further validate the capabilities in larger-scale industrial scenarios.\n38. To address the aforementioned challenges, we propose the AntM 2 C dataset, a large-scale multi-scenario multi-modal dataset for CTR prediction.\n39. Compared with existing CTR datasets, AntM 2 C has the following advantages: • Diverse business scenarios and item types: AntM 2 C contains different types of items from five typical business scenarios on the Alipay platform, including advertisements, vouchers, mini-programs, contents, and videos.\n40. Each business scenario has a unique data distribution.\n41. The abundant intersecting users and similar items between scenarios enable a more comprehensive evaluation for multi-scenario CTR modeling.\n42. • Multi-modal feature system: AntM 2 C not only includes ID features but also provides rich multi-modal features such as text and image, which can establish connections between similar items across scenarios and provide better evaluation for multi-modal CTR models.\n43. • Largest data scale: AntM 2 C comprises 200 million users and 6 million items, with a total of 1 billion samples 5 .\n44. The average number of interactions per user is above 50.\n45. To the best of our knowledge, AntM 2 C is the largest public CTR dataset in terms of scale, which can provide comprehensive and reliable CTR evaluation results.\n46. • Comprehensive benchmark: Based on AntM 2 C, three typical CTR tasks have been built, including multi-scenario modeling, cold-start modeling, and multi-modal modeling.\n47. Benchmark evaluation results based on state-of-the-art models are also provided.\n48. The rest of the paper is organized as follows.\n49. In Section 2, we briefly review some related works about public CTR datasets.\n50. In Section 3, we give a detailed introduction to the dataset collection and data analysis.\n51. In Section 4, we conduct empirical studies with baseline CTR methods on different CTR tasks.\n52. The existing public CTR datasets can be roughly divided into two categories: single-scenario and multi-scenario.\n53. Both have been widely adopted by the evaluation of CTR methods.\n54. The Criteo dataset is one of the publicly available datasets for CTR prediction.\n55. It contains over 45 million records of user interactions with advertisements, including features such as click-through rates, impression rates, and user demographics.\n56. Similar to the Criteo dataset, the Avazu dataset contains over 40 million records of user interactions with mobile advertisements.\n57. It includes features such as device information, app category, and user demographics.\n58. One of the main limitations of the Criteo and Avazu dataset is they only include CTR data for advertisements and cannot be used to evaluate CTR for other business scenarios or types of items.\n59. Additionally, the datasets do not provide text information about the advertisement or user, which can limit the scope of the multi-modal modeling.\n60. The AliExpress is a dataset gathered from real-world traffic logs of the search system in AliExpress.\n61. This dataset is collected from 5 countries: Russia, Spain, French, Netherlands, and America, which can be seen as 5 scenarios.\n62. It can be used to develop and evaluate CTR prediction models for e-commerce platforms.\n63. The Tenrec dataset is a multipurpose dataset for CTR prediction where click data was collected from two scenarios: articles and videos.\n64. Although the above datasets cover different scenarios, the items within these scenarios are similar.\n65. The AliExpress dataset only consists of ecommerce items, and Tenrec involves videos and articles that only reflect the personal interests of users in the entertainment and cultural aspects.\n66. Additionally, similar to single-scenario datasets, both  of these datasets lack textual modal information and only provide features such as IDs.\n67. This limitation restricts the application of multi-modal modeling.\n68. AntM 2 C\'s data is collected from Alipay, a leading platform for payments and digital services.\n69. In order to meet the growing demands of users, Alipay recommends various types of items from different business scenarios to users.\n70. 3.1.1Scenarios.\n71. AntM 2 C collects CTR data in five scenarios on Alipay, and there are differences in the types of items in each scenario.\n72. As shown in Figure1, the CTR prediction occurs in multiple scenarios, including services and content on search, vouchers on marketing, videos on Tab3 page, and advertisements on the membership page.\n73. In the search scenario, when a user enters search words, several relevant mini-apps of services or content are displayed for the user to click on.\n74. Marketing scenarios recommend some consumer vouchers, and users click the coupons they are willing to use.\n75. On the Tab3 page, the recommended items are primarily short videos, and users will click to watch the videos they are interested in.\n76. On the membership page, users may click on some online advertisements.\n77. In conclusion, AntM 2 C includes various types of items from different business scenarios.\n78. 2,we will show that there are differences in the data distribution of these different scenarios.\n79. The rich and diverse items provide a more comprehensive evaluation for CTR prediction.\n80. 3.1.2Data Sampling.\n81. AntM 2 C collects 9-day (from 20230709 to 20230717) CTR samples from the above-mentioned five scenarios and then filters out 1 billion samples of relatively high-activity users who have a total click count ≥ 30 across all scenarios.\n82. In the first stage of open sourcing, we randomly sampled 10 million data from these 1 billion samples, and their statistical properties are shown in Table1.\n83. We will open all 1 billion data in the subsequent stage.\n84. For the purpose of protecting user privacy, we do not explicitly indicate Table1: Data statistics of AntM 2 C.\n85. To protect user privacy, AntM 2 C anonymizes the scenario names as A-E.\n86. The click rate is calculated by dividing the number of clicks by the number of exposures.\n87. Since negative sampling is applied to the samples, the click rate may be higher than the actual value.A-E).\n88. The horizontal axis represents the number of frequencies for users/items, while the vertical axis represents the number of users/items at that frequency.\n89. It can be observed that, in terms of item distribution, all scenarios exhibit a long-tail distribution, with 80% of the sample appearing less than 5 frequencies.\n90. This long-tail distribution is consistent with real-world situations.\n91. As for user distribution, there are differences between scenarios.\n92. In scenario B, the distribution of user frequency has two peaks, one at less than 5 times and the other around 50 times.\n93. After the frequency is greater than 50, the number of users decreases as the frequency increases.\n94. In other scenarios, the exposure frequency of users follows a long-tail distribution similar to that of items, where more exposure frequency leads to fewer users.\n95. Due to the overlapping users between scenarios, the long-tail distribution of users in multiple scenarios becomes a normal distribution in the global samples.\n96. Most users have an exposure frequency of around 50.\n97. Overall, the distribution of items and users in AntM 2 C reflects CTR prediction in practice.\n98. The feature system of AntM 2 C, as shown in Table3, includes ID features of users and items, as well as raw text features.\n99. The user features consist of static profile features6and user sequence features.\n100. The static profile features include basic user attributes such as gender, age, occupation, etc.The sequence features provide the user\'s recent activities on Alipay, including clicked mini-apps, searched services, purchased items, etc.\n101. As mentioned in Section 3.1.3,these user features have been desensitized and encrypted for the purpose of user privacy protection and appear in the dataset in an encrypted ID format, making it impossible to reconstruct the original user features.\n102. In addition to the ID-based features, AntM 2 C also includes the raw text of user search entities to provide multi-modal evaluation.1.\n103. It should be noted that there are a large number of negative samples in the actual online logs (samples that were exposed but not clicked on).\n104. To address this issue, negative sampling was performed which resulted in a higher click-through rate in the AntM 2 C dataset compared to that in the actual online logs.\n105. In this section, we describe the applications of AntM 2 C in several CTR prediction tasks.\n106. We briefly introduce each task and report the results of some baseline methods.\n107. We select the commonly used AUC (Area Under the Curve) as the metrics for all experiments.\n108. The baseline methods and evaluation results in the experiment provide a demo of using AntM 2 C.\n109. More baselines and evaluations will continue to be updated in future work.\n110. Multi-scenario CTR prediction is a common issue in industrial recommendation systems.\n111. It builds a unified model by leveraging CTR data from multiple scenarios.\n112. The knowledge sharing between scenarios enables the multi-scenario model to achieve better performance compared to single-scene modeling.\n113. We conduct an evaluation on multi-scenario CTR prediction using different baseline methods based on the 5 scenarios in the AntM 2 C dataset.\n114. 3. The text features will be used for multi-modal evaluation (see in Section 4.3).\n115. We mainly choose the multitask methods as the baseline methods for multi-scenario CTR prediction.\n116. We treat the CTR estimation for each scenario as a task and share the knowledge among the scenarios at the bottom layer, with each scenario\'s CTR score output at the tower layer.\n117. The baseline methods and hyperparameter settings are as follows: • DNN: The DNN is trained on a mixture of samples from all scenarios without tasks, serving as the baseline for multiscenario CTR prediction.\n118. The DNN consists of three layers with 128, 32, and 2 units, respectively.\n119. • Shared Bottom[10]: Shared bottom is the most fundamental model in multi-task learning, where the knowledge is shared among the tasks at the bottom layer.\n120. Each task has its own independent tower layer and outputs the corresponding CTR score7.\n121. • MMoE[7]: Based on the shared bottom, MMOE introduces multiple expert networks, each specialized in predicting a specific task, sharing a common input layer.\n122. Additionally, MMOE adds a gating network that assigns different weights to each expert based on the input data to determine their influence on predicting the output for a specific task.\n123. • PLE[12]: Based on MMOE, PLE further designs task-specific experts for each task, while retaining the shared expert.\n124. This structure allows the model to better learn the differences and correlations among tasks.\n125. We set the number of experts in PLE to be the same as MMOE, with each of the five scenarios having its own specific expert and one globally shared expert 7 .\n126. All baseline methods utilized the Adam[5]optimizer with a learning rate of 1e-3 for parameter optimization.\n127. The models were trained for 5 epochs with a batch size of 512.5shows the evaluation results of different baseline methods on multi-scenario CTR prediction, from which we can draw the following conclusions.\n128. Firstly, compared to the DNN model that trains all data together without considering scenario characteristics, all multi-task models achieve better performance.\n129. This demonstrates that in AntM 2 C, there are differences and commonalities between scenarios, and simply mixing training data will not achieve the best results.\n130. Secondly, the CTR performance varies across each scenario, indicating different levels of difficulty between scenarios.\n131. For example, in scenario B, where there is a large amount of data, the AUC is generally above 0.93, while in scenario D, the AUC is only around 0.68.\n132. The diverse business scenarios and items in AntM 2 C enable a more comprehensive and diverse evaluation of CTR.\n133. Finally, the expert-structured MMOE and PLE outperform the shared bottom model, demonstrating that refined model design can enhance the performance on AntM 2 C.\n134. AntM 2 C is capable of reflecting the differences between different models.\n135. The cold-start problem is a challenging issue in recommendation systems.\n136. Training high-quality CTR models using sparse user-item interaction data is a challenging task.\n137. Cold-start primarily involves two aspects: users and items.\n138. As shown in Figure2, the AntM 2 C dataset exhibits a natural long-tail distribution in both users and items.\n139. Therefore, we conduct a comprehensive evaluation of coldstart baseline methods based on AntM 2 C dataset.\n140. In cold-start CTR prediction, we split the dataset based on time, using data before 20230717 as the training set and data on 20230717 as the validation and test sets.\n141. Based on this data division, we simulated two common cold-start problems in practice: few-shot and zero-shot.\n142. • Few-shot: users and items that appear in the training set with a count greater than 0 and less than9, meaning there is only a small amount of training data for these users and items.\n143. • Zero-shot: users and items that have never appeared in the training set, indicating that either the user is visiting the scenario for the first time or the item has been launched and added to the scenario on the first day.\n144. Table6shows the data distribution of the test set under cold-start CTR evaluation.\n145. By using this dataset division, we can comprehensively evaluate and compare the performance of CTR models on few-shot and zero-shot samples.\n146. For few-shot samples, we can observe the model\'s performance with only a small amount of training data and evaluate the model\'s generalization ability.\n147. For zero-shot samples, we can evaluate the model\'s recommendation ability on samples that it has never seen before.\n148. The key issue in cold-start modeling is how to learn user preferences and embeddings of users and items with limited data.\n149. In recent years, meta-learning-based cold-start methods have become state-of-the-art methods.\n150. We selected several representative methods with publicly available code as our baseline models.\n151. • DropoutNet[13]: The DropoutNet is a popular cold-start method which applies dropout to control input, and exploits the average representations of interacted items/users to enhance the embeddings of users/items.\n152. • MAML[2]: The MAML algorithm is a popular meta-learning approach that aims to enable fast adaptation to new tasks with limited data.\n153. MAML learns a good initialization of model parameters that can be effectively adapted to new tasks quickly.\n154. We treat each user and item as a task in MAML, and conduct meta-training on warm items.\n155. Then we perform meta-testing on cold-start items.\n156. • MeLU[6]: The MeLU algorithm is the first to apply the MAML to address the cold-start problem in recommender systems.\n157. Although MetaEmb only optimizes the embeddings of items, we have also applied the same approach to optimize the embeddings of users.\n158. These base models share the common embedding and DNN structure.\n159. The dimensionality of embedding vectors of each input field is fixed to 32 for all our experiments.\n160. The Adam optimizer with a learning rate of 1e-3 is used to optimize the model parameters, and the training is performed for 3 epochs with a batch size of 512.\n161. 7shows the CTR performance for cold-start users and items.\n162. Because there is limited data for cold start users and items, we do not calculate AUC by scenarios, and evaluate the overall performance of cold start users and items.\n163. From the table, we can observe several phenomena.\n164. Firstly, compared to the results shown in Table5, the AUC for cold-start users and items are generally lower than the overall level, which demonstrates that AntM 2 C\'s data can effectively reflect the differences between cold and warm items and users.\n165. Secondly, different cold-start methods show distinguishable results in AntM 2 C, and all of them are significantly better than the DNN model without cold-start optimization.\n166. This indicates that AntM 2 C can effectively compare the effects of different cold-start methods and demonstrate the distinctiveness between methods.\n167. Finally, the lower performance of zero-shot compared to few-shot indicates that zero-shot CTR prediction is more challenging than few-shot.\n168. The two cold start modes provided by AntM 2 C can comprehensively evaluate cold-start CTR prediction.\n169. With the rise of large language models (LLMs), it has become a hot research topic to effectively transfer the knowledge of LLM to CTR prediction.\n170. There have been many works[3,4,9,11]based on multi-modal CTR modeling using features such as item and user text.\n171. In multi-modal evaluation, we adapt the same data processing approach as in multi-scenario evaluation mentioned in Section 4.1.1,and additionally include the text features from Table3: user query entities and item entities.\n172. The text features will be used as inputs to the model together with other ID features.\n173. For the baseline model, we use the language model to process the text features, and then concatenate the text embedding with other ID features and input them into the multi-scenario model described in Section 4.1.2.\n174. For ease of evaluation, we choose MMoE as the backbone and pre-trained Bertbase13[1]as the text embedding extractor.\n175. The output dimension of Bert\'s embeddings is 768.\n176. Then, a DNN with two layers, each layer having [768, 32] units, is used to reduce the dimension of Bert\'s embedding to 32.\n177. This reduced embedding is concatenated with other features and input into the MMOE model.\n178. More powerful language models and the application of text features will continue to be supplemented in future works.\n179. Table8shows the evaluation results of the multimodal CTR.\n180. It can be observed that, after adding the text modality, the CTR performance is better in data-sparse scenarios C, D, and E compared to using only the ID modality in the MMoE.\n181. Since the current baseline for using the text modality is relatively simple, the improvement in performance is not significant.\n182. However, this shows the potential of the text modality provided in AntM 2 C to improve CTR performance.\n\n请严格按照JSON格式返回：{"labels": [0,1,0,...]}，不要包含任何其他文字。'}], 'model': 'qwen-plus', 'response_format': {'type': 'json_object'}}}
2025-10-14 19:48:50,803 - DEBUG - Sending HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
2025-10-14 19:48:50,803 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-10-14 19:48:50,803 - DEBUG - send_request_headers.complete
2025-10-14 19:48:50,803 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-10-14 19:48:50,803 - DEBUG - send_request_body.complete
2025-10-14 19:48:50,803 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-10-14 19:48:59,817 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'vary', b'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding'), (b'x-request-id', b'f97ceaf5-2573-4eb7-9e39-131d53b04035'), (b'x-dashscope-call-gateway', b'true'), (b'content-type', b'application/json'), (b'req-cost-time', b'8976'), (b'req-arrive-time', b'1760442522123'), (b'resp-start-time', b'1760442531100'), (b'x-envoy-upstream-service-time', b'8941'), (b'content-encoding', b'gzip'), (b'date', b'Tue, 14 Oct 2025 11:48:50 GMT'), (b'server', b'istio-envoy'), (b'transfer-encoding', b'chunked')])
2025-10-14 19:48:59,818 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-14 19:48:59,818 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-10-14 19:48:59,818 - DEBUG - receive_response_body.complete
2025-10-14 19:48:59,818 - DEBUG - response_closed.started
2025-10-14 19:48:59,818 - DEBUG - response_closed.complete
2025-10-14 19:48:59,818 - DEBUG - HTTP Response: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "200 OK" Headers({'vary': 'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding', 'x-request-id': 'f97ceaf5-2573-4eb7-9e39-131d53b04035', 'x-dashscope-call-gateway': 'true', 'content-type': 'application/json', 'req-cost-time': '8976', 'req-arrive-time': '1760442522123', 'resp-start-time': '1760442531100', 'x-envoy-upstream-service-time': '8941', 'content-encoding': 'gzip', 'date': 'Tue, 14 Oct 2025 11:48:50 GMT', 'server': 'istio-envoy', 'transfer-encoding': 'chunked'})
2025-10-14 19:48:59,818 - DEBUG - request_id: f97ceaf5-2573-4eb7-9e39-131d53b04035
2025-10-14 19:48:59,819 - DEBUG - API request completed in 9.02 seconds
2025-10-14 19:48:59,819 - DEBUG - Raw model response: {"labels": [0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]}
2025-10-14 19:48:59,819 - INFO - Successfully processed 126 labels
2025-10-14 19:48:59,819 - ERROR - Label count mismatch for AntM 2 C_ A Large Scale Dataset For Multi_Scenario Multi_Modal CTR Prediction
2025-10-14 19:48:59,819 - INFO - Evaluating paper 5/18: AU_AIR_ A Multi_modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance
2025-10-14 19:48:59,819 - INFO - Starting model prediction
2025-10-14 19:48:59,819 - INFO - Attempt 1 of 5
2025-10-14 19:48:59,820 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-d4c0849a-553f-4469-8ec0-8fab369689fc', 'json_data': {'messages': [{'role': 'system', 'content': '你是一个专门用于识别学术论文中数据集描述的助手。你需要判断每个输入的句子是否描述了研究中使用的数据集。请确保：\n    1. 只输出JSON格式的结果\n    2. 结果格式必须为 {"labels": [0,1,0,...]}\n    3. 不要输出任何其他文字或解释'}, {'role': 'user', 'content': '你是一位专业的学术论文数据集描述识别专家。请仔细分析下面这篇论文中的每个句子，判断它们是否描述了本研究特定构建的数据集信息。\n\n    论文标题: AU_AIR_ A Multi_modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance\n\n    判断标准:\n    判断为1(数据集描述)的标准：\n    句子必须直接描述本研究特定构建的数据集,包括:\n    1. 该数据集的构建过程和方法\n    2. 该数据集的具体构成和规模\n    3. 该数据的来源和收集方式\n    4. 该数据的预处理步骤\n    5. 该数据集的可获取方式(如发布地址)\n    6. 或者其他直接或者间接描述了该数据集的句子\n\n    所有其他类型的句子均标记为0。\n    \n    请对每个句子进行分析，返回一个JSON格式的标签数组。格式要求：\n    1. 必须是有效的JSON格式\n    2. 只包含labels字段，值为0和1组成的数组\n    3. 数组长度必须与句子数量相同\n    4. 示例格式：{"labels": [0,1,0,1,...]}\n\n    需要判断的句子：\n\n    1. Abstract: Unmanned aerial vehicles (UAVs) with mounted cameras have the advantage of capturing aerial (bird-view) images.\n2. The availability of aerial visual data and the recent advances in object detection algorithms led the computer vision community to focus on object detection tasks on aerial images.\n3. As a result of this, several aerial datasets have been introduced, including visual data with object annotations.\n4. UAVs are used solely as flying-cameras in these datasets, discarding different data types regarding the flight (e.g., time, location, internal sensors).\n5. In this work, we propose a multi-purpose aerial dataset (AU-AIR) that has multi-modal sensor data (i.e., visual, time, location, altitude, IMU, velocity) collected in real-world outdoor environments.\n6. The AU-AIR dataset includes meta-data for extracted frames (i.e., bounding box annotations for trafficrelated object category) from recorded RGB videos.\n7. Moreover, we emphasize the differences between natural and aerial images in the context of object detection task.\n8. For this end, we train and test mobile object detectors (including YOLOv3-Tiny and MobileNetv2-SSDLite) on the AU-AIR dataset, which are applicable for real-time object detection using on-board computers with UAVs.\n9. Since our dataset has diversity in recorded data types, it contributes to filling the gap between computer vision and robotics.\n10. The dataset is available at https://bozcani.github.io/auairdataset.\n11. Unmanned aerial vehicles (UAVs) are extensively used as flying platforms of sensors for different domains such as traffic surveillance[1], managing the urban environment[2], package delivery[3]or aerial cinematography[4].\n12. For these applications, UAVs are equipped with mounted cameras and mainly gather visual data of the environment.\n13. Then, computer vision algorithms are applied to aerial visual data to extract high-level information regarding the environment.\n14. Object detection is one of the most studied problems in computer vision.\n15. The recent advances in deep learning (variants of convolutional neural networks (CNNs) mainly) have led to breakthrough object detection performances with the availability of large datasets and computing power.\n16. Since these methods require a large number of training samples, several datasets (e.g., COCO[5], Pascal VOC[6]) have been introduced for benchmarking for the object detection task.\n17. The samples in these datasets consist of natural images that are mainly captured by handheld cameras.\n18. The significant differences between natural and aerial images (such as object layouts and sizes) cause these object detectors to have trouble to find objects in aerial images.\n19. Therefore, several datasets (e.g.,[7]-[13]) have been introduced in recent years as a benchmark for object detection in aerial images.\n20. Besides visual data gathered by a camera, the data from other sensors might give crucial information about the environment.\n21. The use of UAVs as only flying cameras cut off the potential advance in multi-modal object detection algorithms for aerial applications.\n22. For instance, the recent advances in perception for autonomous driving have brought new datasets such as[14]-[16]including multi-modal data (e.g., RGB images, Global Positioning System (GPS) coordinates, inertial measurement unit (IMU) data).\n23. Although the data fusion for object detection is still open research topic[17], these multi-modal datasets allow a benchmark for further research.\n24. However, to the best of our knowledge, there is no such multi-modal dataset collected in a real-world outdoor environment for UAVs.\n25. In this work, we present a multi-modal UAV dataset (The AU-AIR dataset) in order to push forward the development of computer vision and robotic algorithms targeted at autonomous aerial surveillance.\n26. The AU-AIR dataset meets vision and robotics for UAVs having the multi-modal data from different on-board sensors.\n27. The dataset consists of 8 video streams (over 2 hours in total) for traffic surveillance.\n28. The videos mainly are recorded at Skejby Nordlandsvej and P.O Pedersensvej roads (Aarhus, Denmark).\n29. The dataset includes aerial videos, time, GPS coordinates and the altitude of the UAV, IMU data, and the velocity.\n30. The videos are recorded at different flight altitudes from 5 meters to 30 meters and in different camera angles from 45 degrees to 90 degrees (i.e., complete bird-view images that the camera is perpendicular to the Earth).\n31. Instances belonging to different object categories related to the traffic surveillance context are annotated with bounding boxes in video frames.\n32. Moreover, each extracted video frame is labeled with the flight data (See Fig.1).\n33. The whole dataset includes 32,823 labeled video frames with object annotations and the corresponding flight data.\n34. Eight object categories are annotated including person, car, van, truck, motorbike, bike, bus, trailer.\n35. The total number of annotated instances is 132,034.\n36. The dataset is split into 30,000 training-validation samples and 2,823 test samples.\n37. In this work, we emphasize differences between aerial and natural images in the context of object detection tasks.\n38. To this end, we compare image samples and object instances between the AU-AIR dataset and the COCO dataset[5].\n39. In our experiments, we train and evaluate two mobile object detectors (including YOLOv3-tiny[18]and MobileNetv2-SSD Lite[19]on the AU-AIR dataset.\n40. We form a baseline, including mobile object detectors since we focus on realtime performance and the applicability of object detection task onboard computers mounted on UAV.\n41. In recent years, several drone datasets have been introduced for object detection tasks ([7]-[13]).\n42. Zhu et al.[7]propose a UAV dataset (VisDrone) consisting of visual data and object annotations in images and frames.\n43. In the VisDrone dataset, object instances belonging the certain categories are annotated by bounding boxes and category labels.\n44. Besides object annotations, VisDrone includes some vision-related attributes such as the visibility of a scene, occlusion status.\n45. Du et al.[8]propose a benchmark dataset for object detection and tracking in aerial images.\n46. The dataset also includes meta information regarding the flight altitude.\n47. Hsieh et al.[9]propose a UAV-based counting dataset (CARPK) including object instances that belong to the car category.\n48. Robicquet et al.[10]introduce a UAV dataset (Stanford) that collects images and videos of six types of objects in the Stanford campus area.\n49. In this dataset, some of the object categories dominate the dataset having a high number of samples, whereas the remaining object categories have significantly less number of instances.\n50. Mueller et al.[11]propose synthetic dataset created by a simulator for target tracking with a UAV.\n51. Collins et al.[12]introduce a benchmarking website (VIVID) with an evaluation dataset collected under the DARPA VIVID program.\n52. Krajewski et al.propose an aerial dataset collected from highways, including object bounding boxes and labels of vehicles.\n53. These datasets are annotated by common objects in an environment such as humans and different types of vehicles (e.g., car, bike, van).\n54. However, they only include visual data and bounding box annotations for objects and discard other sensory data.\n55. Among these studies, only UAVDT[8]includes an attribute that gives limited information about the flight altitude (i.e., labels such as "low-level", "mid-level" and "high-level").\n56. Fonder et al.[20]propose a synthetic dataset (Mid-Air) for low altitude drone flights in unstructured environments (e.g., forest, country).\n57. It includes multi-modal data regarding the flight (e.g., visual, GPS, IMU data) without any annotations for visual data.\n58. There are also multi-modal drone datasets in the literature ([20]-[24]).\n59. However, the visual data are not collected for object detection since the main focus of these studies is the UAV navigation.\n60. Therefore, these datasets do not have object annotations.\n61. The comparison of existing datasets is given in TableI.\n62. Looking also at the summary of the existing studies in TableI, the followings are the main contributions of this work: • To the best of our knowledge, the AU-AIR dataset is the first multi-modal UAV dataset for object detection.\n63. The dataset includes flight data (i.e., time, GPS, altitude, IMU data) in addition to visual data and objects annotations.\n64. • Considering the real-time applicability, we form a baseline training and testing mobile object detectors with the AU-AIR dataset.\n65. We emphasize the differences between object detection in aerial images and natural images.\n66. The availability of large amounts of data and processing power enables deep neural networks to achieve state-of-theart results for object detection.\n67. Currently, deep learningbased object detectors are separated into two groups.\n68. The first group consists of region-based CNNs that ascend on image classifiers.\n69. Region-based CNNs propose image regions that are likely to contain an object and classify the region into a predefined object category.\n70. The second group has only one stage converting to the object detection problem into the bounding box prediction for objects, without re-purposing image classifiers.\n71. Faster-R-CNN[25]is one of the wellknown models belonging to the first group, YOLO[26]and SSD[27]are the popular object detectors that belong to the second group.\n72. Deep learning-based object detectors have trained and performed on large datasets such as COCO[5]and PASCAL[6].\n73. These datasets include natural images that contain a single object or multi objects in their natural environments.\n74. Most of the images in these datasets are captured by humans using a handheld camera so that the vast majority of images have side-view.\n75. There are challenges of the object detection in natural images such as occlusion, illumination changes, rotation, low resolution, crowd existence of instances.\n76. Aerial images have different characteristics from natural images due to having a bird\'s-eye view.\n77. First of all, objects in natural images are much larger than their counterparts in aerial images.\n78. For example, an object category such as humans may occupy a large number of pixels in natural images.\n79. However, it may have a few numbers of pixels   in an aerial image that is quite challenging to detect for object detectors (See Fig.2).\n80. Moreover, aerial images can be fed to a network with higher dimensions that increases computational cost in order to prevent the diminishing of pixels belonging to small objects.\n81. Secondly, an occlusion is observed in different conditions for natural and aerial images.\n82. In natural images, an object instance may be occluded by another foreground object instance (e.g., a human in front of a car).\n83. (See Fig.3.\n84. Thirdly, the perspective in aerial images makes appearances of objects short and squat.\n85. This fact diminishes the information regarding an object height (See Fig.4).\n86. Moreover, although aerial images can supply more contextual information about an environment by a broader view angle, the object instances may be amid cluttered.\n87. Lastly, having a drone to capture aerial images, the altitude changes during the flight can cause varieties in object size and appearance in aerial images.\n88. Therefore, a recording of aerial videos at different altitudes may change the levels of challenges mentioned above.\n89. To address the challenges mentioned in Section II, we propose a multi-modal drone dataset (AU-AIR) including videos, object annotations in the extracted frames and sensor data for the corresponding frames.\n90. The data are captured by low-level flight (max.30 meters) and for the scenario of a traffic surveillance.\n91. The AU-AIR dataset consists of video clips, sensor data, and object bounding box annotations for video frames.\n92. We have used a quadrotor (Parrot Bebop 2) to capture the videos and record the flight data.\n93. An on-board camera has recorded the videos with a resolution of 1920 × 1080 pixels at 30 frames per second (fps).\n94. The sensor data have been recorded for every 20 milliseconds.\n95. The AU-AIR dataset consists of 8 video clips (approximately in 2 hours of a total length) with 32,823 extracted frames.\n96. All videos are recorded for a scenario of aerial traffic surveillance at the intersection of Skejby Nordlandsvej and P.\n97. Moreover, the videos cover various lighting conditions due to the time of the day and the weather conditions (e.g., sunny, partly sunny, cloudy).\n98. Capturing an aerial video with a UAV brings different challenges for visual surveillance that are significantly different from natural images.\n99. To add these challenges in our dataset, we have captured the videos in different flight altitudes and camera angles.\n100. The flight altitude changes between 10 meters to 30 meters in the videos and the camera angle is adjusted from 45 degrees to 90 degrees (perpendicular to the Earth).\n101. An increase in the camera angle makes object detection task more challenging since images get differ from natural images.\n102. Although the videos have been recorded with 30 fps, we have extracted five frames for every second in order to prevent the redundant occurrence of frames.\n103. Both of raw videos and extracted frames have a resolution of 1920×1080 pixels.\n104. Considering a traffic surveillance scenario, we have manually annotated specific object categories in the frames.\n105. For annotation, we used a bounding box and object category index for each instance.\n106. The annotated object categories include eight types of objects which highly occur during the traffic surveillance: person, car, bus, van, truck, bike, motorbike, and trailer.\n107. For annotation, we employed workers on Amazons Mechanical Turk (AMT)[28].\n108. In order to increase the labeling quality, three workers annotated the same frame separately.\n109. Then, we combined annotations if they have the same object labels, and whose bounding boxes overlap more than a certain threshold.\n110. We chose a threshold as a value of 0.75 experimentally.\n111. In case this condition is not satisfied, we manually fine-tuned the bounding boxes and class labels.\n112. The category distribution over the dataset can be seen in Fig.5.\n113. In the context of traffic surveillance, cars appear significantly more than other classes, and three vehicle types (car, van, truck) have a major portion of annotated bounding boxes.\n114. The AU-AIR dataset includes frames that are captured in different flight altitudes (See Fig.6).\n115. We recorded the data mainly for 10 meters, 20 meters, and 30 meters with different camera angles from 45 degrees to 90 degrees.\n116. In addition to visual data and object annotations, the AU-AIR dataset includes sensor data that are logged during the video recording.\n117. In the dataset, we have the following attributes for each extracted frame: • la: latitude of the UAV (read from GPS sensor).\n118. • lo: longitude of the UAV (read from GPS sensor) • a: altitude of the UAV (read from altimeter) • φ: UAV roll angle (rotation around the x axis) (read from IMU sensor) • θ: UAV pitch angle (rotation around the y axis) (read from IMU sensor) • ψ: UAV yaw angle (rotation around the z axis) (read from IMU sensor) • V x : speed on the x axis • V y : speed on the y axis • V z : speed on the z axis TableIIshows unit values and ranges for each attribute except the date.\n119. The date (d) has a format of MMDDYYYY-HHMMSS where MM, DD, YYYY, HH, MM, SS indicates the month, day, year, hour, minutes, and second, respectively.\n120. The velocities (V x , V y , V z ) and rotation angles (φ, θ, ψ) are calculated according to the UAV body-frame given in Fig.7.\n121. We train and evaluate mobile object detectors with our dataset.\n122. During the evaluation, we consider real-time performance rather than achieving a state-of-the-art accuracy for the sake of the applicability.\n123. Therefore, we choose two mobile object detectors (YOLOv3-Tiny[18]and MobileNetv2-SSDLite[19]), which have a reasonable trade-off between the detection accuracy and the inference time.\n124. We configure YOLOv3-Tiny[18]and MobileNetv2-SSDLite[19]for the bench-marking using the default parameters (e.g., learning rate, input size) as suggested in the original papers.\n125. We use the models that are trained on the COCO dataset as backbones.\n126. We split the AU-AIR dataset into %60 training, %10 validation and %30 testing samples.\n127. The object detectors are adapted to the total number of classes in the AU-AIR dataset (8 classes in total) by changing their last layers.\n128. To compare detection performances, we use mean average precision (mAP) that is a prominent metric in object detection[5],[6].\n129. It is the mean of the average precision (AP) values which compute the precision score for an object category at discretized recall values over 0 to 1[6].\n130. We consider 11 different recall values as in[6]and the intersection over union (IoU) threshold as 0.5.\n131. For benchmarking, we train YOLOv3-Tiny and MobileNetv2-SSDLite with the AU-AIR Dataset.\n132. We use the batch size of 32 and Adam optimizer with the default parameters (alpha= 0.001, beta1=0.9,beta2=0.999).\n133. The training is stopped when the validation error starts to increase.\n134. Both networks are pre-trained on the COCO dataset.\n135. In order to see the effect of the training with an aerial dataset and a natural image dataset, we also use YOLOv3-Tiny and MobileNetv2-SSDLite trained on the COCO dataset without further training with the AU-AIR dataset.\n136. The results are given in TableIII.\n137. As shown in TableIII, the networks only trained on the COCO dataset have poor results.\n138. This is expected since the characteristics of natural images are significantly different from natural images.\n139. We observe that the AP values of motorbike and bicycle categories are significantly lower than the AP values of other categories.\n140. This fact might happen due to the class imbalance problem and the small object sizes of these categories.\n141. However, the bus category has the highest AP value, although there are fewer bus instances.\n142. This might result from the large size of bus instances in the frames.\n143. Furthermore, although the size of human instances is usually as small as the sizes of motorbike and bicycles, the AP values of the human category are relatively higher than these classes.\n144. This fact might be a consequence of the high number of human instances.\n145. There is no available AP values for the van and trailer categories in Table III since they do not exist in the COCO dataset.\n146. The baselines trained on the AU-AIR dataset are good at finding objects in aerial images that are captured at different altitudes and view angles.\n147. Qualitative results can be seen in Fig.8.\n148. Among the baselines, YOLOv3-Tiny has higher AP values and mAP value compared to MobileNetv2-SSDLite.\n149. There is no significant difference between inference times (17.5 FPS and 17 FPS for YOLOv3-Tiny and MobileNetv2-SSDLite on TX2, respectively).\n150. Since the number of instances of each object category is imbalanced in the AU-AIR dataset (Fig.5), we consider several methods to solve the imbalanced class problem in the next version of the dataset.\n151. As a first step, we will try to collect more data to balance the number of instances.\n152. Besides, we may consider adding synthetic data (i.e., changing the brightness of images, translation, rotation) to increase the number of object categories which has a low number of samples in the current version.\n153. We use AMT to annotate objects in images.\n154. Although three different people annotate one image and the annotations are manually checked by ourselves, there might be still overlooked samples that have weak annotations (e.g., unlabelled instances, loose bounding box drawings).\n155. Therefore, we consider using a three-step workflow proposed by Su et al.[29].\n156. In this workflow, the first worker draws a bounding box around an instance, the second worker verifies whether the bounding box is correctly drawn, and the third worker checks whether all object instances are annotated.\n157. Unlike other UAV object detection datasets, ours includes sensor data corresponding to each frame.\n158. In this work, we give a baseline only for object annotations and visual data.\n159. As future work, more baselines may be added to encourage research using sensor data (e.g., navigation and control of a UAV, object detection using multi-modal data).\n160. Also, we can add more visual sensors, such as multi-spectral cameras.\n161. We have used a ready-to-fly quadrotor (i.e., Parrot Bebop 2) to collect the whole dataset.\n162. We also consider collecting more samples from other platforms (e.g., different types of UAVs) using cameras that have different resolutions and frame rates.\n163. In this dataset, traffic surveillance is the primary context.\n164. In future work, we consider increasing the number of environment contexts to increase diversity in the dataset.\n165. In this work, we propose the AU-AIR dataset that is a multi-modal UAV dataset collected in an outdoor environment.\n166. Our aim is to fill the gap between computer vision and robotics having a diverse range of recorded data types for UAVs.\n167. Including visual data, object annotations, and flight data, it can be used for different research fields focused on data fusion.\n168. We have emphasized the differences between natural images and aerial images affecting the object detection task.\n169. Moreover, since we consider real-time performance and applicability in real-world scenarios, we have created a baseline, including two mobile object detectors in the literature (i.e., YOLOv3-Tiny[18]and MobileNetv2-SSDLite[19]).\n170. In our experiments, we showed that mobile networks trained on natural images have trouble in detecting objects in aerial images.\n\n请严格按照JSON格式返回：{"labels": [0,1,0,...]}，不要包含任何其他文字。'}], 'model': 'qwen-plus', 'response_format': {'type': 'json_object'}}}
2025-10-14 19:48:59,821 - DEBUG - Sending HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
2025-10-14 19:48:59,821 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-10-14 19:48:59,821 - DEBUG - send_request_headers.complete
2025-10-14 19:48:59,821 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-10-14 19:48:59,821 - DEBUG - send_request_body.complete
2025-10-14 19:48:59,821 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-10-14 19:49:09,604 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'vary', b'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding'), (b'x-request-id', b'5de6a7ae-7078-442a-a521-654eecb8250c'), (b'x-dashscope-call-gateway', b'true'), (b'content-type', b'application/json'), (b'req-cost-time', b'9744'), (b'req-arrive-time', b'1760442531137'), (b'resp-start-time', b'1760442540881'), (b'x-envoy-upstream-service-time', b'9708'), (b'content-encoding', b'gzip'), (b'date', b'Tue, 14 Oct 2025 11:49:00 GMT'), (b'server', b'istio-envoy'), (b'transfer-encoding', b'chunked')])
2025-10-14 19:49:09,604 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-14 19:49:09,604 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-10-14 19:49:09,604 - DEBUG - receive_response_body.complete
2025-10-14 19:49:09,604 - DEBUG - response_closed.started
2025-10-14 19:49:09,604 - DEBUG - response_closed.complete
2025-10-14 19:49:09,604 - DEBUG - HTTP Response: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "200 OK" Headers({'vary': 'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding', 'x-request-id': '5de6a7ae-7078-442a-a521-654eecb8250c', 'x-dashscope-call-gateway': 'true', 'content-type': 'application/json', 'req-cost-time': '9744', 'req-arrive-time': '1760442531137', 'resp-start-time': '1760442540881', 'x-envoy-upstream-service-time': '9708', 'content-encoding': 'gzip', 'date': 'Tue, 14 Oct 2025 11:49:00 GMT', 'server': 'istio-envoy', 'transfer-encoding': 'chunked'})
2025-10-14 19:49:09,604 - DEBUG - request_id: 5de6a7ae-7078-442a-a521-654eecb8250c
2025-10-14 19:49:09,605 - DEBUG - API request completed in 9.79 seconds
2025-10-14 19:49:09,605 - DEBUG - Raw model response: {"labels": [1,0,0,0,1,1,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]}
2025-10-14 19:49:09,605 - INFO - Successfully processed 116 labels
2025-10-14 19:49:09,605 - ERROR - Label count mismatch for AU_AIR_ A Multi_modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance
2025-10-14 19:49:09,605 - INFO - Evaluating paper 6/18: AV_NeRF_ Learning Neural Fields for Real_World Audio_Visual Scene Synthesis
2025-10-14 19:49:09,605 - INFO - Starting model prediction
2025-10-14 19:49:09,605 - INFO - Attempt 1 of 5
2025-10-14 19:49:09,605 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-9f93e12c-71af-4d54-9ebf-e08584a5d803', 'json_data': {'messages': [{'role': 'system', 'content': '你是一个专门用于识别学术论文中数据集描述的助手。你需要判断每个输入的句子是否描述了研究中使用的数据集。请确保：\n    1. 只输出JSON格式的结果\n    2. 结果格式必须为 {"labels": [0,1,0,...]}\n    3. 不要输出任何其他文字或解释'}, {'role': 'user', 'content': '你是一位专业的学术论文数据集描述识别专家。请仔细分析下面这篇论文中的每个句子，判断它们是否描述了本研究特定构建的数据集信息。\n\n    论文标题: AV_NeRF_ Learning Neural Fields for Real_World Audio_Visual Scene Synthesis\n\n    判断标准:\n    判断为1(数据集描述)的标准：\n    句子必须直接描述本研究特定构建的数据集,包括:\n    1. 该数据集的构建过程和方法\n    2. 该数据集的具体构成和规模\n    3. 该数据的来源和收集方式\n    4. 该数据的预处理步骤\n    5. 该数据集的可获取方式(如发布地址)\n    6. 或者其他直接或者间接描述了该数据集的句子\n\n    所有其他类型的句子均标记为0。\n    \n    请对每个句子进行分析，返回一个JSON格式的标签数组。格式要求：\n    1. 必须是有效的JSON格式\n    2. 只包含labels字段，值为0和1组成的数组\n    3. 数组长度必须与句子数量相同\n    4. 示例格式：{"labels": [0,1,0,1,...]}\n\n    需要判断的句子：\n\n    1. Abstract: Can machines recording an audio-visual scene produce realistic, matching audiovisual experiences at novel positions and novel view directions?\n2. We answer it by studying a new task-real-world audio-visual scene synthesis-and a first-of-itskind NeRF-based approach for multimodal learning.\n3. Concretely, given a video recording of an audio-visual scene, the task is to synthesize new videos with spatial audios along arbitrary novel camera trajectories in that scene.\n4. We propose an acoustic-aware audio generation module that integrates prior knowledge of audio propagation into NeRF, in which we implicitly associate audio generation with the 3D geometry and material properties of a visual environment.\n5. Furthermore, we present a coordinate transformation module that expresses a view direction relative to the sound source, enabling the model to learn sound source-centric acoustic fields.\n6. To facilitate the study of this new task, we collect a high-quality Real-World Audio-Visual Scene (RWAVS) dataset.\n7. We demonstrate the advantages of our method on this real-world dataset and the simulation-based SoundSpaces dataset.\n8. We recommend that readers visit our project page for convincing comparisons: https://liangsusan-git.github.io/project/avnerf/.\n9. We study a new task, real-world audio-visual scene synthesis, to generate target videos and audios along novel camera trajectories from source audio-visual recordings of known trajectories.\n10. By learning from real-world source videos with binaural audio, we aim to generate target video frames and spatial audios that exhibit consistency with the given camera trajectory visually and acoustically.\n11. This consistency ensures perceptual realism and immersion, enriching the overall user experience.\n12. As far as we know, attempts in the audio-visual learning literature [1-11] have yet to succeed in solving this challenging task thus far.\n13. Although there are similar works[12][13][14][15], these methods have constraints that limit their ability to solve this new task.\n14. Luo et al. [12]  propose neural acoustic fields to model sound propagation in a room.\n15. Su et al. [13]  introduce representing audio scenes by disentangling the scene\'s geometry features.\n16. These methods are tailored for estimating room impulse response signals in a simulation environment that are difficult to obtain in a real-world scene.\n17. Concurrent to our work, ViGAS proposed by Chen et al. [15]  learns to synthesize new sounds by inferring the audio-visual cues.\n18. However, ViGAS is limited to a few viewpoints for audio generation.\n19. We introduce AV-NeRF, a novel NeRF-based method of synthesizing real-world audio-visual scenes.\n20. AV-NeRF enables the generation of videos and spatial audios, following arbitrary camera trajectories.\n21. It utilizes source videos and camera poses as references.\n22. AV-NeRF consists of two branches: A-NeRF, which learns the acoustic fields of an environment, and V-NeRF, which models color and density fields.\n23. We represent a static audio field as a continuous function using A-NeRF, which takes the listener\'s position and head direction as input.\n24. A-NeRF effectively models the energy decay of sound as the sound travels from the source to the listener by correlating the listener\'s position with the 37th Conference on Neural Information Processing Systems (NeurIPS 2023).\n25. To the best of our knowledge, our method is the first NeRF-based system capable of synthesizing realworld videos with perceptually realistic binaural audios at arbitrary poses.\n26. However, existing datasets do not meet the specific requirements of our experiments, particularly in terms of simultaneously providing camera poses, high-quality binaural audios, and images.\n27. Therefore, we curated a highquality audio-visual scene dataset (real) to address this gap and facilitate further research on this problem.\n28. Additionally, we utilize (synthetic) SoundSpaces dataset[4]to validate our method.\n29. (1) RWAVS Dataset.\n30. We collected the Real-World Audio-Visual Scene (RWAVS) dataset to benchmark our method.\n31. In order to increase the diversity of our dataset, we recorded data across different scenarios.\n32. Fig.5shows the example scenarios we used for data recording, including both indoor and outdoor environments, which we believe represent most daily settings.\n33. RWAVS dataset comprises multimodal data, including camera poses, high-quality binaural audios, and videos.\n34. Unlike Replay-NVAS dataset[15], where the environment and the recording viewpoint are constant, RWAVS dataset contains various viewpoints in diverse environments.\n35. During data recording, we randomly moved around the environment while holding the device, capturing various acoustic and visual signals.\n36. RWAVS dataset encompasses all positions and directions (360 • ) within an environment.\n37. In detail, we employed a 3Dio Free Space XLR binaural microphone for capturing high-quality stereo audio, a TASCAM DR-60DMKII for recording and storing audio, and a GoPro Max for capturing accompanying videos.\n38. Additionally, an LG XBOOM 360 omnidirectional speaker was used as the sound source.\n39. For each environment and sound source combination, we collected data ranging from 10 to 25 minutes, resulting in a total collection of 232 minutes (3.8 hours) of data from diverse environments with varying source positions.\n40. We extract key frames at 1 fps from recorded videos and use COLMAP[46]to estimate the corresponding camera pose.\n41. Each key frame is accompanied by one-second binaural audio and one-second source audio, forming a complete data sample.\n42. For audio clips with noticeable background noise, we perform noise suppression using Adobe Audition[47].\n43. We split 80% data as training samples and the rest as validation samples.\n44. After pre-processing, we obtain 9850 and 2469 samples for training and validation, respectively.\n45. This dataset is challenging because of the diverse environments and various camera poses.\n46. We will release this dataset to the research community.\n47. (2) SoundSpaces Dataset.\n48. While RWAVS offers realistic training samples, its realism restricts its scale because it is time-consuming to record high-quality multimodal data in the real world.\n49. Therefore, we use the synthetic SoundSpaces dataset to augment our experiments.\n50. To evaluate our method on SoundSpaces dataset, we modify AV-NeRF to estimate impulse responses instead of the acoustic mask while keeping all other components intact.\n51. We follow NAF[12]selecting six representative indoor scenes, consisting of two single rooms with rectangular walls, two single rooms with non-rectangular walls, and two multi-room layouts.\n52. In each scene, SoundSpaces dataset provides an extensive collection of impulse response signals for sound source and sound receiver pairs, which are densely sampled from a 2D room grid.\n53. Each pair includes four discrete head orientations (0 • , 90 • , 180 • , and 270 • ), and each orientation is associated with two-channel binaural RIRs.\n54. We render RGB and depth images for each sound receiver pose using Habitat-Sim simulator[48,49].\n55. We maintain the same training/test split as NAF, allocating 90% data for training and 10% data for testing.\n56. Comparison with State-of-the-art.\n57. We compare AV-NeRF with the following baselines: (1) Mono-Mono duplicates the source audio a s twice to generate a fake binaural audio without modifying the source audio; (2) Mono-Energy assumes that the average energy of the target audio a t is known, scales the energy of the input audio to match the target, and duplicates the scaled audio to generate a stereo audio; (3) Stereo-Energy assumes that the energy of the two channels of the target audio a t is known, separately scales the energy of the input audio to match the target, and combines the two scaled channels to generate a stereo audio; (4) IRNAS[13]learns representing audio scenes by disentangling scene\'s geometry features with implicit neural fields, and we adapt INRAS to predict wave masks on RWAVS dataset; (5) NAF[12]designs local feature grids and an implicit decoder to capture the sound propagation in a physical scene, and we modify NAF to predict magnitude masks on RWAVS dataset; (6) ViGAS[15]achieves novel-view acoustic synthesis by analyzing audio-visual cues from source viewpoints.\n58. We select magnitude distance (MAG)[29], which measures the audio quality in the time-frequency domain, and envelope distance (ENV)[30], which measures the audio quality in the time domain, to evaluate various methods.\n59. Please refer to the supplementary material for implementation details.\n60. AV-NeRF outperforms all baselines across different environments, including office, house, apartment, and outdoors, by a significant margin (Table1(2) adding visual information to the input of A-NeRF is the most effective multimodal fusion method compared with concatenation and adding visual information to all layers of A-NeRF (Table2middle); (3) using embeddings represent relative angles outperforms applying positional encoding to either absolute or relative angles (Table2right).\n61. "Absolute Direction" represents applying positional encoding to the absolute angle, "Relative Direction" means transforming the relative angle with the positional encoding, and "Relative Embedding" is the embedding method.\n62. Visualization.\n63. We visualize the synthesized audio-visual scenes in Fig.6to intuitively assess the generation quality of our model.\n64. AV-NeRF can synthesize realistic binaural audios that have the same signal envelope and channel difference as the ground-truth audios.\n65. We compare AV-NeRF with traditional audio coding methods[50,51]and advanced learningbased neural field methods[12,13]using T60, C50, and EDT metrics[13].\n66. Please refer to our supplementary material for implementation details.\n67. Table3shows that AV-NeRF outruns both traditional and advanced methods, achieving 21% relative improvement on T60 metric compared with the previous state-of-the-art method INRAS, 5% on C50, and 16% on EDT.\n\n请严格按照JSON格式返回：{"labels": [0,1,0,...]}，不要包含任何其他文字。'}], 'model': 'qwen-plus', 'response_format': {'type': 'json_object'}}}
2025-10-14 19:49:09,606 - DEBUG - Sending HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
2025-10-14 19:49:09,606 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-10-14 19:49:09,606 - DEBUG - send_request_headers.complete
2025-10-14 19:49:09,606 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-10-14 19:49:09,606 - DEBUG - send_request_body.complete
2025-10-14 19:49:09,606 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-10-14 19:49:13,111 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'vary', b'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding'), (b'x-request-id', b'd10f6ea6-a76b-439c-9509-46e0ee2c18d4'), (b'x-dashscope-call-gateway', b'true'), (b'content-type', b'application/json'), (b'req-cost-time', b'3470'), (b'req-arrive-time', b'1760442540916'), (b'resp-start-time', b'1760442544387'), (b'x-envoy-upstream-service-time', b'3468'), (b'content-encoding', b'gzip'), (b'date', b'Tue, 14 Oct 2025 11:49:03 GMT'), (b'server', b'istio-envoy'), (b'transfer-encoding', b'chunked')])
2025-10-14 19:49:13,111 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-14 19:49:13,111 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-10-14 19:49:13,112 - DEBUG - receive_response_body.complete
2025-10-14 19:49:13,112 - DEBUG - response_closed.started
2025-10-14 19:49:13,112 - DEBUG - response_closed.complete
2025-10-14 19:49:13,112 - DEBUG - HTTP Response: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "200 OK" Headers({'vary': 'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding', 'x-request-id': 'd10f6ea6-a76b-439c-9509-46e0ee2c18d4', 'x-dashscope-call-gateway': 'true', 'content-type': 'application/json', 'req-cost-time': '3470', 'req-arrive-time': '1760442540916', 'resp-start-time': '1760442544387', 'x-envoy-upstream-service-time': '3468', 'content-encoding': 'gzip', 'date': 'Tue, 14 Oct 2025 11:49:03 GMT', 'server': 'istio-envoy', 'transfer-encoding': 'chunked'})
2025-10-14 19:49:13,112 - DEBUG - request_id: d10f6ea6-a76b-439c-9509-46e0ee2c18d4
2025-10-14 19:49:13,112 - DEBUG - API request completed in 3.51 seconds
2025-10-14 19:49:13,112 - DEBUG - Raw model response: {"labels": [1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}
2025-10-14 19:49:13,112 - INFO - Successfully processed 68 labels
2025-10-14 19:49:13,112 - ERROR - Label count mismatch for AV_NeRF_ Learning Neural Fields for Real_World Audio_Visual Scene Synthesis
2025-10-14 19:49:13,112 - INFO - Evaluating paper 7/18: BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment
2025-10-14 19:49:13,112 - INFO - Starting model prediction
2025-10-14 19:49:13,112 - INFO - Attempt 1 of 5
2025-10-14 19:49:13,112 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-e73cf175-ad30-4613-8b85-839fec08c93e', 'json_data': {'messages': [{'role': 'system', 'content': '你是一个专门用于识别学术论文中数据集描述的助手。你需要判断每个输入的句子是否描述了研究中使用的数据集。请确保：\n    1. 只输出JSON格式的结果\n    2. 结果格式必须为 {"labels": [0,1,0,...]}\n    3. 不要输出任何其他文字或解释'}, {'role': 'user', 'content': '你是一位专业的学术论文数据集描述识别专家。请仔细分析下面这篇论文中的每个句子，判断它们是否描述了本研究特定构建的数据集信息。\n\n    论文标题: BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment\n\n    判断标准:\n    判断为1(数据集描述)的标准：\n    句子必须直接描述本研究特定构建的数据集,包括:\n    1. 该数据集的构建过程和方法\n    2. 该数据集的具体构成和规模\n    3. 该数据的来源和收集方式\n    4. 该数据的预处理步骤\n    5. 该数据集的可获取方式(如发布地址)\n    6. 或者其他直接或者间接描述了该数据集的句子\n\n    所有其他类型的句子均标记为0。\n    \n    请对每个句子进行分析，返回一个JSON格式的标签数组。格式要求：\n    1. 必须是有效的JSON格式\n    2. 只包含labels字段，值为0和1组成的数组\n    3. 数组长度必须与句子数量相同\n    4. 示例格式：{"labels": [0,1,0,1,...]}\n\n    需要判断的句子：\n\n    1. Abstract: Banding, also known as staircase-like contours, frequently occurs in flat areas of images/videos processed by the compression or quantization algorithms.\n2. As undesirable artifacts, banding destroys the original image structure, thus inevitably degrading users\' quality of experience (QoE).\n3. In this paper, we systematically investigate the banding image quality assessment (IQA) problem, aiming to detect the image banding artifacts and evaluate their perceptual visual quality.\n4. Considering that the existing image banding databases only contain limited content sources and banding generation methods, and lack perceptual quality labels (i.e.mean opinion scores), we first build the largest banding IQA database so far, named Banding Artifact Noticeable Database (BAND-2k), which consists of 2,000 banding images generated by 15 compression and quantization schemes.\n5. A total of 23 workers participated in the subjective IQA experiment, yielding over 214,000 patch-level banding class labels and 44,371 reliable image-level quality rating scores.\n6. Subsequently, we develop an effective no-reference (NR) banding evaluator for banding detection and quality assessment by leveraging frequency characteristics of banding artifacts.\n7. To be more specific, a dual convolutional neural network (CNN) is employed to concurrently learn the feature representation from the high-frequency and low-frequency maps, thereby enhancing the ability to discern banding artifacts.\n8. The quality score of a banding image is generated by pooling the banding detection maps masked by the spatial frequency filters.\n9. The experimental results demonstrate that our banding evaluator achieves a remarkably high accuracy in banding detection and also exhibits high SRCC and PLCC results with the perceptual quality labels, even without directly learning a regression model for banding quality evaluation.\n10. These findings unveil the strong correlations between the intensity of banding artifacts and the perceptual visual quality, thus validating the necessity of banding quality assessment.\n11. The BAND-2k database and the proposed banding evaluator will be available at https://github.com/zijianchen98/BAND-2k.\n12. R ECENT years have witnessed a rapid emergence of media streaming services and social platforms.\n13. YouTube, Netflix, and TikTok account for more than half of the world\'s video traffic.\n14. Improving the quality of images under limited encoding, Zijian Chen, Wei Sun, Jun Jia, Zicheng Zhang, Xiongkuo Min, and Guangtao Zhai are with the Institute of Image Communication and Information Processing, Shanghai Jiao Tong University, Shanghai 200240, China (e-mail: {zijian.chen,sunguwei, jiajun0302, zzc1998, minxiongkuo, zhaiguangtao}@sjtu.edu.cn).\n15. Fangfang Lu is with the College of Computer Science and Technology, Shanghai University of Electric Power, Shanghai 200290, China (email:lufangfang@shiep.edu.cn).\n16. Jing Liu is with the School of Electrical and Information Engineering, Tianjin University, Tianjin, China.(e-mail:jliu tju@tju.edu.cn).\n17. transmission bandwidth, and storage condition is a necessary prerequisite for meeting the quality of experience (QoE) of users.\n18. In the stages between image acquisition and display, an image may suffer from various types of degradation, while banding artifacts are a kind of false contour distortion that is quite perceptible to the human eye.\n19. Since the visual quality of image contents greatly affects the QoE of end-users, it is highly desirable to design an effective banding image quality assessment (IQA) method, which aims to automatically detect the traces of such false contours and predict the objective quality of banding images that can be used to develop pre-processing or postprocessing debanding algorithms and optimize the performance of streaming media application.\n20. Normally, banding artifacts take on the appearance of annual rings, radiation circles, halos, or geographical contour lines and especially exist in the background regions (e.g., sky, water, and wall surface), where the color transition is not smooth enough.\n21. Nearly all existing image or video encoders, including H.264/AVC[28], VP9[38], and H.265/HEVC[27]can introduce such artifacts more or less.\n22. Current banding IQA research can be divided into two categories: subjective quality assessment and objective quality assessment.\n23. The existing subjective banding IQA research[39],[41]-[43]mainly investigates the limited banding scenarios with internal-used and undisclosed databases while lacking the quality label and may be insufficiently generalizable to large-scale commercial applications.\n24. Meanwhile, general IQA methods aiming at common distortions are inapplicable for banding exacerbated images due to the essential differences between them.\n25. First, banding artifacts usually hold tiny, staircaselike, and regional structures, which can be regarded as a kind of high-frequency artifact in smooth areas, while general distortions occur obviously in the whole image and are globally uniform.\n26. Second, the perceptual severity of banding is quantified based on its fraction of coverage and intensity in an image, which is widely divergent from the design philosophy of many existing IQA approaches.\n27. As a result, it is challenging to design an effective banding IQA method.\n28. To address these limitations, we first conduct a comprehensive subjective study of banding exacerbated images and create the largest banding IQA database to date with reliable mean opinion scores (MOS) and patch-level banding labels.\n29. We also propose a novel no-reference banding evaluator for banding detection and quality assessment by leveraging the frequency characteristics of banding artifacts.\n30. First, due to the dissimilar peculiarities of banding and smooth regions, the same distortion in different regions, e.g., textual and pictorial regions, may lead to different visual perception of human beings.\n31. Considering that banding manifests as a high-frequency artifact that exists in the lowfrequency smooth region, we propose a dual-branch CNN, which takes the high-frequency map and low-frequency map as inputs simultaneously, to hierarchically incorporate different visual features from the first layer and the last layer of our Resnet-50 backbone, thus making the model learn more effective banding feature representation and achieving more accurate banding region discrimination.\n32. Furthermore, inspired by the previous studies[15],[16], spatial frequency extracts information consistent with the human visual system (HVS), which not only reflects the overall active level in an image but also intuitively quantifies the contrast information.\n33. In other words, the value of spatial frequency is large in smooth areas, while becoming small in areas with harsh contrast changes, i.e., banding areas.\n34. Based on this mechanism, we adopt a spatial frequency masking strategy to refine the detected banding map and then pool the masked banding detection map to obtain the image-level banding quality score.\n35. • We propose a novel no-reference banding evaluator for banding detection and quality assessment based on frequency characteristics of banding artifacts, which utilizes a dual-branch CNN model to extract hierarchical banding-related feature representation from the high-frequency maps and low-frequency maps simultaneously.\n36. • Experimental results show that the proposed banding evaluator achieves the best performance in banding detection and significantly surpasses baselines in terms of SRCC and PLCC in the banding IQA task, which demonstrates the effectiveness of the proposed model.\n37. The remainder of this paper is organized as follows.\n38. Section II provides an overview of related works, including the stateof-the-art banding databases and detection methods.\n39. Section III introduces the construction of the BAND-2k database and the subjective assessment study.\n40. Section IV proposes an effective no-reference banding evaluator for banding detection and quality assessment.\n41. Section V gives the experimental results and analysis.\n42. Section VI concludes this paper.\n43. In this section, we first provide an overview of the state-ofthe-art banding-related databases (TableI) and then review the banding detection and evaluation methods.\n44. The first banding artifact-relevant VQA database was proposed by Wang et al.[39], which consists of 21 stimuli with different quantization grades generated by VP9[38]from 7 clips of 1280 × 720 30fps video.\n45. Authors in[41]investigated the effect of encoding parameters and dithering on the visibility of banding.\n46. Nine 4k-10bit source clips from the existing Netflix catalogue between 1 and 5 seconds were used to generate banding distorted videos.\n47. Each source content was downsampled to appropriate resolutions (1080p, 2k, or 4k) with certain bit-depth and further compressed by libaom (an AV1 codec library) at QPs {12, 20, 32}.\n48. More recently, Kapoor et al.[43]constructed one of the first databases for data-driven image banding assessment models.\n49. This research included about 1,440 images shot from over 600 pristine HD videos with a resolution of 1920 × 1080.\n50. Six levels of bit-depth quantization in luminance and chrominance channels are introduced to obtain different intensities of banding.\n51. Meanwhile, the banding images were semi-automatically segmented and labeled into banded and non-banded to form a patch-level banding dataset, which allows for training machine learning-based and deep learning-based banding classification methods.\n52. However, to the best of our knowledge, thus far there still lacking a benchmarking dataset in the banding detection and the corresponding banding IQA domain.\n53. Researchers either resort to image/video quality datasets that do not aim at banding distortion or build a small, attribute-restricted, in-house dataset by themselves.\n54. This motivates us to construct a large-scale subjective assessment database focus on the perceived bandingaffected image quality.\n55. Early research on banding detection mainly focuses on false contour identification, which aims to find the wrong boundary rather than a "true" region edge in the image.\n56. Authors in[31]-[33]utilized monotonicity or non-monotonicity features of local support regions including the gradient, contrast, variance, and entropy information to measure the loss of low-amplitude detail caused by banding.\n57. However, these works ignored the perceptual characteristics of the human visual system (HVS) and thus did not perform a good correlation with subjective tests.\n58. Another banding detection strategy is conducted at the pixel-level estimation and segmentation.\n59. Bhagavathy et al.[30]proposed to identify banding artifacts by calculating the likelihood of pixel difference.\n60. Baugh et al.[34]measured the severity of banding based on the number of a group of connected pixels with the same color.\n61. Wang et al.[39]first detected uniform segments to find possible banding areas and further incorporated edge features (e.g.length and contrast) to capture false boundaries.\n62. Nevertheless, these kinds of methods are typically sensitive to edge noise and are computationally expensive, causing limited application in realtime scenarios.\n63. Towards addressing these problems, Tu et al.[42]presented a completely no-reference banding detection method, which combines various properties of HVS with a number of preprocessing steps to refine banding edge detection.\n64. Instead of regarding banding detection as a false edge detection problem, Tandon et al.[41]heuristically utilized the effect of contrast sensitivity function (CSF) on banding visibility and its dependence on spatial frequency.\n65. Based on this, Krasula et al.[40]further compared the banding annoyance with more commonly studied compression artifacts and proposed a banding-aware video quality metric.\n66. In recent years, deep learning approaches have prevailed in various VQA tasks.\n67. As the pioneering work, Kapoor et al.[43]developed an automated CNN-based banding detector for the first time, which is a simple two-stage algorithm and gives rise to devising other learning-based techniques.\n68. In this work, we build a large-scale banding database and propose a data-driven banding indicator that can generate pixelwise banding visibility maps with corresponding subjectively  consistent quality scores by combining human visual mechanisms and deep learning techniques.\n69. Subjective banding image quality assessment facilitates the development of automatic objective banding image and video quality models.\n70. We created the largest banding database in existence, denoted as the BAND-2k database, which consists of 2,000 banding distorted images and over 214,000 patch-level banding class labels.\n71. Then, a subjective experiment was conducted to obtain the mean opinion scores (MOS) of the BAND-2k database.\n72. The workflow of the banding database construction is shown in Fig.1.\n73. To build a content-rich and balanced database, we manually collected source videos including computer graphics (CG), usergenerated content (UGC), and professionally-generated content (PGC) from two popular media websites Bilibili.comand Youtube.com.\n74. Then, 885 clips with multiple spatial resolutions (i.e., 4096×2160, 3840×2160, 1920×1080) and frame rates (i.e., 60, 50, 30, 25) are chosen as candidate.\n75. Note that videos on the mentioned websites are firstly annotated by the community with assigned a number of favorites, views, and downloads.\n76. These statistics correlate with the content and quality of a video, which guides our choices to some extent.\n77. All videos selected on the website are released under an appropriate creative commons (CC) license that allows further editing and redistribution.\n78. After content selection, we further unified the format of all video clips, especially the spatial resolution and the pixel format, which avoid the effect of other facts on visual quality.\n79. Concretely, we first converted the frame rate of the original clips to 25fps, which is to reduce the storage pressure while ensuring the graphics quality.\n80. Considering the commonly used aspect ratio of the user interface and displays is 16:9, we cropped the partially unqualified videos rather than shrinking images unevenly.\n81. Then, we downsampled the trimmed spatial resolution 3840×2160 to a lower resolution -1920×1080 for the following subjective study.\n82. Before conducting the following experiments, we manually removed the videos that are either too dark or bright, overly blurry or colorful, which helps to obtain more reliable subjective assessments.\n83. To avoid redundancy and to make sure the diversity of selected contents, we also conducted attribute analysis studies on the selected images.\n84. Four metrics that correlated with human perception, i.e., contrast, brightness, sharpness, and colorfulness, are adopted as content diversity metrics.\n85. All video attributes are calculated on every 10 frames to reduce computational complexity, which are then averaged over frames of each video sequence.\n86. Fig.2shows the distribution of attributes extracted from the selected videos.\n87. • Contrast: The contrast metric is simply defined as the standard deviation of pixel gray-scale intensities[25].\n88. • Colorfulness: The colorfulness metric is measured by the R, G, and B components[25].\n89. We first compute two matrices of differences between channels rg = R -G and yb = 1 2 (R + G) -B.\n90. Then, the colorfulness metric can be calculated as µ 2 rg + µ 2 yb + σ 2 rg + σ 2 yb , where µ and σ are the mean and standard deviation of their corresponding terms, respectively.\n91. • Sharpness: The cumulative probability of blur detection (CPBD) metric[24]is used to measure the image sharpness, which estimates the probability of detecting blur at each edge.\n92. • Brightness: The brightness of an image is obtained directly from the pixel gray intensities in R, G, and B channels.\n93. Finally, the number of source videos are reduced to 873.\n94. Fig.3displays thumbnails for 30 selected representative video clips.\n95. To simulate authentic banding artifacts that exist in real viewing scenarios, we introduced four encoding/transcoding strategies including H.264/AVC[28], H.265/HEVC[27], VP9[38], and bit-depth manipulation[43]with fifteen quantization schemes in total.For H.264/AVC and H.265/HEVC, considering the range of their quantization parameter (QP in ffmpeg), we selected three typical QP values, namely, QP = {18, 33, 43}.\n96. This is because that coded video clips with a QP value smaller than 18 normally provide perceptual lossless quality, while coded video clips with a QP value larger than 43 will not be able to offer adequate quality, which may affect the subjective test of banding artifacts.\n97. Similarly, for the VP9 encoder, the QP values under our close inspection are chosen as {28, 45, 60}.\n98. Moreover, we applied the same quantization strategies as in[43], which introduces banding distortion by scaling bit-depth in luminance and chrominance channels.\n99. Here, the color coding scheme YCbCr4:2:0 is applied for maximum compatibility.\n100. To sum up, we generated fifteen levels of banding with different intensities and shapes to enhance the diversity of the database.\n101. The banding database is then built by extracting frames from the distorted video clips, resulting in 2,000 images with a resolution of 1920×1080.\n102. Fig.4shows the visualization results of banding exacerbated images.\n103. In the process of image patch labeling, we initially intend to extract image patches of size 235×235 from banding images directly and perform annotation operations.\n104. However, considering the theoretical number of patches, it will cost a lot of manpower and time to label patch by patch.\n105. Therefore, we first segmented the banding images roughly and label them into banded and non-banded regions.\n106. Then, labelled image patches are generated from these segmented and labelled images by a sliding window.\n107. Specifically, we followed the same demarcation of the banded and non-banded images in[43]that a patch is labelled as banded if it has more than 30% overlap with banded regions in the image.\n108. Eventually, a banding dataset containing 2,000 distorted images with 1920×1080 resolution and 214,324 labelled image patches of size 235×235 is built.\n109. To the best of our knowledge, it is the largest banding dataset in existence, which enables training various machine/deep learning based banding detection models and facilitates the development of image/video debanding techniques.\n110. TableIIreports the composition of labelled image patch dataset.\n111. It can be observed that the number of banded patches is a bit smaller than non-banded patches due to the fact that banding usually appears in smooth background areas.\n112. As shown in Fig.1, the subjective quality study contains four steps.\n113. In addition to preparing the experimental environment, subjects should pass the qualification test first to participate in the study.\n114. After the subjective rating, all resulting scores need to be analyzed and examined before generating the final mean opinion score (MOS).\n115. 1) Experimental Environment Setting: In this study, a total of 25 inexperienced subjects are gathered in a laboratory environment, where relevant experimental configuration must satisfy the following requirements: • Considering the viewing effect, desktops and laptops are allowed as displays.\n116. • The resolution of displays must be larger than or equal to 1920×1080 to show the images without spatial downsampling.\n117. • The viewing distance and optimal horizontal viewing angle are set as 1.9 times the height of the display and respectively.\n118. 500 recommendation[26].\n119. As a result, we used a 27-inch AOC Q27U2D monitor with a resolution of 2560×1440 for assessment with 25 subjects.\n120. Due to the large number of images to be assessed, we divided the dataset into ten sessions to avoid visual fatigue.\n121. Each session of tests took nearly 2 hours with a 30-minute break for each participant.\n122. 2) Qualification Test: Before starting the main experiment, subjects are required to pass a quiz to get the qualification of conducting follow-up experiments.\n123. Firstly, we manually selected 10 labeled banding exacerbated images beyond the database as training images to familiarize subjects with the operation interface and the goal of this subjective test.\n124. The quiz consists of two parts including banding classification and image-level quality rating.\n125. In banding classification, subjects were told to divide the test image into banded or non-banded.\n126. In image-level quality rating, subjects were instructed to focus on the coverage and intensity of banding areas, as well as the overall quality of images to get the final quality score.\n127. The rating scale is continuous from 0 to 100 while a higher value indicates more severe banding (i.e., more visible or occupying larger portion of the image).\n128. To make the quiz objective and fair, we take the banding images labeled by domain experts as the ground truth, while two existing banding metrics BBAND[42]and DBI[43]are used to determine the normal range of the rating scores.\n129. That is the quality scores rated by subjects should not exceed 20% of the above banding metrics.\n130. As a result, only 23 subjects with an accuracy above 80% in banding classification and subjective scores in the normal range were allowed to pass the quiz.\n131. Note that the steps for taking a quiz are the same as the main experiment, which aims to guarantee the consistency of results.\n132. 3) Formal Study: We adopted the single-stimulus (SS) method in this test.\n133. Ten golden images that have the acknowledged high quality or poor quality (assessed by BBAND[42]and DBI[43]) were added to each session for controlling the scoring deviations.\n134. Besides, 3 repeated images are randomly inserted into each session to ensure consistency of scores before and after subjects scoring.\n135. At last, 23 qualified subjects were asked to provide their opinions on the shuffled image groups.\n136. The resulting scores were collected and packed for further analysis.\n137. 4) Result Analysis: In total, 46,000 scores were collected by 23 qualified subjects in the main study phase.\n138. However, considering the qualification quiz cannot completely disallow those unreliable workers to muddle through to the main study while reliable subjects may also occasionally score odd values, which may be caused by the inter-individual differences in perceiving the quality of the unique characteristics of different contents.\n139. Thus, we further investigated the confidence of rating scores and removed outliers following the Grubbs\' test[23],[45].\n140. Concretely, let s = (s 1 , s 2 , . . ., s N ) denote a set of raw scores collected for one distorted image.\n141. The test statistics is defined as the maximum absolute deviation of a sample standard deviation (SD) unit from the sample mean, which is mathematically expressed as where s and σ s denote the mean and standard deviation of the sample, respectively.\n142. Then, given a significant level α, a sample is detected as an outlier if where t 2 α/(2N ),N -2 represents the upper critical value of the tdistribution with N -2 degrees of freedom and a significance level of α/(2N ).\n143. Empirically, we set the significance level α at 0.05.\n144. Then, a sample is identified as an outlier if its distance to the sample mean is larger than 2.5 times SD and is removed.\n145. Following the aforementioned steps, the total number of scores Fig.6.\n146. The overall architecture of the proposed method.\n147. Given a banding distorted image, it is first divided into patches.\n148. Then, the patch-level high-frequency map (HFM) and low-frequency map (LFM) are generated by Sobel operation and piece-wise smooth algorithm[19], respectively.\n149. After that, a dual-branch CNN (CNN H and CNN L ) is deployed to extract hierarchical features with different visual information and thus classify the patches into banded or non-banded.\n150. Lastly, a spatial frequency masking strategy is introduced to refine the banding map and calculate the image-level banding quality score.\n151. Note that the dual-branch networks do not share parameters.\n152. was reduced to 44,371, and MOS was created by averaging the scores for each image.\n153. Fig.5presents the histogram of MOSs over the entire database, showing a broad MOS distribution of banding images.\n154. In this section, we describe the architecture of the proposed banding evaluator in detail, as shown in Fig.6.\n155. As stated before, banding usually appear as high-frequency information in the smooth background, while humans perceive high-frequency texture regions and low-frequency plateau regions through different neural channels concurrently, and transfer the upper visual features into the cerebral cortex for final processing[17],[44].\n156. Inspired by this, we employ high-frequency maps (HFM) and low-frequency maps (LFM) as the deep learning network inputs, which represent the texture and structural information of the image respectively, to mimic the recognition mechanism of the human brain for better banding identification.\n157. High-frequency Maps.\n158. Since gradient has been widely used to represent edge information and has been confirmed beneficial to acquire high-frequency components with low computational cost[20]-[22], we apply the isotropic Sobel operator to each patch for enhancing the details of banding artifacts.\n159. Given an input patch I, the high-frequency map is calculated by where S x and S y are the horizontal and vertical isotropic Sobel operators, respectively." * " denotes the convolution operation.\n160. Low-frequency Maps.\n161. To maintain the principal content of the image and filter out the influence of high-frequency information, we use the piece-wise smooth algorithm[19]to generate the low-frequency map by minimizing a function for image approximation recovery: where L represents the low-frequency map, Ω and E denotes the image domain and edge set, respectively.\n162. P indicates the pixel and E dσ represents the total edge length.\n163. The coefficients α and β are positive regularization constants.\n164. An example of frequency maps is shown in Fig.7.\n165. To obtain the overall banding score, we first divide the banding image into 235×235 patches and apply CNN-based classification to each patch, resulting in a banding classification label for each patch, i.e., banded or non-banded.\n166. As shown in Fig.6, the proposed network consists of two parallel branches, namely CNN H and CNN L , which take the patch-level high-frequency map and low-frequency map as input, respectively.\n167. For each branch, we propose to use Resnet-50[18]as the backbone.\n168. Specifically, we incorporate the feature maps extracted from the first convolutional layer and the last layer of Resnet-50 as hierarchical visual features, which represent different visual information[46],[47]and can be used as predictive information to enhance the discrimination ability of the network for banding and non-banded regions.\n169. Afterward, the features extracted from two branches are concatenated first and reshaped into 128dimensional vectors through two fully-connected layers, which is further followed with the sigmoid activation function to output the final predicted label, namely banded or non-banded.\n170. Of note is that sharing parameters is extremely unfavorable for extracting low-and high-frequency features simultaneously, we thereby deploy two branches that work independently and do not share parameters.\n171. The loss function adopted here is binary cross entropy.\n172. With the trained CNN H and CNN L , each input patch is predicted to obtain a banding or non-banded label.\n173. To better guide the pre-processing and post-processing debanding algorithms, it is necessary to generate a quality score for the entire banding image.\n174. Since the visibility of edge is also affected by content, we further consider the effect of spatially varying content information on the local quality of human perception.\n175. As a consequence, we introduce the spatial frequency masking strategy to determine the weighting factor for the detected banding regions in each patch adaptively and thus obtain the image-level banding severity score while refining the visibility of banding artifacts.\n176. 1) Spatial Frequency Masking: The spatial frequency is defined as the activity level of an image, which establishes a filterbank based on the visual stimulus and is in accordance with HVS[16].\n177. In this paper, we propose to apply spatial frequency as an effective contrast criterion to banding measurement.\n178. Specifically, given an image of size I W × I H , divided into N × N patches, where I W and I H denote the number of columns and rows respectively.\n179. The column (CF k ) and row (RF k ) frequencies of the image patches are given by where I(x, y) is the pixel value of the image patch.\n180. Then, the resulting spatial frequency of an N × N patch is computed as where k is the number of patches (1 ≤ k ≤ I W I H N 2 ).\n181. Since most banding regions are likely to have large contrast including edges and textures, which should be assigned greater weights than the smooth and blurred areas.\n182. Accordingly, we design a banding visibility transfer function to express spatial frequency masking as a function of the local textural feature.\n183. The final spatial frequency masking weight is calculated at each patch as where γ is the scaling constant factor chosen to tune the shape of the transfer function.\n184. We used γ = 1.5 in our implementation.\n185. 2) Building a Banding Metric: The visibility of banding artifacts depends on the combination of multiple visual mechanisms.\n186. In this paper, we propose a simple but effective product model for attribute integration at each predicted banding patch to obtain the entire banding map (BM): where P k denotes the predicted label of k-th patch and w k is the weight parameter that scales the visibility of measured contours, i.e., gradient magnitude of the high-frequency map, |HFM k (i, j) | at region (i, j).\n187. Furthermore, inspired by previous psychovisual findings that the QoE of observers is dominated by those regions having poor quality[14],[42], we thereby leverage the worst p% percentile visual pooling to calculate an average banding score from the generated BM, where p is set to 80 in this experiment.\n188. As a result, the perceptual score of the overall banding image is defined as where M is the total number of patches in image I.\n189. T p% denotes the index set of the top p% non-zero pixel-wise value contained in k-th patch of the BM.\n190. In this section, we first present the experimental protocol in detail and then evaluate the performance of the proposed method on two tasks, namely patch-level banding classification and banding image quality assessment.\n191. After that, the ablation study and cross-database validation are conducted to prove the robustness and effectiveness of the proposed method.\n192. Finally, we test the computational efficiency of our method.\n193. A. Experimental Protocol 1) Databases and Settings: We choose two databases to train and test the effectiveness of the proposed banding IQA method, which are the database released in[43]and our proposed BAND-2k database.\n194. The detail information of these two datasets can be found in TableI.\n195. The proposed model is implemented by PyTorch[13].\n196. Before training, we randomly split the training, validation, and testing set into 8:1:1 (as shown in TableII).\n197. We use the Adam optimizer with the initial learning rate set as 1e-4 and set the batch size as 32.\n198. The training process is stopped after 25 epochs.\n199. The resolution of each cropped patch is fixed to 235×235.\n200. All experiments on both the[43]database and the BAND-2k database are conducted repeatedly 10 times to obtain the mean performance.\n201. 2) Baseline Algorithms: We include a number of representative IQA algorithms in our evaluation as references to be compared against.\n202. These baseline methods include: • General FR IQA methods: We choose PSNR, SSIM[12], MS-SSIM[11], LPIPS[10]as baselines.\n203. These are the most commonly used FR IQA metrics in practical applications such as video coding, image enhancement, etc. • General NR IQA methods: BRISQUE[37], NIQE[36], NIMA[8], DBCNN[35], HyberIQA[9], and StairIQA[1].\n204. • Banding IQA methods: Considering that there exists few research on banding detection and quality assessment, we barely select the BBAND[42], CAMBI[41], VMAF BA[40]and DBI[43]metrics as comparisons.\n205. 3) Evaluation Criteria: To evaluate the IQA methods comprehensively, a total of seven evaluation indexes in two categories are adopted.\n206. For patch-level banding classification, we follow the common procedures as in[43]and utilize the area under the receiver operating characteristics (AUROC), the area under the precision-recall curve (AUPRC), and accuracy as the classification performance metrics.\n207. For banding image quality assessment, four mainstream metrics are selected as the evaluation criteria: Spearman rank-order correlation coefficient (SRCC) and Kendall rank-order correlation coefficient (KRCC) measure the prediction monotonicity, while Pearson linear correlation coefficient (PLCC) and root mean square error (RMSE) are calculated to assess prediction consistency.\n208. Considering the potential nonlinear mapping characteristics between the objective scores and the subjective scores, we perform score alignment by mapping the predicted value using the five-parameter logistic function before calculating PLCC and RMSE values[7].\n209. Since our goal is to develop an effective banding IQA approach, we regard the identification of banding patches as an important preceding process to achieve accurate banding quality prediction.\n210. However, there exist few methods that are designed for banding classification and nearly all IQA methods produce scalar values only while failing in classifying banding regions directly.\n211. Therefore, we adopt a thresholding step to convert the single quality value into binary classification results as[43]does.\n212. Concretely, a half-interval search algorithm[48]is employed to find the optimal threshold value that can generate the best classification result.\n213. Based on the above premise, TableIIIreports the experimental results on both the database from[43]and the BAND-2k database.\n214. We highlight the best results in boldface.\n215. As compared to other state-of-the-art IQA methods, our proposed method yields the best overall performance in terms of AUROC, AUPRC, and accuracy.\n216. It is shown that most general FR IQA and NR IQA models perform poorly on the patch-level banding classification task while performing fairly well on other IQA tasks[1],[9],[35], indicating that the current approaches are not sensitive to banding distortion.\n217. Benefiting from the powerful feature extraction ability of CNNs, our proposed method and the customized NR IQA models for banding artifacts detection (DBI[43]) reach a significant performance in the discrimination of false contours.\n218. However, the performance of banding IQA method BBAND, CAMBI, and VMAF BA is surprisingly poor compared with other methods, which shows their vulnerability in identifying local banding artifacts from texture regions and are not suitable for patch-level banding identification.\n219. In addition, we investigate the computational complexity in terms of execution time per image patch.\n220. It can be observed that except for those traditional FR IQA models, our method achieves comparable speed in patch-level banding classification, which determines the prediction efficiency of the subsequent image-level quality assessment, making it a favorable choice in time-constrained scenarios.\n221. Considering that there exist no image banding databases with subjective scores attached in the public domain, we merely compare the performance of the proposed method with the baseline approaches on the proposed BAND-2k database.\n222. The results are shown in TableIV, from which we can observe that our proposed method largely surpasses all baselines in terms of SRCC, KRCC, and PLCC except for the RMSE.\n223. Compared to the secondbest model, our method achieves 18.07%SRCC improvements, 24.91% KRCC improvements, and 18.01%PLCC improvements on the BAND-2k database.\n224. We also present scatter plots of predictions versus MOS for better visualization in Fig.8.\n225. Overall,  (a) PSNR, (b) SSIM[12], (c) MS-SSIM[11], (d) LPIPS[10], (e) VMAF BA[40], (f) BRISQUE[37], (g) NIQE[36], (h) NIMA[8], (i) DBCNN[35], (j) HyperIQA[9], (k) StairIQA[1], (l) BBAND[42], (m) CAMBI[41], (n) DBI[43], and (o) Ours.\n226. Fig.9. Visual comparisons of the banding map results.\n227. From top to bottom are banding images and their corresponding banding maps generated by BBAND[42], DBI[43], and our proposed method, respectively.\n228. The first five columns of images from left to right are from BAND-2k, while the rest images are from[43].\n229. the performance of traditional FR IQA models such as PSNR, SSIM, and MS-SSIM is remarkably inferior in banding images assessment and is uncorrelated with the MOS, which is consistent with the numerical results presented in TableIV.\n230. The reason is that PSNR and SSIM-based methods do perform not well on tiny, regional, and content-independent banding distortion while lacking the consideration for the mechanism of HVS.\n231. It is also worth mentioning that the deep CNN architecturebased methods (DBCNN[35], HyperIQA[9], and StairIQA[1]), despite performing well on LIVEC[6], KonIQ-10k[29], and other universal image quality evaluation databases[4],[5], underperformed our proposed model by a notable margin on the BAND-2k database.\n232. (2) the perception of banding artifacts is explicitly intensity-aware.\n233. These are the issues that the CNN-based approaches above do not take into account.\n234. To some extent, banding distortion is more like a kind of local distortion than common global distortion such as Gaussian noise, blur, and dither.\n235. This suggests that it is potentially valuable to integrate some local texture, edge, contrast, or other visibility-related features into quality prediction models when assessing banding images.\n236. Fortunately, with the help of the proposed scoring strategy, our method gains the ability to convert the area range of identified banding regions to indicative annoying scores.\n237. Surprisingly, the banding IQA methods BBAND[42], CAMBI[41], and VMAF BA[40], however, did not perform very well on the BAND-2k dataset.\n238. We infer that this is due to differences in the test environment.\n239. First, the source videos selected in CAMBI and VMAF BA are from the internal Netflix 4K catalogue while the source contents in BAND-2k are collected from the public streaming websites with different resolutions, leading to an uneven quality level.\n240. Second, the methods of artificially introducing banding distortion are different.\n241. 264 compression techniques are used to generate the banding artifacts, limiting the adaptive ability of the algorithm in other scenarios.\n242. For further investigation, we compare our method with other two methods, i.e., BBAND[42], DBI[43], and visually study the generated banding maps to verify their effectiveness in banding detection.\n243. The visualization results are shown in Fig.9, where seven representative banding images are selected for reference.\n244. As shown, the banding maps generated from BBAND are mostly disordered and have a lot of discontinuity while the banding maps generated by DBI are too vague to locate the exact location of banding artifacts, making it difficult to develop pixel-level debanding techniques.\n245. Overall, we can compendiously conclude that: (1) in comparison with BBAND and DBI, banding maps computed by our proposed method could provide an accurate, clear indication for banding artifacts; (2) in comparison with BBAND and DBI, the quality prediction of banding images guided by our method could achieve a high consistency with HVS.\n246. Moreover, to make a statistically meaningful comparison among different IQA methods, we further conduct the widely used F-test[2],[3]to assess the statistical significance of the proposed method.\n247. Based on the assumption that the model\'s prediction residuals follow the Gaussian distribution, the lefttailed F-test with a confidence level of 95% is performed on the residuals of every two IQA models.\n248. The results of significance tests on the BAND-2k database are shown in Fig.10.\n249. A value of \'1\' (colored in green) indicates that the model in the row is significantly better than the model in the column, while a value of \'0\' (colored in red) indicates that the model in the row is not significantly better than the model in the column.\n250. It is shown that our proposed method performs significantly better than other models, which is consistent with the observations from the above comparison experiments.\n251. In this section, we explore the effectiveness of our model\'s design philosophy.\n252. To verify the importance of the dual-branch (DB) architecture, the baseline variants use the same backbone as the proposed method, except that only a single branch (SB) is reserved.\n253. Then, we use the original banding images as the input while removing the frequency map generation module (SB-I).\n254. Besides, the high-frequency maps and low-frequency maps generation modules are retained respectively (SB-HFM and SB-LFM).\n255. It can be observed from Table V that SB-LFM achieved the worst results, since the low-frequency map filters out the highfrequency banding information to a certain extent, which reduces the ability of the model to identify false contours.\n256. SB-I performs significantly better than SB-HFM and SB-LFM, resulting from that it contains richer image information, but it is still inferior to our method, which demonstrates the effectiveness of the dualbranch scheme.\n257. To investigate the effect of the banding feature extraction, we further design two variants with different input combinations of frequency maps.\n258. First, the high-frequency map was taken as the input of both the CN N H and CN N L layers (DB-HFM).\n259. Then, we replace the inputs with the low-frequency maps (DB-LFM).\n260. As shown in TableV, the performance of DB-HFM and DB-LFM is far apart from our approach, which matches our hypothesis that the high-frequency texture information contained in HFM and the low-frequency background information contained in LFM are crucial to enhance the capacity of discernment for banding artifacts.\n261. Therefore, we may conclude that our model is the most suitable model among those compared variants in terms of both banding classification and IQA applications.\n262. Due to the effects of different compression techniques, shooting equipment, scenes, etc., the image content and banding distortions may vary significantly in practical applications.\n263. For the database[43], it only includes limited types of image sources and means of triggering banding distortion.\n264. As a result, we conduct a cross-database validation to verify the generalizability of the proposed model, wherein the database presented by[43]and BAND-2k are included.\n265. That is, we trained the model on one full database and report the test performance on the other.\n266. We mainly compare the proposed method with four learningbased models, i.e., DBCNN[35], HyperIQA[9], StairIQA[1], and DBI[43].\n267. Since MOS information is not provided in the database[43], which is an essential part of methods training, we condensed part of the experiments.\n268. TableVIand VII report the experimental results in terms of patch-level banding classification and image quality assessment.\n269. We can observe that our proposed method generalization between database[43]and BAND-2k was surprisingly good.\n270. Besides, it is worth noting that the performance of these methods trained on the BAND-2k has improved a little compared to the previous versions that were trained on the database[43], which further demonstrates the superiority of the proposed database BAND-2k.\n271. The efficiency of an image quality prediction model is of great importance in practical industrial deployments.\n272. Therefore, we measured the average running time of the compared IQA models, as shown in TableVIII.\n273. The experiments were performed in MATLAB R2021a and Python 3.7 under Windows 10 64bit system on a Lenovo laptop with Intel Core i5-9300HF CPU@2.4GHz,16GB RAM, and NVIDIA GTX 1660Ti 6G GPU.\n274. It can be observed that the proposed method achieves a reasonable running time among the FR, NR, and other bandingspecified IQA algorithms.\n275. Generally, the execution time of classical IQA algorithms is significantly less than learning-based methods.\n276. Simpler NSS-based models such as BRISQUE and NIQE still show competitive efficiency relative to CNN models while exhibiting inferior performance in banding image quality assessment.\n277. For CAMBI and VMAF BA , we use the officially launched software package, which is based on the stand-alone C library libvmaf and therefore surpasses other methods in speed.\n278. Moreover, unlike the general quality evaluation using regression to predict scores, the patch-wise prediction strategy that we adopted may increase the complexity.\n279. Note that although we deployed a more complex network structure, a nearly 10 times speedup has been seen when comparing DBI with our method since the sliding window mechanism[43]is removed.\n280. In this paper, we conduct a comprehensive exploration of banding images from both subjective and objective perspectives.\n281. Specifically, we construct the largest ecologically valid banding IQA database to date named BAND-2k database, which consists of 2,000 banding images generated by fifteen compression and quantization schemes, achieving several times larger in number and diversity than the existing banding dataset.\n282. The construction process of the database, including distortion content preparation, subjective test procedure, and the removal of outlying data, is described in detail in this paper.\n283. Relying on this database, we proposed a novel banding evaluator using the frequency characteristic of banding artifacts, which models the banding as high-frequency artifacts that contained in the low-frequency smoothing region.\n284. A dual-branch CNN is devised to extract hierarchical features to classify the banding regions, upon which we introduce the spatial frequency masking to refine and compute an overall banding score.\n285. Experimental results show that our proposed method outperforms the baseline algorithms significantly in patch-level banding classification and banding IQA tasks.\n286. We believe that our study will benefit further development, calibration, and benchmarking of banding IQA models.\n\n请严格按照JSON格式返回：{"labels": [0,1,0,...]}，不要包含任何其他文字。'}], 'model': 'qwen-plus', 'response_format': {'type': 'json_object'}}}
2025-10-14 19:49:13,114 - DEBUG - Sending HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
2025-10-14 19:49:13,114 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-10-14 19:49:13,114 - DEBUG - send_request_headers.complete
2025-10-14 19:49:13,114 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-10-14 19:49:13,114 - DEBUG - send_request_body.complete
2025-10-14 19:49:13,114 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-10-14 19:54:01,861 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'vary', b'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding'), (b'x-request-id', b'b0c6796c-39bb-47c3-8390-0b8f2f79839f'), (b'x-dashscope-call-gateway', b'true'), (b'content-type', b'application/json'), (b'req-cost-time', b'288580'), (b'req-arrive-time', b'1760442544425'), (b'resp-start-time', b'1760442833005'), (b'x-envoy-upstream-service-time', b'288510'), (b'content-encoding', b'gzip'), (b'date', b'Tue, 14 Oct 2025 11:53:52 GMT'), (b'server', b'istio-envoy'), (b'transfer-encoding', b'chunked')])
2025-10-14 19:54:01,861 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-14 19:54:01,861 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-10-14 19:54:01,861 - DEBUG - receive_response_body.complete
2025-10-14 19:54:01,861 - DEBUG - response_closed.started
2025-10-14 19:54:01,861 - DEBUG - response_closed.complete
2025-10-14 19:54:01,861 - DEBUG - HTTP Response: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "200 OK" Headers({'vary': 'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding', 'x-request-id': 'b0c6796c-39bb-47c3-8390-0b8f2f79839f', 'x-dashscope-call-gateway': 'true', 'content-type': 'application/json', 'req-cost-time': '288580', 'req-arrive-time': '1760442544425', 'resp-start-time': '1760442833005', 'x-envoy-upstream-service-time': '288510', 'content-encoding': 'gzip', 'date': 'Tue, 14 Oct 2025 11:53:52 GMT', 'server': 'istio-envoy', 'transfer-encoding': 'chunked'})
2025-10-14 19:54:01,861 - DEBUG - request_id: b0c6796c-39bb-47c3-8390-0b8f2f79839f
2025-10-14 19:54:01,862 - DEBUG - API request completed in 288.75 seconds
2025-10-14 19:54:01,862 - DEBUG - Raw model response: {"labels": [1,0,0,1,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,
2025-10-14 19:54:01,862 - ERROR - JSON parsing error: Expecting value: line 1 column 8475 (char 8474)
2025-10-14 19:54:01,862 - ERROR - Problematic content: {"labels": [1,0,0,1,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,
2025-10-14 19:54:01,863 - INFO - Attempt 2 of 5
2025-10-14 19:54:01,863 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-0d6048da-c9b2-432c-a895-7efba14e6c28', 'json_data': {'messages': [{'role': 'system', 'content': '你是一个专门用于识别学术论文中数据集描述的助手。你需要判断每个输入的句子是否描述了研究中使用的数据集。请确保：\n    1. 只输出JSON格式的结果\n    2. 结果格式必须为 {"labels": [0,1,0,...]}\n    3. 不要输出任何其他文字或解释'}, {'role': 'user', 'content': '你是一位专业的学术论文数据集描述识别专家。请仔细分析下面这篇论文中的每个句子，判断它们是否描述了本研究特定构建的数据集信息。\n\n    论文标题: BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment\n\n    判断标准:\n    判断为1(数据集描述)的标准：\n    句子必须直接描述本研究特定构建的数据集,包括:\n    1. 该数据集的构建过程和方法\n    2. 该数据集的具体构成和规模\n    3. 该数据的来源和收集方式\n    4. 该数据的预处理步骤\n    5. 该数据集的可获取方式(如发布地址)\n    6. 或者其他直接或者间接描述了该数据集的句子\n\n    所有其他类型的句子均标记为0。\n    \n    请对每个句子进行分析，返回一个JSON格式的标签数组。格式要求：\n    1. 必须是有效的JSON格式\n    2. 只包含labels字段，值为0和1组成的数组\n    3. 数组长度必须与句子数量相同\n    4. 示例格式：{"labels": [0,1,0,1,...]}\n\n    需要判断的句子：\n\n    1. Abstract: Banding, also known as staircase-like contours, frequently occurs in flat areas of images/videos processed by the compression or quantization algorithms.\n2. As undesirable artifacts, banding destroys the original image structure, thus inevitably degrading users\' quality of experience (QoE).\n3. In this paper, we systematically investigate the banding image quality assessment (IQA) problem, aiming to detect the image banding artifacts and evaluate their perceptual visual quality.\n4. Considering that the existing image banding databases only contain limited content sources and banding generation methods, and lack perceptual quality labels (i.e.mean opinion scores), we first build the largest banding IQA database so far, named Banding Artifact Noticeable Database (BAND-2k), which consists of 2,000 banding images generated by 15 compression and quantization schemes.\n5. A total of 23 workers participated in the subjective IQA experiment, yielding over 214,000 patch-level banding class labels and 44,371 reliable image-level quality rating scores.\n6. Subsequently, we develop an effective no-reference (NR) banding evaluator for banding detection and quality assessment by leveraging frequency characteristics of banding artifacts.\n7. To be more specific, a dual convolutional neural network (CNN) is employed to concurrently learn the feature representation from the high-frequency and low-frequency maps, thereby enhancing the ability to discern banding artifacts.\n8. The quality score of a banding image is generated by pooling the banding detection maps masked by the spatial frequency filters.\n9. The experimental results demonstrate that our banding evaluator achieves a remarkably high accuracy in banding detection and also exhibits high SRCC and PLCC results with the perceptual quality labels, even without directly learning a regression model for banding quality evaluation.\n10. These findings unveil the strong correlations between the intensity of banding artifacts and the perceptual visual quality, thus validating the necessity of banding quality assessment.\n11. The BAND-2k database and the proposed banding evaluator will be available at https://github.com/zijianchen98/BAND-2k.\n12. R ECENT years have witnessed a rapid emergence of media streaming services and social platforms.\n13. YouTube, Netflix, and TikTok account for more than half of the world\'s video traffic.\n14. Improving the quality of images under limited encoding, Zijian Chen, Wei Sun, Jun Jia, Zicheng Zhang, Xiongkuo Min, and Guangtao Zhai are with the Institute of Image Communication and Information Processing, Shanghai Jiao Tong University, Shanghai 200240, China (e-mail: {zijian.chen,sunguwei, jiajun0302, zzc1998, minxiongkuo, zhaiguangtao}@sjtu.edu.cn).\n15. Fangfang Lu is with the College of Computer Science and Technology, Shanghai University of Electric Power, Shanghai 200290, China (email:lufangfang@shiep.edu.cn).\n16. Jing Liu is with the School of Electrical and Information Engineering, Tianjin University, Tianjin, China.(e-mail:jliu tju@tju.edu.cn).\n17. transmission bandwidth, and storage condition is a necessary prerequisite for meeting the quality of experience (QoE) of users.\n18. In the stages between image acquisition and display, an image may suffer from various types of degradation, while banding artifacts are a kind of false contour distortion that is quite perceptible to the human eye.\n19. Since the visual quality of image contents greatly affects the QoE of end-users, it is highly desirable to design an effective banding image quality assessment (IQA) method, which aims to automatically detect the traces of such false contours and predict the objective quality of banding images that can be used to develop pre-processing or postprocessing debanding algorithms and optimize the performance of streaming media application.\n20. Normally, banding artifacts take on the appearance of annual rings, radiation circles, halos, or geographical contour lines and especially exist in the background regions (e.g., sky, water, and wall surface), where the color transition is not smooth enough.\n21. Nearly all existing image or video encoders, including H.264/AVC[28], VP9[38], and H.265/HEVC[27]can introduce such artifacts more or less.\n22. Current banding IQA research can be divided into two categories: subjective quality assessment and objective quality assessment.\n23. The existing subjective banding IQA research[39],[41]-[43]mainly investigates the limited banding scenarios with internal-used and undisclosed databases while lacking the quality label and may be insufficiently generalizable to large-scale commercial applications.\n24. Meanwhile, general IQA methods aiming at common distortions are inapplicable for banding exacerbated images due to the essential differences between them.\n25. First, banding artifacts usually hold tiny, staircaselike, and regional structures, which can be regarded as a kind of high-frequency artifact in smooth areas, while general distortions occur obviously in the whole image and are globally uniform.\n26. Second, the perceptual severity of banding is quantified based on its fraction of coverage and intensity in an image, which is widely divergent from the design philosophy of many existing IQA approaches.\n27. As a result, it is challenging to design an effective banding IQA method.\n28. To address these limitations, we first conduct a comprehensive subjective study of banding exacerbated images and create the largest banding IQA database to date with reliable mean opinion scores (MOS) and patch-level banding labels.\n29. We also propose a novel no-reference banding evaluator for banding detection and quality assessment by leveraging the frequency characteristics of banding artifacts.\n30. First, due to the dissimilar peculiarities of banding and smooth regions, the same distortion in different regions, e.g., textual and pictorial regions, may lead to different visual perception of human beings.\n31. Considering that banding manifests as a high-frequency artifact that exists in the lowfrequency smooth region, we propose a dual-branch CNN, which takes the high-frequency map and low-frequency map as inputs simultaneously, to hierarchically incorporate different visual features from the first layer and the last layer of our Resnet-50 backbone, thus making the model learn more effective banding feature representation and achieving more accurate banding region discrimination.\n32. Furthermore, inspired by the previous studies[15],[16], spatial frequency extracts information consistent with the human visual system (HVS), which not only reflects the overall active level in an image but also intuitively quantifies the contrast information.\n33. In other words, the value of spatial frequency is large in smooth areas, while becoming small in areas with harsh contrast changes, i.e., banding areas.\n34. Based on this mechanism, we adopt a spatial frequency masking strategy to refine the detected banding map and then pool the masked banding detection map to obtain the image-level banding quality score.\n35. • We propose a novel no-reference banding evaluator for banding detection and quality assessment based on frequency characteristics of banding artifacts, which utilizes a dual-branch CNN model to extract hierarchical banding-related feature representation from the high-frequency maps and low-frequency maps simultaneously.\n36. • Experimental results show that the proposed banding evaluator achieves the best performance in banding detection and significantly surpasses baselines in terms of SRCC and PLCC in the banding IQA task, which demonstrates the effectiveness of the proposed model.\n37. The remainder of this paper is organized as follows.\n38. Section II provides an overview of related works, including the stateof-the-art banding databases and detection methods.\n39. Section III introduces the construction of the BAND-2k database and the subjective assessment study.\n40. Section IV proposes an effective no-reference banding evaluator for banding detection and quality assessment.\n41. Section V gives the experimental results and analysis.\n42. Section VI concludes this paper.\n43. In this section, we first provide an overview of the state-ofthe-art banding-related databases (TableI) and then review the banding detection and evaluation methods.\n44. The first banding artifact-relevant VQA database was proposed by Wang et al.[39], which consists of 21 stimuli with different quantization grades generated by VP9[38]from 7 clips of 1280 × 720 30fps video.\n45. Authors in[41]investigated the effect of encoding parameters and dithering on the visibility of banding.\n46. Nine 4k-10bit source clips from the existing Netflix catalogue between 1 and 5 seconds were used to generate banding distorted videos.\n47. Each source content was downsampled to appropriate resolutions (1080p, 2k, or 4k) with certain bit-depth and further compressed by libaom (an AV1 codec library) at QPs {12, 20, 32}.\n48. More recently, Kapoor et al.[43]constructed one of the first databases for data-driven image banding assessment models.\n49. This research included about 1,440 images shot from over 600 pristine HD videos with a resolution of 1920 × 1080.\n50. Six levels of bit-depth quantization in luminance and chrominance channels are introduced to obtain different intensities of banding.\n51. Meanwhile, the banding images were semi-automatically segmented and labeled into banded and non-banded to form a patch-level banding dataset, which allows for training machine learning-based and deep learning-based banding classification methods.\n52. However, to the best of our knowledge, thus far there still lacking a benchmarking dataset in the banding detection and the corresponding banding IQA domain.\n53. Researchers either resort to image/video quality datasets that do not aim at banding distortion or build a small, attribute-restricted, in-house dataset by themselves.\n54. This motivates us to construct a large-scale subjective assessment database focus on the perceived bandingaffected image quality.\n55. Early research on banding detection mainly focuses on false contour identification, which aims to find the wrong boundary rather than a "true" region edge in the image.\n56. Authors in[31]-[33]utilized monotonicity or non-monotonicity features of local support regions including the gradient, contrast, variance, and entropy information to measure the loss of low-amplitude detail caused by banding.\n57. However, these works ignored the perceptual characteristics of the human visual system (HVS) and thus did not perform a good correlation with subjective tests.\n58. Another banding detection strategy is conducted at the pixel-level estimation and segmentation.\n59. Bhagavathy et al.[30]proposed to identify banding artifacts by calculating the likelihood of pixel difference.\n60. Baugh et al.[34]measured the severity of banding based on the number of a group of connected pixels with the same color.\n61. Wang et al.[39]first detected uniform segments to find possible banding areas and further incorporated edge features (e.g.length and contrast) to capture false boundaries.\n62. Nevertheless, these kinds of methods are typically sensitive to edge noise and are computationally expensive, causing limited application in realtime scenarios.\n63. Towards addressing these problems, Tu et al.[42]presented a completely no-reference banding detection method, which combines various properties of HVS with a number of preprocessing steps to refine banding edge detection.\n64. Instead of regarding banding detection as a false edge detection problem, Tandon et al.[41]heuristically utilized the effect of contrast sensitivity function (CSF) on banding visibility and its dependence on spatial frequency.\n65. Based on this, Krasula et al.[40]further compared the banding annoyance with more commonly studied compression artifacts and proposed a banding-aware video quality metric.\n66. In recent years, deep learning approaches have prevailed in various VQA tasks.\n67. As the pioneering work, Kapoor et al.[43]developed an automated CNN-based banding detector for the first time, which is a simple two-stage algorithm and gives rise to devising other learning-based techniques.\n68. In this work, we build a large-scale banding database and propose a data-driven banding indicator that can generate pixelwise banding visibility maps with corresponding subjectively  consistent quality scores by combining human visual mechanisms and deep learning techniques.\n69. Subjective banding image quality assessment facilitates the development of automatic objective banding image and video quality models.\n70. We created the largest banding database in existence, denoted as the BAND-2k database, which consists of 2,000 banding distorted images and over 214,000 patch-level banding class labels.\n71. Then, a subjective experiment was conducted to obtain the mean opinion scores (MOS) of the BAND-2k database.\n72. The workflow of the banding database construction is shown in Fig.1.\n73. To build a content-rich and balanced database, we manually collected source videos including computer graphics (CG), usergenerated content (UGC), and professionally-generated content (PGC) from two popular media websites Bilibili.comand Youtube.com.\n74. Then, 885 clips with multiple spatial resolutions (i.e., 4096×2160, 3840×2160, 1920×1080) and frame rates (i.e., 60, 50, 30, 25) are chosen as candidate.\n75. Note that videos on the mentioned websites are firstly annotated by the community with assigned a number of favorites, views, and downloads.\n76. These statistics correlate with the content and quality of a video, which guides our choices to some extent.\n77. All videos selected on the website are released under an appropriate creative commons (CC) license that allows further editing and redistribution.\n78. After content selection, we further unified the format of all video clips, especially the spatial resolution and the pixel format, which avoid the effect of other facts on visual quality.\n79. Concretely, we first converted the frame rate of the original clips to 25fps, which is to reduce the storage pressure while ensuring the graphics quality.\n80. Considering the commonly used aspect ratio of the user interface and displays is 16:9, we cropped the partially unqualified videos rather than shrinking images unevenly.\n81. Then, we downsampled the trimmed spatial resolution 3840×2160 to a lower resolution -1920×1080 for the following subjective study.\n82. Before conducting the following experiments, we manually removed the videos that are either too dark or bright, overly blurry or colorful, which helps to obtain more reliable subjective assessments.\n83. To avoid redundancy and to make sure the diversity of selected contents, we also conducted attribute analysis studies on the selected images.\n84. Four metrics that correlated with human perception, i.e., contrast, brightness, sharpness, and colorfulness, are adopted as content diversity metrics.\n85. All video attributes are calculated on every 10 frames to reduce computational complexity, which are then averaged over frames of each video sequence.\n86. Fig.2shows the distribution of attributes extracted from the selected videos.\n87. • Contrast: The contrast metric is simply defined as the standard deviation of pixel gray-scale intensities[25].\n88. • Colorfulness: The colorfulness metric is measured by the R, G, and B components[25].\n89. We first compute two matrices of differences between channels rg = R -G and yb = 1 2 (R + G) -B.\n90. Then, the colorfulness metric can be calculated as µ 2 rg + µ 2 yb + σ 2 rg + σ 2 yb , where µ and σ are the mean and standard deviation of their corresponding terms, respectively.\n91. • Sharpness: The cumulative probability of blur detection (CPBD) metric[24]is used to measure the image sharpness, which estimates the probability of detecting blur at each edge.\n92. • Brightness: The brightness of an image is obtained directly from the pixel gray intensities in R, G, and B channels.\n93. Finally, the number of source videos are reduced to 873.\n94. Fig.3displays thumbnails for 30 selected representative video clips.\n95. To simulate authentic banding artifacts that exist in real viewing scenarios, we introduced four encoding/transcoding strategies including H.264/AVC[28], H.265/HEVC[27], VP9[38], and bit-depth manipulation[43]with fifteen quantization schemes in total.For H.264/AVC and H.265/HEVC, considering the range of their quantization parameter (QP in ffmpeg), we selected three typical QP values, namely, QP = {18, 33, 43}.\n96. This is because that coded video clips with a QP value smaller than 18 normally provide perceptual lossless quality, while coded video clips with a QP value larger than 43 will not be able to offer adequate quality, which may affect the subjective test of banding artifacts.\n97. Similarly, for the VP9 encoder, the QP values under our close inspection are chosen as {28, 45, 60}.\n98. Moreover, we applied the same quantization strategies as in[43], which introduces banding distortion by scaling bit-depth in luminance and chrominance channels.\n99. Here, the color coding scheme YCbCr4:2:0 is applied for maximum compatibility.\n100. To sum up, we generated fifteen levels of banding with different intensities and shapes to enhance the diversity of the database.\n101. The banding database is then built by extracting frames from the distorted video clips, resulting in 2,000 images with a resolution of 1920×1080.\n102. Fig.4shows the visualization results of banding exacerbated images.\n103. In the process of image patch labeling, we initially intend to extract image patches of size 235×235 from banding images directly and perform annotation operations.\n104. However, considering the theoretical number of patches, it will cost a lot of manpower and time to label patch by patch.\n105. Therefore, we first segmented the banding images roughly and label them into banded and non-banded regions.\n106. Then, labelled image patches are generated from these segmented and labelled images by a sliding window.\n107. Specifically, we followed the same demarcation of the banded and non-banded images in[43]that a patch is labelled as banded if it has more than 30% overlap with banded regions in the image.\n108. Eventually, a banding dataset containing 2,000 distorted images with 1920×1080 resolution and 214,324 labelled image patches of size 235×235 is built.\n109. To the best of our knowledge, it is the largest banding dataset in existence, which enables training various machine/deep learning based banding detection models and facilitates the development of image/video debanding techniques.\n110. TableIIreports the composition of labelled image patch dataset.\n111. It can be observed that the number of banded patches is a bit smaller than non-banded patches due to the fact that banding usually appears in smooth background areas.\n112. As shown in Fig.1, the subjective quality study contains four steps.\n113. In addition to preparing the experimental environment, subjects should pass the qualification test first to participate in the study.\n114. After the subjective rating, all resulting scores need to be analyzed and examined before generating the final mean opinion score (MOS).\n115. 1) Experimental Environment Setting: In this study, a total of 25 inexperienced subjects are gathered in a laboratory environment, where relevant experimental configuration must satisfy the following requirements: • Considering the viewing effect, desktops and laptops are allowed as displays.\n116. • The resolution of displays must be larger than or equal to 1920×1080 to show the images without spatial downsampling.\n117. • The viewing distance and optimal horizontal viewing angle are set as 1.9 times the height of the display and respectively.\n118. 500 recommendation[26].\n119. As a result, we used a 27-inch AOC Q27U2D monitor with a resolution of 2560×1440 for assessment with 25 subjects.\n120. Due to the large number of images to be assessed, we divided the dataset into ten sessions to avoid visual fatigue.\n121. Each session of tests took nearly 2 hours with a 30-minute break for each participant.\n122. 2) Qualification Test: Before starting the main experiment, subjects are required to pass a quiz to get the qualification of conducting follow-up experiments.\n123. Firstly, we manually selected 10 labeled banding exacerbated images beyond the database as training images to familiarize subjects with the operation interface and the goal of this subjective test.\n124. The quiz consists of two parts including banding classification and image-level quality rating.\n125. In banding classification, subjects were told to divide the test image into banded or non-banded.\n126. In image-level quality rating, subjects were instructed to focus on the coverage and intensity of banding areas, as well as the overall quality of images to get the final quality score.\n127. The rating scale is continuous from 0 to 100 while a higher value indicates more severe banding (i.e., more visible or occupying larger portion of the image).\n128. To make the quiz objective and fair, we take the banding images labeled by domain experts as the ground truth, while two existing banding metrics BBAND[42]and DBI[43]are used to determine the normal range of the rating scores.\n129. That is the quality scores rated by subjects should not exceed 20% of the above banding metrics.\n130. As a result, only 23 subjects with an accuracy above 80% in banding classification and subjective scores in the normal range were allowed to pass the quiz.\n131. Note that the steps for taking a quiz are the same as the main experiment, which aims to guarantee the consistency of results.\n132. 3) Formal Study: We adopted the single-stimulus (SS) method in this test.\n133. Ten golden images that have the acknowledged high quality or poor quality (assessed by BBAND[42]and DBI[43]) were added to each session for controlling the scoring deviations.\n134. Besides, 3 repeated images are randomly inserted into each session to ensure consistency of scores before and after subjects scoring.\n135. At last, 23 qualified subjects were asked to provide their opinions on the shuffled image groups.\n136. The resulting scores were collected and packed for further analysis.\n137. 4) Result Analysis: In total, 46,000 scores were collected by 23 qualified subjects in the main study phase.\n138. However, considering the qualification quiz cannot completely disallow those unreliable workers to muddle through to the main study while reliable subjects may also occasionally score odd values, which may be caused by the inter-individual differences in perceiving the quality of the unique characteristics of different contents.\n139. Thus, we further investigated the confidence of rating scores and removed outliers following the Grubbs\' test[23],[45].\n140. Concretely, let s = (s 1 , s 2 , . . ., s N ) denote a set of raw scores collected for one distorted image.\n141. The test statistics is defined as the maximum absolute deviation of a sample standard deviation (SD) unit from the sample mean, which is mathematically expressed as where s and σ s denote the mean and standard deviation of the sample, respectively.\n142. Then, given a significant level α, a sample is detected as an outlier if where t 2 α/(2N ),N -2 represents the upper critical value of the tdistribution with N -2 degrees of freedom and a significance level of α/(2N ).\n143. Empirically, we set the significance level α at 0.05.\n144. Then, a sample is identified as an outlier if its distance to the sample mean is larger than 2.5 times SD and is removed.\n145. Following the aforementioned steps, the total number of scores Fig.6.\n146. The overall architecture of the proposed method.\n147. Given a banding distorted image, it is first divided into patches.\n148. Then, the patch-level high-frequency map (HFM) and low-frequency map (LFM) are generated by Sobel operation and piece-wise smooth algorithm[19], respectively.\n149. After that, a dual-branch CNN (CNN H and CNN L ) is deployed to extract hierarchical features with different visual information and thus classify the patches into banded or non-banded.\n150. Lastly, a spatial frequency masking strategy is introduced to refine the banding map and calculate the image-level banding quality score.\n151. Note that the dual-branch networks do not share parameters.\n152. was reduced to 44,371, and MOS was created by averaging the scores for each image.\n153. Fig.5presents the histogram of MOSs over the entire database, showing a broad MOS distribution of banding images.\n154. In this section, we describe the architecture of the proposed banding evaluator in detail, as shown in Fig.6.\n155. As stated before, banding usually appear as high-frequency information in the smooth background, while humans perceive high-frequency texture regions and low-frequency plateau regions through different neural channels concurrently, and transfer the upper visual features into the cerebral cortex for final processing[17],[44].\n156. Inspired by this, we employ high-frequency maps (HFM) and low-frequency maps (LFM) as the deep learning network inputs, which represent the texture and structural information of the image respectively, to mimic the recognition mechanism of the human brain for better banding identification.\n157. High-frequency Maps.\n158. Since gradient has been widely used to represent edge information and has been confirmed beneficial to acquire high-frequency components with low computational cost[20]-[22], we apply the isotropic Sobel operator to each patch for enhancing the details of banding artifacts.\n159. Given an input patch I, the high-frequency map is calculated by where S x and S y are the horizontal and vertical isotropic Sobel operators, respectively." * " denotes the convolution operation.\n160. Low-frequency Maps.\n161. To maintain the principal content of the image and filter out the influence of high-frequency information, we use the piece-wise smooth algorithm[19]to generate the low-frequency map by minimizing a function for image approximation recovery: where L represents the low-frequency map, Ω and E denotes the image domain and edge set, respectively.\n162. P indicates the pixel and E dσ represents the total edge length.\n163. The coefficients α and β are positive regularization constants.\n164. An example of frequency maps is shown in Fig.7.\n165. To obtain the overall banding score, we first divide the banding image into 235×235 patches and apply CNN-based classification to each patch, resulting in a banding classification label for each patch, i.e., banded or non-banded.\n166. As shown in Fig.6, the proposed network consists of two parallel branches, namely CNN H and CNN L , which take the patch-level high-frequency map and low-frequency map as input, respectively.\n167. For each branch, we propose to use Resnet-50[18]as the backbone.\n168. Specifically, we incorporate the feature maps extracted from the first convolutional layer and the last layer of Resnet-50 as hierarchical visual features, which represent different visual information[46],[47]and can be used as predictive information to enhance the discrimination ability of the network for banding and non-banded regions.\n169. Afterward, the features extracted from two branches are concatenated first and reshaped into 128dimensional vectors through two fully-connected layers, which is further followed with the sigmoid activation function to output the final predicted label, namely banded or non-banded.\n170. Of note is that sharing parameters is extremely unfavorable for extracting low-and high-frequency features simultaneously, we thereby deploy two branches that work independently and do not share parameters.\n171. The loss function adopted here is binary cross entropy.\n172. With the trained CNN H and CNN L , each input patch is predicted to obtain a banding or non-banded label.\n173. To better guide the pre-processing and post-processing debanding algorithms, it is necessary to generate a quality score for the entire banding image.\n174. Since the visibility of edge is also affected by content, we further consider the effect of spatially varying content information on the local quality of human perception.\n175. As a consequence, we introduce the spatial frequency masking strategy to determine the weighting factor for the detected banding regions in each patch adaptively and thus obtain the image-level banding severity score while refining the visibility of banding artifacts.\n176. 1) Spatial Frequency Masking: The spatial frequency is defined as the activity level of an image, which establishes a filterbank based on the visual stimulus and is in accordance with HVS[16].\n177. In this paper, we propose to apply spatial frequency as an effective contrast criterion to banding measurement.\n178. Specifically, given an image of size I W × I H , divided into N × N patches, where I W and I H denote the number of columns and rows respectively.\n179. The column (CF k ) and row (RF k ) frequencies of the image patches are given by where I(x, y) is the pixel value of the image patch.\n180. Then, the resulting spatial frequency of an N × N patch is computed as where k is the number of patches (1 ≤ k ≤ I W I H N 2 ).\n181. Since most banding regions are likely to have large contrast including edges and textures, which should be assigned greater weights than the smooth and blurred areas.\n182. Accordingly, we design a banding visibility transfer function to express spatial frequency masking as a function of the local textural feature.\n183. The final spatial frequency masking weight is calculated at each patch as where γ is the scaling constant factor chosen to tune the shape of the transfer function.\n184. We used γ = 1.5 in our implementation.\n185. 2) Building a Banding Metric: The visibility of banding artifacts depends on the combination of multiple visual mechanisms.\n186. In this paper, we propose a simple but effective product model for attribute integration at each predicted banding patch to obtain the entire banding map (BM): where P k denotes the predicted label of k-th patch and w k is the weight parameter that scales the visibility of measured contours, i.e., gradient magnitude of the high-frequency map, |HFM k (i, j) | at region (i, j).\n187. Furthermore, inspired by previous psychovisual findings that the QoE of observers is dominated by those regions having poor quality[14],[42], we thereby leverage the worst p% percentile visual pooling to calculate an average banding score from the generated BM, where p is set to 80 in this experiment.\n188. As a result, the perceptual score of the overall banding image is defined as where M is the total number of patches in image I.\n189. T p% denotes the index set of the top p% non-zero pixel-wise value contained in k-th patch of the BM.\n190. In this section, we first present the experimental protocol in detail and then evaluate the performance of the proposed method on two tasks, namely patch-level banding classification and banding image quality assessment.\n191. After that, the ablation study and cross-database validation are conducted to prove the robustness and effectiveness of the proposed method.\n192. Finally, we test the computational efficiency of our method.\n193. A. Experimental Protocol 1) Databases and Settings: We choose two databases to train and test the effectiveness of the proposed banding IQA method, which are the database released in[43]and our proposed BAND-2k database.\n194. The detail information of these two datasets can be found in TableI.\n195. The proposed model is implemented by PyTorch[13].\n196. Before training, we randomly split the training, validation, and testing set into 8:1:1 (as shown in TableII).\n197. We use the Adam optimizer with the initial learning rate set as 1e-4 and set the batch size as 32.\n198. The training process is stopped after 25 epochs.\n199. The resolution of each cropped patch is fixed to 235×235.\n200. All experiments on both the[43]database and the BAND-2k database are conducted repeatedly 10 times to obtain the mean performance.\n201. 2) Baseline Algorithms: We include a number of representative IQA algorithms in our evaluation as references to be compared against.\n202. These baseline methods include: • General FR IQA methods: We choose PSNR, SSIM[12], MS-SSIM[11], LPIPS[10]as baselines.\n203. These are the most commonly used FR IQA metrics in practical applications such as video coding, image enhancement, etc. • General NR IQA methods: BRISQUE[37], NIQE[36], NIMA[8], DBCNN[35], HyberIQA[9], and StairIQA[1].\n204. • Banding IQA methods: Considering that there exists few research on banding detection and quality assessment, we barely select the BBAND[42], CAMBI[41], VMAF BA[40]and DBI[43]metrics as comparisons.\n205. 3) Evaluation Criteria: To evaluate the IQA methods comprehensively, a total of seven evaluation indexes in two categories are adopted.\n206. For patch-level banding classification, we follow the common procedures as in[43]and utilize the area under the receiver operating characteristics (AUROC), the area under the precision-recall curve (AUPRC), and accuracy as the classification performance metrics.\n207. For banding image quality assessment, four mainstream metrics are selected as the evaluation criteria: Spearman rank-order correlation coefficient (SRCC) and Kendall rank-order correlation coefficient (KRCC) measure the prediction monotonicity, while Pearson linear correlation coefficient (PLCC) and root mean square error (RMSE) are calculated to assess prediction consistency.\n208. Considering the potential nonlinear mapping characteristics between the objective scores and the subjective scores, we perform score alignment by mapping the predicted value using the five-parameter logistic function before calculating PLCC and RMSE values[7].\n209. Since our goal is to develop an effective banding IQA approach, we regard the identification of banding patches as an important preceding process to achieve accurate banding quality prediction.\n210. However, there exist few methods that are designed for banding classification and nearly all IQA methods produce scalar values only while failing in classifying banding regions directly.\n211. Therefore, we adopt a thresholding step to convert the single quality value into binary classification results as[43]does.\n212. Concretely, a half-interval search algorithm[48]is employed to find the optimal threshold value that can generate the best classification result.\n213. Based on the above premise, TableIIIreports the experimental results on both the database from[43]and the BAND-2k database.\n214. We highlight the best results in boldface.\n215. As compared to other state-of-the-art IQA methods, our proposed method yields the best overall performance in terms of AUROC, AUPRC, and accuracy.\n216. It is shown that most general FR IQA and NR IQA models perform poorly on the patch-level banding classification task while performing fairly well on other IQA tasks[1],[9],[35], indicating that the current approaches are not sensitive to banding distortion.\n217. Benefiting from the powerful feature extraction ability of CNNs, our proposed method and the customized NR IQA models for banding artifacts detection (DBI[43]) reach a significant performance in the discrimination of false contours.\n218. However, the performance of banding IQA method BBAND, CAMBI, and VMAF BA is surprisingly poor compared with other methods, which shows their vulnerability in identifying local banding artifacts from texture regions and are not suitable for patch-level banding identification.\n219. In addition, we investigate the computational complexity in terms of execution time per image patch.\n220. It can be observed that except for those traditional FR IQA models, our method achieves comparable speed in patch-level banding classification, which determines the prediction efficiency of the subsequent image-level quality assessment, making it a favorable choice in time-constrained scenarios.\n221. Considering that there exist no image banding databases with subjective scores attached in the public domain, we merely compare the performance of the proposed method with the baseline approaches on the proposed BAND-2k database.\n222. The results are shown in TableIV, from which we can observe that our proposed method largely surpasses all baselines in terms of SRCC, KRCC, and PLCC except for the RMSE.\n223. Compared to the secondbest model, our method achieves 18.07%SRCC improvements, 24.91% KRCC improvements, and 18.01%PLCC improvements on the BAND-2k database.\n224. We also present scatter plots of predictions versus MOS for better visualization in Fig.8.\n225. Overall,  (a) PSNR, (b) SSIM[12], (c) MS-SSIM[11], (d) LPIPS[10], (e) VMAF BA[40], (f) BRISQUE[37], (g) NIQE[36], (h) NIMA[8], (i) DBCNN[35], (j) HyperIQA[9], (k) StairIQA[1], (l) BBAND[42], (m) CAMBI[41], (n) DBI[43], and (o) Ours.\n226. Fig.9. Visual comparisons of the banding map results.\n227. From top to bottom are banding images and their corresponding banding maps generated by BBAND[42], DBI[43], and our proposed method, respectively.\n228. The first five columns of images from left to right are from BAND-2k, while the rest images are from[43].\n229. the performance of traditional FR IQA models such as PSNR, SSIM, and MS-SSIM is remarkably inferior in banding images assessment and is uncorrelated with the MOS, which is consistent with the numerical results presented in TableIV.\n230. The reason is that PSNR and SSIM-based methods do perform not well on tiny, regional, and content-independent banding distortion while lacking the consideration for the mechanism of HVS.\n231. It is also worth mentioning that the deep CNN architecturebased methods (DBCNN[35], HyperIQA[9], and StairIQA[1]), despite performing well on LIVEC[6], KonIQ-10k[29], and other universal image quality evaluation databases[4],[5], underperformed our proposed model by a notable margin on the BAND-2k database.\n232. (2) the perception of banding artifacts is explicitly intensity-aware.\n233. These are the issues that the CNN-based approaches above do not take into account.\n234. To some extent, banding distortion is more like a kind of local distortion than common global distortion such as Gaussian noise, blur, and dither.\n235. This suggests that it is potentially valuable to integrate some local texture, edge, contrast, or other visibility-related features into quality prediction models when assessing banding images.\n236. Fortunately, with the help of the proposed scoring strategy, our method gains the ability to convert the area range of identified banding regions to indicative annoying scores.\n237. Surprisingly, the banding IQA methods BBAND[42], CAMBI[41], and VMAF BA[40], however, did not perform very well on the BAND-2k dataset.\n238. We infer that this is due to differences in the test environment.\n239. First, the source videos selected in CAMBI and VMAF BA are from the internal Netflix 4K catalogue while the source contents in BAND-2k are collected from the public streaming websites with different resolutions, leading to an uneven quality level.\n240. Second, the methods of artificially introducing banding distortion are different.\n241. 264 compression techniques are used to generate the banding artifacts, limiting the adaptive ability of the algorithm in other scenarios.\n242. For further investigation, we compare our method with other two methods, i.e., BBAND[42], DBI[43], and visually study the generated banding maps to verify their effectiveness in banding detection.\n243. The visualization results are shown in Fig.9, where seven representative banding images are selected for reference.\n244. As shown, the banding maps generated from BBAND are mostly disordered and have a lot of discontinuity while the banding maps generated by DBI are too vague to locate the exact location of banding artifacts, making it difficult to develop pixel-level debanding techniques.\n245. Overall, we can compendiously conclude that: (1) in comparison with BBAND and DBI, banding maps computed by our proposed method could provide an accurate, clear indication for banding artifacts; (2) in comparison with BBAND and DBI, the quality prediction of banding images guided by our method could achieve a high consistency with HVS.\n246. Moreover, to make a statistically meaningful comparison among different IQA methods, we further conduct the widely used F-test[2],[3]to assess the statistical significance of the proposed method.\n247. Based on the assumption that the model\'s prediction residuals follow the Gaussian distribution, the lefttailed F-test with a confidence level of 95% is performed on the residuals of every two IQA models.\n248. The results of significance tests on the BAND-2k database are shown in Fig.10.\n249. A value of \'1\' (colored in green) indicates that the model in the row is significantly better than the model in the column, while a value of \'0\' (colored in red) indicates that the model in the row is not significantly better than the model in the column.\n250. It is shown that our proposed method performs significantly better than other models, which is consistent with the observations from the above comparison experiments.\n251. In this section, we explore the effectiveness of our model\'s design philosophy.\n252. To verify the importance of the dual-branch (DB) architecture, the baseline variants use the same backbone as the proposed method, except that only a single branch (SB) is reserved.\n253. Then, we use the original banding images as the input while removing the frequency map generation module (SB-I).\n254. Besides, the high-frequency maps and low-frequency maps generation modules are retained respectively (SB-HFM and SB-LFM).\n255. It can be observed from Table V that SB-LFM achieved the worst results, since the low-frequency map filters out the highfrequency banding information to a certain extent, which reduces the ability of the model to identify false contours.\n256. SB-I performs significantly better than SB-HFM and SB-LFM, resulting from that it contains richer image information, but it is still inferior to our method, which demonstrates the effectiveness of the dualbranch scheme.\n257. To investigate the effect of the banding feature extraction, we further design two variants with different input combinations of frequency maps.\n258. First, the high-frequency map was taken as the input of both the CN N H and CN N L layers (DB-HFM).\n259. Then, we replace the inputs with the low-frequency maps (DB-LFM).\n260. As shown in TableV, the performance of DB-HFM and DB-LFM is far apart from our approach, which matches our hypothesis that the high-frequency texture information contained in HFM and the low-frequency background information contained in LFM are crucial to enhance the capacity of discernment for banding artifacts.\n261. Therefore, we may conclude that our model is the most suitable model among those compared variants in terms of both banding classification and IQA applications.\n262. Due to the effects of different compression techniques, shooting equipment, scenes, etc., the image content and banding distortions may vary significantly in practical applications.\n263. For the database[43], it only includes limited types of image sources and means of triggering banding distortion.\n264. As a result, we conduct a cross-database validation to verify the generalizability of the proposed model, wherein the database presented by[43]and BAND-2k are included.\n265. That is, we trained the model on one full database and report the test performance on the other.\n266. We mainly compare the proposed method with four learningbased models, i.e., DBCNN[35], HyperIQA[9], StairIQA[1], and DBI[43].\n267. Since MOS information is not provided in the database[43], which is an essential part of methods training, we condensed part of the experiments.\n268. TableVIand VII report the experimental results in terms of patch-level banding classification and image quality assessment.\n269. We can observe that our proposed method generalization between database[43]and BAND-2k was surprisingly good.\n270. Besides, it is worth noting that the performance of these methods trained on the BAND-2k has improved a little compared to the previous versions that were trained on the database[43], which further demonstrates the superiority of the proposed database BAND-2k.\n271. The efficiency of an image quality prediction model is of great importance in practical industrial deployments.\n272. Therefore, we measured the average running time of the compared IQA models, as shown in TableVIII.\n273. The experiments were performed in MATLAB R2021a and Python 3.7 under Windows 10 64bit system on a Lenovo laptop with Intel Core i5-9300HF CPU@2.4GHz,16GB RAM, and NVIDIA GTX 1660Ti 6G GPU.\n274. It can be observed that the proposed method achieves a reasonable running time among the FR, NR, and other bandingspecified IQA algorithms.\n275. Generally, the execution time of classical IQA algorithms is significantly less than learning-based methods.\n276. Simpler NSS-based models such as BRISQUE and NIQE still show competitive efficiency relative to CNN models while exhibiting inferior performance in banding image quality assessment.\n277. For CAMBI and VMAF BA , we use the officially launched software package, which is based on the stand-alone C library libvmaf and therefore surpasses other methods in speed.\n278. Moreover, unlike the general quality evaluation using regression to predict scores, the patch-wise prediction strategy that we adopted may increase the complexity.\n279. Note that although we deployed a more complex network structure, a nearly 10 times speedup has been seen when comparing DBI with our method since the sliding window mechanism[43]is removed.\n280. In this paper, we conduct a comprehensive exploration of banding images from both subjective and objective perspectives.\n281. Specifically, we construct the largest ecologically valid banding IQA database to date named BAND-2k database, which consists of 2,000 banding images generated by fifteen compression and quantization schemes, achieving several times larger in number and diversity than the existing banding dataset.\n282. The construction process of the database, including distortion content preparation, subjective test procedure, and the removal of outlying data, is described in detail in this paper.\n283. Relying on this database, we proposed a novel banding evaluator using the frequency characteristic of banding artifacts, which models the banding as high-frequency artifacts that contained in the low-frequency smoothing region.\n284. A dual-branch CNN is devised to extract hierarchical features to classify the banding regions, upon which we introduce the spatial frequency masking to refine and compute an overall banding score.\n285. Experimental results show that our proposed method outperforms the baseline algorithms significantly in patch-level banding classification and banding IQA tasks.\n286. We believe that our study will benefit further development, calibration, and benchmarking of banding IQA models.\n\n请严格按照JSON格式返回：{"labels": [0,1,0,...]}，不要包含任何其他文字。'}], 'model': 'qwen-plus', 'response_format': {'type': 'json_object'}}}
2025-10-14 19:54:01,864 - DEBUG - Sending HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
2025-10-14 19:54:01,864 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-10-14 19:54:01,865 - DEBUG - send_request_headers.complete
2025-10-14 19:54:01,865 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-10-14 19:54:01,865 - DEBUG - send_request_body.complete
2025-10-14 19:54:01,865 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-10-14 19:54:09,106 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'vary', b'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding'), (b'x-request-id', b'9c67380b-9fad-435a-9f65-8b29ec8f35a0'), (b'x-dashscope-call-gateway', b'true'), (b'content-type', b'application/json'), (b'req-cost-time', b'7198'), (b'req-arrive-time', b'1760442833043'), (b'resp-start-time', b'1760442840242'), (b'x-envoy-upstream-service-time', b'7125'), (b'content-encoding', b'gzip'), (b'date', b'Tue, 14 Oct 2025 11:54:00 GMT'), (b'server', b'istio-envoy'), (b'transfer-encoding', b'chunked')])
2025-10-14 19:54:09,107 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-14 19:54:09,107 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-10-14 19:54:09,107 - DEBUG - receive_response_body.complete
2025-10-14 19:54:09,107 - DEBUG - response_closed.started
2025-10-14 19:54:09,107 - DEBUG - response_closed.complete
2025-10-14 19:54:09,107 - DEBUG - HTTP Response: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "200 OK" Headers({'vary': 'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding', 'x-request-id': '9c67380b-9fad-435a-9f65-8b29ec8f35a0', 'x-dashscope-call-gateway': 'true', 'content-type': 'application/json', 'req-cost-time': '7198', 'req-arrive-time': '1760442833043', 'resp-start-time': '1760442840242', 'x-envoy-upstream-service-time': '7125', 'content-encoding': 'gzip', 'date': 'Tue, 14 Oct 2025 11:54:00 GMT', 'server': 'istio-envoy', 'transfer-encoding': 'chunked'})
2025-10-14 19:54:09,107 - DEBUG - request_id: 9c67380b-9fad-435a-9f65-8b29ec8f35a0
2025-10-14 19:54:09,107 - DEBUG - API request completed in 7.24 seconds
2025-10-14 19:54:09,107 - DEBUG - Raw model response: {"labels": [1,0,0,1,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]}
2025-10-14 19:54:09,108 - INFO - Successfully processed 157 labels
2025-10-14 19:54:09,108 - ERROR - Label count mismatch for BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment
2025-10-14 19:54:09,108 - INFO - Evaluating paper 8/18: Comprehensive Named Entity Recognition on CORD_19 with Distant or Weak Supervision
2025-10-14 19:54:09,108 - INFO - Starting model prediction
2025-10-14 19:54:09,108 - INFO - Attempt 1 of 5
2025-10-14 19:54:09,108 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-f7fefc67-bfa8-4376-befb-1de4286e2ff3', 'json_data': {'messages': [{'role': 'system', 'content': '你是一个专门用于识别学术论文中数据集描述的助手。你需要判断每个输入的句子是否描述了研究中使用的数据集。请确保：\n    1. 只输出JSON格式的结果\n    2. 结果格式必须为 {"labels": [0,1,0,...]}\n    3. 不要输出任何其他文字或解释'}, {'role': 'user', 'content': '你是一位专业的学术论文数据集描述识别专家。请仔细分析下面这篇论文中的每个句子，判断它们是否描述了本研究特定构建的数据集信息。\n\n    论文标题: Comprehensive Named Entity Recognition on CORD_19 with Distant or Weak Supervision\n\n    判断标准:\n    判断为1(数据集描述)的标准：\n    句子必须直接描述本研究特定构建的数据集,包括:\n    1. 该数据集的构建过程和方法\n    2. 该数据集的具体构成和规模\n    3. 该数据的来源和收集方式\n    4. 该数据的预处理步骤\n    5. 该数据集的可获取方式(如发布地址)\n    6. 或者其他直接或者间接描述了该数据集的句子\n\n    所有其他类型的句子均标记为0。\n    \n    请对每个句子进行分析，返回一个JSON格式的标签数组。格式要求：\n    1. 必须是有效的JSON格式\n    2. 只包含labels字段，值为0和1组成的数组\n    3. 数组长度必须与句子数量相同\n    4. 示例格式：{"labels": [0,1,0,1,...]}\n\n    需要判断的句子：\n\n    1. Abstract: We created this CORD-NER dataset with comprehensive named entity recognition (NER) on the COVID-19 Open Research  Dataset Challenge (CORD-19) corpus (2020-03-13).\n2. This CORD-NER dataset covers 75 fine-grained entity types: In addition to the common biomedical entity types (e.g., genes, chemicals and diseases), it covers many new entity types related explicitly to the COVID-19 studies (e.g., coronaviruses, viral proteins, evolution, materials, substrates and immune responses), which may benefit research on COVID-19 related virus, spreading mechanisms, and potential vaccines.\n3. CORD-NER annotation is a combination of four sources with different NER methods.\n4. The quality of CORD-NER annotation surpasses SciSpacy (over 10% higher on the F1 score based on a sample set of documents), a fully supervised BioNER tool.\n5. Moreover, CORD-NER supports incrementally adding new documents as well as adding new entity types when needed by adding dozens of seeds as the input examples.\n6. We will constantly update CORD-NER based on the incremental updates of the CORD-19 corpus and the improvement of our system.\n7. Coronavirus disease 2019 (COVID-19) is an infectious disease caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2).\n8. The disease was first identified in 2019 in Wuhan, Central China, and has since spread globally, resulting in the 20192020 coronavirus pandemic.\n9. On March 16th, 2020, researchers and leaders from the Allen Institute for AI, Chan Zuckerberg Initiative (CZI), Georgetown University\'s Center for Security and Emerging Technology (CSET), Microsoft, and the National Library of Medicine (NLM) at the National Institutes of Health released the  Open Research Dataset (CORD-19)1of scholarly literature about COVID-19, SARS-CoV-2, and the coronavirus group.\n10. Named entity recognition (NER) is a fundamental step in text mining system development to facilitate COVID-19 studies.\n11. There is a critical need for NER methods that can quickly adapt to all the COVID-19 related new types without much human effort for training data annotation.\n12. We created this CORD-NER dataset2with comprehensive named entity annotation on theCORD-19 corpus (2020-03-13).\n13. This dataset covers 75 fine-grained named entity types.\n14. CORD-NER is automatically generated by combining the annotation results from four sources.\n15. In the following sections, we introduce the details of CORD-NER dataset construction.\n16. We also show some NER annotation results in this dataset.\n17. The input corpus is generated from the 29,500 documents in theCORD-19 corpus (2020-03-13).\n18. We first merge all the meta-data (all sources metadata 2020-03-13.csv) with their corresponding full-text papers.\n19. Then we create a tokenized corpus (CORD-NER-corpus.json)for further NER annotations.\n20. The input corpus is a combination of the "title", "abstract" and "full-text" from the CORD-19 corpus.\n21. We first conduct automatic phrase mining and tokenization on the input corpus using AutoPhrase(Shang et al., 2018a).\n22. Then we do a second round of tokenization with Spacy3on the phrase-replaced corpus.\n23. We found that keeping the AutoPhrase results will significantly improve the distantly-and weakly-supervised NER performance.\n24. CORD-NER annotation is a combination of four sources with different NER methods: 1.Pre-trained NER on 18 general entity types from Spacy using the model "en core web sm".\n25. 2.Pre-trained NER on 18 biomedical entity types from SciSpacy 4 using the models "en ner bionlp13cg md" and "en ner bc5cdr md".\n\n请严格按照JSON格式返回：{"labels": [0,1,0,...]}，不要包含任何其他文字。'}], 'model': 'qwen-plus', 'response_format': {'type': 'json_object'}}}
2025-10-14 19:54:09,109 - DEBUG - Sending HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
2025-10-14 19:54:09,109 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-10-14 19:54:09,109 - DEBUG - send_request_headers.complete
2025-10-14 19:54:09,109 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-10-14 19:54:09,109 - DEBUG - send_request_body.complete
2025-10-14 19:54:09,109 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-10-14 19:54:10,890 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'vary', b'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding'), (b'x-request-id', b'0e7e81b2-dc9e-47ea-8d59-29d4652bb043'), (b'x-dashscope-call-gateway', b'true'), (b'content-type', b'application/json'), (b'req-cost-time', b'1746'), (b'req-arrive-time', b'1760442840284'), (b'resp-start-time', b'1760442842030'), (b'x-envoy-upstream-service-time', b'1744'), (b'content-encoding', b'gzip'), (b'date', b'Tue, 14 Oct 2025 11:54:01 GMT'), (b'server', b'istio-envoy'), (b'transfer-encoding', b'chunked')])
2025-10-14 19:54:10,890 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-14 19:54:10,890 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-10-14 19:54:10,890 - DEBUG - receive_response_body.complete
2025-10-14 19:54:10,890 - DEBUG - response_closed.started
2025-10-14 19:54:10,890 - DEBUG - response_closed.complete
2025-10-14 19:54:10,890 - DEBUG - HTTP Response: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "200 OK" Headers({'vary': 'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding', 'x-request-id': '0e7e81b2-dc9e-47ea-8d59-29d4652bb043', 'x-dashscope-call-gateway': 'true', 'content-type': 'application/json', 'req-cost-time': '1746', 'req-arrive-time': '1760442840284', 'resp-start-time': '1760442842030', 'x-envoy-upstream-service-time': '1744', 'content-encoding': 'gzip', 'date': 'Tue, 14 Oct 2025 11:54:01 GMT', 'server': 'istio-envoy', 'transfer-encoding': 'chunked'})
2025-10-14 19:54:10,890 - DEBUG - request_id: 0e7e81b2-dc9e-47ea-8d59-29d4652bb043
2025-10-14 19:54:10,891 - DEBUG - API request completed in 1.78 seconds
2025-10-14 19:54:10,891 - DEBUG - Raw model response: {"labels": [1,1,1,0,1,1,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1]}
2025-10-14 19:54:10,891 - INFO - Successfully processed 25 labels
2025-10-14 19:54:10,898 - ERROR - Error processing paper Comprehensive Named Entity Recognition on CORD_19 with Distant or Weak Supervision: 'int' object has no attribute 'capitalize'
2025-10-14 19:54:10,898 - INFO - Evaluating paper 9/18: DCQA_ DOCUMENT_LEVEL CHART QUESTION ANSWERING TOWARDS COMPLEX REASONING AND COMMON_SENSE UNDERSTANDING
2025-10-14 19:54:10,898 - INFO - Starting model prediction
2025-10-14 19:54:10,898 - INFO - Attempt 1 of 5
2025-10-14 19:54:10,898 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-8f64c5a3-5df4-42bf-a4fd-1776ddab3a85', 'json_data': {'messages': [{'role': 'system', 'content': '你是一个专门用于识别学术论文中数据集描述的助手。你需要判断每个输入的句子是否描述了研究中使用的数据集。请确保：\n    1. 只输出JSON格式的结果\n    2. 结果格式必须为 {"labels": [0,1,0,...]}\n    3. 不要输出任何其他文字或解释'}, {'role': 'user', 'content': '你是一位专业的学术论文数据集描述识别专家。请仔细分析下面这篇论文中的每个句子，判断它们是否描述了本研究特定构建的数据集信息。\n\n    论文标题: DCQA_ DOCUMENT_LEVEL CHART QUESTION ANSWERING TOWARDS COMPLEX REASONING AND COMMON_SENSE UNDERSTANDING\n\n    判断标准:\n    判断为1(数据集描述)的标准：\n    句子必须直接描述本研究特定构建的数据集,包括:\n    1. 该数据集的构建过程和方法\n    2. 该数据集的具体构成和规模\n    3. 该数据的来源和收集方式\n    4. 该数据的预处理步骤\n    5. 该数据集的可获取方式(如发布地址)\n    6. 或者其他直接或者间接描述了该数据集的句子\n\n    所有其他类型的句子均标记为0。\n    \n    请对每个句子进行分析，返回一个JSON格式的标签数组。格式要求：\n    1. 必须是有效的JSON格式\n    2. 只包含labels字段，值为0和1组成的数组\n    3. 数组长度必须与句子数量相同\n    4. 示例格式：{"labels": [0,1,0,1,...]}\n\n    需要判断的句子：\n\n    1. Abstract: Visually-situated languages such as charts and plots are omnipresent in real-world documents.\n2. These graphical depictions are human-readable and are often analyzed in visuallyrich documents to address a variety of questions that necessitate complex reasoning and common-sense responses.\n3. Despite the growing number of datasets that aim to answer questions over charts, most only address this task in isolation, without considering the broader context of document-level question answering.\n4. Moreover, such datasets lack adequate common-sense reasoning information in their questions.\n5. In this work, we introduce a novel task named document-level chart question answering (DCQA).\n6. The goal of this task is to conduct document-level question answering, extracting charts or plots in the document via document layout analysis (DLA) first and subsequently performing chart question answering (CQA).\n7. The newly developed benchmark dataset comprises 50,010 synthetic documents integrating charts in a wide range of styles (6 styles in contrast to 3 for PlotQA and ChartQA) and includes 699,051 questions that demand a high degree of reasoning ability and common-sense understanding.\n8. Besides, we present the development of a potent question-answer generation engine that employs table data, a rich color set, and basic question templates to produce a vast array of reasoning question-answer pairs automatically.\n9. Based on DCQA, we devise an OCR-free transformer for document-level chartoriented understanding, capable of DLA and answering complex reasoning and common-sense questions over charts in an OCR-free manner.\n10. Our DCQA dataset is expected to foster research on understanding visualizations in documents, especially for scenarios that require complex reasoning for charts in the visually-rich document.\n11. We implement and evaluate a set of baselines, and our proposed method achieves comparable results.\n12. The emergence of visual language as a novel communicative tool, characterized by a tightly integrated interplay of visual and textual components, can be attributed to a confluence of factors, notably globalization, the growing intricacy of commerce and technology, and the convergence of lexicons from diverse fields that were once disparate[7].\n13. The prevalence of visually-situated language in various document types, such as academic, business, medical, and others, is markedly high[8][9][10].\n14. Gaining a comprehensive understanding of these graphical representations, such as charts and plots, is essential in extracting valuable and pragmatic insights from data[11].\n15. To conduct data analysis, individuals frequently pose intricate queries that require common-sense and arithmetic or logical operations pertaining to graphical representations.\n16. Answering such inquiries demands a substantial level of cognitive and reasoning exertion, as individuals are required to be aware of common sense and integrate numerous logical operations, including but not limited to retrieving entities, comparing trends, calculating averages, finding extremum, etc.Typically, the chart question answering (CQA) system[12]aims to generate the desired answer by taking a chart-question pair as input, constituting a fundamental function within the domain of intelligent document understanding (IDU)[13].\n17. Despite the CQA task has drawn ever-growing attention from visual question answering communities in recent years, existing datasets has encountered certain obstacles: (i) Notably, while charts constitute crucial components of documents, the majority of current datasets treat the CQA task solely at the question-answering level, without taking into account its significance as a document-level task.\n18. (ii) Questions generally prioritize reasoning or visual features, potentially losing sight of common sense information that individuals typically consider when posing questions, which is a misalignment with the typical questioning habits of individuals.\n19. (iii) The quantity of chart types, as exemplified by PlotQA and ChartQA datasets, is comparatively restricted (only three).\n20. Such a limited representation fails to capture the broad range of chart styles that are present in real-world documents.\n21. Furthermore, in real-world settings, users typically first identify the location of charts in documents before querying them.\n22. However, directly analyzing document layout poses challenges, as charts lack explicit annotations.\n23. Manually annotating datasets for document layout analysis related to charts is remarkably laborious and time-consuming.\n24. This motivates an automated system to generate annotations associated with charts, eliminating the need for costly labeled data collection.\n25. Such a system would locate charts in arbitrary documents without annotations and produce bounding boxes highlighting them, providing chart-specific layout information without human intervention.\n26. Additionally, certain baseline models rely on obtaining high-quality optical character recognition (OCR) outcomes to extract the data table structure from the chart image.\n27. Therefore, current models\' efficacy generally relies upon the accuracy of OCR results.\n28. Nevertheless, incorporating an OCR-dependent approach for CQA system poses significant challenges.\n29. For one thing, commercially available OCR techniques often exhibit limited adaptability in addressing diverse languages or changes in the domain, which are commonly encountered in the context of charts.\n30. Such limitations may impede the generalization abil-ity of these methods.\n31. For another, the occurrence of errors during the OCR process is unavoidable, and such erroneous outcomes have the potential to propagate to the CQA system, thereby adversely affecting subsequent processes[14].\n32. To alleviate above issues, we go beyond the traditional dataset by presenting a large-scale document-level chart question answering dataset (DCQA).\n33. DCQA comprises 50,010 synthetic documents and 699,051 question-answer pairs generated using our customized semantic-rich question-answer generation engine.\n34. The dataset includes questions that focus on vision, complex reasoning, and common-sense knowledge.\n35. Common-sense knowledge reasoning primarily involves evaluating the ability of CQA models to distinguish between legend labels and entity names belonging to specific parent classes, and subsequently performing reasoning operations based on this discriminative ability.\n36. Each document in the DCQA includes a chart, unrelated images and a descriptive caption related to the chart.\n37. The language used in the DCQA dataset is English.\n38. The chart types exhibit a diverse range of styles and can be broadly categorized into six major types, namely Bar chart, Line plot, Pie chart, Scatter plot, Box plot, and Mixed chart, each of which is further divided into subtypes, yielding a total of 30 chart subtypes.\n39. Figure1displays some examples from DCQA.\n40. More examples are provided in Appendix E.\n41. Drawing upon the DCQA dataset, we further devise a transformer-based OCR-free architecture to perform document layout analysis and chart question answering.\n42. Initially, we exploit swin transformer[15]as the vision backbone to extract visual features of the input document.\n43. Next, the extracted features are fed into the detection component to perform document layout analysis[16].\n44. Upon successfully identifying the chart image, we extract the relevant visual content from the chart, which is then utilized as input to the textual decoder for answer prediction.\n45. This novel OCR-free architecture provides a plug-and-play solution for performing chart question answering directly from the document.\n46. In a nutshell, our contributions are as follows: • We present a comprehensive and extensive documentlevel chart question answering dataset, DCQA, which features a wide range of chart styles and includes question-answer pairs that incorporate complex reasoning and common-sense knowledge.\n47. The dataset\'s scale and diversity make it a valuable resource for researchers interested in developing and evaluating chart question answering models.\n48. • We conceptualize chart question answering as a documentlevel task and propose a transformer-based OCR-free model to effectively address this task.\n49. • We perform comprehensive experiments and thorough analyses on DCQA, verifying the efficacy of our model.\n50. To date, only a limited number of datasets have been explicitly designed for chart question answering.\n51. These datasets include FigureQA[1], DVQA[2], LEAF-QA[3], LEAFQA++[4], PlotQA[5]and ChartQA[6].\n52. Despite consisting of a diverse set of synthetic charts, FigureQA suffers from a lack of specificity in terms of chart element labeling, utilizing only generic titles and color names.\n53. Furthermore, the questions are limited to a few template-based formats with binary "yes/no" answers.\n54. DVQA is limited to a single chart type, namely the Bar chart, and suffers from inadequate semantic relations between the textual elements.(e.g., bar and legend labels are randomly selected words) as well as restricted Y-axis value ranges.\n55. Numeric answers are primarily integers in both the train and test sets and share the same values.\n56. As with FigureQA, all bar plots in DVQA are artificially generated, and the questions are based on a small number of templates.\n57. Both LEAF-QA and its extended version, LEAFQA++, are not publicly available.\n58. Besides, they share a significant limitation: the absence of regression question-answering pairs.\n59. This is evident from the question templates described in their reference and the discrete answer set employed.\n60. Although PlotQA is currently the largest publicly available dataset for CQA, it is limited by imbalanced question distribution, as it is heavily weighted towards data-related questions and lacks an appropriate proportion of queries pertaining to the visual characteristics of chart elements, including color and shape.\n61. Regarding ChartQA, it is the pioneer dataset to compile realworld charts with a blend of human-created QA pairs and machine-generated QA pairs.\n62. However, despite its innovative contribution, ChartQA is characterized by a limited size and encompasses only three distinct plot types.\n63. Furthermore, the paucity of question-answer pairs (two) per chart undermines the potential for a comprehensive understanding of the underlying information conveyed by these visualizations.\n64. This work presents a novel and intricate CQA dataset, which diverges from prior datasets in several respects.\n65. Firstly, DCQA is introduced, which reformulates the CQA task by integrating document layout analysis and chart question answering.\n66. Secondly, in addition to visual and complex reasoning questions, DCQA incorporates common sense-aware questions.\n67. Last but not least, DCQA covers a broad range of chart types1.\n68. In this section, we describe the construction of DCQA from the following four aspects:(i) Data collection, (ii) Chart generation, (iii) Automatic QA pair generation engine, and (iv) Document generation.\n69. The general workflow of the DCQA generation process is shown in Figure3.\n70. A detailed generation procedure is provided in Appendix A.\n71. Given the variability of chart styles in real-world scenarios, integrating real-world sources and randomly generated data for producing charts can augment the models robustness and adaptability to various chart formats encountered in practical scenarios.\n72. Drawing upon this observation, charts included in our dataset was derived by two means: utilizing real-world sources and randomly generated data.\n73. The detailed process of data collection is shown in Appendix A.\n74. We exploit Pyecharts1, a Python visualization tool library based on the Echarts[21]charting library, to generate our charts.\n75. Our DCQA contains six different chart styles: bar chart, line chart, scatter plot, box plot, pie chart, and combination chart (line and bar).\n76. These chart styles can be further divided into 30 sub-types (As shown in Figure2).\n77. The color of chart elements is randomly picked up from a color set Johndecember, which covers a wide range of colors (595).\n78. Besides, the chart presents two distinct styles of background, namely dark and light, of which the former was not previously observed in any of the CQA datasets.\n79. Detailed chart information is provided in Appendix B.\n80. Since the generated charts are from disparate data sources and encompass a wide range of topics, engaging a cadre of individuals with diverse backgrounds, experiences, and expertise is necessary to craft questions about the corresponding charts.\n81. We have meticulously curated a corpus of 573 charts spanning six categories, comprising data extracted from real-world and randomly generated sources, which serve as paradigmatic instances from which questions can be formulated.\n82. We commission a cohort of post-graduate students affiliated with our academic institution, and employees from Huawei company , to generate ten distinct questions for each of the selected charts, with an emphasis on reasoning and common sense awareness.\n83. We have obtained 5730 questions.\n84. After an exhaustive process of meticulous meetings and indepth discussions, we have successfully distilled a total of 324 question templates from the original pool of 5730 questions.\n85. Out of these templates, 204 are specifically tailored for visual and numeric reasoning, while 120 templates are dedicated to common sense reasoning.\n86. Code will be publicly available at github .\n87. Table2displays the statistics of the dataset.\n88. Details in Appendix C.2.\n89. Visual and numeric reasoning: These kinds of questions necessitate combing visual elements understanding and numerical reasoning techniques (e.g., sum, multiple, average, etc.).\n90. Integrating visual and numerical reasoning inquiries can facilitate CQA systems\' comprehension of chart content, as it encourages the concurrent utilization of their visual and analytic faculties, thereby enabling them to engage in a more profound exploration of the underlying message conveyed by the data and achieve a more precise interpretation of chart figures.\n91. Examples of this type of question are presented in Figure1(a) and (b).\n92. Common sense reasoning: Questions of this type demand combining common sense knowledge and numerical operation.\n93. Common sense is able to serve as a facilitator for CQA systems to gain a more profound understanding of the real-life background and context reflected by the data, thus accurately inferring the meaning behind the data.\n94. Meanwhile, numerical reasoning skills can allow readers to fathom the underlying interconnections and relationships of the data and infer potential outcomes and trends.\n95. The combination of both proficiencies can profoundly equip CQA models with diverse conceptualizations of chart content and enable them to increase the usefulness of data in comprehensively examining and scrutinizing data, identifying patterns and trends, and making predictions and decisions.\n96. Examples of this type of question are presented in Figure1 (c).\n97. Construction of the hierarchical entity database: Common sense reasoning is a crucial aspect of DCQA, which primarily involves evaluating a CQA model\'s ability to discriminate between legend labels and entity names that belong to a specific parent class and then perform reasoning operations based on this discriminatory capacity.\n98. Therefore, a hierarchical entity database with a tree-structured architecture and a well-defined set of parent-child relationships is necessary to serve as a source for both entity names and legend labels.\n99. The construction of the hierarchical entity database is expounded upon in Appendix C.1.\n100. Categorization of question difficulty levels: Additionally, the entire set of question templates has been classified into five distinct levels delineated by their respective levels of complexity, namely, beginner, elementary, intermediate, ad-  vanced, and expert.\n101. The statistic of question levels is displayed in Table3.\n102. The difficulty levels of the question templates are manually annotated based on the following criteria: (1) Beginner includes questions about the overall nature of a chart image, such as whether it is horizontal or vertical or the number of columns it contains, as well as retrieval for the value of a specific chart element.\n103. (2) Elementary primarily involves questions carrying out some form of operation on all chart elements within a chart, such as determining the maximum, minimum, median, or mean value.\n104. (3) Intermediate refers to questions that involve applying specific operations to chart elements that satisfy predetermined criteria, including but not limited to identifying the maximum, minimum, median, mean, sum, or difference of chart elements based on their color, legend, or numerical value.\n105. (4) Advanced questions demand performing composite operations on chart elements that meet a specific property (building upon the operations mentioned for intermediate questions), such as finding the sum or difference of two maximum values after they have been identified.\n106. (5) Building upon advanced questions, questions that involve common sense will be categorized as expert-level.\n107. In this paper, answers are generated through an automated process based on a customized set of procedures.\n108. Specifically, a solution step is designed for each question template, with each step representing an atomic operation that is implemented using specific functions to achieve its intended functionality.\n109. When solving specific questions to generate answers, the designed solution steps are followed by invoking corresponding functions, resulting in answers for the respec- Upon completing the question-answer pair generation process, extra analysis is conducted on the distribution of every answer type.\n110. It is noted that the highest proportion of the answer type in the dataset is the binary classification yes or no.\n111. However, the ratio between yes and no is severely imbalanced, with a vastly larger number of no responses compared to yes.\n112. As a result, a post-processing adjustment is necessary to address this language bias and prevent the model from exploiting the answer distribution pattern to output the answer without paying attention to the visual content.\n113. The debiasing procedure can be concisely described as: Firstly, filter out all question templates with Yes/No answers.\n114. Secondly, count the number of Yes/No answers for each Yes/No question template in the dataset.\n115. For each Yes/No question template, determine whether the number of Yes answers exceeds the number of No answers or vice versa.\n116. For the question with a larger proportion of answers, adjust the values of the replaceable modules in the question to change the answer to the less frequent one, and iterate this process until the number of Yes and No answers for the template are equal or differ by only 1.\n117. Most of the Yes/No question templates can be balanced by modifying the answer using the above approach.\n118. However, a few questions cannot be balanced this way, and their answers are not significantly  imbalanced.\n119. Therefore, we do not handle such question templates in this paper for now.\n120. Before debiasing, the dataset\'s overall proportion of yes and no answers was 35.16% and 64.84%, respectively.\n121. After debiasing, the overall proportion of yes and no answers in the dataset became 49.26% and 50.74%, respectively.\n122. The effectiveness of the debiasing is compared in Figure4, where the noticeable changes in the answer distribution of "Yes" and "No" before and after debiasing demonstrate that the debiasing method employed in this study effectively addresses the answer imbalance in binary questions.\n123. In accordance with the methodology outlined in[22], we utilize LaTex to generate synthetic documents that include diverse multimedia elements.\n124. In addition to the chart image, the generated document also incorporates other visual content 2  and textual content produced by ChatGLM[23,24].\n125. org of the synthetic document beyond the mere inclusion of the chart image, which is more consistent with real-world documents.\n126. Detailed document information is provided in Appendix D.\n127. In this section, we provide a comprehensive analysis of the experimental results to establish the validity of the recently developed DCQA dataset and verify the excellent efficacy of our proposed TOT-Doctor model through a comparative evaluation against other baselines.\n128. We compare the TOT-Doctor with the classical approaches include LayoutLMv2[25], LayoutLMv3[26], Pix2Struct[9], and MATCHA[27].\n129. • LayoutLMv3[26]is a multi-modal Transformer framework without the vision backbone that leverages reconstructive objectives for cross-modal alignment learning, showcasing notable generality in the context of document vision tasks.\n130. • Pix2Struct[9]is an image-to-text model tailored for visual language understanding, which is pre-trained on visually-rich screenshots of web pages with screenshot parsing objective.\n131. • MATCHA[27]is a Pix2Struct-based model pre-trained for chart underlying structure understanding and mathematical reasoning.\n132. Our study employs accuracy the primary evaluation metric, wherein the assessment of the predicted answers correctness depends on the nature of the answer type.\n133. In the case of textual answers, such as binary responses, entities, and integers, the evaluation criterion mandates that the predicted answer should match the ground truth exactly.\n134. For numerical answers in the form of floating-point values, it is not always feasible to expect that the predicted answer will precisely match the correct answer.\n135. Therefore, we consider the answer correct if it falls within 5% of the expected value.\n136. This section primarily supplements experimental configurations, including parameters such as batch size and learning rate, and outlines the preprocessing steps undertaken to ensure a fair comparison for the extractive model.\n137. Table4shows the detailed experimental setup, in which CE refers to Cross Entropy.\n138. All models were verified every 5000 iterations during training.\n139. In our implementation of LayoutLMv2 and v3, we employ a multi-step learning rate schedule.\n140. More specifically, we gradually decrease the learning rate by a factor of 2 after each epoch of training.\n141. For other models, we use the cosine scheduler to adjust the learning rate, where the number of warm-up steps is set to 1000.\n142. The LayoutLM series employs a model-based extraction approach, which requires the system to select answers from the optical character recognition (OCR) results.\n143. To enable the model to perform tasks such as binary classification (e.g., Yes/No), we have implemented a simple yet effective solution: we add two special characters, "Yes" and "No", to the OCR results.\n144. TOT-Doctor consists of two main components, namely the document layout analysis and the chart question answering.\n145. The model parameters of TOT-Doctor are calculated and found to be as follows: the encoder of the Swin Transformer used in the document layout analysis has 74M parameters, while the detector component has 48M parameters.\n146. The encoder of the Swin Transformer used in the chart question answering phase has 74M parameters, and the BART has 127M parameters.\n147. Based on the fine-grained document element annotations present within the DCQA dataset introduced in this paper, encompassing various elements such as chart image, picture, textual content, list, caption, header, footer, and page number, the document layout analysis model of the proposed ToT-Doctor framework was trained.\n148. The conclusive results of the document layout analysis testing on the test set are presented in Table5.\n149. Notably, the detection accuracy for chart image reached an impressive 99.901%, exhibiting a near-complete precision in accurately identifying their respective spatial position.\n150. This achievement serves as a robust foundational prerequisite for facilitating subsequent stage of chart question answering task.\n151. The experiment results of our proposed TOT-Doctor and other baselines are displayed in Table6.\n152. Due to the incapacity of the baseline to perform document layout analysis, we add our document layout analysis framework to them before conduct chart question answering.\n153. Based on the data listed in Table6, it can be seen that TOT-Doctor consistently surpasses its counterparts concerning the accuracy in both the validation and test sets, corroborating the efficacy of TOT-Doctor.\n154. It is noteworthy that despite LayoutLMv2 and LayoutLMv3 being reliant on OCR for obtaining the answers, their performance continues to lag behind our OCR-free TOT-Doctor.\n155. This observation proves the robustness and OCR error mitigation capabilities of the TOT-Doctor proposed in this study.\n156. Furthermore, in comparison to the latest state-of-the-art (SOTA) pre-trained visual language understanding model Pix2Struct, TOT-Doctor demonstrates superior performance, achieving a significant improvement of approximately 21.171% on the development dataset, and a respectable enhancement of 20.796% on the test dataset, respectively.\n157. The outstanding performance of our TOT-Doctor model underscores the significance of integrating vision and language features in an OCR-free manner to address the questions posed in DCQA effectively.\n158. The DCQA dataset incorporates five distinct question levels.\n159. In order to better discern the effectiveness of our proposed TOT-Doctor and other baselines, we conduct a performance analysis on each question level.\n160. The results of our analysis are presented in Table7for reference.\n161. It is evident from the results that TOT-Doctor consistently outperforms other baselines in all five levels of questions.\n162. Notably, TOT-Doctor exhibits superior performance on intermediatelevel questions, demonstrating its efficacy in directly applying specific operations such as identifying maximum, minimum, median, mean, and sum to chart elements or analyzing the differences of chart elements based on their color, legend, or numerical value.\n163. However, when encountering the subdifficult advanced-level questions involving composite oper-ations and the most challenging expert-level questions that necessitate commonsense understanding, the performance of all baselines, including TOT-Doctor, considerably decreases compared to the other three more tractable question levels.\n164. This implies that the ability for complex reasoning and commonsense understanding still requires further improvement for analyzing complex documents.\n165. Fig.6: Performance comparison between common sense and numerical reasoning questions on DCQA test set.\n166. We conduct additional experiments to evaluate the performance of the TOT-Doctor and baseline models on different question types.\n167. As discussed before, DCQA comprises two primary question types: visual and numeric reasoning and commonsense reasoning.\n168. Figure6presents the results of all baselines for each question type on the test set.\n169. Our proposed TOT-Doctor outperforms other baselines significantly, particularly in numerical reasoning questions.\n170. To top it all off, TOT-Doctor demonstrates more proficiency in numerical reasoning compared to commonsense understanding.\n171. To further investigate our TOT-Doctor model, we assess the ability of the TOT-Doctor to generate different answer types.\n172. From Table8, we discover that except for LayoutLMv2, all other baselines perform better in answering Yes/No questions.\n173. We observe that LayoutLMv2 and LayoutLMv3 exhibit frustrating performance in generating numerical and string answers.\n174. We speculate that this is mainly because LayoutLMv2 and LayoutLMv3 are extractive models, which means that they cannot generate answers that have not appeared in the document.\n175. This precisely explains their poor performance on the DCQA dataset, where the answers are largely obtained through data reasoning and involve numerical values.\n176. It is noted that EasyOCR 3 is utilized as the OCR system for these two baselines, which deviates from the OCR employed in their original versions.\n177. Based on this observation, we posit that the accuracy of the OCR system may have contributed to the subpar performance of the models.\n178. Moreover, TOT-Doctor is well versed in generating answers in terms of numerical or string, which verifies the robustness of TOT-Doctor.\n179. However, overall, all baselines achieve abysmal accuracy in generating numerical and string answers, highlighting the significant challenge posed by document-level chart understanding, which calls for further research efforts.\n\n请严格按照JSON格式返回：{"labels": [0,1,0,...]}，不要包含任何其他文字。'}], 'model': 'qwen-plus', 'response_format': {'type': 'json_object'}}}
2025-10-14 19:54:10,899 - DEBUG - Sending HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
2025-10-14 19:54:10,899 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-10-14 19:54:10,899 - DEBUG - send_request_headers.complete
2025-10-14 19:54:10,899 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-10-14 19:54:10,900 - DEBUG - send_request_body.complete
2025-10-14 19:54:10,900 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-10-14 19:54:18,264 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'vary', b'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding'), (b'x-request-id', b'dcef80e5-b576-46f8-af36-ffdc58661fd7'), (b'x-dashscope-call-gateway', b'true'), (b'content-type', b'application/json'), (b'req-cost-time', b'7328'), (b'req-arrive-time', b'1760442842073'), (b'resp-start-time', b'1760442849401'), (b'x-envoy-upstream-service-time', b'7290'), (b'content-encoding', b'gzip'), (b'date', b'Tue, 14 Oct 2025 11:54:09 GMT'), (b'server', b'istio-envoy'), (b'transfer-encoding', b'chunked')])
2025-10-14 19:54:18,265 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-14 19:54:18,265 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-10-14 19:54:18,265 - DEBUG - receive_response_body.complete
2025-10-14 19:54:18,265 - DEBUG - response_closed.started
2025-10-14 19:54:18,265 - DEBUG - response_closed.complete
2025-10-14 19:54:18,265 - DEBUG - HTTP Response: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "200 OK" Headers({'vary': 'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding', 'x-request-id': 'dcef80e5-b576-46f8-af36-ffdc58661fd7', 'x-dashscope-call-gateway': 'true', 'content-type': 'application/json', 'req-cost-time': '7328', 'req-arrive-time': '1760442842073', 'resp-start-time': '1760442849401', 'x-envoy-upstream-service-time': '7290', 'content-encoding': 'gzip', 'date': 'Tue, 14 Oct 2025 11:54:09 GMT', 'server': 'istio-envoy', 'transfer-encoding': 'chunked'})
2025-10-14 19:54:18,265 - DEBUG - request_id: dcef80e5-b576-46f8-af36-ffdc58661fd7
2025-10-14 19:54:18,265 - DEBUG - API request completed in 7.37 seconds
2025-10-14 19:54:18,265 - DEBUG - Raw model response: {"labels": [0,0,0,0,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,1,1,1,1,1,1,0,0,1,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}
2025-10-14 19:54:18,266 - INFO - Successfully processed 130 labels
2025-10-14 19:54:18,266 - ERROR - Label count mismatch for DCQA_ DOCUMENT_LEVEL CHART QUESTION ANSWERING TOWARDS COMPLEX REASONING AND COMMON_SENSE UNDERSTANDING
2025-10-14 19:54:18,266 - INFO - Evaluating paper 10/18: Debate Helps Supervise Unreliable Experts
2025-10-14 19:54:18,266 - INFO - Starting model prediction
2025-10-14 19:54:18,266 - INFO - Attempt 1 of 5
2025-10-14 19:54:18,266 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-3562c2cc-4587-46ae-a98c-cefd2f4a29a7', 'json_data': {'messages': [{'role': 'system', 'content': '你是一个专门用于识别学术论文中数据集描述的助手。你需要判断每个输入的句子是否描述了研究中使用的数据集。请确保：\n    1. 只输出JSON格式的结果\n    2. 结果格式必须为 {"labels": [0,1,0,...]}\n    3. 不要输出任何其他文字或解释'}, {'role': 'user', 'content': '你是一位专业的学术论文数据集描述识别专家。请仔细分析下面这篇论文中的每个句子，判断它们是否描述了本研究特定构建的数据集信息。\n\n    论文标题: Debate Helps Supervise Unreliable Experts\n\n    判断标准:\n    判断为1(数据集描述)的标准：\n    句子必须直接描述本研究特定构建的数据集,包括:\n    1. 该数据集的构建过程和方法\n    2. 该数据集的具体构成和规模\n    3. 该数据的来源和收集方式\n    4. 该数据的预处理步骤\n    5. 该数据集的可获取方式(如发布地址)\n    6. 或者其他直接或者间接描述了该数据集的句子\n\n    所有其他类型的句子均标记为0。\n    \n    请对每个句子进行分析，返回一个JSON格式的标签数组。格式要求：\n    1. 必须是有效的JSON格式\n    2. 只包含labels字段，值为0和1组成的数组\n    3. 数组长度必须与句子数量相同\n    4. 示例格式：{"labels": [0,1,0,1,...]}\n\n    需要判断的句子：\n\n    1. Abstract: As AI systems are used to answer more difficult questions and potentially help create new knowledge, judging the truthfulness of their outputs becomes more difficult and more important.\n2. How can we supervise unreliable experts-which have access to the truth but may not accurately report it-to give answers that are systematically true and don\'t just superficially seem true, when the supervisor can\'t tell the difference between the two on their own?\n3. In this work, we show that debate between two unreliable experts can help a non-expert judge more reliably identify the truth.\n4. We collect a dataset of human-written debates on hard reading comprehension questions where the judge has not read the source passage, only ever seeing expert arguments and short quotes selectively revealed by \'expert\' debaters who have access to the passage.\n5. In our debates, one expert argues for the correct answer, and the other for an incorrect answer.\n6. Comparing debate to a baseline we call consultancy, where a single expert argues for only one answer which is correct half of the time, we find that debate performs significantly better, with 84% judge accuracy compared to consultancy\'s 74%.\n7. Debates are also more efficient, being 68% of the length of consultancies.\n8. By comparing human to AI debaters, we find evidence that with more skilled (in this case, human) debaters, the performance of debate goes up but the performance of consultancy goes down.\n9. Our error analysis also supports this trend, with 46% of errors in human debate attributable to mistakes by the honest debater (which should go away with increased skill); whereas 52% of errors in human consultancy are due to debaters obfuscating the relevant evidence from the judge (which should become worse with increased skill).\n10. Overall, these results show that debate is a promising approach for supervising increasingly capable but potentially unreliable AI systems.\n11. How can we tell if an AI system is telling the truth?\n12. Current language models trained to act as AI assistants, such asGPT-4 (OpenAI, 2023a)and Claude(Anthropic, 2023b,a) can correctly answer a wide variety of questions, construct coherent essays, and perform well on academic and professional exams(Hendrycks et al., 2020;OpenAI, 2023b).\n13. But the truthfulness of their responses is not robust: Such systems are prone to making false claims, giving misleading explanations about their reasoning Figure1: High-level summary of our experimental setup.\n14. We source hard reading comprehension questions from the QuALITY dataset(Pang et al., 2022)and incentivize human judges who can\'t read the passage to answer them correctly.\n15. Experts who have full access to the passage are allowed to reveal snippets of it (highlighted) in addition to free-text prose.\n16. In debate, the experts simultaneously defend their assigned option in their opening statements, and following rounds are sequential.In consultancy, the non-expert judge only interacts with one expert defending one option chosen at random.\n17. In both settings, the judge chooses when to end the session; sessions average at about 1,000 words total.(Turpin et al., 2023), and reinforcing the inferred opinions of their interlocutors(Perez et al., 2022;Bang et al., 2023;Borji, 2023).\n18. Language models have access to a vast array of information from their training data to draw on and synthesize-far beyond the knowledge of any individual human who might be involved in supervising them.\n19. As such, they could hold the potential to help us answer increasingly difficult questions or even create new knowledge that we otherwise couldn\'t.\n20. However, we expect that it will be increasingly hard to verify and supervise the truthfulness of their outputs in these cases.\n21. As language models become more capable and are used in more complex settings, it is likely that subtle mistakes, deceptive arguments, or selective use of evidence will become more difficult to spot.\n22. Making sure the information they provide is reliable requires effective methods for verifying the outputs of systems that know things we don\'t-a task known as scalable oversight(Amodei et al., 2016).\n23. Proposals for scalable oversight often involve leveraging the AI\'s abilities to help evaluators, for example with recursive reward modeling(Leike et al., 2018), model self-critique(Saunders et al., 2022), and debate(Irving et al., 2018).\n24. In debate-the focus of this work-two equally-capable expert debaters (e.g., AI systems) argue with each other over the answer to a question, each aiming to convince a non-expert (human) judge of their side.\n25. With an adversarial expert pointing out flaws in its arguments, neither debater will be able to get away with claims that its opponent can convincingly refute in the eyes of the judge.\n26. Training AI systems to win such debates should incentivize them not to make such claims in the first place.\n27. AsIrving et al. (2018)argue, this means that debate would incentivize an AI to tell the truth, as long as it is harder to lie than to refute a lie-i.e., the most successful strategies for debate lead judges to make good, informed decisions, rather than, for example, tricking them, confusing them, or prolonging the debate indefinitely.\n28. In this paper, we demonstrate for the first time that debate helps judges find truth on a realistic task, using debates on hard reading comprehension questions.\n29. To test this, we compare debate to a baseline we call consultancy, where the judge interacts with a single unreliable expert who has a 50/50 chance of arguing for the correct answer.\n30. By prompting the consultant to argue for the wrong answer half of the time, this baseline explicitly elicits dishonest behavior which may arise implicitly in Reinforcement Learning from Human Feedback (RLHF), as in cases, e.g., of sycophancy(Perez et al., 2022).\n31. To evaluate this with the strongest possible debaters, we collect and analyze a dataset of all-human debates, enlisting competitive debaters from the New York University debate team.\n32. A high-level overview of our setup is illustrated in Figure1.2For each debate, we pose a reading comprehension question from the QuALITY dataset(Pang et al., 2022)together with two answer choices (one correct, one incorrect), and allow the debaters-but not the judge-to read the story the question is about.\n33. The judge then interactively judges a debate on the question, where the debaters can back up their claims by selectively revealing short excerpts drawn from the story.\n34. Judge accuracy in these debates is 84%, compared to with 74% on consultancy (Section 4).\n35. Debate is also more efficient, being 68% of the length and requiring 61% as much ground-truth evidence, suggesting that it will be a more effective method than open-ended dialogue (cf.Bowman et al., 2022)for helping annotators efficiently supervise untrusted models that exceed their expertise.\n36. We also find that our judges are relatively calibrated overall on debates, though they struggle with overconfidence in the high-confidence regime (Figure5).\n37. While there are still cases when the judge of a debate gets the answer wrong, we find that the most common sources of error should be possible to mitigate with further judge training or stronger debaters.\n38. For example, in 33% of mistakes, the judge ended the session prematurely, either after only a single round or immediately after changing their preferred answer, giving the debaters no opportunity to refute the judge\'s final reasoning.\n39. In 46% of mistakes, the debater arguing for the correct answer missed an important argument or piece of evidence that they could have used (Section 5).\n40. We also include experimental results for AI debate, using GPT-4 as a debater (Section 4).\n41. In this setting, we find no difference between debate and consultancy.\n42. However, even if debate does not work better as an oversight method for current models, that may simply be because they have not yet reached human-level capabilities at deception (i.e., as a consultant) and argumentation (as a debater); it is also possible that we do not optimize GPT-4\'s prompt heavily enough to elicit such capabilities.\n43. It seems plausible that AI systems may soon be capable enough of argumentation and persuasion that debate will be important to incorporate into their training; in Section 8 we lay out an agenda for what this may look like, and what challenges will need to be solved to make this work.\n44. As we use AI systems in more difficult and complex settings, we will need stronger mechanisms to verify their arguments-ideally, methods which improve concordantly and at pace with the system\'s capabilities.\n45. Our results with human debaters demonstrate for the first time that debate, where equally-capable experts point out flaws with each other\'s arguments, can allow a non-expert judge to effectively determine the answers to questions they could not answer on their own.\n46. This suggests that debate may soon be important for effectively supervising models to truthfully answer hard questions.\n47. Source Material and Questions We draw the questions to be debated from the Question Answering with Long Input Texts, Yes!\n48. (QuALITY) dataset of reading comprehension questions(Pang et al., 2022).\n49. 4To focus on especially hard questions, we further restrict our results to questions that were marked by the untimed annotators as requiring more than one or two sentences of context to get correct (the idea being to avoid questions which could be easily resolved with a single quote from one of the debaters).\n50. Each question in QuALITY has four answers, one of which is correct; as our debates consider only two answer choices, we use the correct answer and the incorrect option that was labeled as the best distractor most often by the QuALITY dataset\'s untimed validation annotators.\n51. We only use the Project Gutenberg subset of QuALITY-hard, which consists of questions over public-domain science fiction short stories.\n52. Since the stories are entirely fictional, judges can almost never guess the answer on the basis of prior knowledge, and must rely on the information provided by the debaters.5.\n53. On average, the stories used for our debates have 27.7k characters, or 6.3k tokens using the CoreNLP tokenizer(Manning et al., 2014).\n54. For each turn, we use a character limit of ℓ  = 750 and a quote limit of ℓ  = 250, meaning that on average up to 1.8% of the story could be revealed in each round of the debate.\n55. Experimental Conditions While our main experimental results concern human debaters, we also test with AI debaters.\n56. As the AI debater, we use the version of GPT-4 with a 32,000-token context window available through the OpenAI API as gpt-4-32k.\n57. Prompts are provided in Appendix G.\n58. 6This gives us four experimental conditions: human debate, human consultancy, AI debate, and AI consultancy.\n59. We recruit 19 people to serve as both debaters and judges in our experiments.\n60. Our participants, all of whom were New York University employees during data collection, include 12 undergraduates on the NYU debate team, all with at least one year of experience in competitive debate; 6 members of the research team, three of whom have at least one year of experience with competitive debate; and one NYU Master\'s student with 6 years of experience studying Jewish legal reasoning and argumentation.\n61. After running initial pilots in Fall of 2022 to establish the debate protocol (see Appendix B), we collect debates according to the protocol defined in Section 2, with collection running from February to August of 2023.\n62. During collection, participants can log into our data collection platform to read stories or take turns in their debates at any time, but we also set aside specific times each week when we request that the debaters work, to facilitate near-synchronous debates.\n63. To avoid information leakage between debates, each participant is only allowed to judge one question about each story.\n64. After each debate is complete, all participants fill out a feedback survey with quantitative and qualitative observations which we use to help us analyze the results (see Appendix F).\n65. Participants cannot see the identities of the other participants in the debate until after filling out the feedback form.\n66. Data collection was not perfectly controlled between our four experimental conditions, as some components of our experimental design were developed part of the way through data collection: The consultancy baseline was only developed in June of 2023 and the AI debaters were only incorporated 4We ended up drawing 59% of our questions from the QuaLITY training set, which has 3 untimed validators per question, and 41% from the development set, which has 5 untimed validators per question.\n67. 5In our data, judge priors were between 45%-55% in 91% of debates, and between 35%-65% in 97% of debates.\n68. 6Because the QuALITY questions are drawn from public-domain texts available from Project Gutenberg, it is likely that the passages used in our experiments appear in GPT-4\'s training corpus.\n69. This does not pose a data contamination issue when using GPT-4 as a debater, since debaters are meant to be experts and are given full access to the text anyway.\n70. However, this does could pose issues for future work testing AI judging in our setting, since it might be difficult to guarantee that the models do not use prior knowledge of the story in their decisions, instead of relying on the debate.\n71. Quotes and characters per round measure how close debaters came to their character limits; quote totals are calculated only using new quoted material that hasn\'t been used yet in the debate.\n72. Bits/rd is the amount of information conveyed to the judge on average per round, calculated from the information gain between their final and initial judgment log 2 (  * , / * ,0 ),   is the judge\'s score as defined in Equation1, and ECE final is the expected calibration error of the judge\'s final judgments, calculated with a bin size of 10%. into the data collection platform in July.\n73. The set of debaters who participated in the experiment also varied over the course of these months.\n74. These factors were due to us trying to collect as much data as possible subject to practical limits on engineering capacity, annotator availability, and researcher foresight.\n75. We validate our analysis in Section 4 with partial controls in Appendix D.\n\n请严格按照JSON格式返回：{"labels": [0,1,0,...]}，不要包含任何其他文字。'}], 'model': 'qwen-plus', 'response_format': {'type': 'json_object'}}}
2025-10-14 19:54:18,267 - DEBUG - Sending HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
2025-10-14 19:54:18,267 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-10-14 19:54:18,267 - DEBUG - send_request_headers.complete
2025-10-14 19:54:18,268 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-10-14 19:54:18,268 - DEBUG - send_request_body.complete
2025-10-14 19:54:18,268 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-10-14 19:54:24,527 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'vary', b'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding'), (b'x-request-id', b'd146a17c-e497-4c45-bd6d-445f187e3694'), (b'x-dashscope-call-gateway', b'true'), (b'content-type', b'application/json'), (b'req-cost-time', b'6223'), (b'req-arrive-time', b'1760442849437'), (b'resp-start-time', b'1760442855661'), (b'x-envoy-upstream-service-time', b'6186'), (b'content-encoding', b'gzip'), (b'date', b'Tue, 14 Oct 2025 11:54:15 GMT'), (b'server', b'istio-envoy'), (b'transfer-encoding', b'chunked')])
2025-10-14 19:54:24,527 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-14 19:54:24,527 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-10-14 19:54:24,527 - DEBUG - receive_response_body.complete
2025-10-14 19:54:24,527 - DEBUG - response_closed.started
2025-10-14 19:54:24,527 - DEBUG - response_closed.complete
2025-10-14 19:54:24,528 - DEBUG - HTTP Response: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "200 OK" Headers({'vary': 'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding', 'x-request-id': 'd146a17c-e497-4c45-bd6d-445f187e3694', 'x-dashscope-call-gateway': 'true', 'content-type': 'application/json', 'req-cost-time': '6223', 'req-arrive-time': '1760442849437', 'resp-start-time': '1760442855661', 'x-envoy-upstream-service-time': '6186', 'content-encoding': 'gzip', 'date': 'Tue, 14 Oct 2025 11:54:15 GMT', 'server': 'istio-envoy', 'transfer-encoding': 'chunked'})
2025-10-14 19:54:24,528 - DEBUG - request_id: d146a17c-e497-4c45-bd6d-445f187e3694
2025-10-14 19:54:24,528 - DEBUG - API request completed in 6.26 seconds
2025-10-14 19:54:24,528 - DEBUG - Raw model response: {"labels": [0,0,0,1,1,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]}
2025-10-14 19:54:24,528 - INFO - Successfully processed 86 labels
2025-10-14 19:54:24,528 - ERROR - Label count mismatch for Debate Helps Supervise Unreliable Experts
2025-10-14 19:54:24,528 - INFO - Evaluating paper 11/18: Distilling Large Language Models for Matching Patients to Clinical Trials
2025-10-14 19:54:24,528 - INFO - Starting model prediction
2025-10-14 19:54:24,528 - INFO - Attempt 1 of 5
2025-10-14 19:54:24,529 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-663d3165-f6ca-42ed-9708-9f45fa81398f', 'json_data': {'messages': [{'role': 'system', 'content': '你是一个专门用于识别学术论文中数据集描述的助手。你需要判断每个输入的句子是否描述了研究中使用的数据集。请确保：\n    1. 只输出JSON格式的结果\n    2. 结果格式必须为 {"labels": [0,1,0,...]}\n    3. 不要输出任何其他文字或解释'}, {'role': 'user', 'content': '你是一位专业的学术论文数据集描述识别专家。请仔细分析下面这篇论文中的每个句子，判断它们是否描述了本研究特定构建的数据集信息。\n\n    论文标题: Distilling Large Language Models for Matching Patients to Clinical Trials\n\n    判断标准:\n    判断为1(数据集描述)的标准：\n    句子必须直接描述本研究特定构建的数据集,包括:\n    1. 该数据集的构建过程和方法\n    2. 该数据集的具体构成和规模\n    3. 该数据的来源和收集方式\n    4. 该数据的预处理步骤\n    5. 该数据集的可获取方式(如发布地址)\n    6. 或者其他直接或者间接描述了该数据集的句子\n\n    所有其他类型的句子均标记为0。\n    \n    请对每个句子进行分析，返回一个JSON格式的标签数组。格式要求：\n    1. 必须是有效的JSON格式\n    2. 只包含labels字段，值为0和1组成的数组\n    3. 数组长度必须与句子数量相同\n    4. 示例格式：{"labels": [0,1,0,1,...]}\n\n    需要判断的句子：\n\n    1. Abstract: The recent success of large language models (LLMs) has paved the way for their adoption in the high-stakes domain of healthcare.\n2. Specifically, the application of LLMs in patient-trial matching, which involves assessing patient eligibility against clinical trial\'s nuanced inclusion and exclusion criteria, has shown promise.\n3. Recent research has shown that GPT-3.5, a widely recognized LLM developed by OpenAI, can outperform existing methods with minimal \'variable engineering\' by simply comparing clinical trial information against patient summaries.\n4. However, there are significant challenges associated with using closed-source proprietary LLMs like GPT-3.5 in practical healthcare applications, such as cost, privacy and reproducibility concerns.\n5. To address these issues, this study presents the first systematic examination of the efficacy of both proprietary (GPT-3.5, and GPT-4) and open-source LLMs (LLAMA 7B,13B, and 70B) for the task of patient-trial matching.\n6. Employing a multifaceted evaluation framework, we conducted extensive automated and human-centric assessments coupled with a detailed error analysis for each model.\n7. To enhance the adaptability of open-source LLMs, we have created a specialized synthetic dataset utilizing GPT-4, enabling effective fine-tuning under constrained data conditions.\n8. Our findings reveal that open-source LLMs, when fine-tuned on this limited and synthetic dataset, demonstrate performance parity with their proprietary counterparts.\n9. This presents a massive opportunity for their deployment in real-world healthcare applications.\n10. To foster further research and applications in this field, we release both the annotated evaluation dataset along with the fine-tuned LLM -Trial-LLAMA -for public use.\n11. Clinical trials represent both the most important and the most challenging aspect of medical advancements.\n12. These trials serve a dual function: first, as a conduit for patients to access potentially life-altering treatments, and second, as a mechanism for the iterative process of drug development and approval.However, a significant number of trials are beleaguered by extended timelines.\n13. Empirical data suggests that, on average, clinical trials take approximately twice as long as initially projected[22], with approximately 40% of trial sites failing to meet their enrollment targets[15].\n14. Apart from others, one of the major challenges in recruiting patients is matching them against suitable trials[5,20,6,26,12,28,19,29].\n15. The process of matching a patient to trials is challenging.\n16. It requires both the meticulous analysis of electronic health records (EHRs) and the contextual interpretation of this data against the backdrop of clinical trial criteria.\n17. This is particularly challenging because a majority of this data is stored in unstructured documents written in free text.\n18. Even the structured data is difficult to query due to the increasing complexity of inclusion and exclusion criteria.\n19. Automating this process can accelerate trials save healthcare providers\' time spent on manual chart reviews.\n20. Current approaches primarily rely on data extraction or classification pipelines[36,49,27].\n21. Nonetheless, these methods require extensive variable engineering, which frequently results in constrained contextual comprehension and limited scalability when dealing with intricate trial criteria.\n22. The emergence of Large Language Models (LLMs), such as Med-PaLM[37]and GPT-4[25], marks a paradigm shift in the domain of automated interpretation of patient health records.\n23. These models embody the cutting-edge in natural language processing (NLP), facilitating nuanced and context-aware analysis of complex medical data.\n24. Leveraging their capabilities, recent research has used these models for a variety of clinical information interpretation tasks, including patient matching[18].\n25. However, their deployment in healthcare settings presents challenges.\n26. One primary concern relates to the risk of Protected Health Information (PHI) leakage when using such models.\n27. Most healthcare organisations prefer on-premise infrastructure for tools that handle identified patient data.\n28. However, due to the cost and computational complexity associated with these models, they often remain in centralized cloud provider environments.\n29. These challenges can make LLMs prohibitive for widespread clinical application.\n30. Moreover, despite their effectiveness, advanced models are often characterized by opacity and proprietary restrictions, which further complicate their integration into healthcare systems subject to stringent regulatory constraints.\n31. In light of these considerations, there is a growing need for the development of open-source LLMs that can match the accuracy of their proprietary counterparts but at a significantly reduced cost.\n32. This also enables healthcare organizations to seamlessly integrate these technologies into their existing infrastructures, mitigating the risk of Protected Health Information (PHI) leaks.\n33. To the best of our knowledge, this study is the first comprehensive examination of the efficacy of open-source LLMs in this domain.\n34. • Our work thoroughly compares open-source LLMs and their proprietary counterparts for patienttrial matching.\n35. • We further explore and elucidate the impact of fine-tuning on various open-source LLMs for patient-trial matching.\n36. • We define the error taxonomy and thoroughly analyze the nature of errors made by the models on this task.\n37. • Along with the experimental details, we publicly release the evaluation dataset and the LLM trained based on LLAMA for patient-trial matching.\n38. We tested both proprietary (GPT-3.5, and GPT-4) and open-source LLMs (LLAMA-2 7B,13B, and 70B, referred to LLAMA hereafter) for the task of patient-trial matching.\n39. For GPT-3.5, we leveraged the Azure Open AI API, specifically gpt-35-turbo-16k-0613 as the model version.\n40. We set the temperature parameter to 0, aiming for deterministic outputs that would ensure consistency and repeatability in our experiments.\n41. This was coupled with a top p setting of 0.95, aligning with our goal to eliminate randomness in the model\'s response generation process.\n42. Additionally, we refrained from applying any frequency or repetition penalties, allowing the model\'s natural language generation capabilities to function without these constraints.\n43. For GPT-4, we employed a similar configuration with gpt-4-0613 as the model version.\n44. For LLAMA, we changed the configuration from Meta.\n45. We initially encountered challenges in aligning the standard versions of these models to produce outputs in the required format, particularly in the context of complex clinical trial criteria.\n46. To address this, we opted for specific versions tailored for chat applications, namely Llama-2-7b-chat-hf, Llama-2-13b-chat-hf, and Llama-2-70b-chat-hf.\n47. These versions offered a more flexible and adaptable framework for our needs.\n48. We adjusted the temperature setting to 0.4 for all LLAMA models, a decision informed by preliminary tests which indicated that a slightly higher temperature prevented the model from collapsing on certain trials where inclusion/exclusion criteria were not clearly defined.\n49. Maintaining the output format was particularly challenging when working with the base LLAMA models.\n50. Despite employing various techniques such as context-free grammar (CFG) to constrain the model\'s output, the results remained suboptimal.Consequently, the models were unable to generate structured output for complex clinical trials.\n51. To circumvent this, we adjusted the model\'s temperature to foster more exploratory behavior and executed five output generations iteratively till the output matched our JSON schema.\n52. This allowed us to generated structured output for majority of clinical trials even with base models.\n53. Compute Novelty Index: noveltyIndex ← 1 -score τ 6: if noveltyIndex > 0 then end for 10: end for 11: return C novel Each criterion in C final is then annotated with a gold-standard answer and corresponding evidences.\n54. These evidential references serve as a basis for language models to substantiate their answers.\n55. To gauge the faithfulness of various models in accurately citing these pieces of evidence, we calculate precision, recall, and F1 scores for each model.\n56. Additionally, we conduct a direct comparison of model performance at the criterion level to evaluate their relative effectiveness.\n57. Different from the metrics used in[18], we created two distinct aspects of Criterion-Level Accuracy (CLA), namely Explicit CLA and Implicit CLA, to holistically assess the model\'s performance.\n58. For Explicit CLA, our focus is on evaluating how accurately the model categorizes each criterion into the correct class, provided that the criterion has been previously identified as \'explicit\' in our manual annotation exercise.\n59. This evaluation primarily concerns criteria for which the necessary information for classification is clearly and directly stated in the patient documentation, leaving minimal room for interpretation or inference.\n60. The accuracy here reflects the model\'s ability to comprehend and correctly apply these straightforward, unambiguous data points.\n61. On the other hand, Implicit CLA tackles a more nuanced challenge: it assesses the model\'s performance on criteria deemed \'implicit\' by the annotators.\n62. These criteria involve situations where the required information is not explicitly stated but rather implied or inferred from the available data.\n63. This often requires connecting disparate pieces of information, understanding subtleties and nuances in the patient data, and making educated guesses based on the context.\n64. Calculating the Implicit CLA involves a thorough analysis of how well the model navigates these complexities and accurately classifies criteria based on less direct information.\n65. Both Explicit and Implicit CLAs are pivotal in understanding the model\'s overall capability to process and interpret clinical trial criteria.\n66. While Explicit CLA provides insight into the model\'s proficiency with clear-cut, straightforward tasks, Implicit CLA sheds light on its ability to handle ambiguity and complexity -crucial aspects in the realm of clinical data interpretation.\n67. For our dataset, we adopted the similar datasets as used in[18]that incorporate the SIGIR dataset[21]and both the 2021 and 2022 versions of the TREC CT cohorts[34,33], as shown in Table1.\n68. For each patient within these datasets, we extract 50 clinical trials, categorizing them into three distinct classifications: "eligible", "excluded", and "irrelevant".\n69. The categorization within the SIGIR dataset required a different approach, given its classification system.\n70. (a) "Will not refer to the trial": This class aligns with the \'irrelevant\' category in our study.\n71. (b) "Will refer to the trial": Corresponds to the \'eligible\' category.\n72. (c) "May refer to the trial": This class does not map directly to any of our predefined categories.\n73. Due to this non-conformity, we excluded all trial-patient combinations classified under (c) "May refer to the trial", to maintain consistency.\n74. To facilitate the fine-tuning of our models, we partition the dataset into a training and test set, adhering to an 80:20 ratio.\n75. This division is implemented along the patient axis to ensure no test patient record gets leaked into the training set.\n76. Prior to splitting, all datasets are combined and thoroughly shuffled.\n77. The specifics of the training and test sets are displayed in Table1.\n78. It is noteworthy that despite the large volume of records in the training set, they are not fully utilized for model training.\n79. Instead, the large size of this set provides with an easy mechanism to sample diverse training samples for fine-tuning while also allowing us to save on compute costs associated with evaluation a large number of model checkpoints.\n80. Evidently, as shown in Table1the sampled dataset is more diverse than the training set.\n81. It is known that the performance of a model improves with the volume of data it is exposed to[16].\n82. Nevertheless, the quality of data plays a pivotal role in determining the output\'s caliber.\n83. Multiple research works have demonstrated that while the fine-tuning performance of a model initially improves rapidly, it tends to reach a saturation point beyond a certain threshold of data exposure [?].\n84. This phenomenon is consistent with our findings with the fine-tuning of the LLAMA models of different sizes.\n85. As illustrated in Figure5b, the performance of all three LLAMA variants exhibits a significant initial leap with exposure to a small data subset, followed by a gradual enhancement as they are introduced to an increasing number of examples.\n86. Notably, the largest LLAMA variant swiftly surpasses the performance of GPT-3.5.\n87. For the assessment of model performance, we utilize the metric of overall criteria level accuracy, encompassing both implicit and explicit criteria.\n88. The process of fine-tuning LLMs presents both computational and methodological challenges, primarily due to the difficulty in providing a dense, multi-token signal that these models require for effective learning.\n89. While labeling for classification tasks typically involves single-token signals, enhancing model performance necessitates the provision of multi-token feedback, which is inherently more complex to curate due to its diversity and volume requirements.\n90. Despite these challenges, our experiments demonstrate that distillation techniques that have been used to enhance the dialogue capabilities of different models[46,43]can be used for the task of patient matching as well.\n91. This method significantly reduces the necessity for manually crafted examples, thereby streamlining the fine-tuning process and making it more affordable.\n\n请严格按照JSON格式返回：{"labels": [0,1,0,...]}，不要包含任何其他文字。'}], 'model': 'qwen-plus', 'response_format': {'type': 'json_object'}}}
2025-10-14 19:54:24,529 - DEBUG - Sending HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
2025-10-14 19:54:24,529 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-10-14 19:54:24,529 - DEBUG - send_request_headers.complete
2025-10-14 19:54:24,529 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-10-14 19:54:24,529 - DEBUG - send_request_body.complete
2025-10-14 19:54:24,529 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-10-14 19:54:29,120 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'vary', b'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding'), (b'x-request-id', b'b4a2724b-684c-41a7-b118-9b0e3047b1d7'), (b'x-dashscope-call-gateway', b'true'), (b'content-type', b'application/json'), (b'req-cost-time', b'4555'), (b'req-arrive-time', b'1760442855697'), (b'resp-start-time', b'1760442860252'), (b'x-envoy-upstream-service-time', b'4521'), (b'content-encoding', b'gzip'), (b'date', b'Tue, 14 Oct 2025 11:54:20 GMT'), (b'server', b'istio-envoy'), (b'transfer-encoding', b'chunked')])
2025-10-14 19:54:29,120 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-14 19:54:29,120 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-10-14 19:54:29,120 - DEBUG - receive_response_body.complete
2025-10-14 19:54:29,120 - DEBUG - response_closed.started
2025-10-14 19:54:29,120 - DEBUG - response_closed.complete
2025-10-14 19:54:29,120 - DEBUG - HTTP Response: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "200 OK" Headers({'vary': 'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding', 'x-request-id': 'b4a2724b-684c-41a7-b118-9b0e3047b1d7', 'x-dashscope-call-gateway': 'true', 'content-type': 'application/json', 'req-cost-time': '4555', 'req-arrive-time': '1760442855697', 'resp-start-time': '1760442860252', 'x-envoy-upstream-service-time': '4521', 'content-encoding': 'gzip', 'date': 'Tue, 14 Oct 2025 11:54:20 GMT', 'server': 'istio-envoy', 'transfer-encoding': 'chunked'})
2025-10-14 19:54:29,120 - DEBUG - request_id: b4a2724b-684c-41a7-b118-9b0e3047b1d7
2025-10-14 19:54:29,121 - DEBUG - API request completed in 4.59 seconds
2025-10-14 19:54:29,121 - DEBUG - Raw model response: {"labels": [0,0,0,0,1,1,1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}
2025-10-14 19:54:29,121 - INFO - Successfully processed 85 labels
2025-10-14 19:54:29,121 - ERROR - Label count mismatch for Distilling Large Language Models for Matching Patients to Clinical Trials
2025-10-14 19:54:29,121 - INFO - Evaluating paper 12/18: ECMD_ An Event_Centric Multisensory Driving Dataset for SLAM
2025-10-14 19:54:29,121 - INFO - Starting model prediction
2025-10-14 19:54:29,121 - INFO - Attempt 1 of 5
2025-10-14 19:54:29,121 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-6953929b-4e32-47f1-8c6a-16360deddac3', 'json_data': {'messages': [{'role': 'system', 'content': '你是一个专门用于识别学术论文中数据集描述的助手。你需要判断每个输入的句子是否描述了研究中使用的数据集。请确保：\n    1. 只输出JSON格式的结果\n    2. 结果格式必须为 {"labels": [0,1,0,...]}\n    3. 不要输出任何其他文字或解释'}, {'role': 'user', 'content': '你是一位专业的学术论文数据集描述识别专家。请仔细分析下面这篇论文中的每个句子，判断它们是否描述了本研究特定构建的数据集信息。\n\n    论文标题: ECMD_ An Event_Centric Multisensory Driving Dataset for SLAM\n\n    判断标准:\n    判断为1(数据集描述)的标准：\n    句子必须直接描述本研究特定构建的数据集,包括:\n    1. 该数据集的构建过程和方法\n    2. 该数据集的具体构成和规模\n    3. 该数据的来源和收集方式\n    4. 该数据的预处理步骤\n    5. 该数据集的可获取方式(如发布地址)\n    6. 或者其他直接或者间接描述了该数据集的句子\n\n    所有其他类型的句子均标记为0。\n    \n    请对每个句子进行分析，返回一个JSON格式的标签数组。格式要求：\n    1. 必须是有效的JSON格式\n    2. 只包含labels字段，值为0和1组成的数组\n    3. 数组长度必须与句子数量相同\n    4. 示例格式：{"labels": [0,1,0,1,...]}\n\n    需要判断的句子：\n\n    1. Abstract: Leveraging multiple sensors enhances complex environmental perception and increases resilience to varying luminance conditions and high-speed motion patterns, achieving precise localization and mapping.\n2. This paper proposes, ECMD, an event-centric multisensory dataset containing 81 sequences and covering over 200 km of various challenging driving scenarios including high-speed motion, repetitive scenarios, dynamic objects, etc. ECMD provides data from two sets of stereo event cameras with different resolutions (640×480, 346×260), stereo industrial cameras, an infrared camera, a top-installed mechanical LiDAR with two slanted LiDARs, two consumer-level GNSS receivers, and an onboard IMU.\n3. Meanwhile, the ground-truth of the vehicle was obtained using a centimeter-level high-accuracy GNSS-RTK/INS navigation system.\n4. All sensors are well-calibrated and temporally synchronized at the hardware level, with recording data simultaneously.\n5. We additionally evaluate several state-of-the-art SLAM algorithms for benchmarking visual and LiDAR SLAM and identifying their limitations.\n6. The dataset is available at https://arclab-hku.github.io/ecmd/.\n7. V ISUAL and LiDAR simultaneous localization and map- ping (SLAM) achieved notable progress within driving scenarios in recent years.\n8. However, they encounter the challenging task of operating robustly under heterogeneous environments, such as varying lighting conditions, lowtexture scenarios, repetitive structures, diverse motion patterns, dense dynamic objects, etc. Utilizing novel sensors and integrating multiple sensors can provide a comprehensive perception and enhance the robustness of the entire system[1]-[3].\n9. These motivate us to develop a dataset that integrates novel sensors under realistic and complex driving scenarios, thereby promoting SLAM research.\n10. Event cameras have low latency (µs-level) and high dynamic range (140 dB compared to 60 dB with standard cameras) properties, which offers great opportunities for visual (VO) and visual-inertial odometry (VIO) in rough terrain, aggressive motions, and high dynamic range (HDR)[4].\n11. Unlike traditional frame-based cameras that directly capture fixed-rate intensity frames, event cameras are motionactivated sensors that capture pixel-wise intensity differences asynchronously in continuous streams.\n12. However, the widespread commercialization and implementation of event cameras in robotics are still early due to the expensive cost.\n13. In addition, event cameras confront challenges during rapid vibrations and ego-motion, as these conditions generate a substantial quantity of events, leading to intensive computations.\n14. Conversely, in cases where minimal relative motion between the event camera and the scene exists, such as under static states, they only provide limited information or even introduce noise[5].\n15. Therefore, we embark on this research effort to explore the inquiry: Are event cameras ready for autonomous driving?\n16. There exist several stereo event-based driving datasets that are worth mentioning and exploring.\n17. MVSEC[6]was the first stereo event-based driving dataset proposed for evaluating the localization performance.\n18. While MVSEC employs the low resolution of DAVIS346 which limits the feature detection for accurate localization.\n19. DSEC[7]offers stereo event streams with a high resolution of 0.31 Megapixels(MP).\n20. However, this dataset focuses on computer vision tasks segmentation, depth estimation, optical flow estimation, etc., which is not specifically designed for VO/VIO/SLAM domains.\n21. MA-VIED[8]propose a largescale driving dataset under standard urban scenarios and race track-like loops.\n22. The ground-truth trajectory relies on GNSS-RTK, which only ensures high accuracy in open-sky environments and fails to provide high accuracy in GNSSdenied scenarios such as tunnels or densely street areas.\n23. [9]focuses on collecting both stereo event data and stereo intensity images under indoor and urban driving scenes with the ground-truth of GNSS-RTK/INS.\n24. Their sequences do not encompass extremely high-speed or repetitive scenarios that could be challenging to VO/VIO/SLAM algorithms.\n25. To address the above drawbacks, we propose ECMD, a dataset procured from diverse challenging driving scenarios with a comprehensive suite of sensors for benchmarking various VO/VIO/SLAM algorithms.\n26. To the best of our knowledge, this is the first event-based SLAM dataset specifically focused on densely urbanized driving scenarios.\n27. The contributions of our work can be summarized as follows: 1) Our sensor platform consists of various novel sensors shown in Fig.1, including two sets of stereo event cameras with distinct resolutions (640×480, 346×260), an infrared camera, stereo industrial cameras, three mechanical LiDARs (including two slanted LiDARs), a high-quality inertial measurement unit (IMU), and three global navigation satellite system (GNSS) receivers.\n28. 2) ECMD collects 81 sequences covering over 200 kilometers of trajectories in various driving scenarios, including dense streets, urban, tunnels, highways, bridges, and suburbs.\n29. These sequences are recorded under daylight and nighttime, providing challenging situations for Visual and LiDAR SLAM, e.g., dynamic objects, highspeed motion, repetitive scenarios, and HDR scenes.\n30. Meanwhile, we evaluate existing state-of-the-art visual and LiDAR SLAM algorithms with various sensor modalities on our datasets.\n31. Moreover, our dataset and benchmark results are released publicly available on our website.\n32. The remainder of the paper is organized as follows: Section II introduces the related works.\n33. Section III presents the sensor setup and sensor calibration.\n34. Section IV intro-duces the dataset overview.\n35. Section V demonstrates the dataset application.\n36. Section VI introduces known issues.\n37. The conclusion is given in Section VII.II.RELATED WORKS Currently, several event-based datasets combined with various sensors have been released for VO/VIO/SLAM domains, utilizing handheld devices or a variety of robotics platforms.\n38. DAVIS240C[10], TUM-VIE[11], VECtor[12], and HKU-dataset1were collected by handheld / headmounted devices under indoor environments.\n39. M2DGR[13]utilizes ground robots to collect a multi-sensor dataset with an event camera under large-scale scenes, while the event streams exhibit large noises.\n40. FusionPortable[14]proposes multi-sensor campus-scene datasets with stereo event cameras on diverse platforms (handheld, quadruped robot, and UGV).\n41. Moreover, there exist specialized event-based datasets such as UZH-FPV[15]and GRIFFIN[16], which are targeted for flying robots.\n42. Moreover, a number of event-based datasets are published under large-scale driving scenarios for computer vision.\n43. These autopilot datasets offer more realistic and challenging conditions, including high-speed scenarios, repetitive situations, and HDR scenes compared to datasets collected from handheld devices.\n44. The first dataset catering to driving recordings using an event camera is DDD17[17], as well as the follow-up DDD20[18], for studying the end-to-end driving application incorporating diverse vehicle control data.\n45. [22]published their event-based datasets for the computer vision task of object classification, image reconstruction, and vision place recognition in driving scenarios.\n46. MVSEC[6]is a pioneering cross-modal dataset with stereo event and image cameras, as well as LiDAR.\n47. However, a limitation of MVSEC resides in the utilization of low-resolution event cameras (346×260) with a compact baseline of 10 cm, coupled with the imprecision of the ground-truth derived from GNSS or LiDAR-SLAM.\n48. DSEC[7]proposed an event-based dataset whose scenarios are similar to KITTI[23], providing higher resolution stereo event (640×480) and image, LiDAR, and IMU under various illumination conditions.\n49. M3ED[24]encompasses high-resolution event cameras (1280×720) and covers three different robotics platforms: driving, flight, and legged robot.\n50. However, both DSEC and M3ED datasets are primarily utilized for computer vision fields, such as optical flow estimation, segmentation, and disparity estimation, rather than specifically for localization or mapping problems.\n51. Besides, they do not provide sufficient challenges for SLAM, as the majority of these datasets were collected in rural or suburban areas with relatively low-lying structures, light traffic, and less dynamic objects.\n52. ViViD++[25]focuses on diverse vision sensors for handheld and driving platforms, including event, thermal, and standard cameras.\n53. MA-VIED[8]proposes a comprehensive driving dataset that encompasses race track-like loops, maneuvers, and standard driving scenarios.\n54. However, both of these datasets exclusively offer monocular data for each camera type, thereby precluding the possibility of conducting stereo visual SLAM.\n55. [9]introduces a stereo visual localization dataset that exploits both the high-resolution event and standard cameras under indoor and urban scenarios.\n56. TableI. summarizes the differences between our ECMD and other event-based datasets under autonomous driving scenarios.\n57. Compared to other datasets, our ECMD offers several advantages: (i) Capture diverse visual data format (RGB image, event stream, and infrared image) from multiple types of vision sensors in varying luminance conditions and urbanized scenarios; (ii) 1kHz-rate event streams from different resolution event cameras empower in-depth exploration of event-based perception; (iii) Based on our previous work[26], three LiDARs, including two slanted LiDARs, are employed to collect high-rising building structures for LiDAR point cloud maps generation; (iv) We employ a centimeter-level localization system, GNSS-RTK/INS, as ground-truth, enabling a comprehensive evaluation of various SLAM algorithms.\n58. The data collection platform is shown in Fig.1.\n59. Our sensor suite consists of a multi-camera setup (event camera, industrial camera, and infrared camera) equipped with three LiDARs, high-quality IMU, three GNSS receivers, and GNSS-RTK/INS systems.\n60. The specific specifications of each sensor are presented in TableII.\n61. An Intel NUC (i7-1260P, 32GB RAM) and an industrial computer (i7-10610U, 32GB RAM) are used to run sensor drivers, and record data into ROS bags on the Ubuntu system.\n62. 1) Visual Sensors: Two sets of stereo event cameras with different resolutions, DAVIS436 (346×260) and DVXplorer (640×480), are configured at a baseline of 30 cm respectively.\n63. DAVIS346 produces asynchronous events and intensity frames.\n64. In contrast, DVXplorer exclusively generates events, while its resolution surpasses that of DAVIS346, enabling the provision of more intricate scene information.\n65. Each event camera is equipped with additional infrared filters to mitigate interference from LiDAR.\n66. Two FLIR BFLY-U3-23S3C industrial cameras with a resolution of 1920×1200 are used to capture RGB images at 20 Hz in fixed exposure mode.\n67. Forward-facing stereo industrial cameras are installed with a baseline of 30 cm, ensuring fairness by maintaining consistency with the baseline of the stereo event cameras.\n68. Hikrobot MV-CI003-GL-N6 infrared camera collects thermal frames at 20 Hz, encompassing a response band of 8-14µm and equipped with a 6.3mm focal length lens.\n69. 2) Mechanical LiDAR: We configure three mechanical LiDARs including two slanted LiDARs to collect accurate point clouds of surrounding environments.\n70. Velodyne HDL-32E is positioned on the top of the vehicle to capture the surroundings horizontally.\n71. Two slanted LiDARs, Lslidar C16 and Velodyne VLP-16, are mounted on the left and right sides of the sensor kit, respectively.\n72. This configuration facilitates the thorough recording of architectural particulars relevant to high-rising buildings in urbanized areas and all LiDAR data are collected at 10 Hz.\n73. 3) GNSS-RTK/INS Sensor: A tactical-level Xsens-MTI-30 IMU is employed to collect the raw acceleration and angular velocity at 400 Hz.\n74. The accurate ground-truth of localization is furnished by a centimeter-level GNSS-RTK/INS navigation system, further details can be found in Section IV-B1.\n75. We use a Precision Time Protocol (PTP)[29]device to synchronize the clocks of various data collection devices across the sensor network.\n76. The PTP ensures time accuracy within nanoseconds.\n77. The synchronization device acquires the NMEA[30]output and pulse-per-second (PPS) signal from a u-blox M8T GNSS receiver to align the ROS time of the onboard computers with the GPS time.\n78. This enables sensors such as cameras, LiDAR, and IMU to record timestamps based on the synchronized GPS time.\n79. Moreover, to achieve time synchronization between different event cameras, the DAVIS346 on the rightmost side is configured as the master device and transmits trigger signal pulses to the remaining slave event cameras sequentially from left to right via external cables.\n80. To calibrate the IMU, we position it on a level surface for a duration of three hours and record the raw measurements.\n81. Utilizing the Kalibr toolbox, we can accurately calibrate the random walk and Gaussian white noise of the IMU.\n82. 2) Industrial Cameras Calibration: For industrial cameras, we move the sensor platform against the 9×7 checkerboard in the XYZ-axis and collect the sequence of RGB images and IMU.\n83. Then intrinsics calibration of industrial cameras is achieved by Kalibr toolbox[31], where the pinhole and radial-tangential camera models are adopted.\n84. 3) Event Cameras Calibration: For event cameras, DAVIS346 can produce fixed-rated frames, enabling imagebased calibration, while DVXplorer merely produces asynchronous event streams.\n85. Therefore, E2Calib[32][33] is used to achieve image reconstruction from event streams.\n86. 4) Infrared Camera Calibration: Due to infrared cameras solely capturing the temperature rather than the intensity difference, we design a distinct 9×7 checkerboard to make the pattern detectable for infrared cameras.\n87. As shown in Fig.3(a), the checkerboard intervals are affixed with aluminum materials, and then using a heating plate to raise the temperature of the checkerboard.\n88. Since the superior thermal dissipation of aluminum compared to plastic, a temperature contrast emerges between the two materials, enabling infrared cameras to distinctly capture the lattice shape of the checkerboard, as in Fig.3(b).\n89. With the special infrared image of the checkerboard, intrinsic can be calibrated by Kalibr.\n90. After completing intrinsics calibration, we move the sensor suite in front of checkerboards along the XYZ-RPY-axis and collect data simultaneously.\n91. Subsequently, the extrinsics and the temporal offset between all cameras and IMU could be estimated using Kalibr.\n92. For the calibration of mechanical LiDAR, LI-Init[34]is capable of achieving temporal and spatial calibration for LiDAR and IMU without checkerboards or extra devices in Fig.4.\n93. We rotate and move the device around the XYZ-axis to ensure sufficient excitation until the data accumulation is completed, thus we acquire the extrinsic transformation between LiDAR and IMU.\n94. Our dataset encompasses a wide range of driving scenes, including urban streets, urban roads, tunnels, highways, bridges, and suburban roads.\n95. We have specifically focused on scenarios where visual SLAM algorithms encounter difficulties.\n96. These scenarios involve high-speed motion (up to 110 km/h), limited texture, as well as difficult glare conditions in both daytime and nighttime driving.\n97. We also targeted situations where LiDAR SLAM encounters limitations, such as long corridors or areas with sparse geometric structures.\n98. The complete dataset is partitioned into 81 sequences to facilitate researchers in evaluating their algorithms.\n99. Each sequence has an approximate duration of 120 seconds.\n100. Additionally, we have retained a few sequences with long duration, lasting approximately 34 minutes, specifically for the evaluation of loop closure in large-scale environments and loop closure scenarios.\n101. The summary of sequence types can be found in TableIII. A. Scenarios 1) Dense Urban Street: This scenario focuses on lowspeed vehicles, around 30km/h, proceeding on highly urbanized areas and urban canyons in Hong Kong with multiple light conditions.\n102. The streets are narrow at 10m in width and buildings on both sides of the scene are dense.\n103. Meanwhile, the presence of congested traffic and dynamic crowds may produce the degradation of visual or LiDAR localization, such as Dense street day easy b.\n104. To evaluate the loop closure performance of SLAM, we remarkably recorded sequences Dense street difficult circle and Dense street difficult loop where our vehicle was circling in repeated routes.\n105. 2) Urban Road: This type of scenario records the vehicle traveling at an approximate speed of 60km/h on an expressway in Hong Kong with multiple weather conditions.\n106. Compared to the Dense Urban Street scenario, Urban Road sequences travel through Hong Kong city at a higher speed, while the buildings are not as tightly packed on either side and the road is more spacious with four lanes.\n107. Despite the absence of pedestrians on the road, the scene still includes vehicles overtaking, paralleling, and other situations where the relative motion is not consistent with the absolute motion.\n108. The aforementioned discrepancy might pose a challenge for the VIO or LIO system.\n109. Moreover, the sequence comprises the vehicle traveling during nighttime in rainy conditions.\n110. We record trajectories in rainy situations under nighttime like Urban road night difficult rainy a which are commonly faced in practical driving scenarios, whereas they are not present in previous datasets.\n111. 3) Tunnel: Tunnel scenarios commence with a high-speed vehicle on an open-sky highway, entering an enclosed tunnel without satellite reception.\n112. Inside the tunnel, GNSS positioning is unreliable since the satellite signal is completely blocked.\n113. Meanwhile, the scenario represents a typical and challenging scene for VIO and LIO systems due to the repetitive and texture-less environments for vision sensors and LiDAR.\n114. The sequence collections end after the vehicle exits the tunnel and continues to proceed on the highway for twenty seconds.\n115. 4) Highway: The scenario involves vehicles traveling at speeds up to 100km/h on low-texture highways both during the day and night, with sparse buildings alongside the road.\n116. High speeds, rapid changes in vehicle speed, repetitive visual scenes, and low-texture environments present significant challenges for autonomous driving.\n117. Meanwhile, the vibration of the vehicle body at high-speed motion amplifies the random walk and Gaussian white noise of IMU, thereby diminishing its reliability.\n118. 5) Bridge: The motion pattern of vehicles in bridge scenarios resembles that of highways, with vehicles traveling in a straight line at high speed along the bridge.\n119. However, this scene differs as there are no buildings on either side of the bridge, only the sea surrounds it.\n120. Bridges present scenes with limited texture, and the feature information within these scenes tends to be monotonous and repetitive, which further exacerbates the challenge of achieving accurate localization.\n121. 6) Suburban Road: Suburban road scenarios present complex natural environments characterized by winding and rugged roads, steep slopes, and narrow lanes.\n122. The vehicle navigates the serpentine mountain roads at a moderate speed (approximately 50km/h), with significant altitude changes.\n123. The abundant texture information in the mountain road scene facilitates visual algorithms to extract stable features and construct effective constraints.\n124. 1) Ground-truth Poses: We obtained the ground-truth positioning from the NovAtel SPAN-CPT[28], a highperformance GNSS RTK/INS integrated navigation system.\n125. The ground-truth of most existing event-based driving datasets are derived from LiDAR-SLAM[6][24], GPS/GNSS[6], GNSS-RTK[7][25][24].\n126. The ground-truth derived from LiDAR-SLAM relies on the estimation of vehicle trajectories using LiDAR SLAM which only provides relative trajectories.\n127. It is difficult to quantify the accuracy of ground-truth pose, and errors may even exceed ten meters in some cases.\n128. The complex environment or the equipment malfunctions may disrupt the satellite reception of GPS/GNSS, thus relying solely on GPS/GNSS for ground-truth pose may lead to significant drift.\n129. The GNSS-RTK device can only provide centimeter-level accuracy in the open sky[26]In contrast, our SPAN-CPT can provide continuous high accuracy aided by the internal fiber-optic gyroscopes under high-rise buildings, tunnels, and other environments with weak satellite signals.\n130. Furthermore, we post-process the ground-truth positioning from SPAN-CPT using the state-ofthe-art NovAtel Inertial Explorer[28]software to maximize the accuracy of the trajectory.\n131. For the GNSS positioning benchmark, we provide the WGS84 coordinate data for comparison.\n132. For the evaluation of SLAM algorithms, we provide the tools2to transform the ground-truth data from the WGS84 coordinates to the local frame/ENU frame based on the original points.\n133. 2) LiDAR Point Cloud Maps Generation: Utilizing the ground-truth pose for each frame in conjunction with their corresponding LiDAR point clouds, we accumulate these point clouds to construct a highly accurate LiDAR point cloud map to depict the TsingMa Bridge in Fig.6.\n134. The map encompasses rich spatial information, providing a detailed 3D reconstruction of the bridge and its surrounding areas.\n135. As shown in TableIV., we evaluate the performance of VINS-MONO[35], ORB-SLAM3[36], and ESVIO[37]Fig.6.\n136. The vehicle poses ground-truth on Google map with the LiDAR point cloud maps of Tsing Ma bridge.across various scenes and lighting conditions on our dataset.\n137. The accuracy is quantified using mean position error (MPE, %), which aligns the estimated trajectory with ground-truth through 6-DOF transformation (in SE3) computed by the tool[38].\n138. For the VINS-Mono, we evaluate it separately using RGB images and infrared images.\n139. Due to the resolution provided by industrial cameras being higher in contrast to the infrared camera, we achieve superior performance when utilizing RGB images.\n140. The ORB-SLAM3 often fails to robustly track features during high-speed vehicle movements, potentially resulting in the tracking thread restarts.\n141. The ESVIO leverages the complementary advantages of event streams and RGB images, allowing it to handle the lack of texture in RGB images under broad illumination conditions to achieve higher accuracy.\n142. Fig.7compares event, RGB images, and infrared images under different lighting conditions.\n143. The RGB images offer rich texture under regular luminance scenes in contrast to events and infrared images offer comparatively limited information, e.g., the infrared image struggles to accurately discern traffic left-turn symbol on the ground.\n144. Conversely, the RGB image may lose numerous environmental features under the conditions of low light or over-exposure.\n145. The infrared camera can capture infrared radiation beyond the visible spectrum and event cameras can detect pixel-level intensity changes at low latency.\n146. Both the event camera and the infrared camera are more resilient in external varying lighting conditions, providing effective visibility compared to the industrial camera, e.g., in nighttime scenes, event cameras can capture road signs, and the infrared camera can clearly capture the surrounding bushes.\n147. TableV. demonstrates the performance of LIO-SAM[39], LVI-SAM[40], Fast-LIO2[41], Point-LIO[42]across various scenes on our dataset.\n148. We use the same criteria introduced in Section V-A to evaluate the localization accuracy.\n149. Due to the tilt-mounted LiDAR setups (see Fig.1), we are able to acquire point clouds of towering buildings situated on both sides of the street.\n150. This installation approach compensated for the lack of vertical point clouds compared to the horizontally mounted LiDAR.\n151. In Fig.8, red point clouds are generated from a horizontally mounted LiDAR while white and green point clouds are generated from tilt-mounted LiDARs.\n152. We evaluate the performance of LOAM[43]using three different LiDARs (center, left, and right)in Dense street day medium circle a sequences.\n153. The MPE of LOAM using center LiDAR is 1.02%, compared to 8.67% using the left LiDAR and 2.00% using the right LiDAR.\n154. LOAM using the left LiDAR exhibits significant drift since it initially captures minimal point cloud information.\n155. Although the LOAM merely using tilt-mounted LiDAR produces less accurate results compared to the center LiDAR, multi-LiDAR fusion can integrate complementary information, thereby improving localization accuracy and constructing more precise point cloud maps.\n156. Meanwhile, tunnel scenes present challenges for LiDAR SLAM.\n157. We capture three consecutive frames of LiDAR point clouds at two-second intervals in Fig.9.\n158. It is evident that these LiDAR point clouds exhibit high similarity in the tunnel environment, potentially resulting in degradation phenomena and inaccurate state estimation.\n159. Due to space limitations, we positioned LiDAR closer to the event cameras.\n160. As a consequence, the infrared wavelengths emitted by LiDAR directly impinge on the photoreceptor of event cameras, resulting in continuous disturbances and flickering in the captured images and event streams.\n161. To address this issue, we implement infrared filters on event cameras to counteract the effect.\n162. However, this intervention led to a compromise, resulting in a degradation of the quality of the recorded event data.\n163. During the night or low illumination scenarios, we observed that when event cameras were directly toward a glowing light source, such as street lights or store lighting, event streams would exhibit persistent flickering and produce artifacts around the light source.\n164. This could potentially lead to a distorted view of the observed object.\n165. We postulate this phenomenon is related to the inherent principle of event cameras, and presently, there is no known solution to address this issue.\n166. In this paper, we propose an event-centric autonomous driving dataset generated with multiple sensors across various scenarios for developing SLAM algorithms.\n167. All sensors undergo meticulous calibration and are temporally synchronized at the hardware level.\n168. We employ the GNSS-RTK/INS navigation system, which provides centimeter-level accuracy, to acquire precise ground-truth of the vehicle.\n169. Furthermore, we conduct the evaluation of various state-of-the-art visual and LiDAR SLAM algorithms while identifying their constraints.\n170. We hope this dataset could contribute to the development of visual and LiDAR SLAM.\n\n请严格按照JSON格式返回：{"labels": [0,1,0,...]}，不要包含任何其他文字。'}], 'model': 'qwen-plus', 'response_format': {'type': 'json_object'}}}
2025-10-14 19:54:29,122 - DEBUG - Sending HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
2025-10-14 19:54:29,122 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-10-14 19:54:29,122 - DEBUG - send_request_headers.complete
2025-10-14 19:54:29,122 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-10-14 19:54:29,122 - DEBUG - send_request_body.complete
2025-10-14 19:54:29,122 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-10-14 19:54:34,563 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'vary', b'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding'), (b'x-request-id', b'5762c47f-41fa-4357-a90c-3135bce326e5'), (b'x-dashscope-call-gateway', b'true'), (b'content-type', b'application/json'), (b'req-cost-time', b'5405'), (b'req-arrive-time', b'1760442860287'), (b'resp-start-time', b'1760442865693'), (b'x-envoy-upstream-service-time', b'5364'), (b'content-encoding', b'gzip'), (b'date', b'Tue, 14 Oct 2025 11:54:25 GMT'), (b'server', b'istio-envoy'), (b'transfer-encoding', b'chunked')])
2025-10-14 19:54:34,563 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-14 19:54:34,563 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-10-14 19:54:34,563 - DEBUG - receive_response_body.complete
2025-10-14 19:54:34,563 - DEBUG - response_closed.started
2025-10-14 19:54:34,563 - DEBUG - response_closed.complete
2025-10-14 19:54:34,563 - DEBUG - HTTP Response: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "200 OK" Headers({'vary': 'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding', 'x-request-id': '5762c47f-41fa-4357-a90c-3135bce326e5', 'x-dashscope-call-gateway': 'true', 'content-type': 'application/json', 'req-cost-time': '5405', 'req-arrive-time': '1760442860287', 'resp-start-time': '1760442865693', 'x-envoy-upstream-service-time': '5364', 'content-encoding': 'gzip', 'date': 'Tue, 14 Oct 2025 11:54:25 GMT', 'server': 'istio-envoy', 'transfer-encoding': 'chunked'})
2025-10-14 19:54:34,563 - DEBUG - request_id: 5762c47f-41fa-4357-a90c-3135bce326e5
2025-10-14 19:54:34,564 - DEBUG - API request completed in 5.44 seconds
2025-10-14 19:54:34,564 - DEBUG - Raw model response: {"labels": [0,1,1,1,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,1,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]}
2025-10-14 19:54:34,564 - INFO - Successfully processed 125 labels
2025-10-14 19:54:34,564 - ERROR - Label count mismatch for ECMD_ An Event_Centric Multisensory Driving Dataset for SLAM
2025-10-14 19:54:34,564 - INFO - Evaluating paper 13/18: Inline Citation Classification using Peripheral Context and Time_evolving Augmentation
2025-10-14 19:54:34,564 - INFO - Starting model prediction
2025-10-14 19:54:34,564 - INFO - Attempt 1 of 5
2025-10-14 19:54:34,565 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-dc30958d-037f-415f-a3c9-a43aed551a2c', 'json_data': {'messages': [{'role': 'system', 'content': '你是一个专门用于识别学术论文中数据集描述的助手。你需要判断每个输入的句子是否描述了研究中使用的数据集。请确保：\n    1. 只输出JSON格式的结果\n    2. 结果格式必须为 {"labels": [0,1,0,...]}\n    3. 不要输出任何其他文字或解释'}, {'role': 'user', 'content': '你是一位专业的学术论文数据集描述识别专家。请仔细分析下面这篇论文中的每个句子，判断它们是否描述了本研究特定构建的数据集信息。\n\n    论文标题: Inline Citation Classification using Peripheral Context and Time_evolving Augmentation\n\n    判断标准:\n    判断为1(数据集描述)的标准：\n    句子必须直接描述本研究特定构建的数据集,包括:\n    1. 该数据集的构建过程和方法\n    2. 该数据集的具体构成和规模\n    3. 该数据的来源和收集方式\n    4. 该数据的预处理步骤\n    5. 该数据集的可获取方式(如发布地址)\n    6. 或者其他直接或者间接描述了该数据集的句子\n\n    所有其他类型的句子均标记为0。\n    \n    请对每个句子进行分析，返回一个JSON格式的标签数组。格式要求：\n    1. 必须是有效的JSON格式\n    2. 只包含labels字段，值为0和1组成的数组\n    3. 数组长度必须与句子数量相同\n    4. 示例格式：{"labels": [0,1,0,1,...]}\n\n    需要判断的句子：\n\n    1. Abstract: Citation plays a pivotal role in determining the associations among research articles.\n2. It portrays essential information in indicative, supportive, or contrastive studies.\n3. The task of inline citation classification aids in extrapolating these relationships; However, existing studies are still immature and demand further scrutiny.\n4. Current datasets and methods used for inline citation classification only use citation-marked sentences constraining the model to turn a blind eye to domain knowledge and neighboring contextual sentences.\n5. In this paper, we propose a new dataset, named 3Cext, which along with the cited sentences, provides discourse information using the vicinal sentences to analyze the contrasting and entailing relationships as well as domain information.\n6. We propose PeriCite, a Transformer-based deep neural network that fuses peripheral sentences and domain knowledge.\n7. Our model achieves the state-of-the-art on the 3Cext dataset by +0.09 F1 against the best baseline.\n8. We conduct extensive ablations to analyze the efficacy of the proposed dataset and model fusion methods.\n9. For the past several decades, there has been an interest in citation analysis for research evaluation.\n10. Researchers have emphasized the necessity for new methodologies that take into account various components of citing sentences.\n11. A wellknown qualitative technique for assessing the scientific influence is to analyze the sentence in which the research article is mentioned to ascertain the purpose behind the citation.\n12. The context of the citation, or the text in which the cited document is mentioned, has proven to be an effective indicator of the citation\'s intent[25].\n13. Measuring the scientific impact of research articles requires a fundamental understanding of citation intent.\n14. A great way to gauge the significance of a scientific publication is to determine why citations are made in one\'s work and how significant they are.\n15. Previous methods for citation context categorization used a range of annotation techniques with low-to-high granularity.\n16. Comparing the earlier systems is extremely difficult due to the absence of standardized methodologies and annotation schemes.\n17. The 3C shared task[12,13]used a piece of the Academic Citation Typing (ACT) dataset to categorize the reference anchor into \'function\' or \'purpose\' by looking at the citing sentence or the text that contains the citation[19].\n18. Only quantitative elements are considered in traditional citation analysis based solely on the citation count.\n19. One of the biggest obstacles to citation context analysis for citation identification is that there is no multidisciplinary dataset and that there isn\'t any medium to fine-grained schemes that adequately represent the function and its influence[8].\n20. To address this challenge, Kunnath et al.[12]provided a unified task, called the 3C Shared Task, to compare several citation classification approaches on the same dataset to address the shortcomings of citation context categorization.\n21. The main distinction in the second iteration of this task[13]was that the subtasks contained full-text datasets.\n22. However, even with the full text, the metadata associated with the citation sentence was not adequate to understand the reasoning for the citation.\n23. To alleviate the above limitations, we propose a new dataset, named 3Cext, and a new model, named PeriCite that combines the advantages of augmentation and peripheral context.\n24. Experiments show that the cited sentences heavily rely on the peripheral context to strengthen an argument by contrasting or entailing information.\n25. Our main contributions are as follows 1.We extend the 3C dataset[13]-3Cext, which, along with the cited sentence, adds more discourse information by providing contrasting and entailing information using the peripheral sentences.\n26. 2. We propose a novel model, PeriCite, which uses spatial fusion and crosstext attention to attend to contextual information for the peripheral sentences and time-evolving augmentation to counter class imbalance during the training time.\n27. 3. We also compare our proposed model against various baselines and show the efficacy of the module along with ablation studies and error analysis.\n28. We also compare our proposed model against various baselines and show the efficacy of the module along with ablation studies and error analysis.\n29. In this section, we discuss our proposed 3Cext dataset in detail.\n30. Kunnath et al.[12]introduced the ACT dataset, with annotations for 11, 233 citations annotated by 883 authors.\n31. The cited label was masked with "#AUTHOR TAG" denoting the position of the cited object.\n32. Additionally, the 3C dataset contained full text and the label denoting the class of a particle citation (c.f.Table2).\n33. In our work, we extend the 3C dataset to house more discourse information to explain better why a citation is present in a sentence.\n34. Our intuition is that the cited sentences mostly either entail or contrast the adjoining sentences.\n35. To capture the peripheral sentences, we extract the full-text files corresponding to the COREIDs (unique paper ID) in our dataset to follow through on this discovery.\n\n请严格按照JSON格式返回：{"labels": [0,1,0,...]}，不要包含任何其他文字。'}], 'model': 'qwen-plus', 'response_format': {'type': 'json_object'}}}
2025-10-14 19:54:34,565 - DEBUG - Sending HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
2025-10-14 19:54:34,565 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-10-14 19:54:34,565 - DEBUG - send_request_headers.complete
2025-10-14 19:54:34,565 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-10-14 19:54:34,565 - DEBUG - send_request_body.complete
2025-10-14 19:54:34,565 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-10-14 19:54:37,452 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'vary', b'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding'), (b'x-request-id', b'9704f50c-086c-4f12-83e3-2a9b0113ef54'), (b'x-dashscope-call-gateway', b'true'), (b'content-type', b'application/json'), (b'req-cost-time', b'2852'), (b'req-arrive-time', b'1760442865727'), (b'resp-start-time', b'1760442868580'), (b'x-envoy-upstream-service-time', b'2851'), (b'content-encoding', b'gzip'), (b'date', b'Tue, 14 Oct 2025 11:54:28 GMT'), (b'server', b'istio-envoy'), (b'transfer-encoding', b'chunked')])
2025-10-14 19:54:37,452 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-14 19:54:37,452 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-10-14 19:54:37,452 - DEBUG - receive_response_body.complete
2025-10-14 19:54:37,452 - DEBUG - response_closed.started
2025-10-14 19:54:37,452 - DEBUG - response_closed.complete
2025-10-14 19:54:37,452 - DEBUG - HTTP Response: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "200 OK" Headers({'vary': 'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding', 'x-request-id': '9704f50c-086c-4f12-83e3-2a9b0113ef54', 'x-dashscope-call-gateway': 'true', 'content-type': 'application/json', 'req-cost-time': '2852', 'req-arrive-time': '1760442865727', 'resp-start-time': '1760442868580', 'x-envoy-upstream-service-time': '2851', 'content-encoding': 'gzip', 'date': 'Tue, 14 Oct 2025 11:54:28 GMT', 'server': 'istio-envoy', 'transfer-encoding': 'chunked'})
2025-10-14 19:54:37,452 - DEBUG - request_id: 9704f50c-086c-4f12-83e3-2a9b0113ef54
2025-10-14 19:54:37,453 - DEBUG - API request completed in 2.89 seconds
2025-10-14 19:54:37,453 - DEBUG - Raw model response: {"labels": [0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,1,0,0,0,1,0,1]}
2025-10-14 19:54:37,453 - INFO - Successfully processed 35 labels
2025-10-14 19:54:37,457 - ERROR - Error processing paper Inline Citation Classification using Peripheral Context and Time_evolving Augmentation: 'int' object has no attribute 'capitalize'
2025-10-14 19:54:37,457 - INFO - Evaluating paper 14/18: llm_japanese_dataset v0_ Construction of Japanese Chat Dataset for Large Language Models and its Methodology
2025-10-14 19:54:37,457 - INFO - Starting model prediction
2025-10-14 19:54:37,457 - INFO - Attempt 1 of 5
2025-10-14 19:54:37,458 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-bae1fef4-abd4-4f0d-972e-d3e2f04ff599', 'json_data': {'messages': [{'role': 'system', 'content': '你是一个专门用于识别学术论文中数据集描述的助手。你需要判断每个输入的句子是否描述了研究中使用的数据集。请确保：\n    1. 只输出JSON格式的结果\n    2. 结果格式必须为 {"labels": [0,1,0,...]}\n    3. 不要输出任何其他文字或解释'}, {'role': 'user', 'content': '你是一位专业的学术论文数据集描述识别专家。请仔细分析下面这篇论文中的每个句子，判断它们是否描述了本研究特定构建的数据集信息。\n\n    论文标题: llm_japanese_dataset v0_ Construction of Japanese Chat Dataset for Large Language Models and its Methodology\n\n    判断标准:\n    判断为1(数据集描述)的标准：\n    句子必须直接描述本研究特定构建的数据集,包括:\n    1. 该数据集的构建过程和方法\n    2. 该数据集的具体构成和规模\n    3. 该数据的来源和收集方式\n    4. 该数据的预处理步骤\n    5. 该数据集的可获取方式(如发布地址)\n    6. 或者其他直接或者间接描述了该数据集的句子\n\n    所有其他类型的句子均标记为0。\n    \n    请对每个句子进行分析，返回一个JSON格式的标签数组。格式要求：\n    1. 必须是有效的JSON格式\n    2. 只包含labels字段，值为0和1组成的数组\n    3. 数组长度必须与句子数量相同\n    4. 示例格式：{"labels": [0,1,0,1,...]}\n\n    需要判断的句子：\n\n    1. Abstract: This study constructed a Japanese chat dataset for tuning large language models (LLMs), which consist of about 8.4 million records.\n2. Recently, LLMs have been developed and gaining popularity.\n3. However, high-performing LLMs are usually mainly for English.\n4. There are two ways to support languages other than English by those LLMs: constructing LLMs from scratch or tuning existing models.\n5. However, in both ways, datasets are necessary parts.\n6. In this study, we focused on supporting Japanese in those LLMs and making a dataset for training or tuning LLMs in Japanese.\n7. The dataset we constructed consisted of various tasks, such as translation and knowledge tasks.\n8. In our experiment, we tuned an existing LLM using our dataset and evaluated the performance qualitatively.\n9. The results suggest that our dataset is possibly beneficial for LLMs.However, we also revealed some difficulties in constructing LLMs in languages other than English.\n10. 2 Dataset Construction: izumi-lab/llm-japanese-dataset v0 In this study, we created a Japanese chat dataset.\n11. The dataset 1 contains 8,393,726 data points.\n12. In the following, we describe the details of datasets and their creation process.\n13. Large language models (LLMs) have recently achieved remarkable progress in performance and generalization.\n14. Specifically, Transformer-based LLMs such as BERT[3]and the GPT series[17,18,1]have demonstrated high-performance thanks to their pre-training.\n15. Furthermore, models that have evolved from these, such as Chat-GPT[14]and GPT4[15], have gained popularity for their remarkable performance.\n16. Other models such asBard [6], LLaMA[24], Dolly[2], Bloom[21], and Vicuna[26]have also emerged.\n17. Masanori HIRANO, Masahiro SUZUKI, and Hiroki SAKAJI The University of Tokyo, 7-3-1 Hongo, Bunkyo, Tokyo 113-8656 Japan, e-mail: research@mhirano.jp,b2019msuzuki@socsim.org,sakaji@sys.t.utokyo.ac.jpSome of those models are already provided to consumers as a web service.\n18. Moreover, via API, those models and services are also now available for sub-parts of web services, and many spin-off services are emerging.\n19. However, despite the prosperity of language models, there are still challenges in handling diverse prompts, including prompts written in languages other than English.\n20. For example, Alpaca[23]dataset has been proposed due to the incompleteness of LLaMA\'s response.\n21. However, the dataset of Alpaca is only available in English, and the incompleteness pointed out by Alpaca has not been filled yet in the other languages.\n22. Moreover, LLaMA has difficultness to respond appropriately to some prompts in languages other than English.\n23. Considering these challenges, it is necessary to enhance models\' performances in languages other than English.\n24. However, it is not a good idea to study a specific model in terms of performance improvements in the other language.\n25. Moreover, model development is still ongoing and very competitive, and the situation is changing dramatically recently.\n26. It is also easy to assume that newer models with better performance will emerge in a few months or even 1-2 months.\n27. Therefore, enhancing datasets that support model training may be more useful than focusing on specific models.\n28. This approach may also lower the barrier to adapting new models to languages other than English.\n29. Therefore, this study constructed a new chat dataset in Japanese for LLM training, which contains approximately 8.4 million data points, and demonstrated the performance of the dataset qualitatively.\n30. The dataset and trained models are opensourced and publicly available.\n31. The details are as follows: • Dataset: https://huggingface.co/datasets/izumi-lab/llm-japanesedataset • Trained Models (LLaMA 1 epoch): https://huggingface.co/izumi-lab/llama-13b-japanese-lora-v0-1ep.\n32. The more details are explained in the following.\n33. Moreover, data expansion and additional model training are planned as future tasks.\n34. The format of the chat data used for model training is shown below.\n35. In the description of the dataset later, we will omit some of the introductory parts and line breaks.\n36. Below is an instruction that describes a task, paired with an input that provides further context.\n37. Write a response that appropriately completes the request.\n38. Note that, in the following examples, the underlined sentences are originally written in Japanese.\n39. We utilized the aforementioned ParaNatCom[25]to create tasks related to our research paper.\n40. The license for the dataset is CC BY 4.0, and the size of the created dataset is 1,732.\n41. ### Instruction: Please make a title from the abstract of the paper.\n42. 1  ### Input: Superthin nanostructures, particularly with atomic-level thicknesses, typically display unique optical properties because of their exceptional light-matter interactions.\n43. ### Instruction: Please rephrase the following Japanese into easy Japanese.\n44. 1 ### Input: Bill has no sense of adventure at all.\n45. 1  ### Response: Bill has no desire to do anything dangerous.\n46. In addition, we incorporated Japanese-translated versions of existing publicly available chat datasets.\n47. The following datasets were included: • Japanese-Alpaca-LoRA17: A translation of the Alpaca[23]dataset into Japanese.\n48. The license is Apache License 2.0.\n49. • databricks-dolly-15k-ja18: A Japanese-translated version of the dataset used for training Dolly[2].\n50. The license is CC BY-SA 3.0.\n51. The dataset size is 15,015.\n52. This study used LoRA[7]as a method to fine-tune LLMs without significant performance degradations.\n53. It is because building LLMs from scratch requires a massive amount of computational resources.\n54. Furthermore, LLMs with a large number of parameters require GPU resources not only for pre-training but also for fine-tuning.\n55. On the other hand, LoRA updates only small parts of LLM parameters.\n56. Therefore, LoRA is a feasible option for us to evaluate the benefits of our dataset.\n57. The main parameters used in the experiment are shown below.\n58. • Base model: LLaMA 13B[24]• Learning rate: 3e-4 We used PEFT[12]and DeepSpeed ZeRO 2[19]for the implementation.\n59. This tuned model is publicly available at https://huggingface.co/izumilab/llama-13b-japanese-lora-v0-1ep.\n60. In order to increase the reproducibility of the evaluation experiment, the temperature parameter for prompt generation was set to 0.0.\n61. Below are some qualitative comparisons we conducted to assess performance.\n62. The phone rings.\n63. When the call is received, the person receiving the call should receive the call.\n64. 1   Response Example(5)### Input: What are the three major festivals in Kyoto?\n65. )### Output(LLaMA): What are the three major festivals in Kyoto?\n66. What are the three major festivals in Kyoto?\n67. What are the three major festivals in Kyoto?\n\n请严格按照JSON格式返回：{"labels": [0,1,0,...]}，不要包含任何其他文字。'}], 'model': 'qwen-plus', 'response_format': {'type': 'json_object'}}}
2025-10-14 19:55:06,326 - DEBUG - Sending HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
2025-10-14 19:55:06,327 - DEBUG - close.started
2025-10-14 19:55:06,327 - DEBUG - close.complete
2025-10-14 19:55:06,327 - DEBUG - connect_tcp.started host='dashscope.aliyuncs.com' port=443 local_address=None timeout=5.0 socket_options=None
2025-10-14 19:55:06,330 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7fb32adfc880>
2025-10-14 19:55:06,330 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x7fb27d3d5640> server_hostname='dashscope.aliyuncs.com' timeout=5.0
2025-10-14 19:55:06,418 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7fb32adfce80>
2025-10-14 19:55:06,418 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-10-14 19:55:06,418 - DEBUG - send_request_headers.complete
2025-10-14 19:55:06,419 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-10-14 19:55:06,419 - DEBUG - send_request_body.complete
2025-10-14 19:55:06,419 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-10-14 19:55:09,991 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'vary', b'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding'), (b'x-request-id', b'8fdae080-d904-442c-8be7-025f34a791f6'), (b'x-dashscope-call-gateway', b'true'), (b'content-type', b'application/json'), (b'req-cost-time', b'3533'), (b'req-arrive-time', b'1760442897567'), (b'resp-start-time', b'1760442901100'), (b'x-envoy-upstream-service-time', b'3531'), (b'content-encoding', b'gzip'), (b'date', b'Tue, 14 Oct 2025 11:55:01 GMT'), (b'server', b'istio-envoy'), (b'transfer-encoding', b'chunked')])
2025-10-14 19:55:09,992 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-14 19:55:09,992 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-10-14 19:55:09,992 - DEBUG - receive_response_body.complete
2025-10-14 19:55:09,992 - DEBUG - response_closed.started
2025-10-14 19:55:09,992 - DEBUG - response_closed.complete
2025-10-14 19:55:09,992 - DEBUG - HTTP Response: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "200 OK" Headers({'vary': 'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding', 'x-request-id': '8fdae080-d904-442c-8be7-025f34a791f6', 'x-dashscope-call-gateway': 'true', 'content-type': 'application/json', 'req-cost-time': '3533', 'req-arrive-time': '1760442897567', 'resp-start-time': '1760442901100', 'x-envoy-upstream-service-time': '3531', 'content-encoding': 'gzip', 'date': 'Tue, 14 Oct 2025 11:55:01 GMT', 'server': 'istio-envoy', 'transfer-encoding': 'chunked'})
2025-10-14 19:55:09,992 - DEBUG - request_id: 8fdae080-d904-442c-8be7-025f34a791f6
2025-10-14 19:55:09,992 - DEBUG - API request completed in 32.53 seconds
2025-10-14 19:55:09,992 - DEBUG - Raw model response: {"labels": [1,0,0,0,0,1,1,0,0,1,1,1,0,0,0,0,0,0,0,1,1,0,0,0,0,0,1,0,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0]}
2025-10-14 19:55:09,992 - INFO - Successfully processed 62 labels
2025-10-14 19:55:09,992 - ERROR - Label count mismatch for llm_japanese_dataset v0_ Construction of Japanese Chat Dataset for Large Language Models and its Methodology
2025-10-14 19:55:09,992 - INFO - Evaluating paper 15/18: Re 3 Dial_ Retrieve_ Reorganize and Rescale Conversations for Long_Turn Open_Domain Dialogue Pre_training
2025-10-14 19:55:09,992 - INFO - Starting model prediction
2025-10-14 19:55:09,992 - INFO - Attempt 1 of 5
2025-10-14 19:55:09,993 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-5a4e2c98-632e-461b-9804-3284e70ddee7', 'json_data': {'messages': [{'role': 'system', 'content': '你是一个专门用于识别学术论文中数据集描述的助手。你需要判断每个输入的句子是否描述了研究中使用的数据集。请确保：\n    1. 只输出JSON格式的结果\n    2. 结果格式必须为 {"labels": [0,1,0,...]}\n    3. 不要输出任何其他文字或解释'}, {'role': 'user', 'content': '你是一位专业的学术论文数据集描述识别专家。请仔细分析下面这篇论文中的每个句子，判断它们是否描述了本研究特定构建的数据集信息。\n\n    论文标题: Re 3 Dial_ Retrieve_ Reorganize and Rescale Conversations for Long_Turn Open_Domain Dialogue Pre_training\n\n    判断标准:\n    判断为1(数据集描述)的标准：\n    句子必须直接描述本研究特定构建的数据集,包括:\n    1. 该数据集的构建过程和方法\n    2. 该数据集的具体构成和规模\n    3. 该数据的来源和收集方式\n    4. 该数据的预处理步骤\n    5. 该数据集的可获取方式(如发布地址)\n    6. 或者其他直接或者间接描述了该数据集的句子\n\n    所有其他类型的句子均标记为0。\n    \n    请对每个句子进行分析，返回一个JSON格式的标签数组。格式要求：\n    1. 必须是有效的JSON格式\n    2. 只包含labels字段，值为0和1组成的数组\n    3. 数组长度必须与句子数量相同\n    4. 示例格式：{"labels": [0,1,0,1,...]}\n\n    需要判断的句子：\n\n    1. Abstract: Pre-training on large-scale open-domain dialogue data can substantially improve the performance of dialogue models.\n2. However, the pre-trained dialogue model\'s ability to utilize long-range context is limited due to the scarcity of long-turn dialogue sessions.\n3. Most dialogues in existing pre-training corpora contain fewer than three turns of dialogue.\n4. To alleviate this issue, we propose the Retrieve, Reorganize and Rescale framework (Re 3 Dial), which can automatically construct billion-scale long-turn dialogues by reorganizing existing short-turn ones.\n5. Given a short-turn session, Re 3 Dial first employs a session retriever to retrieve coherent consecutive sessions.\n6. To this end, we train the retriever to capture semantic and discourse relations within multi-turn dialogues through contrastive training.\n7. Next, Re 3 Dial samples a session from retrieved results following a diversity sampling strategy, which is designed to penalize repetitive or generic sessions.\n8. A longer session is then derived by concatenating the original session and the sampled session.\n9. By repeating the above process, Re 3 Dial can yield a coherent long-turn dialogue.\n10. Extensive experiments on multiple multi-turn dialogue benchmarks demonstrate that Re 3 Dial significantly improves the dialogue model\'s ability to utilize long-range context and thus generate more sensible and informative responses.\n11. Finally, we build a toolkit for efficiently rescaling conversations with Re 3 Dial, which enables us to construct a corpus containing 1B Chinese dialogue sessions with 11.3 turns on average (5× longer than the original corpus).\n12. Our retriever model, code, and data is publicly available at https://github.com/thu-coai/Re3Dial.\n13. Building intelligent open-domain dialogue systems that can generate coherent and engaging multi-turn dialogues with humans has been one of the long-Parents really expect too much from their children in this society, and such children are 100% mentally unhealthy even if they achieve success in the future.\n14. There is no need for children to start working so hard from such a young age.standing goals in AI.\n15. Recently, a variety of largescale open-domain pre-trained dialogue models have dramatically promoted this progress(Roller et al., 2020;Zhou et al., 2021;Shuster et al., 2022b).\n16. And a critical ingredient to the success of these models is the pre-training dialogue corpus.\n17. However, while existing dialogue pre-training corpus collects millions to billions of dialogues from public social media, e.g., Reddit for English(Roller et al., 2020)and Weibo for Chinese(Zhou et al., 2021), long-turn dialogues are highly scarce.\n18. More specifically, based on the publicly reported data statistics shown in Figure1(a), most dialogues in existing pre-training corpora only have less than three turns.\n19. The lack of large-scale long-turn di-alogue data restricts dialogue models from deriving more advanced abilities to utilize long-range context for modeling multi-turn dialogues during pre-training(Xu et al., 2021(Xu et al., , 2022b)).\n20. In this paper, we focus on answering the following research question: Can we automatically build a billionscale long-turn dialogue corpus by reorganizing existing short-turn dialogues?\n21. Our basic idea is to construct a long-turn dialogue via recursively retrieving and selecting one consecutive session from the existing dialogue corpus.\n22. Despite the simplicity of this idea, we still face several challenges to make the constructed corpus effective in enhancing long-turn dialogue pre-training.\n23. First, the selected session should be coherent with the query session.\n24. Otherwise, it will introduce noisy utterances without long-range dependency or break the conversation flow(Liu et al., 2021), which may impact the performance of dialogue models.\n25. Second, our in-depth analysis reveals that the retrieved sessions tend to be biased to be relevant but semantically repetitive with the query or overly generic (e.g., "A: Haha, it\'s so cute.B: Haha! LMAO.") due to both the data bias in the dialogue corpus(Zhou et al., 2021;Lee et al., 2021;Li et al., 2015;Liu et al., 2018)and the model bias of the retriever(Thakur et al., 2021).\n26. These biases significantly lower the diversity and informativeness of the reorganized long-turn dialogues.\n27. To tackle the above challenges, we propose the Retrieve, Reorganize and Rescale framework (Re 3 Dial), which employs an Unsupervised Dense Session Retriever (UDSR) to retrieve coherent short-turn dialogues and reorganize them into a long-turn one.\n28. We train UDSR through contrastive learning by taking consecutive dialogue segments from the same dialogue as positive pairs and those from different dialogues as negative pairs.\n29. To avoid overly retrieving semantically repetitive or generic sessions, we propose a diversity sampling strategy, effectively improving the diversity and informativeness of the reorganized long-turn dialogues.\n30. We verify the effectiveness of Re 3 Dial on three Chinese multi-turn open-domain dialogue benchmarks.\n31. Extensive experiments demonstrate that Re 3 Dial consistently and significantly enhances the dialogue model\'s ability to utilize long-range context, leading to more sensible and informative responses in multi-turn dialogue.\n32. Finally, we develop a toolkit for efficiently rescaling conversations with Re 3 Dial, which enables us to construct a corpus containing 1B Chinese dialogue sessions with 11.3 turns on average (5× longer than that of the original EVA corpus).\n33. We will make our retriever model, toolkit, and data public.\n34. We believe our work provides new opportunities in long-turn dialogue pre-training to the research community.\n35. • We propose to train a dense session retriever on massive unlabeled plain dialogue data with contrastive learning to capture the global semantic and discourse relations within multiturn dialogues.\n36. We also propose the diversity sampling strategy to improve the diversity and informativeness of the automatically constructed corpus.\n37. In the past few years, large-scale pre-training has greatly promoted the progress of the NLP community(Brown et al., 2020).\n38. Recently, large-scale pre-training has also become the mainstream approach to building open-domain dialogue models, both in English(Zhang et al., 2019;Roller et al., 2020;Thoppilan et al., 2022)and Chinese(Bao et al., 2020;Zhou et al., 2021;Gu et al., 2022;Wen et al., 2022).\n39. Through pre-training on massive dialogue data crawled from public social media, these models exhibit strong conversational ability, significantly outperforming traditional non-pre-trained dialogue models.\n40. However, the scarcity of longturn dialogues in the pre-training corpus hinders these models from deriving a better ability to utilize long-range context for modeling multi-turn dialogues during pre-training.\n41. To alleviate this issue, we study how to automatically and efficiently build a large-scale long-turn dialogue corpus based on the existing short-turn dialogue corpus.\n42. We train UDSR on a subset of the EVA pretraining corpus(Zhou et al., 2021), which contains 1,000,000/49,000/1,000 examples for the train/validation/test split.\n43. More details of data processing are provided in Appendix A.1.\n44. We adopt BERT-base(Devlin et al., 2018)as the encoder backbone.\n45. The parameters of E q and E c are not shared according to our preliminary experiments.\n46. Settings We consider three general scenarios where Re Benchmarks We conduct evaluations on three widely-adopted Chinese open-domain multi-turn dialogue benchmarks, including KdConv(Zhou et al., 2020), DuLeMon(Xu et al., 2022b), and NaturalConv(Wang et al., 2021), each has 16~20 turns on average.\n47. Data statistics are shown in Table9.\n48. Metrics We adopt the following automatic metrics for evaluation.\n49. PPL zero-shot measures the perplexity on the test set without fine-tuning on the downstream training sets.\n50. PPL measures the perplexity on the test set after fine-tuning.\n51. BLEU-N measures the precision of the n-gram overlap between generated and ground-truth responses(Papineni et al., 2002)after fine-tuning.\n52. ROUGE-L measures the recall of the n-gram overlap between generated and ground-truth responses(Lin, 2004)after fine-tuning.\n53. Distinct-N measures the percentage of the unique n-grams over all the generated n-grams after fine-tuning(Li et al., 2015).\n54. Table2shows the automatic evaluation results.\n55. In the zero-shot setting, Re  46.25 on DuLeMon, compared to the original baseline\'s performance of 48.79.\n56. This indicates a better ability in multi-turn dialogue modeling.\n57. Moreover, beyond benefiting zero-shot performance, Re 3 Dial can also significantly improve the model\'s performance after fine-tuning on sizable crowdsourcing high-quality long-turn datasets.\n58. Specifically, the Re 3 Dial-trained model achieves better perplexity, BLEU, and ROUGE scores, while showing an improved or comparable generation diversity.\n59. In summary, these results demonstrate that Re 3 Dial provides a well-generalized data foundation in the era of large-scale dialogue pre-training.\n60. We conduct a pair-wise human evaluation to study the models\' performance when provided with dialogue contexts of different lengths.\n61. We first randomly sample 100 long-turn contexts (consisting of at least six turns) from DuLeMon as the Longturn test set.\n62. We then extract the last utterances from these contexts to form the Short-turn test set.\n63. We hence obtain 400 generated responses from the two models.\n64. For each pair of responses (one by the Re 3 Dial-trained model and the other by the Original-trained model), three annotators are hired to give a preference in sensibleness and informativeness, respectively.\n65. Sensibleness mea- sures whether the response is relevant and consistent with the context.\n66. Informativeness measures whether the response is informative given the context.\n67. We adopt majority voting to make final decisions among three annotators.\n68. Effect of Retriever We compare different approaches to retrieve dialogue sessions and evaluate the final dialogue model performance.\n69. We try Random sampling, a term-based retriever BM25, and a state-of-the-art dense retriever Contriever.\n70. Table3presents the results.\n71. All baselines bring fewer improvements or even inversely hurt model performance, especially zero-shot performance in the further pre-training setting.\n72. In contrast, using the retriever in Re 3 Dial achieves consistent and significant improvements across different benchmarks and pre-training settings.\n73. To gain a deeper understanding of the effectiveness of different retrievers in capturing global semantic and discourse relations within multi-turn dialogues, we propose to evaluate the retriever using individual tests in different aspects(Ribeiro et al., 2020).\n74. To this end, we first construct positive pairs following the strategy illustrated in Section 3.1 and introduce perturbations to create negative pairs.\n75. We then compute the retriever\'s accuracy in discriminating between positive and negative pairs, expecting it assigns a higher score to positive pairs.\n76. Our evaluation focuses on three aspects: Irrelevance, Local Relevance, and Discourse Incoherence.\n77. For example, to create a locally relevant negative pair, we keep one utterance from the positive session unchanged while replacing the other utterances with a randomly sampled session.\n78. More details can be found in Appendix E.\n79. The results shown in Table4Overall, these results indicate that automatically building long-turn dialogues to enhance pretraining is non-trivial.Simply improving dialogue turns is insufficient.\n80. It is important to retrieve coherent sessions based on both global semantic relevance and discourse coherence within multi-turn dialogues rather than relying solely on word overlap or semantic similarity.\n81. Otherwise, it will introduce unexpected noise or biases and lead to slightly improved or even decreased model performance.\n82. To further investigate the influence of the proposed diversity sampling strategy in Re 3 Dial, we conduct an ablation study.\n83. As shown in Table5, the dialogue-level and corpus-level weights reduce the bias towards repetitive and generic sessions and improve the diversity and the informativeness of the constructed corpus as expected.\n84. Finally, both of them contribute to the pre-trained dialogue model\'s performance.\n85. To manifest the benefits of Re 3 Dial, we visualize the distribution of PPL zero-shot on samples with varying numbers of dialogue context turns.\n86. Specifically, we first (2) Although other retrieval baselines also exhibit a sharper decreasing trend in perplexity compared to the Original-trained model, they generally yield higher perplexity.\n87. This implies that while these baselines enhance the utilization of long-range context, they capture fewer long-range dependencies compared to Re 3 Dial and may even exhibit inferior performance when the local context is more effectively utilized.\n88. While Re 3 Dial aims to construct a long-turn dialogue pre-training corpus to enhance the utilization of long-range context, there is another line of work that focuses on compressing long contexts into short contexts.\n89. We hence additionally conduct experiments on a retrieval-based baseline and a summarization-based baseline for long-term context modeling and compare them with Re 3 Dial.\n90. We introduce an additional summarization model to summarize long-term context into short sentences.\n91. We try two summarization models: (1) Pegasus-523M(Zhang et al., 2020): It is a widely-adopted encoder-decoder model specifically pre-trained and fine-tuned for text summarization.\n92. (2) ChatGLM-66B(Zeng et al., 2022): It is a widely-adopted instruction-tuned large language model.\n93. We report the average PPL zero-shot over three multi-turn dialogue benchmarks.\n94. From the results shown in Table6, we observe that Re 3 Dial significantly outperforms all baselines in long-turn dialogue benchmarks.\n95. Moreover, augmenting the dialogue model with a context summarization model or a retriever shows less improvement or inversely hurts model performance in several cases.\n96. On the one hand, the two-stage framework suffers from error propagation due to the introduced summarization model or the retriever.\n97. For example, both the summarization model and the retriever may lose important information in the original context.\n98. Moreover, the summarization model could also suffer from hallucination problems(Maynez et al., 2020), thereby introducing new noises.\n99. In contrast, Re 3 Dial keeps the original long-turn context unchanged and thus does not lead to information loss or introduce new noises.\n100. On the other hand, we conjecture that augmenting dialogue models with the context summarization model requires further training on summarizationbased dialogue datasets(Xu et al., 2022a).\n101. In contrast, Re 3 Dial does not require collecting additional training datasets and greatly improves the model performance.\n102. As shown in Table7, the Original-trained model mainly focuses on local context and tends to generate more generic responses (e.g., "I think the same" in responding to the preceding utterance, "they thought it was too risky").\n103. In contrast, the Re 3 Dial-trained dialogue model generates words related to the long-range context (e.g., "fashion designer" which has been mentioned nine turns prior), Original: Well, actually I think the same.\n104. Re 3 Dial: I think it would be a good idea to find another experienced fashion designer , which will help you to achieve your dream.\n105. Table7: Generated responses from the model pretrained on Re 3 Dial and Original corpus (translated from Chinese to English).\n106. We highlight the generated spans that are related to long-range context. leading to a more sensible and specific response.\n107. To show the efficiency of constructing large-scale long-turn dialogue data with Re 3 Dial and allow researchers to explore Re 3 Dial easily, we finally release Re 3 Dial-1B, an improved corpus based on the original EVA corpus that contains 1B sessions with 11.3 turns on average (5× longer than that of the original EVA corpus).\n108. The whole pipeline costs about five days with 32 V100 32G GPUs.\n\n请严格按照JSON格式返回：{"labels": [0,1,0,...]}，不要包含任何其他文字。'}], 'model': 'qwen-plus', 'response_format': {'type': 'json_object'}}}
2025-10-14 19:55:09,993 - DEBUG - Sending HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
2025-10-14 19:55:09,993 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-10-14 19:55:09,994 - DEBUG - send_request_headers.complete
2025-10-14 19:55:09,994 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-10-14 19:55:09,994 - DEBUG - send_request_body.complete
2025-10-14 19:55:09,994 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-10-14 19:55:15,703 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'vary', b'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding'), (b'x-request-id', b'6e8f2e89-a39d-480d-a49d-79b5e1463599'), (b'x-dashscope-call-gateway', b'true'), (b'content-type', b'application/json'), (b'req-cost-time', b'5668'), (b'req-arrive-time', b'1760442901141'), (b'resp-start-time', b'1760442906810'), (b'x-envoy-upstream-service-time', b'5628'), (b'content-encoding', b'gzip'), (b'date', b'Tue, 14 Oct 2025 11:55:06 GMT'), (b'server', b'istio-envoy'), (b'transfer-encoding', b'chunked')])
2025-10-14 19:55:15,703 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-14 19:55:15,703 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-10-14 19:55:15,703 - DEBUG - receive_response_body.complete
2025-10-14 19:55:15,704 - DEBUG - response_closed.started
2025-10-14 19:55:15,704 - DEBUG - response_closed.complete
2025-10-14 19:55:15,704 - DEBUG - HTTP Response: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "200 OK" Headers({'vary': 'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding', 'x-request-id': '6e8f2e89-a39d-480d-a49d-79b5e1463599', 'x-dashscope-call-gateway': 'true', 'content-type': 'application/json', 'req-cost-time': '5668', 'req-arrive-time': '1760442901141', 'resp-start-time': '1760442906810', 'x-envoy-upstream-service-time': '5628', 'content-encoding': 'gzip', 'date': 'Tue, 14 Oct 2025 11:55:06 GMT', 'server': 'istio-envoy', 'transfer-encoding': 'chunked'})
2025-10-14 19:55:15,704 - DEBUG - request_id: 6e8f2e89-a39d-480d-a49d-79b5e1463599
2025-10-14 19:55:15,704 - DEBUG - API request completed in 5.71 seconds
2025-10-14 19:55:15,704 - DEBUG - Raw model response: {"labels": [1,0,0,1,1,1,1,1,1,0,1,1,0,0,0,0,0,0,0,1,1,0,1,1,1,1,1,1,1,0,0,1,1,0,1,1,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}
2025-10-14 19:55:15,704 - INFO - Successfully processed 110 labels
2025-10-14 19:55:15,704 - ERROR - Label count mismatch for Re 3 Dial_ Retrieve_ Reorganize and Rescale Conversations for Long_Turn Open_Domain Dialogue Pre_training
2025-10-14 19:55:15,704 - INFO - Evaluating paper 16/18: Syn2Real_ Forgery Classification via Unsupervised Domain Adaptation
2025-10-14 19:55:15,704 - INFO - Starting model prediction
2025-10-14 19:55:15,704 - INFO - Attempt 1 of 5
2025-10-14 19:55:15,705 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-a4d1e04a-1ea1-41c3-bc89-65a3880b9e2f', 'json_data': {'messages': [{'role': 'system', 'content': '你是一个专门用于识别学术论文中数据集描述的助手。你需要判断每个输入的句子是否描述了研究中使用的数据集。请确保：\n    1. 只输出JSON格式的结果\n    2. 结果格式必须为 {"labels": [0,1,0,...]}\n    3. 不要输出任何其他文字或解释'}, {'role': 'user', 'content': '你是一位专业的学术论文数据集描述识别专家。请仔细分析下面这篇论文中的每个句子，判断它们是否描述了本研究特定构建的数据集信息。\n\n    论文标题: Syn2Real_ Forgery Classification via Unsupervised Domain Adaptation\n\n    判断标准:\n    判断为1(数据集描述)的标准：\n    句子必须直接描述本研究特定构建的数据集,包括:\n    1. 该数据集的构建过程和方法\n    2. 该数据集的具体构成和规模\n    3. 该数据的来源和收集方式\n    4. 该数据的预处理步骤\n    5. 该数据集的可获取方式(如发布地址)\n    6. 或者其他直接或者间接描述了该数据集的句子\n\n    所有其他类型的句子均标记为0。\n    \n    请对每个句子进行分析，返回一个JSON格式的标签数组。格式要求：\n    1. 必须是有效的JSON格式\n    2. 只包含labels字段，值为0和1组成的数组\n    3. 数组长度必须与句子数量相同\n    4. 示例格式：{"labels": [0,1,0,1,...]}\n\n    需要判断的句子：\n\n    1. Abstract: In recent years, image manipulation is becoming increasingly more accessible, yielding more natural-looking images, owing to the modern tools in image processing and computer vision techniques.\n2. The task of the identification of forged images has become very challenging.\n3. Amongst different types of forgeries, the cases of Copy-Move forgery are increasing manifold, due to the difficulties involved to detect this tampering.\n4. To tackle such problems, publicly available datasets are insufficient.\n5. In this paper, we propose to create a synthetic forged dataset using deep semantic image inpainting and copy-move forgery algorithm.\n6. However, models trained on these datasets have a significant drop in performance when tested on more realistic data.\n7. To alleviate this problem, we use unsupervised domain adaptation networks to detect copy-move forgery in new domains by mapping the feature space from our synthetically generated dataset.\n8. Furthermore, we improvised the F1 score on CA-SIA and CoMoFoD dataset to 80.3% and 78.8%, respectively.\n9. Our approach can be helpful in those cases where the classification of data is unavailable.\n10. With the advancement of new image editing technologies, there is a sharp increase in the number of forgery cases.\n11. While sophisticated image editing tools are meant to enhance the quality of images, they are misused to create forged images for nefarious purposes.\n12. These images look so natural that it is difficult to tell with naked eyes whether they have been tampered or are they authentic.\n13. [10]There are diverse ways of forging images, of which Copy-Move, Splicing, Retouching, and Resampling forgeries are the most common ones.\n14. Copy-Move Forgery (CMF) is a type of passive image forgery technique in which a section of an image is copied and pasted within the same image.\n15. Many post-image processing operations such as rescaling, affine transformations, resizing, and blurring are applied to the copied region.\n16. As the source and target image remains the same, the photometric characteristics of the image remain mostly invariable.\n17. Thus, the detection becomes even more difficult.\n18. For instance, in contrast to CMF, splicing forgery is a composition of two images.\n19. A section is cut from an image and pasted on another image.\n20. As a result, there is an edge discrepancy that makes the detection of splicing forgery relatively easier.\n21. Image tampering can have significant effects in various domains.\n22. For instance, in medical imaging, the images are procured with the utmost sensitivity and is a tiresome process.\n23. There can be ulterior economical motives for tampering these confidential and sophisticated images.\n24. Consequently, it could misguide the patients about their illnesses and injuries.\n25. In the field of education, students can tamper their documents with online available software tools.\n26. The significant impact of image tampering can happen in the socio-political area, as manipulated images can affect the perception of a large group of people.\n27. Many magazines and newspaper editors tamper the images in such a way that they can change the semantic meaning of the image.\n28. There have been several traditional approaches for forgery detection that include mostly block-based and keypoint feature extraction[7,16,21]and matching procedures.\n29. Nowadays, deep learning approaches[20,1,17]have been proposed to counterattack the problem of image forgery.\n30. However, most of the approaches are based on supervised learning.\n31. When there are a lot of labeled examples, then it is easy to train the model via supervised learning.\n32. To counter the problems of training data, we generally surrogate the training data by including the dataset from adjacent modality or use synthetic imagery.\n33. When the same model is evaluated on these datasets, it results in a significant drop in the performance.\n34. It happens due to the shift in style, content, or appearance distribution between various datasets.\n35. In these cases, domain adaptation is needed to learn the distribution shift.\n36. In this work, we show that manipulations in images across different domains can be detected via domain adaptation.\n37. We leverage the power of Convolutional Neural Figure1.\n38. The first and second column shows the example of target domain dataset (CASIA and CoMoFoD respectively).\n39. Subsequent column shows the generated synthetic data from COCO dataset using semantic inpainting and copy-move forgery algorithm.\n40. First row is authentic image of each category and second row is forged image.networks (CNNs) to perceive the distinguishable features of authentic and tampered images.\n41. We tackle the problem of performance drop by incorporating the feature space alignment between our synthetic generated datasets and datasets that are publicly available.\n42. We generate the synthetic dataset using Edge-connect semantic inpainting and CMF algorithm.\n43. Contributions Our main contributions in the paper are summarized as follows: 1) The primary task is to classify images as forged or authentic, for which we employ Unsupervised Domain Adaptation (DA), due to the difference in content and style between our source and target dataset, 2) As the publicly available datasets are small, we generate a new dataset comprising of 80,000 images using deep semantic inpainting and copy-move forgery algorithms on COCO[6]dataset, and 3) We explore two Unsupervised DA methods to adapt the features from source dataset to target dataset, such that the variation between the domains is minimized.\n44. The paper is organized as follows: Section II describes the traditional and deep learning solutions that evolved over the years for forgery classification and a review of domain adaptation methods.\n45. Section III describes our methodology in detail that involves dataset generation, Unsupervised DA, and final architectures used for training.\n46. After that, in Section IV, we evaluate the performance of our architecture on CASIA[2]and CoMoFoD[13]dataset.\n47. Section V discusses the conclusion and future directions of our work.\n48. We applied two methods to generate the dataset.\n49. The inclusion of any one of them shows an increase in performance.\n50. Semantic Inpainting helps the model to learn edge discrepancies when the objects are removed.\n51. Copy-Move tampered images improve the focus of the network to recognize similar patches.\n52. We evaluated our architecture on CASIA V2 and CoMo-FoD datasets.\n53. In our case, the source domain constitutes of COCO CMF and semantic inpainted images, and, target domain comprises CASIA V2 and CoMoFoD datasets.\n54. Exhaustive experiments were done using AlexNet[5]and VGG-7[11]for feature extraction.\n55. These datasets are explained briefly in the following sections: It contains 12,614 images in total, of which 7,497 are authentic, and 5,123 are forged images.\n56. The resolution of images ranges from 240 x 160 to 900 x 600.\n57. The tampered images have been applied to post-processing operations and saved in JPEG and TIFF formats.\n58. Out of these 5,123 tampered images, 3,274 images are copy-move, and 1,788 are splicing.\n59. The number of authentic images presents, respectively, for forged images, are 1,701.\n60. Henceforth, our total dataset size comes out to be of 4,975 images.\n61. This dataset contains 400 images, 200 authentic, and 200 forged.\n62. It contains only copy-move forgery cases in PNG format.\n63. The dimension of images in this dataset is 512 x 512.\n64. Various distortions such as translation, rotation, and scaling are applied to tampered images.\n65. We explored diverse color spaces to get a sense of the behavior of CMF images in different color spaces.\n66. Using Alexnet for feature extraction and DANN for domain adaptation, we varied the number of CMF images across RGB and YCrCb color space for the CASIA dataset.\n67. Chrominance component of YCrCb illuminates the identical regions in images with the same luminosity.\n68. It helps the deep networks to visualize copy-pasted regions in images.\n69. In DANN, we used categorical cross-entropy as loss function and Adam optimizer with learning rate 0.001.\n70. The DDC network is trained using Stochastic Gradient Descent optimizer, with a momentum value of 0.9, and learning rate value of 0.0001.\n71. At the time of training, we initially used only CMF images for unsupervised domain adaptation.\n72. Then, we included semantic inpainted images to study the effects of edge discrepancy in recognizing forged images.\n73. There are no labels used at the time of training.\n74. For the target domain, images are passed with a domain label attached to it, and the source domain has a class label also assigned to it.\n75. The source model adapts the weights to classify target images with the same features into a particular category.\n76. During testing, the target images are passed through the source classifier model, whose weights are now adapted to features specific to target data.\n77. 80% of the data used for training, and then, 20% used for testing.\n78. As CoMoFoD contains only 200 images, all the images were used to learn the discriminative features, as well as for evaluation.\n79. We used classification accuracy, precision, recall, and F1-score as performance metrics to evaluate our architectures.\n80. Precision is expressed as the number of true positives divided by the sum of true and false positives.\n81. The recall is defined as the ratio of true positives by true positives and false negatives.\n82. F1-score is the harmonic mean of recall and precision score.\n83. We will now discuss the results summarized in Table1, 2 and 3.\n84. We trained our architecture on source dataset and evaluated it on target dataset.\n85. From Table As the number of images increased, the results improved for domain adaptation.\n86. Due to complex post-processing operations, YCrCb space was unable to localize same tampered regions.\n87. As RGB color space performed better, therefore, for our future training of domain adaptation algorithms, we chose RGB images for source and target domains.\n88. In starting, we only used CMF images for unsupervised DA.\n89. DANN and DDC were able to minimize the distance between the two datasets distributions, but using only CMF images makes the network biased towards objects resembling the same feature characteristics.\n90. To analyze the contribution of the amount of CMF images for domain adaptation, we examined each time with an increment of 10,000 images.\n91. We saw that just by increasing CMF images, there are no noteworthy changes in the accuracy and F1-score on the target domain.\n92. In cases where the copied and background region are the same, e.g., grass, then the model is unable to distinguish the image as authentic or tampered.\n93. To alleviate this problem, we incorporated semantic inpainted images to learn the edge discriminative features.\n94. Itmodel to learn the dissimilarities near the edges of the images are copy-pasted.\n95. As the target domain contains CMF images, increasing the distribution of semantic images beyond 10,000 images leads to drop in performance.\n96. Table2shows the effect of utilizing both semantic inpainted and copy-move tampered images.\n97. In contrast to contemporary networks such as Inception[12]and ResNet[4], we used AlexNet and VGG-7 as our base models, because, these networks have a huge number of parameters and due to limited amount of target domain images, the model doesn\'t generalize well.\n98. COCO → CASIA: In CASIA, there are 3979 images used for training purposes and 996 images for evaluation.\n99. With DDC, we saw a sudden jump by including semantic inpainted images.\n100. Using DANN, we achieved the best score, when the highest number of images were used.\n101. As there is a large number of images available at the time of training, we can see from Table3that DANN+VGG-7 achieves the highest recall and F1-score.\n102. COCO → CoMoFoD: CoMoFoD dataset is very small.\n103. Due to the presence of 200 images only, we trained and evaluated on the whole dataset.\n104. With the increase in the number of images in the source domain, accuracy and F1 score decreased in DDC, and, insignificant increase using DANN.\n105. As the dataset was small, we can see from Table3that DDC+ MMD with Alexnet as base model performed better compared to VGG-7.\n106. VGG-7 has a huge number of parameters that can\'t be optimized; hence, they performed poorly at the test time.\n107. To compare with previous work, we analyzed our results with BusterNet architecture.\n108. They mainly took into account of CASIA CMF and CoMoFoD dataset.\n109. Other works, mainly used all images of CASIA dataset, not explicitly for CMF images.\n110. In BusterNet, they created and trained on 1 lakh images for supervised training, and then evaluated on these datasets.\n111. In CoMoFoD, they used 200 images as ours, but, in CASIA, they took only 1356 CMFD images into account compared to 4975 of ours.\n112. Our approach improves the accuracy by 5-6% in the case of CASIA and 27-28% in the case of CoMoFoD.\n113. Table3shows the performance comparison between ours and BusterNet.\n114. Whereas Buster-Net has used pixel-wise annotations to learn the class of images, we have not used any label at the time of training.\n115. In our case, as the data distribution is too much imbalanced, precision and recall score plays a significant role.\n116. We can see that our precision score is not in the comparable range of recall scores.\n117. It is due to the reason, as we have less num-ber of positive class images in contrast to the negative class.\n118. As we look into the denominator of precision and recall, in the first case, the denominator is the sum of true plus false positives.\n119. Now, we have too many images in a false class.\n120. It attributes to a large number of false positives.\n121. Whereas in the recall, the denominator is the sum of true positives plus false negatives.\n122. The false-negative number is less as the number of images in the correct class is fewer.\n\n请严格按照JSON格式返回：{"labels": [0,1,0,...]}，不要包含任何其他文字。'}], 'model': 'qwen-plus', 'response_format': {'type': 'json_object'}}}
2025-10-14 19:55:15,705 - DEBUG - Sending HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
2025-10-14 19:55:15,705 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-10-14 19:55:15,705 - DEBUG - send_request_headers.complete
2025-10-14 19:55:15,705 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-10-14 19:55:15,705 - DEBUG - send_request_body.complete
2025-10-14 19:55:15,705 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-10-14 19:55:25,585 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'vary', b'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding'), (b'x-request-id', b'16c95e19-7e6c-4da8-bd7e-b5947f05cf4e'), (b'x-dashscope-call-gateway', b'true'), (b'content-type', b'application/json'), (b'req-cost-time', b'9836'), (b'req-arrive-time', b'1760442906850'), (b'resp-start-time', b'1760442916687'), (b'x-envoy-upstream-service-time', b'9796'), (b'content-encoding', b'gzip'), (b'date', b'Tue, 14 Oct 2025 11:55:16 GMT'), (b'server', b'istio-envoy'), (b'transfer-encoding', b'chunked')])
2025-10-14 19:55:25,585 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-14 19:55:25,585 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-10-14 19:55:25,585 - DEBUG - receive_response_body.complete
2025-10-14 19:55:25,585 - DEBUG - response_closed.started
2025-10-14 19:55:25,585 - DEBUG - response_closed.complete
2025-10-14 19:55:25,586 - DEBUG - HTTP Response: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "200 OK" Headers({'vary': 'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding', 'x-request-id': '16c95e19-7e6c-4da8-bd7e-b5947f05cf4e', 'x-dashscope-call-gateway': 'true', 'content-type': 'application/json', 'req-cost-time': '9836', 'req-arrive-time': '1760442906850', 'resp-start-time': '1760442916687', 'x-envoy-upstream-service-time': '9796', 'content-encoding': 'gzip', 'date': 'Tue, 14 Oct 2025 11:55:16 GMT', 'server': 'istio-envoy', 'transfer-encoding': 'chunked'})
2025-10-14 19:55:25,586 - DEBUG - request_id: 16c95e19-7e6c-4da8-bd7e-b5947f05cf4e
2025-10-14 19:55:25,586 - DEBUG - API request completed in 9.88 seconds
2025-10-14 19:55:25,586 - DEBUG - Raw model response: {"labels": [0,0,0,1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,1,0,1,1,1,1,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}
2025-10-14 19:55:25,586 - INFO - Successfully processed 104 labels
2025-10-14 19:55:25,586 - ERROR - Label count mismatch for Syn2Real_ Forgery Classification via Unsupervised Domain Adaptation
2025-10-14 19:55:25,586 - INFO - Evaluating paper 17/18: Topic Shift Detection in Chinese Dialogues_ Corpus and Benchmark
2025-10-14 19:55:25,586 - INFO - Starting model prediction
2025-10-14 19:55:25,586 - INFO - Attempt 1 of 5
2025-10-14 19:55:25,587 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-140f64cd-8ca7-4306-b747-e14255c3287c', 'json_data': {'messages': [{'role': 'system', 'content': '你是一个专门用于识别学术论文中数据集描述的助手。你需要判断每个输入的句子是否描述了研究中使用的数据集。请确保：\n    1. 只输出JSON格式的结果\n    2. 结果格式必须为 {"labels": [0,1,0,...]}\n    3. 不要输出任何其他文字或解释'}, {'role': 'user', 'content': '你是一位专业的学术论文数据集描述识别专家。请仔细分析下面这篇论文中的每个句子，判断它们是否描述了本研究特定构建的数据集信息。\n\n    论文标题: Topic Shift Detection in Chinese Dialogues_ Corpus and Benchmark\n\n    判断标准:\n    判断为1(数据集描述)的标准：\n    句子必须直接描述本研究特定构建的数据集,包括:\n    1. 该数据集的构建过程和方法\n    2. 该数据集的具体构成和规模\n    3. 该数据的来源和收集方式\n    4. 该数据的预处理步骤\n    5. 该数据集的可获取方式(如发布地址)\n    6. 或者其他直接或者间接描述了该数据集的句子\n\n    所有其他类型的句子均标记为0。\n    \n    请对每个句子进行分析，返回一个JSON格式的标签数组。格式要求：\n    1. 必须是有效的JSON格式\n    2. 只包含labels字段，值为0和1组成的数组\n    3. 数组长度必须与句子数量相同\n    4. 示例格式：{"labels": [0,1,0,1,...]}\n\n    需要判断的句子：\n\n    1. Abstract: Dialogue topic shift detection is to detect whether an ongoing topic has shifted or should shift in a dialogue, which can be divided into two categories, i.e., response-known task and response-unknown task.\n2. Currently, only a few investigated the latter, because it is still a challenge to predict the topic shift without the response information.\n3. In this paper, we first annotate a Chinese Natural Topic Dialogue (CNTD) corpus consisting of 1308 dialogues to fill the gap in the Chinese natural conversation topic corpus.\n4. And then we focus on the response-unknown task and propose a teacher-student framework based on hierarchical contrastive learning to predict the topic shift without the response.\n5. Specifically, the response at high-level teacher-student is introduced to build the contrastive learning between the response and the context, while the label contrastive learning is constructed at low-level student.\n6. The experimental results on our Chinese CNTD and English TIAGE show the effectiveness of our proposed model.\n7. Dialogue topic shift detection is to detect whether a dialogue\'s utterance has shifted in the topic, which can help the dialog system to change the topic and guide the dialogue actively.\n8. Although dialog topic shift detection is a new task, it has become a hotspot due to its remarkable benefit to many downstream tasks, such as response generation[1]and reading comprehension[2,3], and can help those real-time applications produce on-topic or topic-shift responses which perform well in dialogue scenarios[4,5,6].\n9. The task of dialogue topic shift detection can be divided into two lines, i.e., response-known task and response-unknown task, as shown in Fig.1.\n10. The former can gain the response information and obtain a better result, while the latter is the opposite.\n11. Moreover, both of them are not accessible to future information.\n12. This is the biggest difference from the task of text topic segmentation, in which all the basic utterances are visible to each other.\n13. That is, those existing topic segmentation models cannot be applied to dialogue topic shift detection since it depends on the response and its subsequent utterances heavily.\n14. Therefore, it is more difficult to discern differences between utterances in the task of dialogue topic shift detection.\n15. Due to the absence of future utterances, dialogue topic shift detection is still a challenging task.\n16. In this paper, we focus on the response-unknown task of topic shift detection in Chinese dialogues.\n17. There are two issues in the response-unknown task of topic shift detection in Chinese dialogues, i.e., lack of annotated corpus in Chinese and how to predict the response.\n18. Fig.1.\n19. Two lines of dialogue topic shift detection tasks to detect whether it exists topic shift between the utterances ui-1 and ui, where the response-known task (left) can use the response ui, while the response-unknown task (right) can be regarded as topic shift prediction without the response ui.\n20. There are only a few publicly dialogue topic shift corpus available and most of them are provided for the segmentation task, which does not satisfy natural conversation.\n21. Xie et al.[7]provided a detailed definition of the dialogue topic shift detection task, and annotated an English dialogue topics corpus TIAGE.\n22. Although it can fill the gap in the corpus of English conversation topics, its scale is still too small.\n23. In Chinese, Xu et al.[8]annotated a Chinese dialogue topic corpus.\n24. However, due to its small size and poor quality, this is detrimental to the further research and development of Chinese dialogue topic shift tasks.\n25. To fill the gap in the Chinese natural dialogue topic corpus, we first annotated a Chinese Natural Topic Dialogue (CNTD) corpus which consists of 1308 dialogues with high quality.\n26. Xie et al.[7]also established a benchmark for this response-unknown task based on the T5 model[9]and this benchmark only used the context to predict topic shift and performed poorly due to the lack of the response information.\n27. Thus, it is more challenging to predict the topic shift in natural dialogue without useful response information.\n28. The teacher-student framework has been used widely to obtain information that is not available to the model[1].\n29. To solve the issue of the lack of response information, we propose a teacher-student framework to introduce the response information.\n30. The teacher can obtain the response information, and the student can learn the response information from the teacher through knowledge distillation.\n31. To facilitate knowledge transfer, the student mimics the teacher on every layer instead of just the top layer, which alleviates the delayed supervised signal problem using hierarchical semantic information in the teacher[10].\n32. Besides, we construct hierarchical contrastive learning in which we consider the teacher-student as high-level and the student as low-level.\n33. At high-level, we build an information simulation loss between the context and the response to improve the semantic information of the student model with more reliable predictive information.\n34. At low-level, we design a semantic coherence-aware loss to better distinguish the different shift cases and produce more reliable prediction results.\n35. Finally, the experimental results on our Chinese CNTD and the English TIAGE show that our proposed model outperforms the baselines.\n36. The contributions of this paper are as follows.\n37. -We introduce hierarchical contrastive learning to further improve performance.\n38. -The experimental results both on the CNTD and TIAGE datasets show that our model outperforms the baselines.\n39. Previous studies explored the dialogue topic tasks and published the annotated topic dialogue corpus.\n40. For English, Xie et al.[7]annotated the TIAGE consisting of 500 dialogues with 7861 turns based on PersonaChat[11].\n41. Xu et al.[8]built a dataset including 711 dialogues by joining dialogues from existing multi-turn dialogue datasets: MultiWOZ Corpus[12], and Stanford Dialog Dataset[13].\n42. Both corpora are either small or limited to a particular domain, and neither applies to the study of the natural dialogue domain.\n43. For Chinese, Xu et al.[8]annotated a dataset including 505 phone records of customer service on banking consultation.\n44. However, this corpus is likewise restricted to a few specialized domains while natural dialogues are more complicated.\n45. Natural dialogues have a range of topic shift scenarios, unrestricted topics, and more free colloquialisms in the utterances.\n46. The above corpus is insufficient to fill the gap in the Chinese natural dialogue topic corpus.\n47. The existing corpus of Chinese dialogue topic detection[8]is small and does not satisfy natural conversation.\n48. Although the English dialogue topic corpora can be converted into Chinese by machine translation, they lack natural conversation colloquiality and are small in size.\n49. Therefore, we annotate a Chinese dialogue topic detection corpus CNTD based on NaturalConv dataset[20].\n50. In this section, we show our annotation guidelines and outline the reasons for our selection of corpus sources, as well as the manual annotation procedure and data statistics.\n51. We also analyze the topic shift distribution in CNTD.\n52. Each dialogue in our corpus has a piece of news as a base document, which is not available in other corpus and can be used as additional information for further research and expansion.\n53. The news is from six domains, which brings our conversations closer to natural dialogue.\n54. Besides, the speakers in our corpus are not restricted in any way, which also makes it closer to natural dialogues.\n55. In addition, we annotated the fine-grained dialogues topics, refer to Section 3.2.\n56. Fine-grained labels are beneficial to promote further research on dialogue topics.\n57. Compared with the existing Chinese topic corpus annotated by Xu et al.[8], the dialogues in our corpus do not have meaningless and repetitive turns.\n58. Also, the corpus is more than twice the size of the other corpus.\n59. In addition, the news in the corpus can be studied as additional information for the dialogues.\n60. Following the annotation guidelines in TIAGE[7], we distinguish each dialogue turns whether changed the topic compared with the context.\n61. -Commenting on the previous context: The response is a comment on what is said by the speaker previously; -Question answering: The response is an answer to the question that comes from the speaker previously; -Developing the dialogue to sub-topics: The response develops to a sub-topic compared to the context; -Introducing a relevant but different topic: The response introduces a relevant but different topic compared to the context; -Completely changing the topic: The response completely changes the topic compared to the context.\n62. Among them, we uniformly identify the two cases of greeting and farewell specific to CNTD as the topic shift.\n63. We chose the NaturalConv dataset[20]as the source corpus, which contains about 400K utterances and 19.9K dialogues in multiple domains.\n64. It is designed to collect a multi-turn document grounded dialogue dataset with scenario and naturalness properties of dialogue.\n65. We consider NaturalConv as a promising dataset for dialogue topic detection for the following reasons: 1) NaturalConv is much closer to human-like dialogue with the natural property, including a full and natural setting such as scenario assumption, free topic extension, greetings, etc.; 2) NaturalConv contains about 400K utterances and 19.9K dialogues in multiple domains; 3) The average turn number of this corpus is 20, and longer dialogue contexts tend to exhibit a flow with more topics; 4) The corpus has almost no restrictions or assumptions about the speakers, e.g., no explicit goal is proposed[21].\n66. We have three annotators for coarse-grained annotations and two for fine-grained annotations.\n67. Both annotations are divided into three stages as follows.\n68. Co-annotation Stage First, for coarse-grained annotations, we draw a total of 100 dialogues from each domain of the NaturalConv dataset proportionally for a total of 2014 dialogue turns.\n69. In this stage, three annotators are asked to discuss every 20 dialogues they annotated, and each annotator is asked to give a reason for the annotation during the discussion.\n70. Finally, the Kappa value of all annotators for coarse-grained annotations at this stage is 0.7426.\n71. In addition, we annotated the fine-grained information based on the results of the complete coarse-grained annotations.\n72. Two annotators annotated the same 150 dialogues and discussed them several times for consistency.\n73. Finally, the kappa value of all annotators for fine-grained annotations at this stage is 0.9032.\n74. These kappa values confirm that our annotators already have sufficient annotation capabilities for independent annotation, as well as the high quality of our corpus.\n75. Independent-annotation Stage We ensured the quality of each annotator\'s annotation and judging criteria before starting the second phase of annotation.\n76. For both granularity annotations, we randomly assign the dialogues drawn from each domain to each annotator for independent annotation.\n77. At this stage, we annotate 1208 dialogues for coarse-grained annotations and 1158 dialogues for fine-grained annotations.\n78. Semi-automatic Rechecking Stage Finally, we use a semi-automatic rechecking process to ensure that the corpus is still of high quality.\n79. On the one hand, we automatically format the dialogues with annotations to detect formatting problems caused by manual annotation.\n80. On the other hand, we automatically match the related news to each dialogue and check that the topic attributes are consistent with the dialogue to rule out any possible errors.\n81. Due to the limited time, we randomly select 1308 dialogues from the Natural-Conv dataset and annotate them with four annotators.\n82. Finally, we construct a Chinese natural topic dialogues corpus containing 26K dialogue turns.\n83. As shown in Table2, we randomly split them into 1041 train, 134 validation, and 133 test dialogues respectively, according to the percentage of different categories.\n84. In addition, we show the details of CNTD in Table3, which shows that our corpus has enough topics and long turns which is suitable for dialogue topic detection.\n85. Finally, there are the statistics of our fine-grained labels, as shown in Table4.\n86. We count the number of dialogues with different numbers of topics, as shown in Fig.2.\n87. On another side, we count the distribution of topic shift signals in dialogues, shown in Fig.3.\n88. We can see there are a total of 21 turns and three peaks of topic shift signals, which occur in 2 nd , 4 th , and 18 th turns, respectively.\n89. The reason is that the dialogue in our corpus usually starts with a greeting and   ends with a farewell, which leads to more topic shifts at the beginning and end of the dialogues.\n90. In addition, the NaturalConv corpus gives a piece of news as the base document of the dialogue, so there are more frequent transitions from news to derived topics, leading to the third highest peak in 4 th turn.\n91. However, we think this is consistent with a natural dialogue scenario because people often talk about recent news after daily greetings.\n92. Based on the train/validation/test dataset of CNTD we partitioned in Table2and previous work on TIAGE[7], we extract (context, response) pairs from each dialogue as input and the label of response as a target for the responseunknown task.\n93. In our experiments, every utterance except the first utterance of the dialogue can be considered as a response.\n94. As for evaluation, we report Precision (P), Recall (R), and Micro-F1 scores.\n95. We use BERT as an encoder and fine-tune it during training.\n96. For both the TIAGE and CNTD corpus, all pre-trained model parameters are set to default values.\n97. We conduct our experiments on NVIDIA GeForce GTX 1080 Ti and NVIDIA GeForce GTX 3090 with batch sizes of 2 and 6 for both CNTD and TIAGE, with the initial learning rates of 2e-5.\n98. And we set the epochs of training to 20, and the dropout to 0.5.\n99. For the pre-trained models in the experiment, we apply BERT-base-Chinese and MT5-base to obtain the semantic representation of the dialogues in CNTD, and we apply BERT-base-uncased and T5-base to obtain the semantic representation of the dialogues in TIAGE.\n100. Dialogue topic shift detection is a new task and there is no complex model available, besides a simple T5[7]that can be considered as the SOTA model.\n101. Since we employ BERT as our encoder and the T5 model is used in TIAGE, we use the pre-trained models of T5[9]and BERT[27]as baselines.\n102. For BERT, we For T5, we also connect utterances in the context and classify the undecidable predicted results to the \'not a topic shift\' category.\n103. Table5shows the performance comparison between our model and the baselines, in which TS denotes our teacher-student model without the hierarchical comparative learning (HCL) and Ours denotes our final model, i.e., the addition of SCL on the student side based on the addition of ISL on both the teacher and student sides.\n104. It can be found that on CNTD, our model achieves a good improvement and improves both precision and recall in comparison with the baselines.\n105. Although T5 does not perform poorly on recall, its precision is inadequate in comparison with BERT, and it is clear that T5 is not effective in predicting topics.\n106. In contrast, TS improved by 1.0 in Micro-F1 in comparison with BERT, which confirms that the teacher-student framework is effective in introducing response information.\n107. As well, Ours improved by 4.0 in micro-F1 in comparison with TS, and also showed significant improvement in P and R, which fully demonstrates that our HCL can improve the model\'s ability to discriminate between different topic situations.\n108. In particular, our model improves on CNTD by 5.0 in comparison with the best baseline BERT, which shows the effectiveness of our proposed model.\n109. To verify the effectiveness of the components used in our model, we conduct ablation studies on CTND, and the experimental results are shown in Table6.\n110. If we remove ISL on the teacher side (-ISL S ) or the student side (-ISL T ), the performance of the model decreased by 1.5 and 1.3 on the Micro-F1 value, respectively, with the largest decrease after removing the ISL on the student side.\n111. Although -ISL T has the highest precision in predicting topics and lower error probability than Ours and -ISL S .\n112. However, it can be seen that adding ISL at both the teacher and student sides can better improve the correct prediction rate.\n113. Moreover, if we remove ISL both on the teacher and student side (-ISL T S ), it achieves a similar performance on Micro-F1, in comparison with -ISL S and -ISL T .\n114. However, it achieves the highest precision (58.8%).\n115. If we remove SCL (-SCL) or HCL (-HCL) from our model, the Micro-F1 value of the models -SCL and -HCL drop from 53.9 to 52.4 (-1.5) and 49.9 (-4.0), respectively.\n116. These results show that our Semantic Conherent-aware Loss(SCL), and Hierarchical Contrastive Learning(HCL) are effective for this task, especially HCL.\n117. In addition, we explore the performance of the dialogues with different numbers of topics to analyze our model in comparison with BERT, as shown in Table7.\n118. It can be found that our model has a better performance than BERT on dialogues with fewer topics.\n119. Our model gets at least a 6% improvement in topic shift prediction on dialogues with 2 to 5 topics and obtains above-average performance.\n120. And when the number of topics increases to 9, the performance improves because the conversation length is still about 20 and the topics shift more significantly.\n121. In Table8, we also investigate the recall of the topic shift detection for various topic turns.\n122. Our model is improved for varying degrees across topic turns, with the most significant improvements in turns 7-9.\n123. Even in long topic shift cases, our model can obtain an effective boost.\n124. However, the performance of our model inevitably decreases compared to short topic shift cases.\n125. When there are fewer topic turns, the topic shift situation is simpler, so it is easier to determine.\n126. When the length of turns becomes longer and the situation becomes complicated, the topic of long turns has more information so it is easier to identify.\n127. As shown in Table9, it can be found that our model also achieves a good improvement on English TIAGE.\n128. et al. we obtain the best performance on both recall and Micro-F1 values, especially on micro-F1 with a 5.8% improvement over T5.\n129. This proves that our model achieves the best performance both in English and Chinese.\n130. We also conducted a case study.\n131. The prediction made by our model, the BERT model on the instance, and the manual labels are shown in Table10.\n132. "etc., belonging to the questionanswering scenario.\n133. "etc. belonging to the commenting on the previous context scenario, our model or BERT cannot accurately predict the topic shift in this scenario.\n134. This shows that detecting the topic shifts in natural dialogue is still challenging.\n135. We further analyze the errors of the prediction produced in our experiments.\n136. Specifically, we analyzed the example to explore whether the error in the results of this example is prevalent in other dialogues.\n137. From Table10, we can find that the wrong predictions at 14 th and 18 th turn.\n138. "as \'topic shift\'.\n139. We counted the appearance of many errors, and the errors are mainly divided into two categories.\n140. One is for the "Introducing a relevant but different topic" type of utterance.\n141. It was predicted that no topic shift occurred due to the lack of information about the future of the conversation.\n142. The other is the "commenting on the previous context" category.\n143. Since this type of response does not affect the integrity of the previous topic, it is mostly predicted to be a topic shift.\n144. Table10.\n145. The results of BERT, Ours, and Human of different turns where "1" indicates that a topic shift has occurred and "0" indicates the opposite.\n146. We omit the lines with all 0.\n\n请严格按照JSON格式返回：{"labels": [0,1,0,...]}，不要包含任何其他文字。'}], 'model': 'qwen-plus', 'response_format': {'type': 'json_object'}}}
2025-10-14 19:55:25,588 - DEBUG - Sending HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
2025-10-14 19:55:25,588 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-10-14 19:55:25,588 - DEBUG - send_request_headers.complete
2025-10-14 19:55:25,588 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-10-14 19:55:25,588 - DEBUG - send_request_body.complete
2025-10-14 19:55:25,588 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-10-14 19:55:33,258 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'vary', b'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding'), (b'x-request-id', b'd7db8e3d-9846-45b4-bb21-d4f6368fd52a'), (b'x-dashscope-call-gateway', b'true'), (b'content-type', b'application/json'), (b'req-cost-time', b'7627'), (b'req-arrive-time', b'1760442916729'), (b'resp-start-time', b'1760442924356'), (b'x-envoy-upstream-service-time', b'7581'), (b'content-encoding', b'gzip'), (b'date', b'Tue, 14 Oct 2025 11:55:24 GMT'), (b'server', b'istio-envoy'), (b'transfer-encoding', b'chunked')])
2025-10-14 19:55:33,258 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-14 19:55:33,258 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-10-14 19:55:33,258 - DEBUG - receive_response_body.complete
2025-10-14 19:55:33,258 - DEBUG - response_closed.started
2025-10-14 19:55:33,258 - DEBUG - response_closed.complete
2025-10-14 19:55:33,258 - DEBUG - HTTP Response: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "200 OK" Headers({'vary': 'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding', 'x-request-id': 'd7db8e3d-9846-45b4-bb21-d4f6368fd52a', 'x-dashscope-call-gateway': 'true', 'content-type': 'application/json', 'req-cost-time': '7627', 'req-arrive-time': '1760442916729', 'resp-start-time': '1760442924356', 'x-envoy-upstream-service-time': '7581', 'content-encoding': 'gzip', 'date': 'Tue, 14 Oct 2025 11:55:24 GMT', 'server': 'istio-envoy', 'transfer-encoding': 'chunked'})
2025-10-14 19:55:33,259 - DEBUG - request_id: d7db8e3d-9846-45b4-bb21-d4f6368fd52a
2025-10-14 19:55:33,259 - DEBUG - API request completed in 7.67 seconds
2025-10-14 19:55:33,259 - DEBUG - Raw model response: {"labels": [0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]}
2025-10-14 19:55:33,259 - INFO - Successfully processed 111 labels
2025-10-14 19:55:33,259 - ERROR - Label count mismatch for Topic Shift Detection in Chinese Dialogues_ Corpus and Benchmark
2025-10-14 19:55:33,259 - INFO - Evaluating paper 18/18: Towards Equitable Representation in Text_to_Image Synthesis Models with the Cross_Cultural Understanding Benchmark _CCUB_ Dataset
2025-10-14 19:55:33,259 - INFO - Starting model prediction
2025-10-14 19:55:33,259 - INFO - Attempt 1 of 5
2025-10-14 19:55:33,260 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-3ea6a529-db39-4eba-ad6e-1835a7be4cb5', 'json_data': {'messages': [{'role': 'system', 'content': '你是一个专门用于识别学术论文中数据集描述的助手。你需要判断每个输入的句子是否描述了研究中使用的数据集。请确保：\n    1. 只输出JSON格式的结果\n    2. 结果格式必须为 {"labels": [0,1,0,...]}\n    3. 不要输出任何其他文字或解释'}, {'role': 'user', 'content': '你是一位专业的学术论文数据集描述识别专家。请仔细分析下面这篇论文中的每个句子，判断它们是否描述了本研究特定构建的数据集信息。\n\n    论文标题: Towards Equitable Representation in Text_to_Image Synthesis Models with the Cross_Cultural Understanding Benchmark _CCUB_ Dataset\n\n    判断标准:\n    判断为1(数据集描述)的标准：\n    句子必须直接描述本研究特定构建的数据集,包括:\n    1. 该数据集的构建过程和方法\n    2. 该数据集的具体构成和规模\n    3. 该数据的来源和收集方式\n    4. 该数据的预处理步骤\n    5. 该数据集的可获取方式(如发布地址)\n    6. 或者其他直接或者间接描述了该数据集的句子\n\n    所有其他类型的句子均标记为0。\n    \n    请对每个句子进行分析，返回一个JSON格式的标签数组。格式要求：\n    1. 必须是有效的JSON格式\n    2. 只包含labels字段，值为0和1组成的数组\n    3. 数组长度必须与句子数量相同\n    4. 示例格式：{"labels": [0,1,0,1,...]}\n\n    需要判断的句子：\n\n    1. Abstract: It has been shown that accurate representation in media improves the well-being of the people who consume it.\n2. By contrast, inaccurate representations can negatively affect viewers and lead to harmful perceptions of other cultures.\n3. To achieve inclusive representation in generated images, we propose a culturally-aware priming approach for text-to-image synthesis using a small but culturally curated dataset that we collected, known here as Cross-Cultural Understanding Benchmark (CCUB) Dataset, to fight the bias prevalent in giant datasets.\n4. Our proposed approach is comprised of two fine-tuning techniques: (1) Adding visual context via fine-tuning a pre-trained text-to-image synthesis model, Stable Diffusion, on the CCUB text-image pairs, and (2) Adding semantic context via automated prompt engineering using the finetuned large language model, GPT-3, trained on our CCUB culturally-aware text data.\n5. CCUB dataset is curated and our approach is evaluated by people who have a personal relationship with that particular culture.\n6. Our experiments indicate that priming using both text and image is effective in improving the cultural relevance and decreasing the offensiveness of generated images while maintaining quality.\n7. † indicates corresponding authors.\n8. In media, studies repeatedly show that representation affects the well-being of its viewers[Shaw, 2010;Caswell et al., 2017;Elbaba, 2019].\n9. Representation can positively affect viewers by providing them with role models that they identify with, but it can also negatively affect viewers by creating harmful, stereotypical understandings of people and culture[Castañeda, 2018].\n10. When people are accurately represented in media, it allows people to properly understand cultures without harmful stereo- types forming[Dixon and Linz, 2000;Mastro and Greenberg, 2000].\n11. Despite the benefits of representation, many media generating Artificial Intelligence (AI) models show poor representation in their results[Ntoutsi et al., 2020].\n12. Many of these issues stem from their large training datasets which are gathered by crawling the Internet without filtering supervision and contain malign stereotypes and ethnic slurs among other problematic content[Birhane et al., 2021].\n13. As AI models are increasingly used to create and aid in the production of visual content, it is important that the models have a true understanding of culture such that it can give accurate and proper representation leading to well-being rewards for its consumers.\n14. In this paper, we aim to address such a representation issue in image generation and introduce a new task of culturally-aware image synthesis: generating visual content within a cultural context that is both accurate and inoffensive.\n15. Our overarching goal is to improve the well-being of consumers of the AI generated images with particular attention to those consumers from underrepresented groups.\n16. Specifically, we formulate the culturally-aware text-to-image synthesis task to take an additional input of a country name to specify a cultural context in addition to language description.\n17. It was found that large datasets such as the LAION-5B[Schuhmann et al., 2021]used to train many text-toimage synthesis models such as Stable Diffusion[Rombach et al., 2021]are Anglo-centric and Euro-centric[Birhane et al., 2021]as shown in the top row of Figure1.\n18. As a consequence, these powerful models may generate culturally offensive images due to misrepresentation during training.\n19. Our research question is, how can effective existing text-to-image models be improved to become more culturally representative and thus less offensive?\n20. It may be infeasible to vet billions of training examples for accurate cultural content.\n21. We hypothesize that a small dataset that is veritably representative of a culture can be used to prime pre-trained textto-image models to guide the model towards more culturally accurate content creation.\n22. To verify the hypothesis, we collected a dataset of image and caption pairs for 8 cultures.\n23. For each culture, data was collected by a few people who are native of that culture as they are the people who properly understand it and are most affected by its misrepresentations.\n24. We call this the Cross-Cultural Understanding Benchmark (CCUB) dataset which comprises of 100-200 images each with a manually written caption as shown in Figure2.\n25. We propose two techniques for enhancing the text-toimage pipelines using CCUB.\n26. First, we fine-tune a text-toimage synthesis model, Stable Diffusion, on the CCUB textimage pairs to generate images tailored for a given cultural context.\n27. We evaluate our approach\'s two components individually as well as combined against the baseline of simply specifying the culture in the text prompt.\n28. Our evaluation was performed by native people of each country.\n29. Our survey results based on 2,244 image comparisions conducted by 72 participants from 5 countries indicate that our proposed approach is both less offensive and more cultural relevant than simply adding the country name as a suffix to the prompt.\n30. Our contributions are as follows: 1.\n31. Following the definition of culture in[Halpern, 1955]and [Key and Comrie, 2021], nine categories are used to represent cultural elements in our dataset: food & drink, clothing, artwork, dance and music, religion, architecture, people, city and nature.\n32. The categories are further divided into traditional and modern to reflect a characteristic of the culture that culture changes over time.\n33. Our CCUB image are collected based on the nine cultural categories.\n34. For collection, we recruited cultural experts who confidently know this culture well or belong to it.\n35. Cultural experts are asked to collect 10-20 relevant images containing different objects for each cultural category.\n36. The images were collected either from Creative Commons licensed images from Google searches or the collectors own photographs.\n37. Cultural experts were also asked to select images with common or culturally representative items.\n38. Each image in the CCUB dataset is also captioned by cultural experts forming paired image-text data.\n39. Cultural experts were asked to focus on the general and specific items in each cultural image, rather than adding captions to subtle components of the image.\n40. The captions accurately express cultural contents in English as opposed to large datasets such asLAION [Schuhmann et al., 2021]which are scraped from the internet and not vetted for cultural accuracy.\n41. We produced surveys to evaluate the effectiveness of our two proposed techniques for culturally-aware text-to-image synthesis and compare them to a baseline of simply appending the culture to the prompt, e.g., "A family eating dinner , China." and using an existing text-to-image model.\n42. In setting up our study, we consider a comparative structure between images: the baseline image versus another image from our results.\n43. The setup of a single question in our survey was as follows: Given two images, the participant selects which image best fits three given comparative properties.\n44. The properties analyzed were: (1) Text and Image Alignment: Participants are given a text prompt and consider which of the two images is more similar to the prompt; (2) Cultural Alignment: Participants decide which of the two images is a better representation of the country\'s culture; and (3) Offensiveness: Participants consider which of the two images is more offensive to them.\n45. The participants for the study were selected based on whether they had a personal understanding of the culture for which the images in the survey were generated.\n46. Participants were recruited among university students, friends, and family members of the authors.\n47. It was ensured that the participants would not be able to discern the approaches used to generate the compared images by randomizing the order of questions and images in the survey.\n\n请严格按照JSON格式返回：{"labels": [0,1,0,...]}，不要包含任何其他文字。'}], 'model': 'qwen-plus', 'response_format': {'type': 'json_object'}}}
2025-10-14 19:55:33,261 - DEBUG - Sending HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
2025-10-14 19:55:33,261 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-10-14 19:55:33,261 - DEBUG - send_request_headers.complete
2025-10-14 19:55:33,261 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-10-14 19:55:33,261 - DEBUG - send_request_body.complete
2025-10-14 19:55:33,261 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-10-14 19:55:37,007 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'vary', b'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding'), (b'x-request-id', b'3d733ff6-aa8b-4026-a093-25153e641652'), (b'x-dashscope-call-gateway', b'true'), (b'content-type', b'application/json'), (b'req-cost-time', b'3704'), (b'req-arrive-time', b'1760442924398'), (b'resp-start-time', b'1760442928103'), (b'x-envoy-upstream-service-time', b'3702'), (b'content-encoding', b'gzip'), (b'date', b'Tue, 14 Oct 2025 11:55:28 GMT'), (b'server', b'istio-envoy'), (b'transfer-encoding', b'chunked')])
2025-10-14 19:55:37,008 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-14 19:55:37,008 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-10-14 19:55:37,008 - DEBUG - receive_response_body.complete
2025-10-14 19:55:37,008 - DEBUG - response_closed.started
2025-10-14 19:55:37,008 - DEBUG - response_closed.complete
2025-10-14 19:55:37,008 - DEBUG - HTTP Response: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "200 OK" Headers({'vary': 'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding', 'x-request-id': '3d733ff6-aa8b-4026-a093-25153e641652', 'x-dashscope-call-gateway': 'true', 'content-type': 'application/json', 'req-cost-time': '3704', 'req-arrive-time': '1760442924398', 'resp-start-time': '1760442928103', 'x-envoy-upstream-service-time': '3702', 'content-encoding': 'gzip', 'date': 'Tue, 14 Oct 2025 11:55:28 GMT', 'server': 'istio-envoy', 'transfer-encoding': 'chunked'})
2025-10-14 19:55:37,008 - DEBUG - request_id: 3d733ff6-aa8b-4026-a093-25153e641652
2025-10-14 19:55:37,008 - DEBUG - API request completed in 3.75 seconds
2025-10-14 19:55:37,008 - DEBUG - Raw model response: {"labels": [0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,1,0,0,0,0,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0]}
2025-10-14 19:55:37,008 - INFO - Successfully processed 47 labels
2025-10-14 19:55:37,013 - ERROR - Error processing paper Towards Equitable Representation in Text_to_Image Synthesis Models with the Cross_Cultural Understanding Benchmark _CCUB_ Dataset: 'int' object has no attribute 'capitalize'
2025-10-14 19:55:37,013 - ERROR - No valid results obtained from any paper
2025-10-14 20:46:07,939 - INFO - Initializing ResearchDatasetEvaluator
2025-10-14 20:46:07,965 - INFO - Starting data loading process
2025-10-14 20:46:07,967 - DEBUG - Processing paper: A Multilingual Multi_Target Dataset for Stance Detection
2025-10-14 20:46:07,967 - DEBUG - Processed 139 sentences for paper A Multilingual Multi_Target Dataset for Stance Detection
2025-10-14 20:46:07,967 - DEBUG - Processing paper: Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation
2025-10-14 20:46:07,967 - DEBUG - Processed 256 sentences for paper Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation
2025-10-14 20:46:07,967 - DEBUG - Processing paper: Amazon_M2_ A Multilingual Multi_locale Shopping Session Dataset for Recommendation and Text Generation
2025-10-14 20:46:07,967 - DEBUG - Processed 121 sentences for paper Amazon_M2_ A Multilingual Multi_locale Shopping Session Dataset for Recommendation and Text Generation
2025-10-14 20:46:07,967 - DEBUG - Processing paper: AntM 2 C_ A Large Scale Dataset For Multi_Scenario Multi_Modal CTR Prediction
2025-10-14 20:46:07,967 - DEBUG - Processed 182 sentences for paper AntM 2 C_ A Large Scale Dataset For Multi_Scenario Multi_Modal CTR Prediction
2025-10-14 20:46:07,967 - DEBUG - Processing paper: AU_AIR_ A Multi_modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance
2025-10-14 20:46:07,967 - DEBUG - Processed 170 sentences for paper AU_AIR_ A Multi_modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance
2025-10-14 20:46:07,967 - DEBUG - Processing paper: AV_NeRF_ Learning Neural Fields for Real_World Audio_Visual Scene Synthesis
2025-10-14 20:46:07,967 - DEBUG - Processed 67 sentences for paper AV_NeRF_ Learning Neural Fields for Real_World Audio_Visual Scene Synthesis
2025-10-14 20:46:07,967 - DEBUG - Processing paper: BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment
2025-10-14 20:46:07,967 - DEBUG - Processed 286 sentences for paper BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment
2025-10-14 20:46:07,967 - DEBUG - Processing paper: Comprehensive Named Entity Recognition on CORD_19 with Distant or Weak Supervision
2025-10-14 20:46:07,967 - DEBUG - Processed 25 sentences for paper Comprehensive Named Entity Recognition on CORD_19 with Distant or Weak Supervision
2025-10-14 20:46:07,967 - DEBUG - Processing paper: DCQA_ DOCUMENT_LEVEL CHART QUESTION ANSWERING TOWARDS COMPLEX REASONING AND COMMON_SENSE UNDERSTANDING
2025-10-14 20:46:07,967 - DEBUG - Processed 179 sentences for paper DCQA_ DOCUMENT_LEVEL CHART QUESTION ANSWERING TOWARDS COMPLEX REASONING AND COMMON_SENSE UNDERSTANDING
2025-10-14 20:46:07,967 - DEBUG - Processing paper: Debate Helps Supervise Unreliable Experts
2025-10-14 20:46:07,967 - DEBUG - Processed 75 sentences for paper Debate Helps Supervise Unreliable Experts
2025-10-14 20:46:07,967 - DEBUG - Processing paper: Distilling Large Language Models for Matching Patients to Clinical Trials
2025-10-14 20:46:07,967 - DEBUG - Processed 91 sentences for paper Distilling Large Language Models for Matching Patients to Clinical Trials
2025-10-14 20:46:07,967 - DEBUG - Processing paper: ECMD_ An Event_Centric Multisensory Driving Dataset for SLAM
2025-10-14 20:46:07,967 - DEBUG - Processed 170 sentences for paper ECMD_ An Event_Centric Multisensory Driving Dataset for SLAM
2025-10-14 20:46:07,967 - DEBUG - Processing paper: Inline Citation Classification using Peripheral Context and Time_evolving Augmentation
2025-10-14 20:46:07,967 - DEBUG - Processed 35 sentences for paper Inline Citation Classification using Peripheral Context and Time_evolving Augmentation
2025-10-14 20:46:07,967 - DEBUG - Processing paper: llm_japanese_dataset v0_ Construction of Japanese Chat Dataset for Large Language Models and its Methodology
2025-10-14 20:46:07,967 - DEBUG - Processed 67 sentences for paper llm_japanese_dataset v0_ Construction of Japanese Chat Dataset for Large Language Models and its Methodology
2025-10-14 20:46:07,967 - DEBUG - Processing paper: Re 3 Dial_ Retrieve_ Reorganize and Rescale Conversations for Long_Turn Open_Domain Dialogue Pre_training
2025-10-14 20:46:07,967 - DEBUG - Processed 108 sentences for paper Re 3 Dial_ Retrieve_ Reorganize and Rescale Conversations for Long_Turn Open_Domain Dialogue Pre_training
2025-10-14 20:46:07,967 - DEBUG - Processing paper: Syn2Real_ Forgery Classification via Unsupervised Domain Adaptation
2025-10-14 20:46:07,967 - DEBUG - Processed 122 sentences for paper Syn2Real_ Forgery Classification via Unsupervised Domain Adaptation
2025-10-14 20:46:07,967 - DEBUG - Processing paper: Topic Shift Detection in Chinese Dialogues_ Corpus and Benchmark
2025-10-14 20:46:07,967 - DEBUG - Processed 146 sentences for paper Topic Shift Detection in Chinese Dialogues_ Corpus and Benchmark
2025-10-14 20:46:07,967 - DEBUG - Processing paper: Towards Equitable Representation in Text_to_Image Synthesis Models with the Cross_Cultural Understanding Benchmark _CCUB_ Dataset
2025-10-14 20:46:07,967 - DEBUG - Processed 47 sentences for paper Towards Equitable Representation in Text_to_Image Synthesis Models with the Cross_Cultural Understanding Benchmark _CCUB_ Dataset
2025-10-14 20:46:07,967 - INFO - Starting evaluation of 18 papers
2025-10-14 20:46:07,968 - INFO - Evaluating paper 1/18: A Multilingual Multi_Target Dataset for Stance Detection
2025-10-14 20:46:07,968 - INFO - Starting model prediction
2025-10-14 20:46:07,968 - INFO - Attempt 1 of 5
2025-10-14 20:46:08,061 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-7379881f-6c21-4442-83f0-6db19f5dd0d4', 'json_data': {'messages': [{'role': 'system', 'content': '你是一个专门用于识别学术论文中数据集描述的助手。你需要判断每个输入的句子是否描述了研究中使用的数据集。请确保：\n    1. 只输出JSON格式的结果\n    2. 结果格式必须为 {"labels": [0,1,0,...]}\n    3. 不要输出任何其他文字或解释'}, {'role': 'user', 'content': '你是一位专业的学术论文数据集描述识别专家。请仔细分析下面这篇论文中的每个句子，判断它们是否描述了本研究特定构建的数据集信息。\n\n    论文标题: A Multilingual Multi_Target Dataset for Stance Detection\n\n    判断标准:\n    判断为1(数据集描述)的标准：\n    句子必须直接描述本研究特定构建的数据集,包括:\n    1. 该数据集的构建过程和方法\n    2. 该数据集的具体构成和规模\n    3. 该数据的来源和收集方式\n    4. 该数据的预处理步骤\n    5. 该数据集的可获取方式(如发布地址)\n    6. 或者其他直接或者间接描述了该数据集的句子\n\n    所有其他类型的句子均标记为0。\n    \n    请对每个句子进行分析，返回一个JSON格式的标签数组。格式要求：\n    1. 必须是有效的JSON格式\n    2. 只包含labels字段，值为0和1组成的数组\n    3. 数组长度必须与句子数量相同\n    4. 示例格式：{"labels": [0,1,0,1,...]}\n\n    需要判断的句子：\n\n    1. Abstract: We extract a large-scale stance detection dataset from comments written by candidates of elections in Switzerland.\n2. The dataset consists of German, French and Italian text, allowing for a cross-lingual evaluation of stance detection.\n3. It contains 67 000 comments on more than 150 political issues (targets).\n4. Unlike stance detection models that have specific target issues, we use the dataset to train a single model on all the issues.\n5. To make learning across targets possible, we prepend to each instance a natural question that represents the target (e.g."Do you support X?").\n6. Baseline results from multilingual BERT show that zero-shot crosslingual and cross-target transfer of stance detection is moderately successful with this approach.\n7. In recent years many datasets have been created for the task of automated stance detection, advancing natural language understanding systems for political science, opinion research and other application areas.\n8. Typically, such benchmarks(Mohammad et al., 2016a)are composed of short pieces of text commenting on politicians or public issues and are manually annotated with their stance towards a target entity (e.g.Climate Change, or Trump).\n9. However, they are limited in scope on multiple levels(Küçük and Can, 2020).\n10. First of all, it is questionable how well current stance detection methods perform in a crosslingual setting, as the multilingual datasets avail-able today are relatively small, and specific to a single target(Taulé et al., 2017(Taulé et al., , 2018)).\n11. Furthermore, specific models tend to be developed for each single target or pair of targets(Sobhani et al., 2017).\n12. Concerns have been raised that cross-target performance is often considerably lower than fully supervised performance(Küçük and Can, 2020).\n13. In this paper we propose a much larger dataset that combines multilinguality and a multitude of topics and targets.\n14. X-stance comprises more than 150 questions about Swiss politics and more than 67k answers given by candidates running for political office in Switzerland.\n15. Questions are available in four languages: English, Swiss Standard German, French, and Italian.\n16. The language of a comment depends on the candidate\'s region of origin.\n17. We have extracted the data from the voting advice application Smartvote.\n18. Candidates respond to questions mainly in categorical form (yes / rather yes / rather no / no).\n19. They can also submit a freetext comment to justify or explain their categorical answer.\n20. An example is given in Figure1.\n21. We transform the dataset into a stance detection task by interpreting the question as a naturallanguage representation of the target, and the commentary as the input to be classified.\n22. The dataset is split into a multilingual training set and into several test sets to evaluate zeroshot cross-lingual and cross-target transfer.\n23. To provide a baseline, we fine-tune a multilingual BERT model(Devlin et al., 2019)on X-stance.\n24. We show that the baseline accuracy is comparable to previous stance detection benchmarks while leaving ample room for improvement.\n25. In addition, the model can generalize to a degree both crosslingually and in a cross-target setting.\n26. We have made the dataset and the code for reproducing the baseline models publicly available.\n27. Figure1: Example of a question and two answers in the X-stance dataset.\n28. The answers were submitted by electoral candidates on a voting advice website.\n29. The author of the German comment was in favor of the issue; the author of the French comment against.\n30. Both authors use comments to explain their respective stance.\n31. Provenance We downloaded the questions and answers via the Smartvote API 2 .\n32. The downloaded data cover 175 communal, cantonal and national elections between 2011 and 2020.\n33. All candidates in an election who participate in Smartvote are asked the same set of questions, but 2 https://smartvote.chdepending on the locale they see translated versions of the questions.\n34. They can answer each question with either \'yes\', \'rather yes\', \'rather no\', or \'no\'.\n35. They can supplement each answer with a comment of at most 500 characters.\n36. The questions asked on Smartvote have been edited by a team of political scientists.\n37. They are intended to cover a broad range of political issues relevant at the time of the election.\n38. A detailed documentation of the design of Smartvote and the editing process of the questions is provided byThurman and Gasser (2009).\n39. Preprocessing We merged the two labels on each pole into a single label: \'yes\' and \'rather yes\' were combined into \'favor\'; \'rather no\', or \'no\' into \'against\'.\n40. This improves the consistency of the data and the comparability to previous stance detection datasets.\n41. We did not further preprocess the text of the comments.\n42. Language Identification As the API does not provide the language of comments, we employed a language identifier to automatically annotate this information.\n43. We used the langdetect library(Shuyo, 2010).\n44. For each responder we classified all the comments jointly, assuming that responders did not switch code during the answering of the questionnaire.\n45. We applied the identifier in a two-step approach.\n46. In the first run we allowed the identifier to output all 55 languages that it supports out of the box, plus Romansh, the fourth official language in Switzerland3.\n47. We found that no Romansh comments were detected and that all unexpected outputs were misclassifications of German, French or Italian comments.\n48. We further concluded that little or no Swiss German comments are in the dataset; otherwise, some of them would have manifested themselves via misclassifications (e.g. as Dutch).\n49. In the second run, drawing from these conclusions, we restricted the identifier\'s set of choices to English, French, German and Italian.\n50. Filtering We pre-filtered the questions and answers to improve the quality of the dataset.\n51. In the right column the model encounters unseen answers to unseen questions within an unseen topic.\n52. The two test sets in parentheses are too small for a significant evaluation. questions and corresponding answers pertaining to national elections were included.\n53. In the context of communal and cantonal elections, candidates have answered both local questions and a subset of the national questions.\n54. Of those elections, we only considered answers to the questions that also had been asked in a national election.\n55. They were only used to augment the training set while the validation and test sets were restricted to answers from national elections.\n56. We discarded the fewer than 20 comments classified as English.\n57. Furthermore, we discarded instances that met any of the following conditions: • Question is not a closed question or does not address a clearly defined political issue.\n58. • No comment was submitted by the candidate or the comment is shorter than 50 characters.\n59. • Comment starts with "but" or a similar indicator that the comment is not self-contained.\n60. • Comment contains a URL.\n61. In total, a fifth of the comments were filtered out.\n62. Topics The questions have been organized by the Smartvote editors into categories (such as "Economy").\n63. We further consolidated the predefined categories into 12 broad topics (Table1).\n64. Compliance The dataset is shared under a CC BY-NC 4.0 license.\n65. Copyright remains with www.smartvote.ch.\n66. Given the sensitive nature of the data, we increase the anonymity of the data by hashing the respondents\' IDs.\n67. No personal attributes of the respondents are included in the dataset.\n68. We provide a data statement(Bender and Friedman, 2018)in Appendix B.\n69. We held out the topics "Healthcare" and "Political System" from the training data and created a separate cross-topic test set that contains the questions and answers related to those topics.\n70. Furthermore, in order to test cross-question generalization performance within previously seen topics, we manually selected 16 held-out questions that are distributed over the remaining 10 topics.\n71. We selected the held-out questions manually because we wanted to make sure that they are truly unseen and that no paraphrases of the questions are found in the training set.\n72. We designated Italian as a test-only language, since relatively few comments have been written in Italian.\n73. From the remaining German and French data we randomly selected a percentage of respondents as validation or as test respondents.\n74. As a result we received one training set, one validation set and four test sets.\n75. The sizes of the sets are listed in Table 2.\n76. We did not consider test sets that are cross-lingual and cross-target at the same time, as they would have been too small to yield significant results.\n77. We evaluate four baselines to obtain an impression of the difficulty of the task.\n78. The first pair of baselines uses the most frequent class in the training set for prediction.\n79. Specifically, the global majority class baseline predicts the most frequent class across all training targets while the target-wise majority class baseline predicts the class that is most frequent for a given target question.\n80. The latter can only be applied to the intra-target test sets.\n81. As a second baseline, we train a fastText bag-ofwords linear classifier(Joulin et al., 2017).\n82. For each comment, we select the translation of the question that matches its language, and concatenate it to the comment.\n83. We tokenize the text using the Europarl preprocessing tools(Koehn, 2005).\n84. The \'against\' class was slightly upsampled in the training data so that the classes are balanced when summing over all questions and topics.\n85. We use the standard settings provided by the fastText library.\n86. The word vectors were set to a size of 300.\n87. We do not initialize them with pre-trained multilingual embeddings since preliminary experiments did not show a beneficial effect.\n88. As our main baseline model we fine-tune multilingual BERT (M-BERT) on the task(Devlin et al., 2019)which has been pre-trained jointly in 104 languages 5 and has established itself as a state of the art for various multilingual tasks(Wu and Dredze, 2019;Pires et al., 2019).\n89. Within the field of stance detection, BERT can outperform both feature-based and other neural approaches in a monolingual English setting(Ghosh et al., 2019).\n90. Architecture In the context of BERT we interpret the X-stance task as sequence pair classification inspired by natural language inference tasks(Bowman et al., 2015).\n91. We follow the procedure outlined byDevlin et al. (2019)for such tasks.\n92. We designate the question as segment A and the comment as segment B.\n93. The two segments are separated with the special token [SEP], and the special token [CLS] is prepended to the sequence.\n94. The final hidden state corresponding to [CLS] is then classified by a linear layer.\n95. We fine-tune the full model with a cross-entropy loss, using the AllenNLP library(Gardner et al., 2018)as a basis for our implementation.\n96. Training As above, we balanced out the number of classes in the training set.\n97. We use a batch size of 16 and a maximum sequence length of 512 subwords, and performed a grid search over the following hyperparameters based on the validation accuracy: • Learning rate: 5e-5, 3e-5, 2e-5 No Italian samples were seen during training, making this a case of zero-shot cross-lingual transfer.\n98. The scores are reported as the macro-average of the F1scores for \'favor\' and for \'against\'.\n99. The grid search was repeated independently for every variant that we test in the following subsections.\n100. Furthermore, the standard recommendations for fine-tuning BERT were used: Adam with β 1 = 0.9 and β 2 = 0.999; an L2 weight decay of 0.01; a learning rate warmup over the first 10% of the steps; and a linear decay of the learning rate.\n101. A dropout probability of 0.1 was set on all layers.\n102. Results Table3shows the results for the crosslingual setting.\n103. M-BERT performs consistently better than the previous baselines.\n104. Even the zeroshot performance in Italian, while significantly lower than the supervised scores, is much better than the target-wise majority class baseline.\n105. Results for the cross-target setting are given in Table4.\n106. Similar to the cross-lingual setting, model performance drops in the cross-target setting, but M-BERT remains the strongest baseline and easily surpasses the majority class baselines.\n107. Furthermore, the cross-question score of M-BERT is slightly lower than the cross-topic score.\n108. The default setup preserves horizontal language consistency in that the language of the questions always corresponds to the language of the comments.\n109. For example, the Italian test instances are combined with the Italian version of the questions, even though during training the model has only ever seen the German and French version of them.\n110. An alternative concept is vertical language consistency, whereby the questions are consistently presented in one language, regardless of the comment.\n111. To test whether horizontal or vertical consistency is more helpful, we train and evaluate M-BERT on a dataset variant where all questions are in their English version.\n112. We chose English as a lingua franca because it had the largest share of data during the pre-training of M-BERT.\n113. Cross-topic Results are shown in Table5.\n114. While the effect is negligible in most settings, cross-lingual performance increases when all questions are in English.\n115. In order to rule out that only the questions or only the comments are necessary to optimally solve the task, we conduct some additional experiments: • Only use a single segment containing the comment, removing the questions from the training and test data (missing questions).\n116. • Only use the question and remove the comment (missing comments).\n117. In both cases the performance decreases across all evaluation settings (Table5).\n118. The loss in performance is much higher when comments are missing, indicating that the comments contain the most important information about stance.\n119. As can be expected, the score achieved without comments is only slightly different from the target-wise majority class baseline.\n120. But there is also a loss in performance when the questions are missing, which underlines the importance of pairing both pieces of text.\n121. The effect of missing questions is especially strong in the supervised and cross-lingual settings.\n122. To illustrate this, we provide in TableA8some examples of comments that occur with multiple different targets in the training set.\n123. Those examples can explain why the target can be essential for disambiguating a stance detection problem.\n124. On the other hand, the effect of omitting the questions is less pronounced in the cross-target settings.\n125. The above single-segment experiments tell us that both the comment and the question provide crucial information.\n126. But it is possible that the M-BERT model, even though trained on both segments, mainly looks at a single segment at test time.\n127. To rule this out, we probe the model with randomized data at test time: • Test the model on versions of the test sets where the comments remain in place but the questions are shuffled randomly (random questions).\n128. We make sure that the random questions come from the same test set and language as the original questions.\n129. • Keep the questions in place and randomize the comments (random comments).\n130. Again we shuffle the comments only within test set boundaries.\n131. The results in Table5show that the performance of the model decreases in both cases, confirming that it learns to take into account both segments.\n132. 4.6 How Important are Spelled-Out Targets?\n133. Finally we test whether the target really needs to be represented by natural language (e.g."Do you support X?").\n134. An alternative is to represent the target with a trainable embedding instead.\n135. In order to fit target embeddings smoothly into our architecture, we represent each target type with a different reserved symbol from the M-BERT vocabulary.\n136. Segment A is then set to this symbol instead of a natural language question.\n137. The results for this experiment are listed in the bottom row of Table5.\n138. An M-BERT model that learns target embeddings instead of encoding a question performs clearly worse in the supervised and cross-lingual settings.\n139. From this we conclude that spelled-out natural language questions provide important linguistic detail that can help in stance detection.\n\n请严格按照JSON格式返回：{"labels": [0,1,0,...]}，不要包含任何其他文字。'}], 'model': 'qwen-plus', 'response_format': {'type': 'json_object'}}}
2025-10-14 20:46:08,066 - DEBUG - Sending HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
2025-10-14 20:46:08,066 - DEBUG - connect_tcp.started host='dashscope.aliyuncs.com' port=443 local_address=None timeout=5.0 socket_options=None
2025-10-14 20:46:08,069 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7fb016122280>
2025-10-14 20:46:08,069 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x7fb016a88640> server_hostname='dashscope.aliyuncs.com' timeout=5.0
2025-10-14 20:46:08,170 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7fb016122340>
2025-10-14 20:46:08,170 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-10-14 20:46:08,170 - DEBUG - send_request_headers.complete
2025-10-14 20:46:08,170 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-10-14 20:46:08,171 - DEBUG - send_request_body.complete
2025-10-14 20:46:08,171 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-10-14 20:46:15,919 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'vary', b'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding'), (b'x-request-id', b'7d036543-d81d-4b6c-8ea5-f50d441feb23'), (b'x-dashscope-call-gateway', b'true'), (b'content-type', b'application/json'), (b'req-cost-time', b'7707'), (b'req-arrive-time', b'1760445958221'), (b'resp-start-time', b'1760445965928'), (b'x-envoy-upstream-service-time', b'7661'), (b'set-cookie', b'acw_tc=7d036543-d81d-4b6c-8ea5-f50d441feb234992a61a500f600df13d64c44db996f4;path=/;HttpOnly;Max-Age=1800'), (b'content-encoding', b'gzip'), (b'date', b'Tue, 14 Oct 2025 12:46:05 GMT'), (b'server', b'istio-envoy'), (b'transfer-encoding', b'chunked')])
2025-10-14 20:46:15,920 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-14 20:46:15,920 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-10-14 20:46:15,920 - DEBUG - receive_response_body.complete
2025-10-14 20:46:15,920 - DEBUG - response_closed.started
2025-10-14 20:46:15,921 - DEBUG - response_closed.complete
2025-10-14 20:46:15,921 - DEBUG - HTTP Response: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "200 OK" Headers({'vary': 'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding', 'x-request-id': '7d036543-d81d-4b6c-8ea5-f50d441feb23', 'x-dashscope-call-gateway': 'true', 'content-type': 'application/json', 'req-cost-time': '7707', 'req-arrive-time': '1760445958221', 'resp-start-time': '1760445965928', 'x-envoy-upstream-service-time': '7661', 'set-cookie': 'acw_tc=7d036543-d81d-4b6c-8ea5-f50d441feb234992a61a500f600df13d64c44db996f4;path=/;HttpOnly;Max-Age=1800', 'content-encoding': 'gzip', 'date': 'Tue, 14 Oct 2025 12:46:05 GMT', 'server': 'istio-envoy', 'transfer-encoding': 'chunked'})
2025-10-14 20:46:15,921 - DEBUG - request_id: 7d036543-d81d-4b6c-8ea5-f50d441feb23
2025-10-14 20:46:15,928 - DEBUG - API request completed in 7.96 seconds
2025-10-14 20:46:15,928 - DEBUG - Raw model response: {"labels": [1,1,1,0,1,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]}
2025-10-14 20:46:15,928 - INFO - Successfully processed 103 labels
2025-10-14 20:46:15,928 - ERROR - Label count mismatch for A Multilingual Multi_Target Dataset for Stance Detection
2025-10-14 20:46:15,928 - INFO - Evaluating paper 2/18: Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation
2025-10-14 20:46:15,928 - INFO - Starting model prediction
2025-10-14 20:46:15,928 - INFO - Attempt 1 of 5
2025-10-14 20:46:15,929 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-e9d227d6-58c3-4bb1-b2c9-ed03700644a9', 'json_data': {'messages': [{'role': 'system', 'content': '你是一个专门用于识别学术论文中数据集描述的助手。你需要判断每个输入的句子是否描述了研究中使用的数据集。请确保：\n    1. 只输出JSON格式的结果\n    2. 结果格式必须为 {"labels": [0,1,0,...]}\n    3. 不要输出任何其他文字或解释'}, {'role': 'user', 'content': '你是一位专业的学术论文数据集描述识别专家。请仔细分析下面这篇论文中的每个句子，判断它们是否描述了本研究特定构建的数据集信息。\n\n    论文标题: Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation\n\n    判断标准:\n    判断为1(数据集描述)的标准：\n    句子必须直接描述本研究特定构建的数据集,包括:\n    1. 该数据集的构建过程和方法\n    2. 该数据集的具体构成和规模\n    3. 该数据的来源和收集方式\n    4. 该数据的预处理步骤\n    5. 该数据集的可获取方式(如发布地址)\n    6. 或者其他直接或者间接描述了该数据集的句子\n\n    所有其他类型的句子均标记为0。\n    \n    请对每个句子进行分析，返回一个JSON格式的标签数组。格式要求：\n    1. 必须是有效的JSON格式\n    2. 只包含labels字段，值为0和1组成的数组\n    3. 数组长度必须与句子数量相同\n    4. 示例格式：{"labels": [0,1,0,1,...]}\n\n    需要判断的句子：\n\n    1. Abstract: Digital humans have witnessed extensive applications in various domains, necessitating related quality assessment studies.\n2. However, there is a lack of comprehensive digital human quality assessment (DHQA) databases.\n3. To address this gap, we propose SJTU-H3D, a subjective quality assessment database specifically designed for full-body digital humans.\n4. It comprises 40 high-quality reference digital humans and 1,120 labeled distorted counterparts generated with seven types of distortions.\n5. The SJTU-H3D database can serve as a benchmark for DHQA research, allowing evaluation and refinement of processing algorithms.\n6. Further, we propose a zero-shot DHQA approach that focuses on no-reference (NR) scenarios to ensure generalization capabilities while mitigating database bias.\n7. Our method leverages semantic and distortion features extracted from projections, as well as geometry features derived from the mesh structure of digital humans.\n8. Specifically, we employ the Contrastive Language-Image Pre-training (CLIP) model to measure semantic affinity and incorporate the Naturalness Image Quality Evaluator (NIQE) model to capture low-level distortion information.\n9. Additionally, we utilize dihedral angles as geometry descriptors to extract mesh features.\n10. By aggregating these measures, we introduce the Digital Human Quality Index (DHQI), which demonstrates significant improvements in zeroshot performance.\n11. The DHQI can also serve as a robust baseline for DHQA tasks, facilitating advancements in the field.\n12. The database and the code are available at https://github.com/zzc-1998/SJTU-H3D.\n13. Fig.1.Motovation of our works.\n14. Unlike 2D images/videos, collection of 3D digital humans is more difficult and expensive.\n15. Therefore there is a lack of subjective databases for 3D digital humans currently.\n16. To tackle this issue, we propose the first perceptual quality assessment database for full-body digital humans (SJTU-H3D).\n17. Furthermore, in contrast to the numerous large-scale image/video quality assessment (I/VQA) databases that facilitate data-driven methodologies, supervised methods can be easily bothered by the bias of the limited number of DHQA databases, affecting generalization ability.\n18. Thus we propose a zero-shot no-reference quality assessment method to address this concern. humans.\n19. Regrettably, acquisition of digital human models is a laborious and costly process compared with 2D media such as images and videos, requiring specialized three-dimensional (3D) scanning devices and professional post-production, which makes it quite difficult to carry out digital human quality assessment (DHQA) databases.\n20. Therefore, few works about subjective DHQA have been carried out in the literature.\n21. Then the absence of large-scale subjective experiments for assessing the visual quality of digital humans further hinders progress in this domain.\n22. Therefore, in this paper, we propose a comprehensive subjective quality assessment database (SJTU-H3D) targeted at digital humans, aiming to address this research gap and contribute to the advancement of DHQA.\n23. The SJTU-H3D database introduced in this study comprises 40 high-quality reference digital humans, represented by textured meshes in full-body format, and the database includes 1,120 distorted digital humans that have been generated using seven different types of distortions.\n24. The perceptual mean opinion scores (MOSs) of these distorted digital humans are collected through a meticulously controlled subjective experiment.\n25. Notably, the SJTU-H3D database is the first large-scale database specifically designed for digital human quality assessment (DHQA) that focuses on full-body representations.\n26. The primary objective of this database is to advance the research and development of DHQA within the scientific community.\n27. Furthermore, it serves as an ideal platform for evaluating and refining various processing algorithms, including but not being limited to denoising and compression techniques.\n28. By providing a comprehensive database consisting of high-quality reference models and distorted counterparts, the proposed SJTU-H3D database offers researchers and practitioners an opportunity to explore and enhance their DHQA methodologies.\n29. The availability of such a resource is expected to significantly contribute to the growth and advancement of the DHQA research community.\n30. During recent years, data-driven image and video quality assessment (I/VQA) approaches[2],[3],[4],[5]have garnered significant attention and have demonstrated remarkable performance in various application domains.\n31. The success of these approaches can be partly attributed to the availability of largescale I/VQA databases such as the SPAQ database (containing 11,125 labeled images)[6]and the LSVQ database (comprising up to 38,811 annotated videos)[7].\n32. These databases have also contributed to ensuring the generalization capability and robustness of data-driven methods.\n33. However, in the realm of DHQA research, the availability of suitable perceptual quality assessment databases is limited.\n34. With the exception of the proposed SJTU-H3D database, only one perceptual quality assessment database, DHHQA[8], focusing solely on digital human heads rather than full-body representations, exists.\n35. This scarcity of databases makes it challenging to develop datadriven DHQA methods and ensure their generalization ability in practical scenarios.\n36. Hence, this challenge serves as a motivation for us to devise a zero-shot DHQA method that does not necessitate training on labeled DHQA databases.\n37. To cater to most practical applications where pristine references may not be readily available, our focus is only on no-reference (NR) methods.\n38. To extract both semantic and distortion features for evaluating the visual quality of digital humans, we employ projection rendering techniques.\n39. From a semantic perspective, we utilize the Contrastive Language-Image Pre-training (CLIP) model[9]to measure the correlation between the input projections and quality-related texts.\n40. Our hypothesis is that high-quality digital human projections should exhibit a strong correlation with positive quality-related texts and a weak correlation with negative ones.\n41. To determine the quality levels of the input projections, we design several positive-negative text pairs.\n42. The semantic affinity quality measure is then derived by computing the difference in affinity between positive and negative texts.\n43. However, CLIP operates on low-resolution images, which limits its ability to capture low-level distortion information.\n44. To address this limitation, we incorporate the completely blind Naturalness Image Quality Evaluator (NIQE)[10]to extract low-level quality representations from the raw resolution.\n45. To further enhance the accuracy of quality prediction, we also extract features from the mesh modality.\n46. For robustness and effectiveness, we choose the dihedral angle as the geometry descriptor, as it has been widely recognized for effectively capturing geometric features relevant to visual quality[11],[12],[13],[14]and its values are confined within the range of [0, π].\n47. By analyzing the changing tendency of dihedral angles corresponding to geometry compression and simplification levels, we average-pool the dihedral angles to derive the geometry loss quality measure.\n48. Finally, all three quality measures (semantic affinity quality measure, spatial naturalness quality measure, and geometry loss quality measure) are aggregated using a sum function to form the proposed Digital Human Quality Index (DHQI).\n49. Experimental results demonstrate that DHQI significantly improves zero-shot performance and even achieves competitiveness with supervised methods.\n50. In summary, our contributions are as follows: • We propose the first large-scale full-body DHQA database, SJTU-H3D, which consists of 40 high-quality digital humans represented by textured meshes and 1,120 distorted digital humans generated by 7 types of distortions.\n51. 40 human subjects are invited and a total of 44,800 ratings are collected to gather the mean opinion scores (MOSs) for 1,120 distorted digital humans.\n52. In this section, we give a brief introduction to the development of 3D model quality assessment (3DQA) and noreference image quality assessment (NR-IQA) methods.\n53. A. 3DQA Development 1) 3DQA Databases: Early subjective 3D quality assessment (3DQA) databases primarily employ colorless point clouds and are relatively small in scale[20],[21],[22].\n54. However, recent efforts have been directed towards addressing the challenge of assessing visual quality in colored 3D models, resulting in the development of substantial 3DQA databases[20],[21],[22],[15],[16],[17],[18],[19].\n55. A detailed comparison between these databases and the proposed database is presented in TableI.\n56. From the table, it is evident that the recent 3DQA databases, with the exception of DHHQA, encompass general 3D objects and do not specifically focus on 3D digital humans.\n57. Although the DHHQA database comprises real human heads, it neglects the consideration of the body part.\n58. This highlights the significance of the proposed SJTU-H3D database.\n59. 2) 3DQA Methods: In the field of 3D quality assessment (3DQA), metrics can be broadly categorized into model-based and projection-based methods.\n60. Model-based methods[23],[24],[11],[25],[26],[27],[28],[29],[30],[31]involve extracting features directly from the 3D model, which offers the advantage of being viewpoint-invariant and relatively straightforward.\n61. However, due to the inherent complexity of 3D models, these methods can be computationally expensive and time-consuming.\n62. On the other hand, projection-based methods[15],[32],[14],[33],[34]infer the visual quality of a 3D model based on its corresponding projections.\n63. These methods leverage mature and successful 2D media analysis tools, which often lead to excellent performance.\n64. However, projection-based methods are highly dependent on the selection of viewpoints and can be susceptible to instability when subjected to various rendering setups.\n65. More recently,[36], which utilizes natural scene statistics (NSS) in the spatial domain to analyze image quality.\n66. CPBD[37]estimates blur levels by computing the cumulative probability of blur detection.\n67. BMPRI[38]predicts image quality by generating multiple pseudo-reference images obtained through further degradation of the distorted image and comparing their similarities.\n68. NFERM[39]investigates image quality using the free energy principle.\n69. Deep learning-based IQA methods have gained momentum with the advancement of deep neural networks.\n70. DBCNN[40]consists of two streams of deep neural networks to address both synthetic and authentic distortions.\n71. HyperIQA[41]employs a self-adaptive hyper network to handle challenges arising from distortion diversity and content variation in IQA tasks.\n72. MUSIQ[42]utilizes a multi-scale image quality transformer to represent image quality at different levels of granularity.\n73. StairIQA[43]hierarchically integrates features extracted from intermediate layers to leverage low-level and high-level visual information.\n74. 2) Zero-shot NR-IQA: Zero-shot IQA methods, also known as opinion-unaware methods, have emerged, which do not rely on training on subjective-rated quality assessment databases and can operate on unseen images.\n75. The earliest zero-shot NR-IQA methods are NIQE[10]and IL-NIQE[44].\n76. NIQE extracts handcrafted natural scene statistics (NSS) features from raw-resolution images and quantifies naturalness quality by computing the Multivariate Gaussian (MVG) distance to high-quality images.\n77. IL-NIQE enhances the feature set by incorporating additional quality-aware features, including gradient features, log Gabor filter responses, and color statistics.\n78. In this section, we mainly present the construction details of the proposed SJTU-H3D database, which includes reference collection, reference characterization, distortion generation, and subjective experiment.\n79. In order to ensure the visual quality and content diversity of the reference 3D digital humans, a manual selection process is conducted to choose all reference digital humans from the HumanAlloy1, a wonderful platform that provides high-quality 3D humans.\n80. A total of 40 digital humans are purchased and collected for this study.\n81. These digital humans are represented as textured meshes, with texture resolutions of 2048×2048.\n82. Fig.2illustrates the rendered projections of the selected digital humans, and Table II provides detailed information regarding the number of vertices and faces for each model.\n83. The primary objective of our study is to curate a database that exhibits high diversity and generality while minimizing biases associated with the selection of source models.\n84. 1) Geometry Information: In the domain of image quality assessment (IQA), the analysis of spatial information often involves computing the standard deviation of the Sobel-filtered image.\n85. Motivated by this concept, we propose a novel approach to quantify the geometry information by utilizing the standard deviation of the dihedral angles in a mesh.\n86. The dihedral angle is a fundamental metric employed in computer graphics and geometric modeling to characterize the shape and curvature of meshes[11],[12], thus drawing a parallel to the Sobel-filtering process in image analysis.\n87. It denotes the angle between two neighboring faces that share an edge within the mesh, providing valuable insights into the smoothness or sharpness of the surface.\n88. Specifically, the geometry information can be obtained as: where GI represents the geometry information and std(•) stands for the standard deviation function.\n89. By leveraging the standard deviation of dihedral angles, we aim to capture and assess the geometric characteristics of the mesh, enabling a more comprehensive evaluation of its structure and shape.\n90. 2) Colorfulness: To evaluate the color characteristics, we focus solely on the texture map.\n91. Following the common color calculation process[45],[46], we first convert the texture from RGB channels to LAB channels and combine the standard deviation of A and B channels, which can be mathematically expressed as: where CF represents the colorfulness measure, A and B denote the corresponding color channels of the texture.\n92. 3) Characterization Visualization: We apply the extracted geometry information and colorfulness measure to the collection of 40 reference digital humans.\n93. The results are visualized in Fig.3.\n94. The analysis demonstrates that the selected reference 3D digital humans exhibit a wide spectrum of geometry information and colorfulness.\n95. Notably, model #24 positioned in the top-right corner showcases intricate geometry details and vibrant colorfulness.\n96. In contrast, model #15 portrays simpler geometry information and relatively subdued colorfulness.\n97. The proposed measures thoroughly capture the distinctiveness of 3D digital humans concerning their geometry and color characteristics.\n98. It is important to emphasize that these measures are directly computed from the underlying model files, thereby ensuring their stability and viewpoint invariance.\n99. To account for the common sources of distortion, we incorporate distortions arising from both the generation process and the transmission process.\n100. During the generation process, we consider geometry noise resulting from erroneous scanning procedures, as well as color noise introduced by cameras.\n101. Furthermore, compression and simplification techniques are widely employed during the transmission process.\n102. Hence, these factors are also taken into consideration in our assessment.\n103. By considering the full range of distortion sources, we aim to provide a comprehensive evaluation of the quality of 3D digital humans.\n104. Therefore, to degrade the quality of the reference 3D digital humans, we apply seven types of distortions and the specific settings for each distortion type are listed in TableIII.\n105. We manually select the distortion parameters to cover most visual quality range and the details are illustrated as follows: • Geometry Noise (GN): Gaussian noise with standard deviations σ g of 0.05, 0.1, 0.15, and 0.2 is added to the vertices\' geometry coordinates of the digital humans.\n106. In accordance with the recommended procedure outlined in[15],[16], passive watching is chosen over interactive watching for the subjective experiment to mitigate potential viewing bias.\n107. The 3D digital humans are rendered into video sequences for exhibition purposes.\n108. The open3d library is utilized to generate the projections[52].\n109. The rendering window is configured with a resolution of 1080 × 1920.\n110. To capture the video frames, a horizontal and a vertical circle are employed as the predefined camera paths.\n111. Each 3D digital human is captured at one frame every 3 degree, resulting in a total of 240 frames (360 × 2 ÷ 3).\n112. These frames are then compiled into an 8-second video with a framerate of 30 frames per second.\n113. This approach ensures that the viewers can effectively perceive the significant quality information.\n114. The rendering process is depicted in Fig.5.\n115. 2) Experiment Process: A total of 40 human subjects, comprising 20 males and 20 females, are recruited to participate in the subjective experiment.\n116. Prior to the experiment, a training session is conducted, wherein additional videos generated using the same aforementioned process are presented to familiarize the subjects with the tasks.\n117. The rating process takes place within a well-controlled laboratory environment, maintaining a normal level of illumination.\n118. The viewers are seated at a distance of twice the screen height.\n119. The videos are displayed on an iMac monitor capable of supporting resolutions up to 4096×2304.\n120. The order of video presentations is randomized.\n121. To facilitate the evaluation process, a double stimuli strategy is employed, where the reference and distorted videos are simultaneously displayed on the screen.\n122. The rating interface is excited in Fig.5and the quality score ranges from 0 to 5.\n123. In order to mitigate viewer fatigue, the entire experiment is divided into 20 sessions, with each session featuring 56 digital humans.\n124. Ultimately, a total of 44,800 subjective ratings (1, 120 × 40) are collected.\n125. 3) Subjective Data Analysis: After the subjective experiment, we calculate the z-scores from the raw ratings as follows: where and N i is the number of digital humans judged by subject i.\n126. 500-13[53]standard, ratings from unreliable subjects are excluded from the analysis.\n127. The corresponding z-scores are linearly rescaled to the range of [0, 5].\n128. Finally, the mean opinion scores (MOSs) are computed by averaging the rescaled z-scores.\n129. Fig.6illustrates the distribution of MOSs and the corresponding probability distributions for different distortion types.\n130. Interestingly, the probability distributions reveal that visual quality is less sensitive to varying levels of FS distortions compared to other distortion types.\n131. Even when reducing the face numbers to a ratio of 0.05 (only about 2k faces are preserved), the visual quality score remains higher than other distortions with similar levels.\n132. This observation indicates that visual quality is relatively resilient to FS distortions, implying that the reduction in face complexity may not significantly impact the perceived quality.\n133. In this section, we introduce the three indexes that make up the whole proposed digital human quality index (DHQI), which includes the text-prompted semantic affinity quality measure, spatial naturalness quality measure, and geometry loss quality measure.\n134. These three indexes are then aligned and aggregated into the proposed DHQI quality index.\n135. The framework is exhibited in Fig.7.\n136. We acquire the cube-like projection set of the given digital human as follows: P = ψ(DH), where P represents the set of the 6 rendered projections and ψ(•) stands for the rendering process.\n137. Such rendering process has been employed in the popular point cloud compression standard MPEG VPCC[54]and many other 3DQA works[15],[34].\n138. The projections are utilized as the input information for the text-prompted semantic affinity and spatial naturalness measure.\n139. To assess the perception of quality related to semantic content, specifically evaluating the quality of contents and the ability to discern semantic distortions, we design the text-prompted semantic affinity quality measure.\n140. Inspired by CLIP[9]-based quality assessment tasks[55],[56], we hold the hypothesis that the projections of the high-quality digital humans should have higher affinity with positive qualityrelated descriptions (e.g.good, perfect) and lower affinity with negative quality-related descriptions (e.g.bad, distorted).\n141. 1) Text Prompt Format: In accordance with the official recommendation provided by CLIP[9]and drawing from established practices, our text prompts are designed as a concatenation of three components: a prefix, a description, and a suffix.\n142. To be more precise, the text prompt T corresponding to the raw description D is defined as: T = "a" + D + "projection of 3d human model",(5)where the suffix "projection of 3d human model" is specifically designed to fit the task of DHQA.\n143. This carefully chosen suffix can encourage the CLIP model to prioritize and focus its attention on the detection and evaluation of content-aware distortions that may arise in the context of 3D digital humans.\n144. 2) Description Selection: We have identified descriptions pertaining to quality assessment that encompass broad evaluation aspects to ensure robustness.\n145. In this study, the general quality-related descriptions employed comprise the contrasting pairs of high quality ↔ low quality, good ↔ bad, and perfect ↔ distorted.\n146. The utilization of the high quality ↔ low quality as well as the good ↔ bad text pair assists in directing the attention of the CLIP model towards general subjective impressions.\n147. Conversely, the perfect ↔ distorted pair compels the CLIP model to prioritize the existence of distortions.\n148. 3) Affinity Difference Computation: Given the input image I and text T , the senmantic affinity can be calculated with the assistance of CLIP as: where E I and E T stand for the image and text encoders of CLIP, F I and F T represent the CLIP-encoded features, and A(I, T ) indicates the affinity between the input image and text.\n149. Afterward, the computation of zero-shot quality affinity can be derived from the aforementioned selected descriptions by calculating the disparity between the probabilities assigned to positive and negative textual inputs:\n150. A(P k , T ), where the averaged affinity to the given text T , denoted by A(P, T ), is calculated by CLIP across the six projections P.\n151. In this context, T i + and T i -refer to the positive and negative text descriptions, respectively, from the i-th text pair.\n152. The variable N T represents the total number of text pairs.\n153. Furthermore, A dif f signifies the cumulative difference between the averaged positive and negative affinity.\n154. The sigmoid remapping technique is then used to map the raw difference scores A dif f obtained from perceptual quality evaluation into a range of [0, 1].\n155. This remapping is done based on the guidance provided by the Video Quality Experts Group (VQEG)[57].\n156. Apart from evaluating semantic affinity, we incorporate the use of NIQE (Naturalness Image Quality Evaluator[10]) as a blind quality evaluator to assess the spatial naturalness of the digital humans.\n157. The purpose of employing NIQE is to identify and quantify common low-level distortions encountered in practical digital humans, including Gaussian noise, blur, and JPEG compression artifacts.\n158. By incorporating NIQE alongside semantic affinity evaluation, we aim to complement the assessment of high-level information with an evaluation of low-level technical quality.\n159. The NIQE index operates by quantifying the disparity between the characteristics of the input image features and the anticipated distribution of features observed in "high-quality" images, which are derived from a diverse set of pristine natural images.\n160. Since the raw NIQE scores and the raw affinity difference scores are on different scales, it is necessary to normalize the NIQE scores to facilitate meaningful comparison.\n161. To achieve this, we divide the NIQE scores by a constant value, denoted as c 1 , which effectively restricts the majority of NIQE scores to the range of [0,1].\n162. Consequently, the spatial naturalness quality measure can be computed as follows: where N (P k ) denotes the NIQE value for the k-th projection, N (P) represents the average NIQE value across the 6 projections, and Q N stands for the spatial naturalness quality measure.\n163. It\'s worth noting that the NIQE scores are inversely correlated with quality and the negative sign is incorporated into the sigmoid function, allowing for a consistent interpretation and alignment of the NIQE scores with the quality evaluation framework.\n164. The aforementioned measures are applied to projections, specifically the image modality.\n165. In order to enhance the model\'s understanding of digital humans, it is proposed to directly extract features from the mesh modality to capture the loss in geometry with respect to visual quality.\n166. 1) Descriptor Selection: Various geometry attributes have been utilized to describe the quality-related geometric characteristics of meshes[14], including curvature, dihedral angle, face angle, face area, etc.For the purpose of preserving stability and improving the robustness of the proposed zeroshot method, the dihedral angle is selected as the geometry descriptor for the following reasons: a) Extensive evidence supports the effectiveness of the dihedral angle in describing geometric features relevant to visual quality[11],[12],[13],[14].b).\n167. b) Unlike other geometry attributes, the dihedral angle is invariant to scale.\n168. Its values are confined within the range of [0, π], thereby contributing to its robustness.\n169. The dihedral angle is the angle between two adjacent faces, which can be calculated as the dot product of corresponding normal vectors: where θj π indicates the scaled dihedral angle corresponding to the j-th edge of the mesh, Θ indicates the set of the scaled dihedral angle values, n j1 and n j2 stand for the normal vectors of the two adjacent faces whose co-edge is the j-th edge.\n170. 2) Quality Correlation with Dihedral Angle: Lossy mesh compression and simplification techniques can potentially diminish a mesh\'s structural details, resulting in a smoother and simpler surface representation.\n171. In such cases, the faces comprising the smoother and simpler surface tend to exhibit dihedral angles that approach π, leading to an inherent inclination for larger dihedral angles.\n172. To substantiate this observation, we present the tendencies of the mean values of the dihedral angles in Fig.8, from which we can find a consistent upward trend in dihedral angle means as compression/simplification levels increase.\n173. Therefore, the mean values of the dihedral angle can be generally taken as an indicator of geometry detail loss caused by compression/simplification.\n174. Then geometry loss quality measure can be calculated as: where Q G represents the geometry loss quality measure, Θ indicates the mean value of the dihedral angles, and the negative sign is added to the sigmoid function due to the positive correlation between the dihedral angles\' mean values and compression/simplification levels.\n175. In order to develop a reliable zero-shot perceptual quality index, we adopt a direct aggregation approach wherein we sum up the scale-aligned scores of various indices without performing any fine-tuning processes.\n176. Considering that the Q A , Q N , and Q G have undergone sigmoid rescaling, all three measures are bounded within the range of [0, 1].\n177. Consequently, we define the comprehensive unified DHQI (digital human quality index) as follows: where Q DHQI indicates the final quality values for the digital humans.\n178. V. EXPERIMENT A.\n179. Validation Setup 1) Benchmark Databases: In addition to the proposed SJTU-H3D database, we have incorporated the digital human quality assessment (DHHQA) database[8]as an additional resource for benchmark validation.\n180. The DHHQA database comprises a total of 55 scanned digital human heads that serve as reference samples, along with 1,540 labeled distorted digital human heads.\n181. These distorted samples have been intentionally degraded through the introduction of noise and compression/simplification.\n182. 2) k-fold Cross-Validation: To ensure robust evaluation, we adopt a k-fold cross-validation strategy.\n183. This approach involves dividing the database into k equally sized folds.\n184. The model is then trained on k-1 of these folds and subsequently tested on the remaining fold.\n185. This process is repeated k times, with each fold being used as the test set once.\n186. By averaging the performance across these k iterations, we obtain a more reliable estimate of the model\'s effectiveness, minimizing the impact of random variations.\n187. For both the SJTU-H3D and DHHQA databases, we have selected a value of k = 5 to conduct the k-fold cross-validation, ensuring a balanced evaluation across multiple subsets.\n188. It\'s worth mentioning that there is no content overlap between the training and testing folds.\n189. To facilitate a direct and fair comparison between zeroshot and supervised methods, we validate their performance in the following way.\n190. Zero-shot methods are directly applied to the testing folds, as they do not require any training.\n191. The performance is then averaged across the testing folds and reported as the final performance.\n192. On the other hand, supervised methods undergo training on the training folds and are subsequently tested on the testing folds.\n193. Similar to zero-shot methods, the average performance is calculated and reported as the final performance.\n194. Adopting this methodology enables a direct and unbiased comparison of the performance between zero-shot and supervised methods, insights into their respective strengths and limitations.\n195. 3) Implemetation Details: The cube-like projection process described in Section IV-A is conducted with the assistance of open3d[52]library with a resolution of 1080P.\n196. The white background is cropped out.\n197. The projections are downsampled to 224×224 as the input of the CLIP[9]image encoder.\n198. The ViT-B-32[64]backbone with LAION-2B[65]pretrained weights is utilized as the CLIP model.\n199. To fit the DHHQA database, we replace the suffix "projection of 3d human model" as described in Equation5with "projection of 3d human face".\n200. The scale parameter c 1 constant described in Section IV-C is set as 100.\n201. The supervised training of the proposed DHQA method is conducted with the Support Vector Regression (SVR) model with RBF kernel.\n202. The official source code is used for the competitors and default parameters are maintained.\n203. The default 5-fold crossvalidation is strictly followed for the competitors to make the comparison fair.\n204. In addition, the predicted scores of all the methods are followed by a five-parameter logistic regression to map the scores to the MOS scale.\n205. The competitors\' selection is conducted to ensure high diversity, which includes the zero-shot FR methods, zero-shot NR methods, and the supervised NR methods.\n206. 1) Zero-shot FR Methods: We consider several classical projection-based FR methods: PSNR, SSIM[58], MS-SSIM[59], and GMSD[60].\n207. These methods are applied to the six perpendicular projections, and the resulting scores are averaged and recorded.\n208. Additionally, we incorporate three popular point-based FR metrics proposed by MPEG: PSNR p2po[61], PSNR p2pl[62], and PSNR yuv[63].\n209. For the purpose of validation, we convert the digital human models into point clouds.\n210. Furthermore, we utilize G-LPIPS*[19], which is a projection-based FR metric modified from LPIPS[66]and is designed for textured meshes.\n211. The official pretrained weights are employed for this metric.\n212. 2) Zero-shot NR Methods: These methods comprise CPBD[37], pretrained BRISQUE*[36], NIQE[10], and IL-NIQE[44].\n213. 3) Supervised NR Methods: These methods encompass handcrafted approaches such as BRISQUE[36], NFERM[39], and BMPRI[38], which are supervised using the Support Vector Regression (SVR) model.\n214. Additionally, we include deep learning-based methods, namely DBCNN[40], Hyper-IQA[41], MUSIQ[42], and StaiIQA[43], which have been retrained for our evaluation.\n215. The overall performance on the SJTU-H3D and DHHQA databases are exhibited in TableIV, from which we can draw several conclusions.\n216. 1) Zero-shot Performance: a) Among all the zero-shot methods compared on the SJTU-H3D database, the DHQI method demonstrates superior performance and outperforms them all.\n217. b) Nevertheless, the FR metrics that exhibit the highest performance on the DHHQA database, namely MS-SSIM & GMSD, suffer significant performance degradation when applied to the SJTU-H3D database.\n218. c) In contrast, all the competing zero-shot NR methods consistently exhibit lower performance compared to the proposed DHQI method.\n219. The reason for this disparity lies in the focus of these methods on addressing low-level distortions, which restricts their ability to effectively capture and model high-level semantic quality representations.\n220. By leveraging the semantic affinity quality measure, the DHQI method can enhance the performance of zero-shot NR approaches even further.\n221. 2) Supervised Performance: Due to the significant advancements achieved by deep neural networks, deep learning-based methods such as HyperIQA and MUSIQ have demonstrated superior performance compared to traditional handcrafted methods.\n222. Despite this, the proposed DHQI method, which is solely supervised by Support Vector Regression (SVR) model, achieves the top-ranking performance on the SJTU-H3D database.\n223. One notable advantage of the proposed supervised DHQI index is its cost-effectiveness in terms of time and computational resources.\n224. b) The point-based methods proposed by MPEG exhibit high sensitivity to noise-related distortions.\n225. This can be attributed to the direct impact of geometry and color noise on the point-level quality characteristics.\n226. Additionally, the PSNR yuv metric demonstrates a strong discriminative ability in distinguishing quality differences within CN, PC, UMC, and TD distortions.\n227. c) The zeroshot NR methods NIQE and IL-NIQE show competitive performance for UMC, TD, and TC distortions.\n228. This can be attributed to the fact that UMC and TD distortions introduce blurring effects to digital human projections, which aligns with the strengths of these methods.\n229. d) FS distortion proves to be the most challenging distortion to evaluate.\n230. This is due to the fact that the MOS distribution for FS distortion tends to be more centered, as shown in Fig.6, indicating a more fine-grained quality level that is less distinctive.\n231. FS distortion primarily causes digital humans to exhibit more geometric characteristics, which may lead to small differences in NSS reflected by the projections and result in the poor performance of NIQE and IL-NIQE.\n232. In this section, we present an analysis of the effects of different quality measures: Q A , Q N , and Q G , on the experimental performance.\n233. The combinations of these quality measures are tested, and the results are summarized in TableVI.\n234. Throughout the experiments, we maintain the default experimental setup.\n235. Table VI clearly demonstrates that among the single quality measures, Q A achieves the highest performance.\n236. This finding indicates a strong correlation between quality-aware semantic affinity and the visual quality of digital humans.\n237. It suggests that considering the quality of semantic representations is crucial for accurately assessing the visual fidelity of digital human models.\n238. Furthermore, excluding any of the three quality measures leads to a drop in performance compared to utilizing all quality measures together.\n239. This observation implies that each quality measure contributes significantly to the final results.\n240. The effectiveness of the proposed framework is thereby validated by the consistent performance improvements achieved when all quality measures are incorporated.\n241. To further analyze the performance of the proposed method, we conduct the statistical test in this section.\n242. We follow the same experiment setup as in[67]and compare the difference between the predicted quality scores with the subjective ratings.\n243. All possible pairs of models are tested and the results are listed in Fig.9.\n244. Our method demonstrates remarkable superiority over 12 zero-shot methods and 5 supervised methods when compared on the SJTU-H3D database.\n245. On the DHHQA database, our method exhibits substantial outperformance compared to 9 zero-shot methods and 3 supervised methods.\n246. The increasing applications of digital humans across various domains have highlighted the need for comprehensive A black/white block means the row method is statistically worse/better than the column one.\n247. A gray block means the row method and the column method are statistically indistinguishable.\n248. The methods are denoted by the same index as in TableIV. quality assessment studies.\n249. However, the limited availability of comprehensive digital human quality assessment (DHQA) databases has posed challenges in this area.\n250. To address this gap, we have introduced the SJTU-H3D subjective quality assessment database, specifically designed for full-body digital humans.\n251. This database consists of 40 high-quality reference digital humans and 1,120 labeled distorted counterparts created with seven types of distortions.\n252. Nonetheless, the scarcity of suitable DHQA databases remains a hindrance to the development of data-driven methods.\n253. To overcome this limitation and enhance generalization capabilities, we propose a zero-shot DHQA approach that focuses on no-reference (NR) scenarios.\n254. Our approach leverages semantic and distortion features obtained from projections, as well as geometry features derived from the mesh structure of digital humans.\n255. The proposed DHQI not only serves as a robust baseline for DHQA tasks but also facilitates advancements in the field.\n256. We hope our work can contribute to the establishment of effective evaluation frameworks and methodologies for digital humans, enabling their widespread application in diverse domains.\n\n请严格按照JSON格式返回：{"labels": [0,1,0,...]}，不要包含任何其他文字。'}], 'model': 'qwen-plus', 'response_format': {'type': 'json_object'}}}
2025-10-14 20:46:15,931 - DEBUG - Sending HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
2025-10-14 20:46:15,931 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-10-14 20:46:15,931 - DEBUG - send_request_headers.complete
2025-10-14 20:46:15,931 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-10-14 20:46:15,931 - DEBUG - send_request_body.complete
2025-10-14 20:46:15,931 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-10-14 20:46:28,515 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'vary', b'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding'), (b'x-request-id', b'6ef98be4-b307-4bca-be24-542be08cbc75'), (b'x-dashscope-call-gateway', b'true'), (b'content-type', b'application/json'), (b'req-cost-time', b'12543'), (b'req-arrive-time', b'1760445965981'), (b'resp-start-time', b'1760445978524'), (b'x-envoy-upstream-service-time', b'12487'), (b'content-encoding', b'gzip'), (b'date', b'Tue, 14 Oct 2025 12:46:18 GMT'), (b'server', b'istio-envoy'), (b'transfer-encoding', b'chunked')])
2025-10-14 20:46:28,516 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-14 20:46:28,516 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-10-14 20:46:28,517 - DEBUG - receive_response_body.complete
2025-10-14 20:46:28,517 - DEBUG - response_closed.started
2025-10-14 20:46:28,517 - DEBUG - response_closed.complete
2025-10-14 20:46:28,517 - DEBUG - HTTP Response: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "200 OK" Headers({'vary': 'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding', 'x-request-id': '6ef98be4-b307-4bca-be24-542be08cbc75', 'x-dashscope-call-gateway': 'true', 'content-type': 'application/json', 'req-cost-time': '12543', 'req-arrive-time': '1760445965981', 'resp-start-time': '1760445978524', 'x-envoy-upstream-service-time': '12487', 'content-encoding': 'gzip', 'date': 'Tue, 14 Oct 2025 12:46:18 GMT', 'server': 'istio-envoy', 'transfer-encoding': 'chunked'})
2025-10-14 20:46:28,517 - DEBUG - request_id: 6ef98be4-b307-4bca-be24-542be08cbc75
2025-10-14 20:46:28,518 - DEBUG - API request completed in 12.59 seconds
2025-10-14 20:46:28,518 - DEBUG - Raw model response: {"labels": [1,1,1,1,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,1,1,1,1,1,1,1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}
2025-10-14 20:46:28,519 - INFO - Successfully processed 147 labels
2025-10-14 20:46:28,519 - ERROR - Label count mismatch for Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation
2025-10-14 20:46:28,519 - INFO - Evaluating paper 3/18: Amazon_M2_ A Multilingual Multi_locale Shopping Session Dataset for Recommendation and Text Generation
2025-10-14 20:46:28,519 - INFO - Starting model prediction
2025-10-14 20:46:28,519 - INFO - Attempt 1 of 5
2025-10-14 20:46:28,521 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-3140c7df-0f3c-44a8-8696-16ce1a6b1782', 'json_data': {'messages': [{'role': 'system', 'content': '你是一个专门用于识别学术论文中数据集描述的助手。你需要判断每个输入的句子是否描述了研究中使用的数据集。请确保：\n    1. 只输出JSON格式的结果\n    2. 结果格式必须为 {"labels": [0,1,0,...]}\n    3. 不要输出任何其他文字或解释'}, {'role': 'user', 'content': '你是一位专业的学术论文数据集描述识别专家。请仔细分析下面这篇论文中的每个句子，判断它们是否描述了本研究特定构建的数据集信息。\n\n    论文标题: Amazon_M2_ A Multilingual Multi_locale Shopping Session Dataset for Recommendation and Text Generation\n\n    判断标准:\n    判断为1(数据集描述)的标准：\n    句子必须直接描述本研究特定构建的数据集,包括:\n    1. 该数据集的构建过程和方法\n    2. 该数据集的具体构成和规模\n    3. 该数据的来源和收集方式\n    4. 该数据的预处理步骤\n    5. 该数据集的可获取方式(如发布地址)\n    6. 或者其他直接或者间接描述了该数据集的句子\n\n    所有其他类型的句子均标记为0。\n    \n    请对每个句子进行分析，返回一个JSON格式的标签数组。格式要求：\n    1. 必须是有效的JSON格式\n    2. 只包含labels字段，值为0和1组成的数组\n    3. 数组长度必须与句子数量相同\n    4. 示例格式：{"labels": [0,1,0,1,...]}\n\n    需要判断的句子：\n\n    1. Abstract: Modeling customer shopping intentions is a crucial task for e-commerce, as it directly impacts user experience and engagement.\n2. Thus, accurately understanding customer preferences is essential for providing personalized recommendations.\n3. Session-based recommendation, which utilizes customer session data to predict their next interaction, has become increasingly popular.\n4. However, existing session datasets have limitations in terms of item attributes, user diversity, and dataset scale.\n5. As a result, they cannot comprehensively capture the spectrum of user behaviors and preferences.\n6. To bridge this gap, we present the Amazon Multilingual Multilocale Shopping Session Dataset, namely Amazon-M2.\n7. It is the first multilingual dataset consisting of millions of user sessions from six different locales, where the major languages of products are English, German, Japanese, French, Italian, and Spanish.\n8. Remarkably, the dataset can help us enhance personalization and understanding of user preferences, which can benefit various existing tasks as well as enable new tasks.\n9. To test the potential of the dataset, we introduce three tasks in this work: (1) next-product recommendation, (2) next-product recommendation with domain shifts, and (3) next-product title generation.\n10. With the above tasks, we benchmark a range of algorithms on our proposed dataset, drawing new insights for further research and practice.\n11. In addition, based on the proposed dataset and tasks, we hosted a competition in the KDD CUP 2023 2 and have attracted thousands of users and submissions.\n12. The winning solutions and the associated workshop can be accessed at our website https://kddcup23.github.io/.* Equal contribution. 2https://www.aicrowd.com/challenges/amazon-kdd-cup-23-multilingual-recommendationchallenge37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks.\n13. In the era of information explosion, recommender systems have become a prevalent tool for understanding user preferences and reducing information overload[1,2,3,4,5,6].\n14. Traditionally, the majority of recommendation algorithms focus on understanding long-term user interests through utilizing user-profiles and behavioral records.\n15. However, they tend to overlook the user\'s current purpose which often has a dominant impact on user\'s next behavior.\n16. Besides, many recommendation algorithms require access to user profiles[7,8,9], which can be incomplete or even missing in real-world situations especially when users are browsing in an incognito mode.\n17. In these cases, only the most recent user interactions in the current session can be utilized for understanding their preferences.\n18. Consequently, the session-based recommendation has emerged as an effective solution for modeling user\'s short-term interest, focusing on a user\'s most recent interactions within the current session to predict the next product.\n19. Over the past few years, the session-based recommendation has gained significant attention and has prompted the development of numerous models[10,11,12,13,14,15,16,17].\n20. A critical ingredient for evaluating the efficacy of these methods is the session dataset.\n21. While numerous session datasets[18,19,20,11,21]have been carefully curated to meet the requirements of modeling user intent and are extensively employed for evaluating session-based recommender systems, they have several drawbacks.\n22. First, existing datasets only provide limited product attributes, resulting in incomplete product information and obscuring studies that leverage attribute information to advance the recommendation.\n23. Second, the user diversity within these datasets is limited and may not adequately represent the diversity of user-profiles and behaviors.\n24. Consequently, it can result in biased or less accurate recommendations, as the models may not capture the full range of customer preferences.\n25. Third, the dataset scale, particularly in terms of the product set, is limited, which falls short of reflecting real-world recommendation scenarios with vast product and user bases.\n26. To break the aforementioned limitations, we introduce the Amazon Multilingual Multi-Locale Shopping Session Dataset, namely Amazon-M2, a large dataset of anonymized user sessions with their interacted products collected from multiple language sources at Amazon.\n27. Specifically, the dataset contains samples constructed from real user session data, where each sample contains a list of user-engaged products in chronological order.\n28. In addition, we provide a table of product attributes, which contains all the interacted products with their associated attributes such as title, brand, color, etc. Modeling such session data can help us better understand customers\' shopping intentions, which is also the main focus of e-commerce.\n29. Particularly, the proposed dataset exhibits the following characteristics that make it unique from existing session datasets.\n30. (a) Rich semantic attributes: Amazon-M2 includes rich product attributes (categorical, textual, and numerical attributes) as product features including title, price, brand, description, etc.These attributes provide a great opportunity to accurately comprehend the user\'s interests.\n31. To our best knowledge, it is the first session dataset to provide textual features.\n32. (b) Large scale: Amazon-M2 is a large-scale dataset with millions of user sessions and products, while existing datasets only contain tens of thousands of products.\n33. (c) Multiple locales: Amazon-M2 collected data from diverse sources, i.e., six different locales including the United Kingdom, Japan, Italian, Spanish, French, and Germany.\n34. (d) Multiple languages: Given the included locales, Amazon-M2 is special for its multilingual property.\n35. Particularly, six different languages (English, Japanese, Italian, Spanish, French, and German) are provided.\n36. It enables us to leverage recent advances such as language models[22,23,24]to model different languages in user sessions.\n37. By utilizing this dataset, we can perform diverse downstream tasks for evaluating relevant algorithms in recommendation and text generation.\n38. Here, we focus on three different tasks, consisting of (1) next-product recommendation, (2) next-product recommendation with domain shifts, and (3) nextproduct title generation.\n39. The first task is the classic session-based recommendation which requires models to predict the ID of the next product, where the training dataset and test dataset are from the same domain.\n40. The second task is similar to the first task but requires the models to pre-train on the large dataset from large locales and transfer the knowledge to make predictions on downstream datasets from different domains (i.e., underrepresented locales).\n41. The third task is a novel task proposed by us, which asks models to predict the title of the next product which has never been shown in the training set.\n42. Based on these tasks, we benchmark representative baselines along with simple heuristic methods.\n43. Our empirical observations suggest that the representative baselines fail to outperform simple heuristic methods in certain evaluation metrics in these new settings.\n44. Therefore, we believe that Amazon-M2 can inspire novel solutions for session-based recommendation and enable new opportunities for tasks that revolve around large language models and recommender systems.\n45. denote a dictionary of unique products that appeared in the sessions, and each product is associated with some attributes.\n46. Designed for session-based recommendation, Amazon-M2 is a large-scale dataset composed of customer shopping sessions with interacted products.\n47. Specifically, the dataset consists of two components: (1) user sessions where each session is a list of product IDs interacted by the current user (Figure1a), and (2) a table of products with each row representing the attributes of one product (Figure1b).\n48. Particularly, the user sessions come from six different locales, i.e., the United Kingdom (UK), Japan (JP), German (DE), Spain (ES), Italian (IT), and France (FR).\n49. Given its multi-locale nature, the dataset is also multilingual: the textual attributes (e.g., title and description) of the products in the user sessions are in multiple languages, namely, English, Italian, French, Germany, and Spanish.\n50. Based on this dataset, we construct the training/test dataset for each task.\n51. A summary of our session dataset is given in Table2.\n52. It includes the number of sessions, the number of interactions, the number of products, and the average session length for six different locales.\n53. We can find that UK, DE, and JP have approximately 10 times the number of sessions/products compared to ES, FR, and IT.\n54. More details about the collection process of the dataset can be found in Appendix B.\n55. Comparison with Existing Datasets.\n56. We summarize the differences between existing session datasets (especially from session-based recommendation) and Amazon-M2 in Table1.\n57. First of all, Amazon-M2 is the first dataset to provide textural information while other datasets majorly focus on the product ID information or categorical attributes.\n58. Without comprehensive product attributes, the recommendation models may struggle to capture the nuanced preferences of customers.\n59. Second, existing datasets only provide sessions from a single locale (or country) which limits their user diversity.\n60. Consequently, it may lead to biased or less accurate recommendations, as the models may not capture the full range of customer preferences.\n61. By contrast, our proposed Amazon-M2 is collected from multiple locales and is multilingual in nature.\n62. Third, our proposed Amazon-M2 provides a large number of user sessions and is on a much larger product scale, which can better reflect real-world recommendation scenarios with huge product bases.\n63. In this section, we offer a comprehensive analysis of the Amazon-M2 dataset to uncover valuable insights.\n64. Our analysis covers several essential perspectives: long-tail phenomenon, product overlap between locales, session lengths, repeat pattern, and collaborative filtering pattern.\n65. Corresponding codes can be found here.\n66. Long-tail phenomenon[26,27]is a significant challenge in the session recommendation domain.\n67. It refers to the situation where only a handful of products enjoy high popularity, while the majority of products receive only a limited number of interactions.\n68. To investigate the presence of the long-tail phenomenon in Amazon-M2 dataset, we analyze the distribution of product frequencies, as depicted  in Figure2a.\n69. The results clearly demonstrate the existence of a long-tail distribution, where the head of the distribution represents popular items and the tail consists of less popular ones.\n70. Furthermore, we observe that the long-tail phenomenon is also evident within each individual locale.\n71. For detailed experimental results regarding this phenomenon in each locale, please refer to Appendix B.2.\n72. The long-tail distribution makes it difficult to effectively recommend less popular products, as a small number of popular items dominate the recommendations.\n73. Product overlap ratio between locales is the proportion of the same products shared by different locales.\n74. A large number of overlapping products indicates a better transferability potential when transferring the knowledge from one locale to the other.\n75. For example, cross-domain recommendation algorithms like[28]can then be successfully applied, which directly transfers the learned embedding of the overlapping products from popular locales to the underrepresented locales.\n76. We then examine product overlap between locales in Amazon-M2 with the product overlap ratio.\n77. |Na| , where N a and N b correspond to the products set of locale a and b, respectively.\n78. In Figure2bwe use a heatmap to show the overlap ratio, where x and y axes stand for locale a and b, respectively.\n79. (2) Considering the product overlap ratio between large locales and underrepresented locales, i.e., ES, FR, and IT, we can see a large product overlapping, indicating products in the underrepresented domain also appear in the large locales.\n80. Particularly, the overlap ratio between small locales and DE can reach around 0.4.\n81. Thus, it has the potential to facilitate knowledge transfer from large locales and areas to underrepresented regions.\n82. Notably, despite the existence of overlapping products between different locales, there still remains a large proportion of distinguished products in each locale, indicating the difficulty of transferability with distribution shift.\n83. Moreover, the multilingual property, where the product textual description from different locales is in different languages, also induces to distribution shift issue.\n84. Such a multilingual issue is a long-established topic in the NLP domain.\n85. For instance,[29,30,31]point out morphology disparity, tokenization differences, and negative transfer in the multilingual scenario, leading to distribution shift.\n86. Session length is an important factor in the session recommendation domain.\n87. Typically, a longer session length may lead to the interest shift challenge problem[15]with difficulties in capturing multiple user interests in one single session.\n88. Most existing algorithms[13,16]show a better performance on the shorter sessions while failing down on the longer ones.\n89. As shown in Figure2c.\n90. We can observe that the session length also exhibits a long-tail distribution: most sessions are short while only few sessions are with a length larger than 100.\n91. Repeat pattern[32,33,34,35]is also an important user pattern, which refers to the phenomenon that a user repeatedly engages the same products multiple times in a single session.\n92. The presence of repeat patterns in recommender systems can potentially result in the system offering more familiar products that match users\' existing preferences, which may lead to a less diverse and potentially less satisfying user experience.\n93. On the other hand, the repeat pattern is also an important property utilized in the graph-based session recommendation algorithms[13,17,36,37].\n94. Typically, those graph-based algorithms construct a session graph where each node represents a product and each edge indicates two products interacted by the user consecutively.\n95. Complicated session graphs with different structure patterns can be built when sessions exhibit evident repeat patterns.\n96. In Figure2d, we report the proportion of sessions with repeat patterns for the six locales and we can observe that there are around 35% sessions with repeat patterns across different locales.\n97. Furthermore, we examine the number of repeat products in those sessions with repeat patterns and report results on the distribution of repeated products in Figure2e.\n98. We make two observations: (1) the number of repeated products varies on different sessions; and (2) the number of repeated products in a session also follows the long-tail distribution where most sessions only appear with a few repeated products.\n99. Collaborative filtering pattern.\n100. Collaborative filtering is a widely used technique that generates recommended products based on the behavior of other similar users.\n101. It is generally utilized as an important data argumentation technique to alleviate the data sparsity issue, especially for short sessions[38,39].\n102. Since Amazon-M2 encompasses a much larger product set than existing datasets, we investigate whether collaborative filtering techniques can potentially operate in this challenging data environment.\n103. Specifically, we utilize the session collaborative filtering algorithm, Session-KNN (SKNN)[11], to identify sessions that are similar to the target user\'s current session.\n104. The similarity score of SKNN can be calculated in the following steps.\n105. First, for a particular session s, we first determines a set of its most similar sessions N (s) ⊆ S with the cosine similarity sim(s, s j ) = |s ∩ s j |/ |s||s j |.\n106. • SRGNN[13]is the first to employ GNN layer to capture user interest in the current session.\n107. • CORE[41]ensures that sessions and items are in the same representation space via encoding the session embedding as a linear combination of item embeddings.\n108. • MGS[42]incorporates product attribute information to construct a mirror graph, aiming to learn better preference understanding via combining session graph and mirror graph.\n109. Notably, MGS can only adapt categorical attributes.\n110. Therefore, we discretize the price attribute as the input feature.\n111. In addition, we include a simple yet effective method, Popularity, by simply recommending all users the most popular products.\n112. We utilize Mean Reciprocal Rank@K (MRR@100) and Recall@100 to evaluate various recommendation algorithms.\n113. More results on NDCG@100 metric can be found in Appendix C.\n114. Corresponding codes can be found here.\n115. Results & Observations.\n116. The experiment results across different locales can be found in Table3.\n117. We can observe that the popularity heuristic generally outperforms all the deep models with respect to both MRR and Recall.\n118. The only exception is that CORE achieves better performance on Recall.\n119. On one hand, the success of the popularity heuristic indicates that the product popularity is a strong bias for this dataset.\n120. On the other hand, it indicates that the large product set in Amazon-M2 poses great challenges in developing effective recommendation algorithms.\n121. Thus, more advanced recommendation strategies are needed for handling the challenging Amazon-M2 dataset.\n\n请严格按照JSON格式返回：{"labels": [0,1,0,...]}，不要包含任何其他文字。'}], 'model': 'qwen-plus', 'response_format': {'type': 'json_object'}}}
2025-10-14 20:46:28,523 - DEBUG - Sending HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
2025-10-14 20:46:28,524 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-10-14 20:46:28,524 - DEBUG - send_request_headers.complete
2025-10-14 20:46:28,524 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-10-14 20:46:28,524 - DEBUG - send_request_body.complete
2025-10-14 20:46:28,524 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-10-14 20:46:39,562 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'vary', b'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding'), (b'x-request-id', b'fbe5e871-ec2f-4c71-b5dd-3ee2add24b0d'), (b'x-dashscope-call-gateway', b'true'), (b'content-type', b'application/json'), (b'req-cost-time', b'10995'), (b'req-arrive-time', b'1760445978575'), (b'resp-start-time', b'1760445989571'), (b'x-envoy-upstream-service-time', b'10948'), (b'content-encoding', b'gzip'), (b'date', b'Tue, 14 Oct 2025 12:46:29 GMT'), (b'server', b'istio-envoy'), (b'transfer-encoding', b'chunked')])
2025-10-14 20:46:39,563 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-14 20:46:39,563 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-10-14 20:46:39,563 - DEBUG - receive_response_body.complete
2025-10-14 20:46:39,564 - DEBUG - response_closed.started
2025-10-14 20:46:39,564 - DEBUG - response_closed.complete
2025-10-14 20:46:39,564 - DEBUG - HTTP Response: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "200 OK" Headers({'vary': 'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding', 'x-request-id': 'fbe5e871-ec2f-4c71-b5dd-3ee2add24b0d', 'x-dashscope-call-gateway': 'true', 'content-type': 'application/json', 'req-cost-time': '10995', 'req-arrive-time': '1760445978575', 'resp-start-time': '1760445989571', 'x-envoy-upstream-service-time': '10948', 'content-encoding': 'gzip', 'date': 'Tue, 14 Oct 2025 12:46:29 GMT', 'server': 'istio-envoy', 'transfer-encoding': 'chunked'})
2025-10-14 20:46:39,564 - DEBUG - request_id: fbe5e871-ec2f-4c71-b5dd-3ee2add24b0d
2025-10-14 20:46:39,565 - DEBUG - API request completed in 11.05 seconds
2025-10-14 20:46:39,565 - DEBUG - Raw model response: {"labels": [0,0,0,0,0,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}
2025-10-14 20:46:39,566 - INFO - Successfully processed 104 labels
2025-10-14 20:46:39,566 - ERROR - Label count mismatch for Amazon_M2_ A Multilingual Multi_locale Shopping Session Dataset for Recommendation and Text Generation
2025-10-14 20:46:39,566 - INFO - Evaluating paper 4/18: AntM 2 C_ A Large Scale Dataset For Multi_Scenario Multi_Modal CTR Prediction
2025-10-14 20:46:39,566 - INFO - Starting model prediction
2025-10-14 20:46:39,566 - INFO - Attempt 1 of 5
2025-10-14 20:46:39,568 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-74866b12-d354-4ef7-96a6-a12ca7d5d835', 'json_data': {'messages': [{'role': 'system', 'content': '你是一个专门用于识别学术论文中数据集描述的助手。你需要判断每个输入的句子是否描述了研究中使用的数据集。请确保：\n    1. 只输出JSON格式的结果\n    2. 结果格式必须为 {"labels": [0,1,0,...]}\n    3. 不要输出任何其他文字或解释'}, {'role': 'user', 'content': '你是一位专业的学术论文数据集描述识别专家。请仔细分析下面这篇论文中的每个句子，判断它们是否描述了本研究特定构建的数据集信息。\n\n    论文标题: AntM 2 C_ A Large Scale Dataset For Multi_Scenario Multi_Modal CTR Prediction\n\n    判断标准:\n    判断为1(数据集描述)的标准：\n    句子必须直接描述本研究特定构建的数据集,包括:\n    1. 该数据集的构建过程和方法\n    2. 该数据集的具体构成和规模\n    3. 该数据的来源和收集方式\n    4. 该数据的预处理步骤\n    5. 该数据集的可获取方式(如发布地址)\n    6. 或者其他直接或者间接描述了该数据集的句子\n\n    所有其他类型的句子均标记为0。\n    \n    请对每个句子进行分析，返回一个JSON格式的标签数组。格式要求：\n    1. 必须是有效的JSON格式\n    2. 只包含labels字段，值为0和1组成的数组\n    3. 数组长度必须与句子数量相同\n    4. 示例格式：{"labels": [0,1,0,1,...]}\n\n    需要判断的句子：\n\n    1. Abstract: Click-through rate (CTR) prediction is a crucial issue in recommendation systems, directly impacting user experience and platform revenue.\n2. In recent years, CTR has garnered attention from both industry and academia, leading to the emergence of various public CTR datasets.\n3. However, existing CTR datasets primarily suffer from the following limitations.\n4. Firstly, users generally click different types of items from multiple scenarios, and modeling the CTR from multiple scenarios can provide a more comprehensive understanding of users and share knowledge between different scenarios.\n5. Existing datasets only include CTR data for the same type of items from a single scenario.\n6. Secondly, multi-modal features are essential in multi-scenario CTR prediction as they effectively address the issue of inconsistent ID encoding between different scenarios.\n7. The existing datasets are based on ID features and lack multi-modal features.\n8. Third, a large-scale CTR dataset can provide a more reliable and comprehensive evaluation of complex models, fully reflecting the performance differences between models.\n9. While the scale of existing datasets is around 100 million, which is relatively small compared to the real-world industrial CTR prediction.\n10. To address these limitations, we propose AntM 2 C, a Multi-Scenario Multi-Modal CTR dataset based on real industrial data from the Alipay platform.\n11. Specifically, AntM 2 C possesses the following characteristics: 1) It covers CTR data of 5 different types of items from Alipay, providing insights into the preferences of users for different items, including advertisements, vouchers, mini-programs, contents, and videos.\n12. 2) Apart from ID-based features, AntM 2 C also provides 2 multi-modal features, raw text and image features, which can effectively establish connections between items with different IDs.\n13. 3) AntM 2 C provides 1 billion CTR data with 200 features, including:\n14. Click-through rate (CTR) prediction plays a significant role in various domains, including online advertising, search engines, and recommendation systems.\n15. CTR prediction refers to the task of estimating the probability that a user will click on a given item.\n16. It is essential for optimizing ad revenue, enhancing user experience, and improving engagement.\n17. One of the challenging issues in CTR prediction lies in the faithful evaluation of the model.\n18. Public CTR datasets provide a standardized and benchmarked environment for evaluating the performance of different CTR models.\n19. This enables researchers to compare the effectiveness of different models and identify the most suitable ones for specific applications.\n20. However, in order to meet the constantly growing demands of users, the current CTR scenarios and items are becoming increasingly diverse, and the amount of CTR data is also increasing.\n21. For example, in Alipay, CTR occurs in the consumer coupons at marketing campaigns, videos on the tab3 page, and mini-programs after a search.\n22. As a result, the existing CTR datasets suffer from the following limitations.\n23. Firstly, in real-world industrial CTR prediction, users generally click various types of items from different business scenarios, reflecting their preferences for different items.\n24. For example, on Alipay, a user may browse a video about coffee on the Tab3 page, then click on a coffee coupon during a marketing campaign, and finally use the Alipay search to click a coffee ordering mini-program to place an order.\n25. Jointly modeling this multi-scenario CTR data can provide a more comprehensive understanding of user preferences, and the knowledge across scenarios can be shared to improve the CTR performance in each scenario.\n26. However, existing CTR datasets have a limited range of item types and generally originate from the same business scenario, which fails to capture the multi-scenario preferences of users.\n27. For example, Criteo1and Avazu2only involve CTR data for advertisements.\n28. As e-commerce platforms, both Amazon3and AliExpress4provide CTR data for their e-commerce items.\n29. Tenrec[14]focuses more on video and article recommendations.\n30. Secondly, multi-modal features can address the issue of inconsistent IDs for similar items in different business scenarios and effectively establish a bridge between different scenarios.\n31. For example, a video about coffee and a coffee coupon have different IDs in different business scenarios.\n32. Directly using ID features cannot perceive the relationship between these two items.\n33. Multi-modal features inherently carry semantic meaning and can better compensate for the inconsistency of ID features across different domains.\n34. Additionally, with the rise of large language models (LLMs), combining LLMs with CTR prediction has become an emerging research field.\n35. Existing CTR datasets are based on ID features and lack abundant multi-modal features, resulting in the CTR model being unable to test the performance in multi-scenarios and multi-modal settings.\n36. Furthermore, large-scale datasets can reliably and comprehensively reflect the performance of CTR models, while also highlighting the differences between CTR models.\n37. The existing datasets are typically at the scale of 100 million, which is insufficient to further validate the capabilities in larger-scale industrial scenarios.\n38. To address the aforementioned challenges, we propose the AntM 2 C dataset, a large-scale multi-scenario multi-modal dataset for CTR prediction.\n39. Compared with existing CTR datasets, AntM 2 C has the following advantages: • Diverse business scenarios and item types: AntM 2 C contains different types of items from five typical business scenarios on the Alipay platform, including advertisements, vouchers, mini-programs, contents, and videos.\n40. Each business scenario has a unique data distribution.\n41. The abundant intersecting users and similar items between scenarios enable a more comprehensive evaluation for multi-scenario CTR modeling.\n42. • Multi-modal feature system: AntM 2 C not only includes ID features but also provides rich multi-modal features such as text and image, which can establish connections between similar items across scenarios and provide better evaluation for multi-modal CTR models.\n43. • Largest data scale: AntM 2 C comprises 200 million users and 6 million items, with a total of 1 billion samples 5 .\n44. The average number of interactions per user is above 50.\n45. To the best of our knowledge, AntM 2 C is the largest public CTR dataset in terms of scale, which can provide comprehensive and reliable CTR evaluation results.\n46. • Comprehensive benchmark: Based on AntM 2 C, three typical CTR tasks have been built, including multi-scenario modeling, cold-start modeling, and multi-modal modeling.\n47. Benchmark evaluation results based on state-of-the-art models are also provided.\n48. The rest of the paper is organized as follows.\n49. In Section 2, we briefly review some related works about public CTR datasets.\n50. In Section 3, we give a detailed introduction to the dataset collection and data analysis.\n51. In Section 4, we conduct empirical studies with baseline CTR methods on different CTR tasks.\n52. The existing public CTR datasets can be roughly divided into two categories: single-scenario and multi-scenario.\n53. Both have been widely adopted by the evaluation of CTR methods.\n54. The Criteo dataset is one of the publicly available datasets for CTR prediction.\n55. It contains over 45 million records of user interactions with advertisements, including features such as click-through rates, impression rates, and user demographics.\n56. Similar to the Criteo dataset, the Avazu dataset contains over 40 million records of user interactions with mobile advertisements.\n57. It includes features such as device information, app category, and user demographics.\n58. One of the main limitations of the Criteo and Avazu dataset is they only include CTR data for advertisements and cannot be used to evaluate CTR for other business scenarios or types of items.\n59. Additionally, the datasets do not provide text information about the advertisement or user, which can limit the scope of the multi-modal modeling.\n60. The AliExpress is a dataset gathered from real-world traffic logs of the search system in AliExpress.\n61. This dataset is collected from 5 countries: Russia, Spain, French, Netherlands, and America, which can be seen as 5 scenarios.\n62. It can be used to develop and evaluate CTR prediction models for e-commerce platforms.\n63. The Tenrec dataset is a multipurpose dataset for CTR prediction where click data was collected from two scenarios: articles and videos.\n64. Although the above datasets cover different scenarios, the items within these scenarios are similar.\n65. The AliExpress dataset only consists of ecommerce items, and Tenrec involves videos and articles that only reflect the personal interests of users in the entertainment and cultural aspects.\n66. Additionally, similar to single-scenario datasets, both  of these datasets lack textual modal information and only provide features such as IDs.\n67. This limitation restricts the application of multi-modal modeling.\n68. AntM 2 C\'s data is collected from Alipay, a leading platform for payments and digital services.\n69. In order to meet the growing demands of users, Alipay recommends various types of items from different business scenarios to users.\n70. 3.1.1Scenarios.\n71. AntM 2 C collects CTR data in five scenarios on Alipay, and there are differences in the types of items in each scenario.\n72. As shown in Figure1, the CTR prediction occurs in multiple scenarios, including services and content on search, vouchers on marketing, videos on Tab3 page, and advertisements on the membership page.\n73. In the search scenario, when a user enters search words, several relevant mini-apps of services or content are displayed for the user to click on.\n74. Marketing scenarios recommend some consumer vouchers, and users click the coupons they are willing to use.\n75. On the Tab3 page, the recommended items are primarily short videos, and users will click to watch the videos they are interested in.\n76. On the membership page, users may click on some online advertisements.\n77. In conclusion, AntM 2 C includes various types of items from different business scenarios.\n78. 2,we will show that there are differences in the data distribution of these different scenarios.\n79. The rich and diverse items provide a more comprehensive evaluation for CTR prediction.\n80. 3.1.2Data Sampling.\n81. AntM 2 C collects 9-day (from 20230709 to 20230717) CTR samples from the above-mentioned five scenarios and then filters out 1 billion samples of relatively high-activity users who have a total click count ≥ 30 across all scenarios.\n82. In the first stage of open sourcing, we randomly sampled 10 million data from these 1 billion samples, and their statistical properties are shown in Table1.\n83. We will open all 1 billion data in the subsequent stage.\n84. For the purpose of protecting user privacy, we do not explicitly indicate Table1: Data statistics of AntM 2 C.\n85. To protect user privacy, AntM 2 C anonymizes the scenario names as A-E.\n86. The click rate is calculated by dividing the number of clicks by the number of exposures.\n87. Since negative sampling is applied to the samples, the click rate may be higher than the actual value.A-E).\n88. The horizontal axis represents the number of frequencies for users/items, while the vertical axis represents the number of users/items at that frequency.\n89. It can be observed that, in terms of item distribution, all scenarios exhibit a long-tail distribution, with 80% of the sample appearing less than 5 frequencies.\n90. This long-tail distribution is consistent with real-world situations.\n91. As for user distribution, there are differences between scenarios.\n92. In scenario B, the distribution of user frequency has two peaks, one at less than 5 times and the other around 50 times.\n93. After the frequency is greater than 50, the number of users decreases as the frequency increases.\n94. In other scenarios, the exposure frequency of users follows a long-tail distribution similar to that of items, where more exposure frequency leads to fewer users.\n95. Due to the overlapping users between scenarios, the long-tail distribution of users in multiple scenarios becomes a normal distribution in the global samples.\n96. Most users have an exposure frequency of around 50.\n97. Overall, the distribution of items and users in AntM 2 C reflects CTR prediction in practice.\n98. The feature system of AntM 2 C, as shown in Table3, includes ID features of users and items, as well as raw text features.\n99. The user features consist of static profile features6and user sequence features.\n100. The static profile features include basic user attributes such as gender, age, occupation, etc.The sequence features provide the user\'s recent activities on Alipay, including clicked mini-apps, searched services, purchased items, etc.\n101. As mentioned in Section 3.1.3,these user features have been desensitized and encrypted for the purpose of user privacy protection and appear in the dataset in an encrypted ID format, making it impossible to reconstruct the original user features.\n102. In addition to the ID-based features, AntM 2 C also includes the raw text of user search entities to provide multi-modal evaluation.1.\n103. It should be noted that there are a large number of negative samples in the actual online logs (samples that were exposed but not clicked on).\n104. To address this issue, negative sampling was performed which resulted in a higher click-through rate in the AntM 2 C dataset compared to that in the actual online logs.\n105. In this section, we describe the applications of AntM 2 C in several CTR prediction tasks.\n106. We briefly introduce each task and report the results of some baseline methods.\n107. We select the commonly used AUC (Area Under the Curve) as the metrics for all experiments.\n108. The baseline methods and evaluation results in the experiment provide a demo of using AntM 2 C.\n109. More baselines and evaluations will continue to be updated in future work.\n110. Multi-scenario CTR prediction is a common issue in industrial recommendation systems.\n111. It builds a unified model by leveraging CTR data from multiple scenarios.\n112. The knowledge sharing between scenarios enables the multi-scenario model to achieve better performance compared to single-scene modeling.\n113. We conduct an evaluation on multi-scenario CTR prediction using different baseline methods based on the 5 scenarios in the AntM 2 C dataset.\n114. 3. The text features will be used for multi-modal evaluation (see in Section 4.3).\n115. We mainly choose the multitask methods as the baseline methods for multi-scenario CTR prediction.\n116. We treat the CTR estimation for each scenario as a task and share the knowledge among the scenarios at the bottom layer, with each scenario\'s CTR score output at the tower layer.\n117. The baseline methods and hyperparameter settings are as follows: • DNN: The DNN is trained on a mixture of samples from all scenarios without tasks, serving as the baseline for multiscenario CTR prediction.\n118. The DNN consists of three layers with 128, 32, and 2 units, respectively.\n119. • Shared Bottom[10]: Shared bottom is the most fundamental model in multi-task learning, where the knowledge is shared among the tasks at the bottom layer.\n120. Each task has its own independent tower layer and outputs the corresponding CTR score7.\n121. • MMoE[7]: Based on the shared bottom, MMOE introduces multiple expert networks, each specialized in predicting a specific task, sharing a common input layer.\n122. Additionally, MMOE adds a gating network that assigns different weights to each expert based on the input data to determine their influence on predicting the output for a specific task.\n123. • PLE[12]: Based on MMOE, PLE further designs task-specific experts for each task, while retaining the shared expert.\n124. This structure allows the model to better learn the differences and correlations among tasks.\n125. We set the number of experts in PLE to be the same as MMOE, with each of the five scenarios having its own specific expert and one globally shared expert 7 .\n126. All baseline methods utilized the Adam[5]optimizer with a learning rate of 1e-3 for parameter optimization.\n127. The models were trained for 5 epochs with a batch size of 512.5shows the evaluation results of different baseline methods on multi-scenario CTR prediction, from which we can draw the following conclusions.\n128. Firstly, compared to the DNN model that trains all data together without considering scenario characteristics, all multi-task models achieve better performance.\n129. This demonstrates that in AntM 2 C, there are differences and commonalities between scenarios, and simply mixing training data will not achieve the best results.\n130. Secondly, the CTR performance varies across each scenario, indicating different levels of difficulty between scenarios.\n131. For example, in scenario B, where there is a large amount of data, the AUC is generally above 0.93, while in scenario D, the AUC is only around 0.68.\n132. The diverse business scenarios and items in AntM 2 C enable a more comprehensive and diverse evaluation of CTR.\n133. Finally, the expert-structured MMOE and PLE outperform the shared bottom model, demonstrating that refined model design can enhance the performance on AntM 2 C.\n134. AntM 2 C is capable of reflecting the differences between different models.\n135. The cold-start problem is a challenging issue in recommendation systems.\n136. Training high-quality CTR models using sparse user-item interaction data is a challenging task.\n137. Cold-start primarily involves two aspects: users and items.\n138. As shown in Figure2, the AntM 2 C dataset exhibits a natural long-tail distribution in both users and items.\n139. Therefore, we conduct a comprehensive evaluation of coldstart baseline methods based on AntM 2 C dataset.\n140. In cold-start CTR prediction, we split the dataset based on time, using data before 20230717 as the training set and data on 20230717 as the validation and test sets.\n141. Based on this data division, we simulated two common cold-start problems in practice: few-shot and zero-shot.\n142. • Few-shot: users and items that appear in the training set with a count greater than 0 and less than9, meaning there is only a small amount of training data for these users and items.\n143. • Zero-shot: users and items that have never appeared in the training set, indicating that either the user is visiting the scenario for the first time or the item has been launched and added to the scenario on the first day.\n144. Table6shows the data distribution of the test set under cold-start CTR evaluation.\n145. By using this dataset division, we can comprehensively evaluate and compare the performance of CTR models on few-shot and zero-shot samples.\n146. For few-shot samples, we can observe the model\'s performance with only a small amount of training data and evaluate the model\'s generalization ability.\n147. For zero-shot samples, we can evaluate the model\'s recommendation ability on samples that it has never seen before.\n148. The key issue in cold-start modeling is how to learn user preferences and embeddings of users and items with limited data.\n149. In recent years, meta-learning-based cold-start methods have become state-of-the-art methods.\n150. We selected several representative methods with publicly available code as our baseline models.\n151. • DropoutNet[13]: The DropoutNet is a popular cold-start method which applies dropout to control input, and exploits the average representations of interacted items/users to enhance the embeddings of users/items.\n152. • MAML[2]: The MAML algorithm is a popular meta-learning approach that aims to enable fast adaptation to new tasks with limited data.\n153. MAML learns a good initialization of model parameters that can be effectively adapted to new tasks quickly.\n154. We treat each user and item as a task in MAML, and conduct meta-training on warm items.\n155. Then we perform meta-testing on cold-start items.\n156. • MeLU[6]: The MeLU algorithm is the first to apply the MAML to address the cold-start problem in recommender systems.\n157. Although MetaEmb only optimizes the embeddings of items, we have also applied the same approach to optimize the embeddings of users.\n158. These base models share the common embedding and DNN structure.\n159. The dimensionality of embedding vectors of each input field is fixed to 32 for all our experiments.\n160. The Adam optimizer with a learning rate of 1e-3 is used to optimize the model parameters, and the training is performed for 3 epochs with a batch size of 512.\n161. 7shows the CTR performance for cold-start users and items.\n162. Because there is limited data for cold start users and items, we do not calculate AUC by scenarios, and evaluate the overall performance of cold start users and items.\n163. From the table, we can observe several phenomena.\n164. Firstly, compared to the results shown in Table5, the AUC for cold-start users and items are generally lower than the overall level, which demonstrates that AntM 2 C\'s data can effectively reflect the differences between cold and warm items and users.\n165. Secondly, different cold-start methods show distinguishable results in AntM 2 C, and all of them are significantly better than the DNN model without cold-start optimization.\n166. This indicates that AntM 2 C can effectively compare the effects of different cold-start methods and demonstrate the distinctiveness between methods.\n167. Finally, the lower performance of zero-shot compared to few-shot indicates that zero-shot CTR prediction is more challenging than few-shot.\n168. The two cold start modes provided by AntM 2 C can comprehensively evaluate cold-start CTR prediction.\n169. With the rise of large language models (LLMs), it has become a hot research topic to effectively transfer the knowledge of LLM to CTR prediction.\n170. There have been many works[3,4,9,11]based on multi-modal CTR modeling using features such as item and user text.\n171. In multi-modal evaluation, we adapt the same data processing approach as in multi-scenario evaluation mentioned in Section 4.1.1,and additionally include the text features from Table3: user query entities and item entities.\n172. The text features will be used as inputs to the model together with other ID features.\n173. For the baseline model, we use the language model to process the text features, and then concatenate the text embedding with other ID features and input them into the multi-scenario model described in Section 4.1.2.\n174. For ease of evaluation, we choose MMoE as the backbone and pre-trained Bertbase13[1]as the text embedding extractor.\n175. The output dimension of Bert\'s embeddings is 768.\n176. Then, a DNN with two layers, each layer having [768, 32] units, is used to reduce the dimension of Bert\'s embedding to 32.\n177. This reduced embedding is concatenated with other features and input into the MMOE model.\n178. More powerful language models and the application of text features will continue to be supplemented in future works.\n179. Table8shows the evaluation results of the multimodal CTR.\n180. It can be observed that, after adding the text modality, the CTR performance is better in data-sparse scenarios C, D, and E compared to using only the ID modality in the MMoE.\n181. Since the current baseline for using the text modality is relatively simple, the improvement in performance is not significant.\n182. However, this shows the potential of the text modality provided in AntM 2 C to improve CTR performance.\n\n请严格按照JSON格式返回：{"labels": [0,1,0,...]}，不要包含任何其他文字。'}], 'model': 'qwen-plus', 'response_format': {'type': 'json_object'}}}
2025-10-14 20:46:39,572 - DEBUG - Sending HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
2025-10-14 20:46:39,572 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-10-14 20:46:39,573 - DEBUG - send_request_headers.complete
2025-10-14 20:46:39,573 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-10-14 20:46:39,573 - DEBUG - send_request_body.complete
2025-10-14 20:46:39,573 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-10-14 20:46:50,482 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'vary', b'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding'), (b'x-request-id', b'7149f565-8ee1-4770-a76a-793fa553c3cb'), (b'x-dashscope-call-gateway', b'true'), (b'content-type', b'application/json'), (b'req-cost-time', b'10864'), (b'req-arrive-time', b'1760445989626'), (b'resp-start-time', b'1760446000491'), (b'x-envoy-upstream-service-time', b'10816'), (b'content-encoding', b'gzip'), (b'date', b'Tue, 14 Oct 2025 12:46:40 GMT'), (b'server', b'istio-envoy'), (b'transfer-encoding', b'chunked')])
2025-10-14 20:46:50,482 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-14 20:46:50,483 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-10-14 20:46:50,483 - DEBUG - receive_response_body.complete
2025-10-14 20:46:50,483 - DEBUG - response_closed.started
2025-10-14 20:46:50,483 - DEBUG - response_closed.complete
2025-10-14 20:46:50,483 - DEBUG - HTTP Response: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "200 OK" Headers({'vary': 'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding', 'x-request-id': '7149f565-8ee1-4770-a76a-793fa553c3cb', 'x-dashscope-call-gateway': 'true', 'content-type': 'application/json', 'req-cost-time': '10864', 'req-arrive-time': '1760445989626', 'resp-start-time': '1760446000491', 'x-envoy-upstream-service-time': '10816', 'content-encoding': 'gzip', 'date': 'Tue, 14 Oct 2025 12:46:40 GMT', 'server': 'istio-envoy', 'transfer-encoding': 'chunked'})
2025-10-14 20:46:50,484 - DEBUG - request_id: 7149f565-8ee1-4770-a76a-793fa553c3cb
2025-10-14 20:46:50,485 - DEBUG - API request completed in 10.92 seconds
2025-10-14 20:46:50,485 - DEBUG - Raw model response: {"labels": [0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]}
2025-10-14 20:46:50,485 - INFO - Successfully processed 141 labels
2025-10-14 20:46:50,485 - ERROR - Label count mismatch for AntM 2 C_ A Large Scale Dataset For Multi_Scenario Multi_Modal CTR Prediction
2025-10-14 20:46:50,485 - INFO - Evaluating paper 5/18: AU_AIR_ A Multi_modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance
2025-10-14 20:46:50,485 - INFO - Starting model prediction
2025-10-14 20:46:50,485 - INFO - Attempt 1 of 5
2025-10-14 20:46:50,487 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-46564073-e32a-48ce-83b1-f3704e43912d', 'json_data': {'messages': [{'role': 'system', 'content': '你是一个专门用于识别学术论文中数据集描述的助手。你需要判断每个输入的句子是否描述了研究中使用的数据集。请确保：\n    1. 只输出JSON格式的结果\n    2. 结果格式必须为 {"labels": [0,1,0,...]}\n    3. 不要输出任何其他文字或解释'}, {'role': 'user', 'content': '你是一位专业的学术论文数据集描述识别专家。请仔细分析下面这篇论文中的每个句子，判断它们是否描述了本研究特定构建的数据集信息。\n\n    论文标题: AU_AIR_ A Multi_modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance\n\n    判断标准:\n    判断为1(数据集描述)的标准：\n    句子必须直接描述本研究特定构建的数据集,包括:\n    1. 该数据集的构建过程和方法\n    2. 该数据集的具体构成和规模\n    3. 该数据的来源和收集方式\n    4. 该数据的预处理步骤\n    5. 该数据集的可获取方式(如发布地址)\n    6. 或者其他直接或者间接描述了该数据集的句子\n\n    所有其他类型的句子均标记为0。\n    \n    请对每个句子进行分析，返回一个JSON格式的标签数组。格式要求：\n    1. 必须是有效的JSON格式\n    2. 只包含labels字段，值为0和1组成的数组\n    3. 数组长度必须与句子数量相同\n    4. 示例格式：{"labels": [0,1,0,1,...]}\n\n    需要判断的句子：\n\n    1. Abstract: Unmanned aerial vehicles (UAVs) with mounted cameras have the advantage of capturing aerial (bird-view) images.\n2. The availability of aerial visual data and the recent advances in object detection algorithms led the computer vision community to focus on object detection tasks on aerial images.\n3. As a result of this, several aerial datasets have been introduced, including visual data with object annotations.\n4. UAVs are used solely as flying-cameras in these datasets, discarding different data types regarding the flight (e.g., time, location, internal sensors).\n5. In this work, we propose a multi-purpose aerial dataset (AU-AIR) that has multi-modal sensor data (i.e., visual, time, location, altitude, IMU, velocity) collected in real-world outdoor environments.\n6. The AU-AIR dataset includes meta-data for extracted frames (i.e., bounding box annotations for trafficrelated object category) from recorded RGB videos.\n7. Moreover, we emphasize the differences between natural and aerial images in the context of object detection task.\n8. For this end, we train and test mobile object detectors (including YOLOv3-Tiny and MobileNetv2-SSDLite) on the AU-AIR dataset, which are applicable for real-time object detection using on-board computers with UAVs.\n9. Since our dataset has diversity in recorded data types, it contributes to filling the gap between computer vision and robotics.\n10. The dataset is available at https://bozcani.github.io/auairdataset.\n11. Unmanned aerial vehicles (UAVs) are extensively used as flying platforms of sensors for different domains such as traffic surveillance[1], managing the urban environment[2], package delivery[3]or aerial cinematography[4].\n12. For these applications, UAVs are equipped with mounted cameras and mainly gather visual data of the environment.\n13. Then, computer vision algorithms are applied to aerial visual data to extract high-level information regarding the environment.\n14. Object detection is one of the most studied problems in computer vision.\n15. The recent advances in deep learning (variants of convolutional neural networks (CNNs) mainly) have led to breakthrough object detection performances with the availability of large datasets and computing power.\n16. Since these methods require a large number of training samples, several datasets (e.g., COCO[5], Pascal VOC[6]) have been introduced for benchmarking for the object detection task.\n17. The samples in these datasets consist of natural images that are mainly captured by handheld cameras.\n18. The significant differences between natural and aerial images (such as object layouts and sizes) cause these object detectors to have trouble to find objects in aerial images.\n19. Therefore, several datasets (e.g.,[7]-[13]) have been introduced in recent years as a benchmark for object detection in aerial images.\n20. Besides visual data gathered by a camera, the data from other sensors might give crucial information about the environment.\n21. The use of UAVs as only flying cameras cut off the potential advance in multi-modal object detection algorithms for aerial applications.\n22. For instance, the recent advances in perception for autonomous driving have brought new datasets such as[14]-[16]including multi-modal data (e.g., RGB images, Global Positioning System (GPS) coordinates, inertial measurement unit (IMU) data).\n23. Although the data fusion for object detection is still open research topic[17], these multi-modal datasets allow a benchmark for further research.\n24. However, to the best of our knowledge, there is no such multi-modal dataset collected in a real-world outdoor environment for UAVs.\n25. In this work, we present a multi-modal UAV dataset (The AU-AIR dataset) in order to push forward the development of computer vision and robotic algorithms targeted at autonomous aerial surveillance.\n26. The AU-AIR dataset meets vision and robotics for UAVs having the multi-modal data from different on-board sensors.\n27. The dataset consists of 8 video streams (over 2 hours in total) for traffic surveillance.\n28. The videos mainly are recorded at Skejby Nordlandsvej and P.O Pedersensvej roads (Aarhus, Denmark).\n29. The dataset includes aerial videos, time, GPS coordinates and the altitude of the UAV, IMU data, and the velocity.\n30. The videos are recorded at different flight altitudes from 5 meters to 30 meters and in different camera angles from 45 degrees to 90 degrees (i.e., complete bird-view images that the camera is perpendicular to the Earth).\n31. Instances belonging to different object categories related to the traffic surveillance context are annotated with bounding boxes in video frames.\n32. Moreover, each extracted video frame is labeled with the flight data (See Fig.1).\n33. The whole dataset includes 32,823 labeled video frames with object annotations and the corresponding flight data.\n34. Eight object categories are annotated including person, car, van, truck, motorbike, bike, bus, trailer.\n35. The total number of annotated instances is 132,034.\n36. The dataset is split into 30,000 training-validation samples and 2,823 test samples.\n37. In this work, we emphasize differences between aerial and natural images in the context of object detection tasks.\n38. To this end, we compare image samples and object instances between the AU-AIR dataset and the COCO dataset[5].\n39. In our experiments, we train and evaluate two mobile object detectors (including YOLOv3-tiny[18]and MobileNetv2-SSD Lite[19]on the AU-AIR dataset.\n40. We form a baseline, including mobile object detectors since we focus on realtime performance and the applicability of object detection task onboard computers mounted on UAV.\n41. In recent years, several drone datasets have been introduced for object detection tasks ([7]-[13]).\n42. Zhu et al.[7]propose a UAV dataset (VisDrone) consisting of visual data and object annotations in images and frames.\n43. In the VisDrone dataset, object instances belonging the certain categories are annotated by bounding boxes and category labels.\n44. Besides object annotations, VisDrone includes some vision-related attributes such as the visibility of a scene, occlusion status.\n45. Du et al.[8]propose a benchmark dataset for object detection and tracking in aerial images.\n46. The dataset also includes meta information regarding the flight altitude.\n47. Hsieh et al.[9]propose a UAV-based counting dataset (CARPK) including object instances that belong to the car category.\n48. Robicquet et al.[10]introduce a UAV dataset (Stanford) that collects images and videos of six types of objects in the Stanford campus area.\n49. In this dataset, some of the object categories dominate the dataset having a high number of samples, whereas the remaining object categories have significantly less number of instances.\n50. Mueller et al.[11]propose synthetic dataset created by a simulator for target tracking with a UAV.\n51. Collins et al.[12]introduce a benchmarking website (VIVID) with an evaluation dataset collected under the DARPA VIVID program.\n52. Krajewski et al.propose an aerial dataset collected from highways, including object bounding boxes and labels of vehicles.\n53. These datasets are annotated by common objects in an environment such as humans and different types of vehicles (e.g., car, bike, van).\n54. However, they only include visual data and bounding box annotations for objects and discard other sensory data.\n55. Among these studies, only UAVDT[8]includes an attribute that gives limited information about the flight altitude (i.e., labels such as "low-level", "mid-level" and "high-level").\n56. Fonder et al.[20]propose a synthetic dataset (Mid-Air) for low altitude drone flights in unstructured environments (e.g., forest, country).\n57. It includes multi-modal data regarding the flight (e.g., visual, GPS, IMU data) without any annotations for visual data.\n58. There are also multi-modal drone datasets in the literature ([20]-[24]).\n59. However, the visual data are not collected for object detection since the main focus of these studies is the UAV navigation.\n60. Therefore, these datasets do not have object annotations.\n61. The comparison of existing datasets is given in TableI.\n62. Looking also at the summary of the existing studies in TableI, the followings are the main contributions of this work: • To the best of our knowledge, the AU-AIR dataset is the first multi-modal UAV dataset for object detection.\n63. The dataset includes flight data (i.e., time, GPS, altitude, IMU data) in addition to visual data and objects annotations.\n64. • Considering the real-time applicability, we form a baseline training and testing mobile object detectors with the AU-AIR dataset.\n65. We emphasize the differences between object detection in aerial images and natural images.\n66. The availability of large amounts of data and processing power enables deep neural networks to achieve state-of-theart results for object detection.\n67. Currently, deep learningbased object detectors are separated into two groups.\n68. The first group consists of region-based CNNs that ascend on image classifiers.\n69. Region-based CNNs propose image regions that are likely to contain an object and classify the region into a predefined object category.\n70. The second group has only one stage converting to the object detection problem into the bounding box prediction for objects, without re-purposing image classifiers.\n71. Faster-R-CNN[25]is one of the wellknown models belonging to the first group, YOLO[26]and SSD[27]are the popular object detectors that belong to the second group.\n72. Deep learning-based object detectors have trained and performed on large datasets such as COCO[5]and PASCAL[6].\n73. These datasets include natural images that contain a single object or multi objects in their natural environments.\n74. Most of the images in these datasets are captured by humans using a handheld camera so that the vast majority of images have side-view.\n75. There are challenges of the object detection in natural images such as occlusion, illumination changes, rotation, low resolution, crowd existence of instances.\n76. Aerial images have different characteristics from natural images due to having a bird\'s-eye view.\n77. First of all, objects in natural images are much larger than their counterparts in aerial images.\n78. For example, an object category such as humans may occupy a large number of pixels in natural images.\n79. However, it may have a few numbers of pixels   in an aerial image that is quite challenging to detect for object detectors (See Fig.2).\n80. Moreover, aerial images can be fed to a network with higher dimensions that increases computational cost in order to prevent the diminishing of pixels belonging to small objects.\n81. Secondly, an occlusion is observed in different conditions for natural and aerial images.\n82. In natural images, an object instance may be occluded by another foreground object instance (e.g., a human in front of a car).\n83. (See Fig.3.\n84. Thirdly, the perspective in aerial images makes appearances of objects short and squat.\n85. This fact diminishes the information regarding an object height (See Fig.4).\n86. Moreover, although aerial images can supply more contextual information about an environment by a broader view angle, the object instances may be amid cluttered.\n87. Lastly, having a drone to capture aerial images, the altitude changes during the flight can cause varieties in object size and appearance in aerial images.\n88. Therefore, a recording of aerial videos at different altitudes may change the levels of challenges mentioned above.\n89. To address the challenges mentioned in Section II, we propose a multi-modal drone dataset (AU-AIR) including videos, object annotations in the extracted frames and sensor data for the corresponding frames.\n90. The data are captured by low-level flight (max.30 meters) and for the scenario of a traffic surveillance.\n91. The AU-AIR dataset consists of video clips, sensor data, and object bounding box annotations for video frames.\n92. We have used a quadrotor (Parrot Bebop 2) to capture the videos and record the flight data.\n93. An on-board camera has recorded the videos with a resolution of 1920 × 1080 pixels at 30 frames per second (fps).\n94. The sensor data have been recorded for every 20 milliseconds.\n95. The AU-AIR dataset consists of 8 video clips (approximately in 2 hours of a total length) with 32,823 extracted frames.\n96. All videos are recorded for a scenario of aerial traffic surveillance at the intersection of Skejby Nordlandsvej and P.\n97. Moreover, the videos cover various lighting conditions due to the time of the day and the weather conditions (e.g., sunny, partly sunny, cloudy).\n98. Capturing an aerial video with a UAV brings different challenges for visual surveillance that are significantly different from natural images.\n99. To add these challenges in our dataset, we have captured the videos in different flight altitudes and camera angles.\n100. The flight altitude changes between 10 meters to 30 meters in the videos and the camera angle is adjusted from 45 degrees to 90 degrees (perpendicular to the Earth).\n101. An increase in the camera angle makes object detection task more challenging since images get differ from natural images.\n102. Although the videos have been recorded with 30 fps, we have extracted five frames for every second in order to prevent the redundant occurrence of frames.\n103. Both of raw videos and extracted frames have a resolution of 1920×1080 pixels.\n104. Considering a traffic surveillance scenario, we have manually annotated specific object categories in the frames.\n105. For annotation, we used a bounding box and object category index for each instance.\n106. The annotated object categories include eight types of objects which highly occur during the traffic surveillance: person, car, bus, van, truck, bike, motorbike, and trailer.\n107. For annotation, we employed workers on Amazons Mechanical Turk (AMT)[28].\n108. In order to increase the labeling quality, three workers annotated the same frame separately.\n109. Then, we combined annotations if they have the same object labels, and whose bounding boxes overlap more than a certain threshold.\n110. We chose a threshold as a value of 0.75 experimentally.\n111. In case this condition is not satisfied, we manually fine-tuned the bounding boxes and class labels.\n112. The category distribution over the dataset can be seen in Fig.5.\n113. In the context of traffic surveillance, cars appear significantly more than other classes, and three vehicle types (car, van, truck) have a major portion of annotated bounding boxes.\n114. The AU-AIR dataset includes frames that are captured in different flight altitudes (See Fig.6).\n115. We recorded the data mainly for 10 meters, 20 meters, and 30 meters with different camera angles from 45 degrees to 90 degrees.\n116. In addition to visual data and object annotations, the AU-AIR dataset includes sensor data that are logged during the video recording.\n117. In the dataset, we have the following attributes for each extracted frame: • la: latitude of the UAV (read from GPS sensor).\n118. • lo: longitude of the UAV (read from GPS sensor) • a: altitude of the UAV (read from altimeter) • φ: UAV roll angle (rotation around the x axis) (read from IMU sensor) • θ: UAV pitch angle (rotation around the y axis) (read from IMU sensor) • ψ: UAV yaw angle (rotation around the z axis) (read from IMU sensor) • V x : speed on the x axis • V y : speed on the y axis • V z : speed on the z axis TableIIshows unit values and ranges for each attribute except the date.\n119. The date (d) has a format of MMDDYYYY-HHMMSS where MM, DD, YYYY, HH, MM, SS indicates the month, day, year, hour, minutes, and second, respectively.\n120. The velocities (V x , V y , V z ) and rotation angles (φ, θ, ψ) are calculated according to the UAV body-frame given in Fig.7.\n121. We train and evaluate mobile object detectors with our dataset.\n122. During the evaluation, we consider real-time performance rather than achieving a state-of-the-art accuracy for the sake of the applicability.\n123. Therefore, we choose two mobile object detectors (YOLOv3-Tiny[18]and MobileNetv2-SSDLite[19]), which have a reasonable trade-off between the detection accuracy and the inference time.\n124. We configure YOLOv3-Tiny[18]and MobileNetv2-SSDLite[19]for the bench-marking using the default parameters (e.g., learning rate, input size) as suggested in the original papers.\n125. We use the models that are trained on the COCO dataset as backbones.\n126. We split the AU-AIR dataset into %60 training, %10 validation and %30 testing samples.\n127. The object detectors are adapted to the total number of classes in the AU-AIR dataset (8 classes in total) by changing their last layers.\n128. To compare detection performances, we use mean average precision (mAP) that is a prominent metric in object detection[5],[6].\n129. It is the mean of the average precision (AP) values which compute the precision score for an object category at discretized recall values over 0 to 1[6].\n130. We consider 11 different recall values as in[6]and the intersection over union (IoU) threshold as 0.5.\n131. For benchmarking, we train YOLOv3-Tiny and MobileNetv2-SSDLite with the AU-AIR Dataset.\n132. We use the batch size of 32 and Adam optimizer with the default parameters (alpha= 0.001, beta1=0.9,beta2=0.999).\n133. The training is stopped when the validation error starts to increase.\n134. Both networks are pre-trained on the COCO dataset.\n135. In order to see the effect of the training with an aerial dataset and a natural image dataset, we also use YOLOv3-Tiny and MobileNetv2-SSDLite trained on the COCO dataset without further training with the AU-AIR dataset.\n136. The results are given in TableIII.\n137. As shown in TableIII, the networks only trained on the COCO dataset have poor results.\n138. This is expected since the characteristics of natural images are significantly different from natural images.\n139. We observe that the AP values of motorbike and bicycle categories are significantly lower than the AP values of other categories.\n140. This fact might happen due to the class imbalance problem and the small object sizes of these categories.\n141. However, the bus category has the highest AP value, although there are fewer bus instances.\n142. This might result from the large size of bus instances in the frames.\n143. Furthermore, although the size of human instances is usually as small as the sizes of motorbike and bicycles, the AP values of the human category are relatively higher than these classes.\n144. This fact might be a consequence of the high number of human instances.\n145. There is no available AP values for the van and trailer categories in Table III since they do not exist in the COCO dataset.\n146. The baselines trained on the AU-AIR dataset are good at finding objects in aerial images that are captured at different altitudes and view angles.\n147. Qualitative results can be seen in Fig.8.\n148. Among the baselines, YOLOv3-Tiny has higher AP values and mAP value compared to MobileNetv2-SSDLite.\n149. There is no significant difference between inference times (17.5 FPS and 17 FPS for YOLOv3-Tiny and MobileNetv2-SSDLite on TX2, respectively).\n150. Since the number of instances of each object category is imbalanced in the AU-AIR dataset (Fig.5), we consider several methods to solve the imbalanced class problem in the next version of the dataset.\n151. As a first step, we will try to collect more data to balance the number of instances.\n152. Besides, we may consider adding synthetic data (i.e., changing the brightness of images, translation, rotation) to increase the number of object categories which has a low number of samples in the current version.\n153. We use AMT to annotate objects in images.\n154. Although three different people annotate one image and the annotations are manually checked by ourselves, there might be still overlooked samples that have weak annotations (e.g., unlabelled instances, loose bounding box drawings).\n155. Therefore, we consider using a three-step workflow proposed by Su et al.[29].\n156. In this workflow, the first worker draws a bounding box around an instance, the second worker verifies whether the bounding box is correctly drawn, and the third worker checks whether all object instances are annotated.\n157. Unlike other UAV object detection datasets, ours includes sensor data corresponding to each frame.\n158. In this work, we give a baseline only for object annotations and visual data.\n159. As future work, more baselines may be added to encourage research using sensor data (e.g., navigation and control of a UAV, object detection using multi-modal data).\n160. Also, we can add more visual sensors, such as multi-spectral cameras.\n161. We have used a ready-to-fly quadrotor (i.e., Parrot Bebop 2) to collect the whole dataset.\n162. We also consider collecting more samples from other platforms (e.g., different types of UAVs) using cameras that have different resolutions and frame rates.\n163. In this dataset, traffic surveillance is the primary context.\n164. In future work, we consider increasing the number of environment contexts to increase diversity in the dataset.\n165. In this work, we propose the AU-AIR dataset that is a multi-modal UAV dataset collected in an outdoor environment.\n166. Our aim is to fill the gap between computer vision and robotics having a diverse range of recorded data types for UAVs.\n167. Including visual data, object annotations, and flight data, it can be used for different research fields focused on data fusion.\n168. We have emphasized the differences between natural images and aerial images affecting the object detection task.\n169. Moreover, since we consider real-time performance and applicability in real-world scenarios, we have created a baseline, including two mobile object detectors in the literature (i.e., YOLOv3-Tiny[18]and MobileNetv2-SSDLite[19]).\n170. In our experiments, we showed that mobile networks trained on natural images have trouble in detecting objects in aerial images.\n\n请严格按照JSON格式返回：{"labels": [0,1,0,...]}，不要包含任何其他文字。'}], 'model': 'qwen-plus', 'response_format': {'type': 'json_object'}}}
2025-10-14 20:46:50,489 - DEBUG - Sending HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
2025-10-14 20:46:50,489 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-10-14 20:46:50,489 - DEBUG - send_request_headers.complete
2025-10-14 20:46:50,489 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-10-14 20:46:50,489 - DEBUG - send_request_body.complete
2025-10-14 20:46:50,489 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-10-14 20:46:58,014 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'vary', b'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding'), (b'x-request-id', b'33fcaf35-5097-4266-b225-6a34451d9135'), (b'x-dashscope-call-gateway', b'true'), (b'content-type', b'application/json'), (b'req-cost-time', b'7482'), (b'req-arrive-time', b'1760446000542'), (b'resp-start-time', b'1760446008024'), (b'x-envoy-upstream-service-time', b'7436'), (b'content-encoding', b'gzip'), (b'date', b'Tue, 14 Oct 2025 12:46:47 GMT'), (b'server', b'istio-envoy'), (b'transfer-encoding', b'chunked')])
2025-10-14 20:46:58,015 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-14 20:46:58,016 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-10-14 20:46:58,016 - DEBUG - receive_response_body.complete
2025-10-14 20:46:58,016 - DEBUG - response_closed.started
2025-10-14 20:46:58,016 - DEBUG - response_closed.complete
2025-10-14 20:46:58,016 - DEBUG - HTTP Response: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "200 OK" Headers({'vary': 'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding', 'x-request-id': '33fcaf35-5097-4266-b225-6a34451d9135', 'x-dashscope-call-gateway': 'true', 'content-type': 'application/json', 'req-cost-time': '7482', 'req-arrive-time': '1760446000542', 'resp-start-time': '1760446008024', 'x-envoy-upstream-service-time': '7436', 'content-encoding': 'gzip', 'date': 'Tue, 14 Oct 2025 12:46:47 GMT', 'server': 'istio-envoy', 'transfer-encoding': 'chunked'})
2025-10-14 20:46:58,017 - DEBUG - request_id: 33fcaf35-5097-4266-b225-6a34451d9135
2025-10-14 20:46:58,018 - DEBUG - API request completed in 7.53 seconds
2025-10-14 20:46:58,018 - DEBUG - Raw model response: {"labels": [1,0,0,0,1,1,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]}
2025-10-14 20:46:58,018 - INFO - Successfully processed 127 labels
2025-10-14 20:46:58,018 - ERROR - Label count mismatch for AU_AIR_ A Multi_modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance
2025-10-14 20:46:58,018 - INFO - Evaluating paper 6/18: AV_NeRF_ Learning Neural Fields for Real_World Audio_Visual Scene Synthesis
2025-10-14 20:46:58,018 - INFO - Starting model prediction
2025-10-14 20:46:58,018 - INFO - Attempt 1 of 5
2025-10-14 20:46:58,020 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-e7890ea8-f6d6-4ffc-8b1b-ec6bffa892af', 'json_data': {'messages': [{'role': 'system', 'content': '你是一个专门用于识别学术论文中数据集描述的助手。你需要判断每个输入的句子是否描述了研究中使用的数据集。请确保：\n    1. 只输出JSON格式的结果\n    2. 结果格式必须为 {"labels": [0,1,0,...]}\n    3. 不要输出任何其他文字或解释'}, {'role': 'user', 'content': '你是一位专业的学术论文数据集描述识别专家。请仔细分析下面这篇论文中的每个句子，判断它们是否描述了本研究特定构建的数据集信息。\n\n    论文标题: AV_NeRF_ Learning Neural Fields for Real_World Audio_Visual Scene Synthesis\n\n    判断标准:\n    判断为1(数据集描述)的标准：\n    句子必须直接描述本研究特定构建的数据集,包括:\n    1. 该数据集的构建过程和方法\n    2. 该数据集的具体构成和规模\n    3. 该数据的来源和收集方式\n    4. 该数据的预处理步骤\n    5. 该数据集的可获取方式(如发布地址)\n    6. 或者其他直接或者间接描述了该数据集的句子\n\n    所有其他类型的句子均标记为0。\n    \n    请对每个句子进行分析，返回一个JSON格式的标签数组。格式要求：\n    1. 必须是有效的JSON格式\n    2. 只包含labels字段，值为0和1组成的数组\n    3. 数组长度必须与句子数量相同\n    4. 示例格式：{"labels": [0,1,0,1,...]}\n\n    需要判断的句子：\n\n    1. Abstract: Can machines recording an audio-visual scene produce realistic, matching audiovisual experiences at novel positions and novel view directions?\n2. We answer it by studying a new task-real-world audio-visual scene synthesis-and a first-of-itskind NeRF-based approach for multimodal learning.\n3. Concretely, given a video recording of an audio-visual scene, the task is to synthesize new videos with spatial audios along arbitrary novel camera trajectories in that scene.\n4. We propose an acoustic-aware audio generation module that integrates prior knowledge of audio propagation into NeRF, in which we implicitly associate audio generation with the 3D geometry and material properties of a visual environment.\n5. Furthermore, we present a coordinate transformation module that expresses a view direction relative to the sound source, enabling the model to learn sound source-centric acoustic fields.\n6. To facilitate the study of this new task, we collect a high-quality Real-World Audio-Visual Scene (RWAVS) dataset.\n7. We demonstrate the advantages of our method on this real-world dataset and the simulation-based SoundSpaces dataset.\n8. We recommend that readers visit our project page for convincing comparisons: https://liangsusan-git.github.io/project/avnerf/.\n9. We study a new task, real-world audio-visual scene synthesis, to generate target videos and audios along novel camera trajectories from source audio-visual recordings of known trajectories.\n10. By learning from real-world source videos with binaural audio, we aim to generate target video frames and spatial audios that exhibit consistency with the given camera trajectory visually and acoustically.\n11. This consistency ensures perceptual realism and immersion, enriching the overall user experience.\n12. As far as we know, attempts in the audio-visual learning literature [1-11] have yet to succeed in solving this challenging task thus far.\n13. Although there are similar works[12][13][14][15], these methods have constraints that limit their ability to solve this new task.\n14. Luo et al. [12]  propose neural acoustic fields to model sound propagation in a room.\n15. Su et al. [13]  introduce representing audio scenes by disentangling the scene\'s geometry features.\n16. These methods are tailored for estimating room impulse response signals in a simulation environment that are difficult to obtain in a real-world scene.\n17. Concurrent to our work, ViGAS proposed by Chen et al. [15]  learns to synthesize new sounds by inferring the audio-visual cues.\n18. However, ViGAS is limited to a few viewpoints for audio generation.\n19. We introduce AV-NeRF, a novel NeRF-based method of synthesizing real-world audio-visual scenes.\n20. AV-NeRF enables the generation of videos and spatial audios, following arbitrary camera trajectories.\n21. It utilizes source videos and camera poses as references.\n22. AV-NeRF consists of two branches: A-NeRF, which learns the acoustic fields of an environment, and V-NeRF, which models color and density fields.\n23. We represent a static audio field as a continuous function using A-NeRF, which takes the listener\'s position and head direction as input.\n24. A-NeRF effectively models the energy decay of sound as the sound travels from the source to the listener by correlating the listener\'s position with the 37th Conference on Neural Information Processing Systems (NeurIPS 2023).\n25. To the best of our knowledge, our method is the first NeRF-based system capable of synthesizing realworld videos with perceptually realistic binaural audios at arbitrary poses.\n26. However, existing datasets do not meet the specific requirements of our experiments, particularly in terms of simultaneously providing camera poses, high-quality binaural audios, and images.\n27. Therefore, we curated a highquality audio-visual scene dataset (real) to address this gap and facilitate further research on this problem.\n28. Additionally, we utilize (synthetic) SoundSpaces dataset[4]to validate our method.\n29. (1) RWAVS Dataset.\n30. We collected the Real-World Audio-Visual Scene (RWAVS) dataset to benchmark our method.\n31. In order to increase the diversity of our dataset, we recorded data across different scenarios.\n32. Fig.5shows the example scenarios we used for data recording, including both indoor and outdoor environments, which we believe represent most daily settings.\n33. RWAVS dataset comprises multimodal data, including camera poses, high-quality binaural audios, and videos.\n34. Unlike Replay-NVAS dataset[15], where the environment and the recording viewpoint are constant, RWAVS dataset contains various viewpoints in diverse environments.\n35. During data recording, we randomly moved around the environment while holding the device, capturing various acoustic and visual signals.\n36. RWAVS dataset encompasses all positions and directions (360 • ) within an environment.\n37. In detail, we employed a 3Dio Free Space XLR binaural microphone for capturing high-quality stereo audio, a TASCAM DR-60DMKII for recording and storing audio, and a GoPro Max for capturing accompanying videos.\n38. Additionally, an LG XBOOM 360 omnidirectional speaker was used as the sound source.\n39. For each environment and sound source combination, we collected data ranging from 10 to 25 minutes, resulting in a total collection of 232 minutes (3.8 hours) of data from diverse environments with varying source positions.\n40. We extract key frames at 1 fps from recorded videos and use COLMAP[46]to estimate the corresponding camera pose.\n41. Each key frame is accompanied by one-second binaural audio and one-second source audio, forming a complete data sample.\n42. For audio clips with noticeable background noise, we perform noise suppression using Adobe Audition[47].\n43. We split 80% data as training samples and the rest as validation samples.\n44. After pre-processing, we obtain 9850 and 2469 samples for training and validation, respectively.\n45. This dataset is challenging because of the diverse environments and various camera poses.\n46. We will release this dataset to the research community.\n47. (2) SoundSpaces Dataset.\n48. While RWAVS offers realistic training samples, its realism restricts its scale because it is time-consuming to record high-quality multimodal data in the real world.\n49. Therefore, we use the synthetic SoundSpaces dataset to augment our experiments.\n50. To evaluate our method on SoundSpaces dataset, we modify AV-NeRF to estimate impulse responses instead of the acoustic mask while keeping all other components intact.\n51. We follow NAF[12]selecting six representative indoor scenes, consisting of two single rooms with rectangular walls, two single rooms with non-rectangular walls, and two multi-room layouts.\n52. In each scene, SoundSpaces dataset provides an extensive collection of impulse response signals for sound source and sound receiver pairs, which are densely sampled from a 2D room grid.\n53. Each pair includes four discrete head orientations (0 • , 90 • , 180 • , and 270 • ), and each orientation is associated with two-channel binaural RIRs.\n54. We render RGB and depth images for each sound receiver pose using Habitat-Sim simulator[48,49].\n55. We maintain the same training/test split as NAF, allocating 90% data for training and 10% data for testing.\n56. Comparison with State-of-the-art.\n57. We compare AV-NeRF with the following baselines: (1) Mono-Mono duplicates the source audio a s twice to generate a fake binaural audio without modifying the source audio; (2) Mono-Energy assumes that the average energy of the target audio a t is known, scales the energy of the input audio to match the target, and duplicates the scaled audio to generate a stereo audio; (3) Stereo-Energy assumes that the energy of the two channels of the target audio a t is known, separately scales the energy of the input audio to match the target, and combines the two scaled channels to generate a stereo audio; (4) IRNAS[13]learns representing audio scenes by disentangling scene\'s geometry features with implicit neural fields, and we adapt INRAS to predict wave masks on RWAVS dataset; (5) NAF[12]designs local feature grids and an implicit decoder to capture the sound propagation in a physical scene, and we modify NAF to predict magnitude masks on RWAVS dataset; (6) ViGAS[15]achieves novel-view acoustic synthesis by analyzing audio-visual cues from source viewpoints.\n58. We select magnitude distance (MAG)[29], which measures the audio quality in the time-frequency domain, and envelope distance (ENV)[30], which measures the audio quality in the time domain, to evaluate various methods.\n59. Please refer to the supplementary material for implementation details.\n60. AV-NeRF outperforms all baselines across different environments, including office, house, apartment, and outdoors, by a significant margin (Table1(2) adding visual information to the input of A-NeRF is the most effective multimodal fusion method compared with concatenation and adding visual information to all layers of A-NeRF (Table2middle); (3) using embeddings represent relative angles outperforms applying positional encoding to either absolute or relative angles (Table2right).\n61. "Absolute Direction" represents applying positional encoding to the absolute angle, "Relative Direction" means transforming the relative angle with the positional encoding, and "Relative Embedding" is the embedding method.\n62. Visualization.\n63. We visualize the synthesized audio-visual scenes in Fig.6to intuitively assess the generation quality of our model.\n64. AV-NeRF can synthesize realistic binaural audios that have the same signal envelope and channel difference as the ground-truth audios.\n65. We compare AV-NeRF with traditional audio coding methods[50,51]and advanced learningbased neural field methods[12,13]using T60, C50, and EDT metrics[13].\n66. Please refer to our supplementary material for implementation details.\n67. Table3shows that AV-NeRF outruns both traditional and advanced methods, achieving 21% relative improvement on T60 metric compared with the previous state-of-the-art method INRAS, 5% on C50, and 16% on EDT.\n\n请严格按照JSON格式返回：{"labels": [0,1,0,...]}，不要包含任何其他文字。'}], 'model': 'qwen-plus', 'response_format': {'type': 'json_object'}}}
2025-10-14 20:46:58,022 - DEBUG - Sending HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
2025-10-14 20:46:58,022 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-10-14 20:46:58,022 - DEBUG - send_request_headers.complete
2025-10-14 20:46:58,022 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-10-14 20:46:58,022 - DEBUG - send_request_body.complete
2025-10-14 20:46:58,022 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-10-14 20:47:04,451 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'vary', b'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding'), (b'x-request-id', b'3ed76f7b-924f-434c-8ebb-cdac7e7f9313'), (b'x-dashscope-call-gateway', b'true'), (b'content-type', b'application/json'), (b'req-cost-time', b'6373'), (b'req-arrive-time', b'1760446008075'), (b'resp-start-time', b'1760446014448'), (b'x-envoy-upstream-service-time', b'6370'), (b'content-encoding', b'gzip'), (b'date', b'Tue, 14 Oct 2025 12:46:54 GMT'), (b'server', b'istio-envoy'), (b'transfer-encoding', b'chunked')])
2025-10-14 20:47:04,452 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-14 20:47:04,452 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-10-14 20:47:04,452 - DEBUG - receive_response_body.complete
2025-10-14 20:47:04,453 - DEBUG - response_closed.started
2025-10-14 20:47:04,453 - DEBUG - response_closed.complete
2025-10-14 20:47:04,453 - DEBUG - HTTP Response: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "200 OK" Headers({'vary': 'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding', 'x-request-id': '3ed76f7b-924f-434c-8ebb-cdac7e7f9313', 'x-dashscope-call-gateway': 'true', 'content-type': 'application/json', 'req-cost-time': '6373', 'req-arrive-time': '1760446008075', 'resp-start-time': '1760446014448', 'x-envoy-upstream-service-time': '6370', 'content-encoding': 'gzip', 'date': 'Tue, 14 Oct 2025 12:46:54 GMT', 'server': 'istio-envoy', 'transfer-encoding': 'chunked'})
2025-10-14 20:47:04,453 - DEBUG - request_id: 3ed76f7b-924f-434c-8ebb-cdac7e7f9313
2025-10-14 20:47:04,454 - DEBUG - API request completed in 6.44 seconds
2025-10-14 20:47:04,454 - DEBUG - Raw model response: {"labels": [0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}
2025-10-14 20:47:04,454 - INFO - Successfully processed 67 labels
2025-10-14 20:47:04,475 - ERROR - Error processing paper AV_NeRF_ Learning Neural Fields for Real_World Audio_Visual Scene Synthesis: 'int' object has no attribute 'capitalize'
2025-10-14 20:47:04,475 - INFO - Evaluating paper 7/18: BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment
2025-10-14 20:47:04,475 - INFO - Starting model prediction
2025-10-14 20:47:04,475 - INFO - Attempt 1 of 5
2025-10-14 20:47:04,476 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-e63b4a29-f3d5-479f-b41b-3f4eae97f761', 'json_data': {'messages': [{'role': 'system', 'content': '你是一个专门用于识别学术论文中数据集描述的助手。你需要判断每个输入的句子是否描述了研究中使用的数据集。请确保：\n    1. 只输出JSON格式的结果\n    2. 结果格式必须为 {"labels": [0,1,0,...]}\n    3. 不要输出任何其他文字或解释'}, {'role': 'user', 'content': '你是一位专业的学术论文数据集描述识别专家。请仔细分析下面这篇论文中的每个句子，判断它们是否描述了本研究特定构建的数据集信息。\n\n    论文标题: BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment\n\n    判断标准:\n    判断为1(数据集描述)的标准：\n    句子必须直接描述本研究特定构建的数据集,包括:\n    1. 该数据集的构建过程和方法\n    2. 该数据集的具体构成和规模\n    3. 该数据的来源和收集方式\n    4. 该数据的预处理步骤\n    5. 该数据集的可获取方式(如发布地址)\n    6. 或者其他直接或者间接描述了该数据集的句子\n\n    所有其他类型的句子均标记为0。\n    \n    请对每个句子进行分析，返回一个JSON格式的标签数组。格式要求：\n    1. 必须是有效的JSON格式\n    2. 只包含labels字段，值为0和1组成的数组\n    3. 数组长度必须与句子数量相同\n    4. 示例格式：{"labels": [0,1,0,1,...]}\n\n    需要判断的句子：\n\n    1. Abstract: Banding, also known as staircase-like contours, frequently occurs in flat areas of images/videos processed by the compression or quantization algorithms.\n2. As undesirable artifacts, banding destroys the original image structure, thus inevitably degrading users\' quality of experience (QoE).\n3. In this paper, we systematically investigate the banding image quality assessment (IQA) problem, aiming to detect the image banding artifacts and evaluate their perceptual visual quality.\n4. Considering that the existing image banding databases only contain limited content sources and banding generation methods, and lack perceptual quality labels (i.e.mean opinion scores), we first build the largest banding IQA database so far, named Banding Artifact Noticeable Database (BAND-2k), which consists of 2,000 banding images generated by 15 compression and quantization schemes.\n5. A total of 23 workers participated in the subjective IQA experiment, yielding over 214,000 patch-level banding class labels and 44,371 reliable image-level quality rating scores.\n6. Subsequently, we develop an effective no-reference (NR) banding evaluator for banding detection and quality assessment by leveraging frequency characteristics of banding artifacts.\n7. To be more specific, a dual convolutional neural network (CNN) is employed to concurrently learn the feature representation from the high-frequency and low-frequency maps, thereby enhancing the ability to discern banding artifacts.\n8. The quality score of a banding image is generated by pooling the banding detection maps masked by the spatial frequency filters.\n9. The experimental results demonstrate that our banding evaluator achieves a remarkably high accuracy in banding detection and also exhibits high SRCC and PLCC results with the perceptual quality labels, even without directly learning a regression model for banding quality evaluation.\n10. These findings unveil the strong correlations between the intensity of banding artifacts and the perceptual visual quality, thus validating the necessity of banding quality assessment.\n11. The BAND-2k database and the proposed banding evaluator will be available at https://github.com/zijianchen98/BAND-2k.\n12. R ECENT years have witnessed a rapid emergence of media streaming services and social platforms.\n13. YouTube, Netflix, and TikTok account for more than half of the world\'s video traffic.\n14. Improving the quality of images under limited encoding, Zijian Chen, Wei Sun, Jun Jia, Zicheng Zhang, Xiongkuo Min, and Guangtao Zhai are with the Institute of Image Communication and Information Processing, Shanghai Jiao Tong University, Shanghai 200240, China (e-mail: {zijian.chen,sunguwei, jiajun0302, zzc1998, minxiongkuo, zhaiguangtao}@sjtu.edu.cn).\n15. Fangfang Lu is with the College of Computer Science and Technology, Shanghai University of Electric Power, Shanghai 200290, China (email:lufangfang@shiep.edu.cn).\n16. Jing Liu is with the School of Electrical and Information Engineering, Tianjin University, Tianjin, China.(e-mail:jliu tju@tju.edu.cn).\n17. transmission bandwidth, and storage condition is a necessary prerequisite for meeting the quality of experience (QoE) of users.\n18. In the stages between image acquisition and display, an image may suffer from various types of degradation, while banding artifacts are a kind of false contour distortion that is quite perceptible to the human eye.\n19. Since the visual quality of image contents greatly affects the QoE of end-users, it is highly desirable to design an effective banding image quality assessment (IQA) method, which aims to automatically detect the traces of such false contours and predict the objective quality of banding images that can be used to develop pre-processing or postprocessing debanding algorithms and optimize the performance of streaming media application.\n20. Normally, banding artifacts take on the appearance of annual rings, radiation circles, halos, or geographical contour lines and especially exist in the background regions (e.g., sky, water, and wall surface), where the color transition is not smooth enough.\n21. Nearly all existing image or video encoders, including H.264/AVC[28], VP9[38], and H.265/HEVC[27]can introduce such artifacts more or less.\n22. Current banding IQA research can be divided into two categories: subjective quality assessment and objective quality assessment.\n23. The existing subjective banding IQA research[39],[41]-[43]mainly investigates the limited banding scenarios with internal-used and undisclosed databases while lacking the quality label and may be insufficiently generalizable to large-scale commercial applications.\n24. Meanwhile, general IQA methods aiming at common distortions are inapplicable for banding exacerbated images due to the essential differences between them.\n25. First, banding artifacts usually hold tiny, staircaselike, and regional structures, which can be regarded as a kind of high-frequency artifact in smooth areas, while general distortions occur obviously in the whole image and are globally uniform.\n26. Second, the perceptual severity of banding is quantified based on its fraction of coverage and intensity in an image, which is widely divergent from the design philosophy of many existing IQA approaches.\n27. As a result, it is challenging to design an effective banding IQA method.\n28. To address these limitations, we first conduct a comprehensive subjective study of banding exacerbated images and create the largest banding IQA database to date with reliable mean opinion scores (MOS) and patch-level banding labels.\n29. We also propose a novel no-reference banding evaluator for banding detection and quality assessment by leveraging the frequency characteristics of banding artifacts.\n30. First, due to the dissimilar peculiarities of banding and smooth regions, the same distortion in different regions, e.g., textual and pictorial regions, may lead to different visual perception of human beings.\n31. Considering that banding manifests as a high-frequency artifact that exists in the lowfrequency smooth region, we propose a dual-branch CNN, which takes the high-frequency map and low-frequency map as inputs simultaneously, to hierarchically incorporate different visual features from the first layer and the last layer of our Resnet-50 backbone, thus making the model learn more effective banding feature representation and achieving more accurate banding region discrimination.\n32. Furthermore, inspired by the previous studies[15],[16], spatial frequency extracts information consistent with the human visual system (HVS), which not only reflects the overall active level in an image but also intuitively quantifies the contrast information.\n33. In other words, the value of spatial frequency is large in smooth areas, while becoming small in areas with harsh contrast changes, i.e., banding areas.\n34. Based on this mechanism, we adopt a spatial frequency masking strategy to refine the detected banding map and then pool the masked banding detection map to obtain the image-level banding quality score.\n35. • We propose a novel no-reference banding evaluator for banding detection and quality assessment based on frequency characteristics of banding artifacts, which utilizes a dual-branch CNN model to extract hierarchical banding-related feature representation from the high-frequency maps and low-frequency maps simultaneously.\n36. • Experimental results show that the proposed banding evaluator achieves the best performance in banding detection and significantly surpasses baselines in terms of SRCC and PLCC in the banding IQA task, which demonstrates the effectiveness of the proposed model.\n37. The remainder of this paper is organized as follows.\n38. Section II provides an overview of related works, including the stateof-the-art banding databases and detection methods.\n39. Section III introduces the construction of the BAND-2k database and the subjective assessment study.\n40. Section IV proposes an effective no-reference banding evaluator for banding detection and quality assessment.\n41. Section V gives the experimental results and analysis.\n42. Section VI concludes this paper.\n43. In this section, we first provide an overview of the state-ofthe-art banding-related databases (TableI) and then review the banding detection and evaluation methods.\n44. The first banding artifact-relevant VQA database was proposed by Wang et al.[39], which consists of 21 stimuli with different quantization grades generated by VP9[38]from 7 clips of 1280 × 720 30fps video.\n45. Authors in[41]investigated the effect of encoding parameters and dithering on the visibility of banding.\n46. Nine 4k-10bit source clips from the existing Netflix catalogue between 1 and 5 seconds were used to generate banding distorted videos.\n47. Each source content was downsampled to appropriate resolutions (1080p, 2k, or 4k) with certain bit-depth and further compressed by libaom (an AV1 codec library) at QPs {12, 20, 32}.\n48. More recently, Kapoor et al.[43]constructed one of the first databases for data-driven image banding assessment models.\n49. This research included about 1,440 images shot from over 600 pristine HD videos with a resolution of 1920 × 1080.\n50. Six levels of bit-depth quantization in luminance and chrominance channels are introduced to obtain different intensities of banding.\n51. Meanwhile, the banding images were semi-automatically segmented and labeled into banded and non-banded to form a patch-level banding dataset, which allows for training machine learning-based and deep learning-based banding classification methods.\n52. However, to the best of our knowledge, thus far there still lacking a benchmarking dataset in the banding detection and the corresponding banding IQA domain.\n53. Researchers either resort to image/video quality datasets that do not aim at banding distortion or build a small, attribute-restricted, in-house dataset by themselves.\n54. This motivates us to construct a large-scale subjective assessment database focus on the perceived bandingaffected image quality.\n55. Early research on banding detection mainly focuses on false contour identification, which aims to find the wrong boundary rather than a "true" region edge in the image.\n56. Authors in[31]-[33]utilized monotonicity or non-monotonicity features of local support regions including the gradient, contrast, variance, and entropy information to measure the loss of low-amplitude detail caused by banding.\n57. However, these works ignored the perceptual characteristics of the human visual system (HVS) and thus did not perform a good correlation with subjective tests.\n58. Another banding detection strategy is conducted at the pixel-level estimation and segmentation.\n59. Bhagavathy et al.[30]proposed to identify banding artifacts by calculating the likelihood of pixel difference.\n60. Baugh et al.[34]measured the severity of banding based on the number of a group of connected pixels with the same color.\n61. Wang et al.[39]first detected uniform segments to find possible banding areas and further incorporated edge features (e.g.length and contrast) to capture false boundaries.\n62. Nevertheless, these kinds of methods are typically sensitive to edge noise and are computationally expensive, causing limited application in realtime scenarios.\n63. Towards addressing these problems, Tu et al.[42]presented a completely no-reference banding detection method, which combines various properties of HVS with a number of preprocessing steps to refine banding edge detection.\n64. Instead of regarding banding detection as a false edge detection problem, Tandon et al.[41]heuristically utilized the effect of contrast sensitivity function (CSF) on banding visibility and its dependence on spatial frequency.\n65. Based on this, Krasula et al.[40]further compared the banding annoyance with more commonly studied compression artifacts and proposed a banding-aware video quality metric.\n66. In recent years, deep learning approaches have prevailed in various VQA tasks.\n67. As the pioneering work, Kapoor et al.[43]developed an automated CNN-based banding detector for the first time, which is a simple two-stage algorithm and gives rise to devising other learning-based techniques.\n68. In this work, we build a large-scale banding database and propose a data-driven banding indicator that can generate pixelwise banding visibility maps with corresponding subjectively  consistent quality scores by combining human visual mechanisms and deep learning techniques.\n69. Subjective banding image quality assessment facilitates the development of automatic objective banding image and video quality models.\n70. We created the largest banding database in existence, denoted as the BAND-2k database, which consists of 2,000 banding distorted images and over 214,000 patch-level banding class labels.\n71. Then, a subjective experiment was conducted to obtain the mean opinion scores (MOS) of the BAND-2k database.\n72. The workflow of the banding database construction is shown in Fig.1.\n73. To build a content-rich and balanced database, we manually collected source videos including computer graphics (CG), usergenerated content (UGC), and professionally-generated content (PGC) from two popular media websites Bilibili.comand Youtube.com.\n74. Then, 885 clips with multiple spatial resolutions (i.e., 4096×2160, 3840×2160, 1920×1080) and frame rates (i.e., 60, 50, 30, 25) are chosen as candidate.\n75. Note that videos on the mentioned websites are firstly annotated by the community with assigned a number of favorites, views, and downloads.\n76. These statistics correlate with the content and quality of a video, which guides our choices to some extent.\n77. All videos selected on the website are released under an appropriate creative commons (CC) license that allows further editing and redistribution.\n78. After content selection, we further unified the format of all video clips, especially the spatial resolution and the pixel format, which avoid the effect of other facts on visual quality.\n79. Concretely, we first converted the frame rate of the original clips to 25fps, which is to reduce the storage pressure while ensuring the graphics quality.\n80. Considering the commonly used aspect ratio of the user interface and displays is 16:9, we cropped the partially unqualified videos rather than shrinking images unevenly.\n81. Then, we downsampled the trimmed spatial resolution 3840×2160 to a lower resolution -1920×1080 for the following subjective study.\n82. Before conducting the following experiments, we manually removed the videos that are either too dark or bright, overly blurry or colorful, which helps to obtain more reliable subjective assessments.\n83. To avoid redundancy and to make sure the diversity of selected contents, we also conducted attribute analysis studies on the selected images.\n84. Four metrics that correlated with human perception, i.e., contrast, brightness, sharpness, and colorfulness, are adopted as content diversity metrics.\n85. All video attributes are calculated on every 10 frames to reduce computational complexity, which are then averaged over frames of each video sequence.\n86. Fig.2shows the distribution of attributes extracted from the selected videos.\n87. • Contrast: The contrast metric is simply defined as the standard deviation of pixel gray-scale intensities[25].\n88. • Colorfulness: The colorfulness metric is measured by the R, G, and B components[25].\n89. We first compute two matrices of differences between channels rg = R -G and yb = 1 2 (R + G) -B.\n90. Then, the colorfulness metric can be calculated as µ 2 rg + µ 2 yb + σ 2 rg + σ 2 yb , where µ and σ are the mean and standard deviation of their corresponding terms, respectively.\n91. • Sharpness: The cumulative probability of blur detection (CPBD) metric[24]is used to measure the image sharpness, which estimates the probability of detecting blur at each edge.\n92. • Brightness: The brightness of an image is obtained directly from the pixel gray intensities in R, G, and B channels.\n93. Finally, the number of source videos are reduced to 873.\n94. Fig.3displays thumbnails for 30 selected representative video clips.\n95. To simulate authentic banding artifacts that exist in real viewing scenarios, we introduced four encoding/transcoding strategies including H.264/AVC[28], H.265/HEVC[27], VP9[38], and bit-depth manipulation[43]with fifteen quantization schemes in total.For H.264/AVC and H.265/HEVC, considering the range of their quantization parameter (QP in ffmpeg), we selected three typical QP values, namely, QP = {18, 33, 43}.\n96. This is because that coded video clips with a QP value smaller than 18 normally provide perceptual lossless quality, while coded video clips with a QP value larger than 43 will not be able to offer adequate quality, which may affect the subjective test of banding artifacts.\n97. Similarly, for the VP9 encoder, the QP values under our close inspection are chosen as {28, 45, 60}.\n98. Moreover, we applied the same quantization strategies as in[43], which introduces banding distortion by scaling bit-depth in luminance and chrominance channels.\n99. Here, the color coding scheme YCbCr4:2:0 is applied for maximum compatibility.\n100. To sum up, we generated fifteen levels of banding with different intensities and shapes to enhance the diversity of the database.\n101. The banding database is then built by extracting frames from the distorted video clips, resulting in 2,000 images with a resolution of 1920×1080.\n102. Fig.4shows the visualization results of banding exacerbated images.\n103. In the process of image patch labeling, we initially intend to extract image patches of size 235×235 from banding images directly and perform annotation operations.\n104. However, considering the theoretical number of patches, it will cost a lot of manpower and time to label patch by patch.\n105. Therefore, we first segmented the banding images roughly and label them into banded and non-banded regions.\n106. Then, labelled image patches are generated from these segmented and labelled images by a sliding window.\n107. Specifically, we followed the same demarcation of the banded and non-banded images in[43]that a patch is labelled as banded if it has more than 30% overlap with banded regions in the image.\n108. Eventually, a banding dataset containing 2,000 distorted images with 1920×1080 resolution and 214,324 labelled image patches of size 235×235 is built.\n109. To the best of our knowledge, it is the largest banding dataset in existence, which enables training various machine/deep learning based banding detection models and facilitates the development of image/video debanding techniques.\n110. TableIIreports the composition of labelled image patch dataset.\n111. It can be observed that the number of banded patches is a bit smaller than non-banded patches due to the fact that banding usually appears in smooth background areas.\n112. As shown in Fig.1, the subjective quality study contains four steps.\n113. In addition to preparing the experimental environment, subjects should pass the qualification test first to participate in the study.\n114. After the subjective rating, all resulting scores need to be analyzed and examined before generating the final mean opinion score (MOS).\n115. 1) Experimental Environment Setting: In this study, a total of 25 inexperienced subjects are gathered in a laboratory environment, where relevant experimental configuration must satisfy the following requirements: • Considering the viewing effect, desktops and laptops are allowed as displays.\n116. • The resolution of displays must be larger than or equal to 1920×1080 to show the images without spatial downsampling.\n117. • The viewing distance and optimal horizontal viewing angle are set as 1.9 times the height of the display and respectively.\n118. 500 recommendation[26].\n119. As a result, we used a 27-inch AOC Q27U2D monitor with a resolution of 2560×1440 for assessment with 25 subjects.\n120. Due to the large number of images to be assessed, we divided the dataset into ten sessions to avoid visual fatigue.\n121. Each session of tests took nearly 2 hours with a 30-minute break for each participant.\n122. 2) Qualification Test: Before starting the main experiment, subjects are required to pass a quiz to get the qualification of conducting follow-up experiments.\n123. Firstly, we manually selected 10 labeled banding exacerbated images beyond the database as training images to familiarize subjects with the operation interface and the goal of this subjective test.\n124. The quiz consists of two parts including banding classification and image-level quality rating.\n125. In banding classification, subjects were told to divide the test image into banded or non-banded.\n126. In image-level quality rating, subjects were instructed to focus on the coverage and intensity of banding areas, as well as the overall quality of images to get the final quality score.\n127. The rating scale is continuous from 0 to 100 while a higher value indicates more severe banding (i.e., more visible or occupying larger portion of the image).\n128. To make the quiz objective and fair, we take the banding images labeled by domain experts as the ground truth, while two existing banding metrics BBAND[42]and DBI[43]are used to determine the normal range of the rating scores.\n129. That is the quality scores rated by subjects should not exceed 20% of the above banding metrics.\n130. As a result, only 23 subjects with an accuracy above 80% in banding classification and subjective scores in the normal range were allowed to pass the quiz.\n131. Note that the steps for taking a quiz are the same as the main experiment, which aims to guarantee the consistency of results.\n132. 3) Formal Study: We adopted the single-stimulus (SS) method in this test.\n133. Ten golden images that have the acknowledged high quality or poor quality (assessed by BBAND[42]and DBI[43]) were added to each session for controlling the scoring deviations.\n134. Besides, 3 repeated images are randomly inserted into each session to ensure consistency of scores before and after subjects scoring.\n135. At last, 23 qualified subjects were asked to provide their opinions on the shuffled image groups.\n136. The resulting scores were collected and packed for further analysis.\n137. 4) Result Analysis: In total, 46,000 scores were collected by 23 qualified subjects in the main study phase.\n138. However, considering the qualification quiz cannot completely disallow those unreliable workers to muddle through to the main study while reliable subjects may also occasionally score odd values, which may be caused by the inter-individual differences in perceiving the quality of the unique characteristics of different contents.\n139. Thus, we further investigated the confidence of rating scores and removed outliers following the Grubbs\' test[23],[45].\n140. Concretely, let s = (s 1 , s 2 , . . ., s N ) denote a set of raw scores collected for one distorted image.\n141. The test statistics is defined as the maximum absolute deviation of a sample standard deviation (SD) unit from the sample mean, which is mathematically expressed as where s and σ s denote the mean and standard deviation of the sample, respectively.\n142. Then, given a significant level α, a sample is detected as an outlier if where t 2 α/(2N ),N -2 represents the upper critical value of the tdistribution with N -2 degrees of freedom and a significance level of α/(2N ).\n143. Empirically, we set the significance level α at 0.05.\n144. Then, a sample is identified as an outlier if its distance to the sample mean is larger than 2.5 times SD and is removed.\n145. Following the aforementioned steps, the total number of scores Fig.6.\n146. The overall architecture of the proposed method.\n147. Given a banding distorted image, it is first divided into patches.\n148. Then, the patch-level high-frequency map (HFM) and low-frequency map (LFM) are generated by Sobel operation and piece-wise smooth algorithm[19], respectively.\n149. After that, a dual-branch CNN (CNN H and CNN L ) is deployed to extract hierarchical features with different visual information and thus classify the patches into banded or non-banded.\n150. Lastly, a spatial frequency masking strategy is introduced to refine the banding map and calculate the image-level banding quality score.\n151. Note that the dual-branch networks do not share parameters.\n152. was reduced to 44,371, and MOS was created by averaging the scores for each image.\n153. Fig.5presents the histogram of MOSs over the entire database, showing a broad MOS distribution of banding images.\n154. In this section, we describe the architecture of the proposed banding evaluator in detail, as shown in Fig.6.\n155. As stated before, banding usually appear as high-frequency information in the smooth background, while humans perceive high-frequency texture regions and low-frequency plateau regions through different neural channels concurrently, and transfer the upper visual features into the cerebral cortex for final processing[17],[44].\n156. Inspired by this, we employ high-frequency maps (HFM) and low-frequency maps (LFM) as the deep learning network inputs, which represent the texture and structural information of the image respectively, to mimic the recognition mechanism of the human brain for better banding identification.\n157. High-frequency Maps.\n158. Since gradient has been widely used to represent edge information and has been confirmed beneficial to acquire high-frequency components with low computational cost[20]-[22], we apply the isotropic Sobel operator to each patch for enhancing the details of banding artifacts.\n159. Given an input patch I, the high-frequency map is calculated by where S x and S y are the horizontal and vertical isotropic Sobel operators, respectively." * " denotes the convolution operation.\n160. Low-frequency Maps.\n161. To maintain the principal content of the image and filter out the influence of high-frequency information, we use the piece-wise smooth algorithm[19]to generate the low-frequency map by minimizing a function for image approximation recovery: where L represents the low-frequency map, Ω and E denotes the image domain and edge set, respectively.\n162. P indicates the pixel and E dσ represents the total edge length.\n163. The coefficients α and β are positive regularization constants.\n164. An example of frequency maps is shown in Fig.7.\n165. To obtain the overall banding score, we first divide the banding image into 235×235 patches and apply CNN-based classification to each patch, resulting in a banding classification label for each patch, i.e., banded or non-banded.\n166. As shown in Fig.6, the proposed network consists of two parallel branches, namely CNN H and CNN L , which take the patch-level high-frequency map and low-frequency map as input, respectively.\n167. For each branch, we propose to use Resnet-50[18]as the backbone.\n168. Specifically, we incorporate the feature maps extracted from the first convolutional layer and the last layer of Resnet-50 as hierarchical visual features, which represent different visual information[46],[47]and can be used as predictive information to enhance the discrimination ability of the network for banding and non-banded regions.\n169. Afterward, the features extracted from two branches are concatenated first and reshaped into 128dimensional vectors through two fully-connected layers, which is further followed with the sigmoid activation function to output the final predicted label, namely banded or non-banded.\n170. Of note is that sharing parameters is extremely unfavorable for extracting low-and high-frequency features simultaneously, we thereby deploy two branches that work independently and do not share parameters.\n171. The loss function adopted here is binary cross entropy.\n172. With the trained CNN H and CNN L , each input patch is predicted to obtain a banding or non-banded label.\n173. To better guide the pre-processing and post-processing debanding algorithms, it is necessary to generate a quality score for the entire banding image.\n174. Since the visibility of edge is also affected by content, we further consider the effect of spatially varying content information on the local quality of human perception.\n175. As a consequence, we introduce the spatial frequency masking strategy to determine the weighting factor for the detected banding regions in each patch adaptively and thus obtain the image-level banding severity score while refining the visibility of banding artifacts.\n176. 1) Spatial Frequency Masking: The spatial frequency is defined as the activity level of an image, which establishes a filterbank based on the visual stimulus and is in accordance with HVS[16].\n177. In this paper, we propose to apply spatial frequency as an effective contrast criterion to banding measurement.\n178. Specifically, given an image of size I W × I H , divided into N × N patches, where I W and I H denote the number of columns and rows respectively.\n179. The column (CF k ) and row (RF k ) frequencies of the image patches are given by where I(x, y) is the pixel value of the image patch.\n180. Then, the resulting spatial frequency of an N × N patch is computed as where k is the number of patches (1 ≤ k ≤ I W I H N 2 ).\n181. Since most banding regions are likely to have large contrast including edges and textures, which should be assigned greater weights than the smooth and blurred areas.\n182. Accordingly, we design a banding visibility transfer function to express spatial frequency masking as a function of the local textural feature.\n183. The final spatial frequency masking weight is calculated at each patch as where γ is the scaling constant factor chosen to tune the shape of the transfer function.\n184. We used γ = 1.5 in our implementation.\n185. 2) Building a Banding Metric: The visibility of banding artifacts depends on the combination of multiple visual mechanisms.\n186. In this paper, we propose a simple but effective product model for attribute integration at each predicted banding patch to obtain the entire banding map (BM): where P k denotes the predicted label of k-th patch and w k is the weight parameter that scales the visibility of measured contours, i.e., gradient magnitude of the high-frequency map, |HFM k (i, j) | at region (i, j).\n187. Furthermore, inspired by previous psychovisual findings that the QoE of observers is dominated by those regions having poor quality[14],[42], we thereby leverage the worst p% percentile visual pooling to calculate an average banding score from the generated BM, where p is set to 80 in this experiment.\n188. As a result, the perceptual score of the overall banding image is defined as where M is the total number of patches in image I.\n189. T p% denotes the index set of the top p% non-zero pixel-wise value contained in k-th patch of the BM.\n190. In this section, we first present the experimental protocol in detail and then evaluate the performance of the proposed method on two tasks, namely patch-level banding classification and banding image quality assessment.\n191. After that, the ablation study and cross-database validation are conducted to prove the robustness and effectiveness of the proposed method.\n192. Finally, we test the computational efficiency of our method.\n193. A. Experimental Protocol 1) Databases and Settings: We choose two databases to train and test the effectiveness of the proposed banding IQA method, which are the database released in[43]and our proposed BAND-2k database.\n194. The detail information of these two datasets can be found in TableI.\n195. The proposed model is implemented by PyTorch[13].\n196. Before training, we randomly split the training, validation, and testing set into 8:1:1 (as shown in TableII).\n197. We use the Adam optimizer with the initial learning rate set as 1e-4 and set the batch size as 32.\n198. The training process is stopped after 25 epochs.\n199. The resolution of each cropped patch is fixed to 235×235.\n200. All experiments on both the[43]database and the BAND-2k database are conducted repeatedly 10 times to obtain the mean performance.\n201. 2) Baseline Algorithms: We include a number of representative IQA algorithms in our evaluation as references to be compared against.\n202. These baseline methods include: • General FR IQA methods: We choose PSNR, SSIM[12], MS-SSIM[11], LPIPS[10]as baselines.\n203. These are the most commonly used FR IQA metrics in practical applications such as video coding, image enhancement, etc. • General NR IQA methods: BRISQUE[37], NIQE[36], NIMA[8], DBCNN[35], HyberIQA[9], and StairIQA[1].\n204. • Banding IQA methods: Considering that there exists few research on banding detection and quality assessment, we barely select the BBAND[42], CAMBI[41], VMAF BA[40]and DBI[43]metrics as comparisons.\n205. 3) Evaluation Criteria: To evaluate the IQA methods comprehensively, a total of seven evaluation indexes in two categories are adopted.\n206. For patch-level banding classification, we follow the common procedures as in[43]and utilize the area under the receiver operating characteristics (AUROC), the area under the precision-recall curve (AUPRC), and accuracy as the classification performance metrics.\n207. For banding image quality assessment, four mainstream metrics are selected as the evaluation criteria: Spearman rank-order correlation coefficient (SRCC) and Kendall rank-order correlation coefficient (KRCC) measure the prediction monotonicity, while Pearson linear correlation coefficient (PLCC) and root mean square error (RMSE) are calculated to assess prediction consistency.\n208. Considering the potential nonlinear mapping characteristics between the objective scores and the subjective scores, we perform score alignment by mapping the predicted value using the five-parameter logistic function before calculating PLCC and RMSE values[7].\n209. Since our goal is to develop an effective banding IQA approach, we regard the identification of banding patches as an important preceding process to achieve accurate banding quality prediction.\n210. However, there exist few methods that are designed for banding classification and nearly all IQA methods produce scalar values only while failing in classifying banding regions directly.\n211. Therefore, we adopt a thresholding step to convert the single quality value into binary classification results as[43]does.\n212. Concretely, a half-interval search algorithm[48]is employed to find the optimal threshold value that can generate the best classification result.\n213. Based on the above premise, TableIIIreports the experimental results on both the database from[43]and the BAND-2k database.\n214. We highlight the best results in boldface.\n215. As compared to other state-of-the-art IQA methods, our proposed method yields the best overall performance in terms of AUROC, AUPRC, and accuracy.\n216. It is shown that most general FR IQA and NR IQA models perform poorly on the patch-level banding classification task while performing fairly well on other IQA tasks[1],[9],[35], indicating that the current approaches are not sensitive to banding distortion.\n217. Benefiting from the powerful feature extraction ability of CNNs, our proposed method and the customized NR IQA models for banding artifacts detection (DBI[43]) reach a significant performance in the discrimination of false contours.\n218. However, the performance of banding IQA method BBAND, CAMBI, and VMAF BA is surprisingly poor compared with other methods, which shows their vulnerability in identifying local banding artifacts from texture regions and are not suitable for patch-level banding identification.\n219. In addition, we investigate the computational complexity in terms of execution time per image patch.\n220. It can be observed that except for those traditional FR IQA models, our method achieves comparable speed in patch-level banding classification, which determines the prediction efficiency of the subsequent image-level quality assessment, making it a favorable choice in time-constrained scenarios.\n221. Considering that there exist no image banding databases with subjective scores attached in the public domain, we merely compare the performance of the proposed method with the baseline approaches on the proposed BAND-2k database.\n222. The results are shown in TableIV, from which we can observe that our proposed method largely surpasses all baselines in terms of SRCC, KRCC, and PLCC except for the RMSE.\n223. Compared to the secondbest model, our method achieves 18.07%SRCC improvements, 24.91% KRCC improvements, and 18.01%PLCC improvements on the BAND-2k database.\n224. We also present scatter plots of predictions versus MOS for better visualization in Fig.8.\n225. Overall,  (a) PSNR, (b) SSIM[12], (c) MS-SSIM[11], (d) LPIPS[10], (e) VMAF BA[40], (f) BRISQUE[37], (g) NIQE[36], (h) NIMA[8], (i) DBCNN[35], (j) HyperIQA[9], (k) StairIQA[1], (l) BBAND[42], (m) CAMBI[41], (n) DBI[43], and (o) Ours.\n226. Fig.9. Visual comparisons of the banding map results.\n227. From top to bottom are banding images and their corresponding banding maps generated by BBAND[42], DBI[43], and our proposed method, respectively.\n228. The first five columns of images from left to right are from BAND-2k, while the rest images are from[43].\n229. the performance of traditional FR IQA models such as PSNR, SSIM, and MS-SSIM is remarkably inferior in banding images assessment and is uncorrelated with the MOS, which is consistent with the numerical results presented in TableIV.\n230. The reason is that PSNR and SSIM-based methods do perform not well on tiny, regional, and content-independent banding distortion while lacking the consideration for the mechanism of HVS.\n231. It is also worth mentioning that the deep CNN architecturebased methods (DBCNN[35], HyperIQA[9], and StairIQA[1]), despite performing well on LIVEC[6], KonIQ-10k[29], and other universal image quality evaluation databases[4],[5], underperformed our proposed model by a notable margin on the BAND-2k database.\n232. (2) the perception of banding artifacts is explicitly intensity-aware.\n233. These are the issues that the CNN-based approaches above do not take into account.\n234. To some extent, banding distortion is more like a kind of local distortion than common global distortion such as Gaussian noise, blur, and dither.\n235. This suggests that it is potentially valuable to integrate some local texture, edge, contrast, or other visibility-related features into quality prediction models when assessing banding images.\n236. Fortunately, with the help of the proposed scoring strategy, our method gains the ability to convert the area range of identified banding regions to indicative annoying scores.\n237. Surprisingly, the banding IQA methods BBAND[42], CAMBI[41], and VMAF BA[40], however, did not perform very well on the BAND-2k dataset.\n238. We infer that this is due to differences in the test environment.\n239. First, the source videos selected in CAMBI and VMAF BA are from the internal Netflix 4K catalogue while the source contents in BAND-2k are collected from the public streaming websites with different resolutions, leading to an uneven quality level.\n240. Second, the methods of artificially introducing banding distortion are different.\n241. 264 compression techniques are used to generate the banding artifacts, limiting the adaptive ability of the algorithm in other scenarios.\n242. For further investigation, we compare our method with other two methods, i.e., BBAND[42], DBI[43], and visually study the generated banding maps to verify their effectiveness in banding detection.\n243. The visualization results are shown in Fig.9, where seven representative banding images are selected for reference.\n244. As shown, the banding maps generated from BBAND are mostly disordered and have a lot of discontinuity while the banding maps generated by DBI are too vague to locate the exact location of banding artifacts, making it difficult to develop pixel-level debanding techniques.\n245. Overall, we can compendiously conclude that: (1) in comparison with BBAND and DBI, banding maps computed by our proposed method could provide an accurate, clear indication for banding artifacts; (2) in comparison with BBAND and DBI, the quality prediction of banding images guided by our method could achieve a high consistency with HVS.\n246. Moreover, to make a statistically meaningful comparison among different IQA methods, we further conduct the widely used F-test[2],[3]to assess the statistical significance of the proposed method.\n247. Based on the assumption that the model\'s prediction residuals follow the Gaussian distribution, the lefttailed F-test with a confidence level of 95% is performed on the residuals of every two IQA models.\n248. The results of significance tests on the BAND-2k database are shown in Fig.10.\n249. A value of \'1\' (colored in green) indicates that the model in the row is significantly better than the model in the column, while a value of \'0\' (colored in red) indicates that the model in the row is not significantly better than the model in the column.\n250. It is shown that our proposed method performs significantly better than other models, which is consistent with the observations from the above comparison experiments.\n251. In this section, we explore the effectiveness of our model\'s design philosophy.\n252. To verify the importance of the dual-branch (DB) architecture, the baseline variants use the same backbone as the proposed method, except that only a single branch (SB) is reserved.\n253. Then, we use the original banding images as the input while removing the frequency map generation module (SB-I).\n254. Besides, the high-frequency maps and low-frequency maps generation modules are retained respectively (SB-HFM and SB-LFM).\n255. It can be observed from Table V that SB-LFM achieved the worst results, since the low-frequency map filters out the highfrequency banding information to a certain extent, which reduces the ability of the model to identify false contours.\n256. SB-I performs significantly better than SB-HFM and SB-LFM, resulting from that it contains richer image information, but it is still inferior to our method, which demonstrates the effectiveness of the dualbranch scheme.\n257. To investigate the effect of the banding feature extraction, we further design two variants with different input combinations of frequency maps.\n258. First, the high-frequency map was taken as the input of both the CN N H and CN N L layers (DB-HFM).\n259. Then, we replace the inputs with the low-frequency maps (DB-LFM).\n260. As shown in TableV, the performance of DB-HFM and DB-LFM is far apart from our approach, which matches our hypothesis that the high-frequency texture information contained in HFM and the low-frequency background information contained in LFM are crucial to enhance the capacity of discernment for banding artifacts.\n261. Therefore, we may conclude that our model is the most suitable model among those compared variants in terms of both banding classification and IQA applications.\n262. Due to the effects of different compression techniques, shooting equipment, scenes, etc., the image content and banding distortions may vary significantly in practical applications.\n263. For the database[43], it only includes limited types of image sources and means of triggering banding distortion.\n264. As a result, we conduct a cross-database validation to verify the generalizability of the proposed model, wherein the database presented by[43]and BAND-2k are included.\n265. That is, we trained the model on one full database and report the test performance on the other.\n266. We mainly compare the proposed method with four learningbased models, i.e., DBCNN[35], HyperIQA[9], StairIQA[1], and DBI[43].\n267. Since MOS information is not provided in the database[43], which is an essential part of methods training, we condensed part of the experiments.\n268. TableVIand VII report the experimental results in terms of patch-level banding classification and image quality assessment.\n269. We can observe that our proposed method generalization between database[43]and BAND-2k was surprisingly good.\n270. Besides, it is worth noting that the performance of these methods trained on the BAND-2k has improved a little compared to the previous versions that were trained on the database[43], which further demonstrates the superiority of the proposed database BAND-2k.\n271. The efficiency of an image quality prediction model is of great importance in practical industrial deployments.\n272. Therefore, we measured the average running time of the compared IQA models, as shown in TableVIII.\n273. The experiments were performed in MATLAB R2021a and Python 3.7 under Windows 10 64bit system on a Lenovo laptop with Intel Core i5-9300HF CPU@2.4GHz,16GB RAM, and NVIDIA GTX 1660Ti 6G GPU.\n274. It can be observed that the proposed method achieves a reasonable running time among the FR, NR, and other bandingspecified IQA algorithms.\n275. Generally, the execution time of classical IQA algorithms is significantly less than learning-based methods.\n276. Simpler NSS-based models such as BRISQUE and NIQE still show competitive efficiency relative to CNN models while exhibiting inferior performance in banding image quality assessment.\n277. For CAMBI and VMAF BA , we use the officially launched software package, which is based on the stand-alone C library libvmaf and therefore surpasses other methods in speed.\n278. Moreover, unlike the general quality evaluation using regression to predict scores, the patch-wise prediction strategy that we adopted may increase the complexity.\n279. Note that although we deployed a more complex network structure, a nearly 10 times speedup has been seen when comparing DBI with our method since the sliding window mechanism[43]is removed.\n280. In this paper, we conduct a comprehensive exploration of banding images from both subjective and objective perspectives.\n281. Specifically, we construct the largest ecologically valid banding IQA database to date named BAND-2k database, which consists of 2,000 banding images generated by fifteen compression and quantization schemes, achieving several times larger in number and diversity than the existing banding dataset.\n282. The construction process of the database, including distortion content preparation, subjective test procedure, and the removal of outlying data, is described in detail in this paper.\n283. Relying on this database, we proposed a novel banding evaluator using the frequency characteristic of banding artifacts, which models the banding as high-frequency artifacts that contained in the low-frequency smoothing region.\n284. A dual-branch CNN is devised to extract hierarchical features to classify the banding regions, upon which we introduce the spatial frequency masking to refine and compute an overall banding score.\n285. Experimental results show that our proposed method outperforms the baseline algorithms significantly in patch-level banding classification and banding IQA tasks.\n286. We believe that our study will benefit further development, calibration, and benchmarking of banding IQA models.\n\n请严格按照JSON格式返回：{"labels": [0,1,0,...]}，不要包含任何其他文字。'}], 'model': 'qwen-plus', 'response_format': {'type': 'json_object'}}}
2025-10-14 20:47:04,478 - DEBUG - Sending HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
2025-10-14 20:47:04,478 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-10-14 20:47:04,478 - DEBUG - send_request_headers.complete
2025-10-14 20:47:04,478 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-10-14 20:47:04,478 - DEBUG - send_request_body.complete
2025-10-14 20:47:04,478 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-10-14 20:51:53,584 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'vary', b'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding'), (b'x-request-id', b'a034da84-4f57-49c6-bba9-e80f02d44ec9'), (b'x-dashscope-call-gateway', b'true'), (b'content-type', b'application/json'), (b'req-cost-time', b'289074'), (b'req-arrive-time', b'1760446014531'), (b'resp-start-time', b'1760446303605'), (b'x-envoy-upstream-service-time', b'288983'), (b'content-encoding', b'gzip'), (b'date', b'Tue, 14 Oct 2025 12:51:43 GMT'), (b'server', b'istio-envoy'), (b'transfer-encoding', b'chunked')])
2025-10-14 20:51:53,585 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-14 20:51:53,585 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-10-14 20:51:53,585 - DEBUG - receive_response_body.complete
2025-10-14 20:51:53,585 - DEBUG - response_closed.started
2025-10-14 20:51:53,585 - DEBUG - response_closed.complete
2025-10-14 20:51:53,585 - DEBUG - HTTP Response: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "200 OK" Headers({'vary': 'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding', 'x-request-id': 'a034da84-4f57-49c6-bba9-e80f02d44ec9', 'x-dashscope-call-gateway': 'true', 'content-type': 'application/json', 'req-cost-time': '289074', 'req-arrive-time': '1760446014531', 'resp-start-time': '1760446303605', 'x-envoy-upstream-service-time': '288983', 'content-encoding': 'gzip', 'date': 'Tue, 14 Oct 2025 12:51:43 GMT', 'server': 'istio-envoy', 'transfer-encoding': 'chunked'})
2025-10-14 20:51:53,585 - DEBUG - request_id: a034da84-4f57-49c6-bba9-e80f02d44ec9
2025-10-14 20:51:53,586 - DEBUG - API request completed in 289.11 seconds
2025-10-14 20:51:53,586 - DEBUG - Raw model response: {"labels": [1,0,0,1,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1
2025-10-14 20:51:53,586 - ERROR - JSON parsing error: Expecting ',' delimiter: line 1 column 12116 (char 12115)
2025-10-14 20:51:53,586 - ERROR - Problematic content: {"labels": [1,0,0,1,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1
2025-10-14 20:51:53,586 - INFO - Attempt 2 of 5
2025-10-14 20:51:53,588 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-04a558db-afd7-4148-b937-58cdea8f4674', 'json_data': {'messages': [{'role': 'system', 'content': '你是一个专门用于识别学术论文中数据集描述的助手。你需要判断每个输入的句子是否描述了研究中使用的数据集。请确保：\n    1. 只输出JSON格式的结果\n    2. 结果格式必须为 {"labels": [0,1,0,...]}\n    3. 不要输出任何其他文字或解释'}, {'role': 'user', 'content': '你是一位专业的学术论文数据集描述识别专家。请仔细分析下面这篇论文中的每个句子，判断它们是否描述了本研究特定构建的数据集信息。\n\n    论文标题: BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment\n\n    判断标准:\n    判断为1(数据集描述)的标准：\n    句子必须直接描述本研究特定构建的数据集,包括:\n    1. 该数据集的构建过程和方法\n    2. 该数据集的具体构成和规模\n    3. 该数据的来源和收集方式\n    4. 该数据的预处理步骤\n    5. 该数据集的可获取方式(如发布地址)\n    6. 或者其他直接或者间接描述了该数据集的句子\n\n    所有其他类型的句子均标记为0。\n    \n    请对每个句子进行分析，返回一个JSON格式的标签数组。格式要求：\n    1. 必须是有效的JSON格式\n    2. 只包含labels字段，值为0和1组成的数组\n    3. 数组长度必须与句子数量相同\n    4. 示例格式：{"labels": [0,1,0,1,...]}\n\n    需要判断的句子：\n\n    1. Abstract: Banding, also known as staircase-like contours, frequently occurs in flat areas of images/videos processed by the compression or quantization algorithms.\n2. As undesirable artifacts, banding destroys the original image structure, thus inevitably degrading users\' quality of experience (QoE).\n3. In this paper, we systematically investigate the banding image quality assessment (IQA) problem, aiming to detect the image banding artifacts and evaluate their perceptual visual quality.\n4. Considering that the existing image banding databases only contain limited content sources and banding generation methods, and lack perceptual quality labels (i.e.mean opinion scores), we first build the largest banding IQA database so far, named Banding Artifact Noticeable Database (BAND-2k), which consists of 2,000 banding images generated by 15 compression and quantization schemes.\n5. A total of 23 workers participated in the subjective IQA experiment, yielding over 214,000 patch-level banding class labels and 44,371 reliable image-level quality rating scores.\n6. Subsequently, we develop an effective no-reference (NR) banding evaluator for banding detection and quality assessment by leveraging frequency characteristics of banding artifacts.\n7. To be more specific, a dual convolutional neural network (CNN) is employed to concurrently learn the feature representation from the high-frequency and low-frequency maps, thereby enhancing the ability to discern banding artifacts.\n8. The quality score of a banding image is generated by pooling the banding detection maps masked by the spatial frequency filters.\n9. The experimental results demonstrate that our banding evaluator achieves a remarkably high accuracy in banding detection and also exhibits high SRCC and PLCC results with the perceptual quality labels, even without directly learning a regression model for banding quality evaluation.\n10. These findings unveil the strong correlations between the intensity of banding artifacts and the perceptual visual quality, thus validating the necessity of banding quality assessment.\n11. The BAND-2k database and the proposed banding evaluator will be available at https://github.com/zijianchen98/BAND-2k.\n12. R ECENT years have witnessed a rapid emergence of media streaming services and social platforms.\n13. YouTube, Netflix, and TikTok account for more than half of the world\'s video traffic.\n14. Improving the quality of images under limited encoding, Zijian Chen, Wei Sun, Jun Jia, Zicheng Zhang, Xiongkuo Min, and Guangtao Zhai are with the Institute of Image Communication and Information Processing, Shanghai Jiao Tong University, Shanghai 200240, China (e-mail: {zijian.chen,sunguwei, jiajun0302, zzc1998, minxiongkuo, zhaiguangtao}@sjtu.edu.cn).\n15. Fangfang Lu is with the College of Computer Science and Technology, Shanghai University of Electric Power, Shanghai 200290, China (email:lufangfang@shiep.edu.cn).\n16. Jing Liu is with the School of Electrical and Information Engineering, Tianjin University, Tianjin, China.(e-mail:jliu tju@tju.edu.cn).\n17. transmission bandwidth, and storage condition is a necessary prerequisite for meeting the quality of experience (QoE) of users.\n18. In the stages between image acquisition and display, an image may suffer from various types of degradation, while banding artifacts are a kind of false contour distortion that is quite perceptible to the human eye.\n19. Since the visual quality of image contents greatly affects the QoE of end-users, it is highly desirable to design an effective banding image quality assessment (IQA) method, which aims to automatically detect the traces of such false contours and predict the objective quality of banding images that can be used to develop pre-processing or postprocessing debanding algorithms and optimize the performance of streaming media application.\n20. Normally, banding artifacts take on the appearance of annual rings, radiation circles, halos, or geographical contour lines and especially exist in the background regions (e.g., sky, water, and wall surface), where the color transition is not smooth enough.\n21. Nearly all existing image or video encoders, including H.264/AVC[28], VP9[38], and H.265/HEVC[27]can introduce such artifacts more or less.\n22. Current banding IQA research can be divided into two categories: subjective quality assessment and objective quality assessment.\n23. The existing subjective banding IQA research[39],[41]-[43]mainly investigates the limited banding scenarios with internal-used and undisclosed databases while lacking the quality label and may be insufficiently generalizable to large-scale commercial applications.\n24. Meanwhile, general IQA methods aiming at common distortions are inapplicable for banding exacerbated images due to the essential differences between them.\n25. First, banding artifacts usually hold tiny, staircaselike, and regional structures, which can be regarded as a kind of high-frequency artifact in smooth areas, while general distortions occur obviously in the whole image and are globally uniform.\n26. Second, the perceptual severity of banding is quantified based on its fraction of coverage and intensity in an image, which is widely divergent from the design philosophy of many existing IQA approaches.\n27. As a result, it is challenging to design an effective banding IQA method.\n28. To address these limitations, we first conduct a comprehensive subjective study of banding exacerbated images and create the largest banding IQA database to date with reliable mean opinion scores (MOS) and patch-level banding labels.\n29. We also propose a novel no-reference banding evaluator for banding detection and quality assessment by leveraging the frequency characteristics of banding artifacts.\n30. First, due to the dissimilar peculiarities of banding and smooth regions, the same distortion in different regions, e.g., textual and pictorial regions, may lead to different visual perception of human beings.\n31. Considering that banding manifests as a high-frequency artifact that exists in the lowfrequency smooth region, we propose a dual-branch CNN, which takes the high-frequency map and low-frequency map as inputs simultaneously, to hierarchically incorporate different visual features from the first layer and the last layer of our Resnet-50 backbone, thus making the model learn more effective banding feature representation and achieving more accurate banding region discrimination.\n32. Furthermore, inspired by the previous studies[15],[16], spatial frequency extracts information consistent with the human visual system (HVS), which not only reflects the overall active level in an image but also intuitively quantifies the contrast information.\n33. In other words, the value of spatial frequency is large in smooth areas, while becoming small in areas with harsh contrast changes, i.e., banding areas.\n34. Based on this mechanism, we adopt a spatial frequency masking strategy to refine the detected banding map and then pool the masked banding detection map to obtain the image-level banding quality score.\n35. • We propose a novel no-reference banding evaluator for banding detection and quality assessment based on frequency characteristics of banding artifacts, which utilizes a dual-branch CNN model to extract hierarchical banding-related feature representation from the high-frequency maps and low-frequency maps simultaneously.\n36. • Experimental results show that the proposed banding evaluator achieves the best performance in banding detection and significantly surpasses baselines in terms of SRCC and PLCC in the banding IQA task, which demonstrates the effectiveness of the proposed model.\n37. The remainder of this paper is organized as follows.\n38. Section II provides an overview of related works, including the stateof-the-art banding databases and detection methods.\n39. Section III introduces the construction of the BAND-2k database and the subjective assessment study.\n40. Section IV proposes an effective no-reference banding evaluator for banding detection and quality assessment.\n41. Section V gives the experimental results and analysis.\n42. Section VI concludes this paper.\n43. In this section, we first provide an overview of the state-ofthe-art banding-related databases (TableI) and then review the banding detection and evaluation methods.\n44. The first banding artifact-relevant VQA database was proposed by Wang et al.[39], which consists of 21 stimuli with different quantization grades generated by VP9[38]from 7 clips of 1280 × 720 30fps video.\n45. Authors in[41]investigated the effect of encoding parameters and dithering on the visibility of banding.\n46. Nine 4k-10bit source clips from the existing Netflix catalogue between 1 and 5 seconds were used to generate banding distorted videos.\n47. Each source content was downsampled to appropriate resolutions (1080p, 2k, or 4k) with certain bit-depth and further compressed by libaom (an AV1 codec library) at QPs {12, 20, 32}.\n48. More recently, Kapoor et al.[43]constructed one of the first databases for data-driven image banding assessment models.\n49. This research included about 1,440 images shot from over 600 pristine HD videos with a resolution of 1920 × 1080.\n50. Six levels of bit-depth quantization in luminance and chrominance channels are introduced to obtain different intensities of banding.\n51. Meanwhile, the banding images were semi-automatically segmented and labeled into banded and non-banded to form a patch-level banding dataset, which allows for training machine learning-based and deep learning-based banding classification methods.\n52. However, to the best of our knowledge, thus far there still lacking a benchmarking dataset in the banding detection and the corresponding banding IQA domain.\n53. Researchers either resort to image/video quality datasets that do not aim at banding distortion or build a small, attribute-restricted, in-house dataset by themselves.\n54. This motivates us to construct a large-scale subjective assessment database focus on the perceived bandingaffected image quality.\n55. Early research on banding detection mainly focuses on false contour identification, which aims to find the wrong boundary rather than a "true" region edge in the image.\n56. Authors in[31]-[33]utilized monotonicity or non-monotonicity features of local support regions including the gradient, contrast, variance, and entropy information to measure the loss of low-amplitude detail caused by banding.\n57. However, these works ignored the perceptual characteristics of the human visual system (HVS) and thus did not perform a good correlation with subjective tests.\n58. Another banding detection strategy is conducted at the pixel-level estimation and segmentation.\n59. Bhagavathy et al.[30]proposed to identify banding artifacts by calculating the likelihood of pixel difference.\n60. Baugh et al.[34]measured the severity of banding based on the number of a group of connected pixels with the same color.\n61. Wang et al.[39]first detected uniform segments to find possible banding areas and further incorporated edge features (e.g.length and contrast) to capture false boundaries.\n62. Nevertheless, these kinds of methods are typically sensitive to edge noise and are computationally expensive, causing limited application in realtime scenarios.\n63. Towards addressing these problems, Tu et al.[42]presented a completely no-reference banding detection method, which combines various properties of HVS with a number of preprocessing steps to refine banding edge detection.\n64. Instead of regarding banding detection as a false edge detection problem, Tandon et al.[41]heuristically utilized the effect of contrast sensitivity function (CSF) on banding visibility and its dependence on spatial frequency.\n65. Based on this, Krasula et al.[40]further compared the banding annoyance with more commonly studied compression artifacts and proposed a banding-aware video quality metric.\n66. In recent years, deep learning approaches have prevailed in various VQA tasks.\n67. As the pioneering work, Kapoor et al.[43]developed an automated CNN-based banding detector for the first time, which is a simple two-stage algorithm and gives rise to devising other learning-based techniques.\n68. In this work, we build a large-scale banding database and propose a data-driven banding indicator that can generate pixelwise banding visibility maps with corresponding subjectively  consistent quality scores by combining human visual mechanisms and deep learning techniques.\n69. Subjective banding image quality assessment facilitates the development of automatic objective banding image and video quality models.\n70. We created the largest banding database in existence, denoted as the BAND-2k database, which consists of 2,000 banding distorted images and over 214,000 patch-level banding class labels.\n71. Then, a subjective experiment was conducted to obtain the mean opinion scores (MOS) of the BAND-2k database.\n72. The workflow of the banding database construction is shown in Fig.1.\n73. To build a content-rich and balanced database, we manually collected source videos including computer graphics (CG), usergenerated content (UGC), and professionally-generated content (PGC) from two popular media websites Bilibili.comand Youtube.com.\n74. Then, 885 clips with multiple spatial resolutions (i.e., 4096×2160, 3840×2160, 1920×1080) and frame rates (i.e., 60, 50, 30, 25) are chosen as candidate.\n75. Note that videos on the mentioned websites are firstly annotated by the community with assigned a number of favorites, views, and downloads.\n76. These statistics correlate with the content and quality of a video, which guides our choices to some extent.\n77. All videos selected on the website are released under an appropriate creative commons (CC) license that allows further editing and redistribution.\n78. After content selection, we further unified the format of all video clips, especially the spatial resolution and the pixel format, which avoid the effect of other facts on visual quality.\n79. Concretely, we first converted the frame rate of the original clips to 25fps, which is to reduce the storage pressure while ensuring the graphics quality.\n80. Considering the commonly used aspect ratio of the user interface and displays is 16:9, we cropped the partially unqualified videos rather than shrinking images unevenly.\n81. Then, we downsampled the trimmed spatial resolution 3840×2160 to a lower resolution -1920×1080 for the following subjective study.\n82. Before conducting the following experiments, we manually removed the videos that are either too dark or bright, overly blurry or colorful, which helps to obtain more reliable subjective assessments.\n83. To avoid redundancy and to make sure the diversity of selected contents, we also conducted attribute analysis studies on the selected images.\n84. Four metrics that correlated with human perception, i.e., contrast, brightness, sharpness, and colorfulness, are adopted as content diversity metrics.\n85. All video attributes are calculated on every 10 frames to reduce computational complexity, which are then averaged over frames of each video sequence.\n86. Fig.2shows the distribution of attributes extracted from the selected videos.\n87. • Contrast: The contrast metric is simply defined as the standard deviation of pixel gray-scale intensities[25].\n88. • Colorfulness: The colorfulness metric is measured by the R, G, and B components[25].\n89. We first compute two matrices of differences between channels rg = R -G and yb = 1 2 (R + G) -B.\n90. Then, the colorfulness metric can be calculated as µ 2 rg + µ 2 yb + σ 2 rg + σ 2 yb , where µ and σ are the mean and standard deviation of their corresponding terms, respectively.\n91. • Sharpness: The cumulative probability of blur detection (CPBD) metric[24]is used to measure the image sharpness, which estimates the probability of detecting blur at each edge.\n92. • Brightness: The brightness of an image is obtained directly from the pixel gray intensities in R, G, and B channels.\n93. Finally, the number of source videos are reduced to 873.\n94. Fig.3displays thumbnails for 30 selected representative video clips.\n95. To simulate authentic banding artifacts that exist in real viewing scenarios, we introduced four encoding/transcoding strategies including H.264/AVC[28], H.265/HEVC[27], VP9[38], and bit-depth manipulation[43]with fifteen quantization schemes in total.For H.264/AVC and H.265/HEVC, considering the range of their quantization parameter (QP in ffmpeg), we selected three typical QP values, namely, QP = {18, 33, 43}.\n96. This is because that coded video clips with a QP value smaller than 18 normally provide perceptual lossless quality, while coded video clips with a QP value larger than 43 will not be able to offer adequate quality, which may affect the subjective test of banding artifacts.\n97. Similarly, for the VP9 encoder, the QP values under our close inspection are chosen as {28, 45, 60}.\n98. Moreover, we applied the same quantization strategies as in[43], which introduces banding distortion by scaling bit-depth in luminance and chrominance channels.\n99. Here, the color coding scheme YCbCr4:2:0 is applied for maximum compatibility.\n100. To sum up, we generated fifteen levels of banding with different intensities and shapes to enhance the diversity of the database.\n101. The banding database is then built by extracting frames from the distorted video clips, resulting in 2,000 images with a resolution of 1920×1080.\n102. Fig.4shows the visualization results of banding exacerbated images.\n103. In the process of image patch labeling, we initially intend to extract image patches of size 235×235 from banding images directly and perform annotation operations.\n104. However, considering the theoretical number of patches, it will cost a lot of manpower and time to label patch by patch.\n105. Therefore, we first segmented the banding images roughly and label them into banded and non-banded regions.\n106. Then, labelled image patches are generated from these segmented and labelled images by a sliding window.\n107. Specifically, we followed the same demarcation of the banded and non-banded images in[43]that a patch is labelled as banded if it has more than 30% overlap with banded regions in the image.\n108. Eventually, a banding dataset containing 2,000 distorted images with 1920×1080 resolution and 214,324 labelled image patches of size 235×235 is built.\n109. To the best of our knowledge, it is the largest banding dataset in existence, which enables training various machine/deep learning based banding detection models and facilitates the development of image/video debanding techniques.\n110. TableIIreports the composition of labelled image patch dataset.\n111. It can be observed that the number of banded patches is a bit smaller than non-banded patches due to the fact that banding usually appears in smooth background areas.\n112. As shown in Fig.1, the subjective quality study contains four steps.\n113. In addition to preparing the experimental environment, subjects should pass the qualification test first to participate in the study.\n114. After the subjective rating, all resulting scores need to be analyzed and examined before generating the final mean opinion score (MOS).\n115. 1) Experimental Environment Setting: In this study, a total of 25 inexperienced subjects are gathered in a laboratory environment, where relevant experimental configuration must satisfy the following requirements: • Considering the viewing effect, desktops and laptops are allowed as displays.\n116. • The resolution of displays must be larger than or equal to 1920×1080 to show the images without spatial downsampling.\n117. • The viewing distance and optimal horizontal viewing angle are set as 1.9 times the height of the display and respectively.\n118. 500 recommendation[26].\n119. As a result, we used a 27-inch AOC Q27U2D monitor with a resolution of 2560×1440 for assessment with 25 subjects.\n120. Due to the large number of images to be assessed, we divided the dataset into ten sessions to avoid visual fatigue.\n121. Each session of tests took nearly 2 hours with a 30-minute break for each participant.\n122. 2) Qualification Test: Before starting the main experiment, subjects are required to pass a quiz to get the qualification of conducting follow-up experiments.\n123. Firstly, we manually selected 10 labeled banding exacerbated images beyond the database as training images to familiarize subjects with the operation interface and the goal of this subjective test.\n124. The quiz consists of two parts including banding classification and image-level quality rating.\n125. In banding classification, subjects were told to divide the test image into banded or non-banded.\n126. In image-level quality rating, subjects were instructed to focus on the coverage and intensity of banding areas, as well as the overall quality of images to get the final quality score.\n127. The rating scale is continuous from 0 to 100 while a higher value indicates more severe banding (i.e., more visible or occupying larger portion of the image).\n128. To make the quiz objective and fair, we take the banding images labeled by domain experts as the ground truth, while two existing banding metrics BBAND[42]and DBI[43]are used to determine the normal range of the rating scores.\n129. That is the quality scores rated by subjects should not exceed 20% of the above banding metrics.\n130. As a result, only 23 subjects with an accuracy above 80% in banding classification and subjective scores in the normal range were allowed to pass the quiz.\n131. Note that the steps for taking a quiz are the same as the main experiment, which aims to guarantee the consistency of results.\n132. 3) Formal Study: We adopted the single-stimulus (SS) method in this test.\n133. Ten golden images that have the acknowledged high quality or poor quality (assessed by BBAND[42]and DBI[43]) were added to each session for controlling the scoring deviations.\n134. Besides, 3 repeated images are randomly inserted into each session to ensure consistency of scores before and after subjects scoring.\n135. At last, 23 qualified subjects were asked to provide their opinions on the shuffled image groups.\n136. The resulting scores were collected and packed for further analysis.\n137. 4) Result Analysis: In total, 46,000 scores were collected by 23 qualified subjects in the main study phase.\n138. However, considering the qualification quiz cannot completely disallow those unreliable workers to muddle through to the main study while reliable subjects may also occasionally score odd values, which may be caused by the inter-individual differences in perceiving the quality of the unique characteristics of different contents.\n139. Thus, we further investigated the confidence of rating scores and removed outliers following the Grubbs\' test[23],[45].\n140. Concretely, let s = (s 1 , s 2 , . . ., s N ) denote a set of raw scores collected for one distorted image.\n141. The test statistics is defined as the maximum absolute deviation of a sample standard deviation (SD) unit from the sample mean, which is mathematically expressed as where s and σ s denote the mean and standard deviation of the sample, respectively.\n142. Then, given a significant level α, a sample is detected as an outlier if where t 2 α/(2N ),N -2 represents the upper critical value of the tdistribution with N -2 degrees of freedom and a significance level of α/(2N ).\n143. Empirically, we set the significance level α at 0.05.\n144. Then, a sample is identified as an outlier if its distance to the sample mean is larger than 2.5 times SD and is removed.\n145. Following the aforementioned steps, the total number of scores Fig.6.\n146. The overall architecture of the proposed method.\n147. Given a banding distorted image, it is first divided into patches.\n148. Then, the patch-level high-frequency map (HFM) and low-frequency map (LFM) are generated by Sobel operation and piece-wise smooth algorithm[19], respectively.\n149. After that, a dual-branch CNN (CNN H and CNN L ) is deployed to extract hierarchical features with different visual information and thus classify the patches into banded or non-banded.\n150. Lastly, a spatial frequency masking strategy is introduced to refine the banding map and calculate the image-level banding quality score.\n151. Note that the dual-branch networks do not share parameters.\n152. was reduced to 44,371, and MOS was created by averaging the scores for each image.\n153. Fig.5presents the histogram of MOSs over the entire database, showing a broad MOS distribution of banding images.\n154. In this section, we describe the architecture of the proposed banding evaluator in detail, as shown in Fig.6.\n155. As stated before, banding usually appear as high-frequency information in the smooth background, while humans perceive high-frequency texture regions and low-frequency plateau regions through different neural channels concurrently, and transfer the upper visual features into the cerebral cortex for final processing[17],[44].\n156. Inspired by this, we employ high-frequency maps (HFM) and low-frequency maps (LFM) as the deep learning network inputs, which represent the texture and structural information of the image respectively, to mimic the recognition mechanism of the human brain for better banding identification.\n157. High-frequency Maps.\n158. Since gradient has been widely used to represent edge information and has been confirmed beneficial to acquire high-frequency components with low computational cost[20]-[22], we apply the isotropic Sobel operator to each patch for enhancing the details of banding artifacts.\n159. Given an input patch I, the high-frequency map is calculated by where S x and S y are the horizontal and vertical isotropic Sobel operators, respectively." * " denotes the convolution operation.\n160. Low-frequency Maps.\n161. To maintain the principal content of the image and filter out the influence of high-frequency information, we use the piece-wise smooth algorithm[19]to generate the low-frequency map by minimizing a function for image approximation recovery: where L represents the low-frequency map, Ω and E denotes the image domain and edge set, respectively.\n162. P indicates the pixel and E dσ represents the total edge length.\n163. The coefficients α and β are positive regularization constants.\n164. An example of frequency maps is shown in Fig.7.\n165. To obtain the overall banding score, we first divide the banding image into 235×235 patches and apply CNN-based classification to each patch, resulting in a banding classification label for each patch, i.e., banded or non-banded.\n166. As shown in Fig.6, the proposed network consists of two parallel branches, namely CNN H and CNN L , which take the patch-level high-frequency map and low-frequency map as input, respectively.\n167. For each branch, we propose to use Resnet-50[18]as the backbone.\n168. Specifically, we incorporate the feature maps extracted from the first convolutional layer and the last layer of Resnet-50 as hierarchical visual features, which represent different visual information[46],[47]and can be used as predictive information to enhance the discrimination ability of the network for banding and non-banded regions.\n169. Afterward, the features extracted from two branches are concatenated first and reshaped into 128dimensional vectors through two fully-connected layers, which is further followed with the sigmoid activation function to output the final predicted label, namely banded or non-banded.\n170. Of note is that sharing parameters is extremely unfavorable for extracting low-and high-frequency features simultaneously, we thereby deploy two branches that work independently and do not share parameters.\n171. The loss function adopted here is binary cross entropy.\n172. With the trained CNN H and CNN L , each input patch is predicted to obtain a banding or non-banded label.\n173. To better guide the pre-processing and post-processing debanding algorithms, it is necessary to generate a quality score for the entire banding image.\n174. Since the visibility of edge is also affected by content, we further consider the effect of spatially varying content information on the local quality of human perception.\n175. As a consequence, we introduce the spatial frequency masking strategy to determine the weighting factor for the detected banding regions in each patch adaptively and thus obtain the image-level banding severity score while refining the visibility of banding artifacts.\n176. 1) Spatial Frequency Masking: The spatial frequency is defined as the activity level of an image, which establishes a filterbank based on the visual stimulus and is in accordance with HVS[16].\n177. In this paper, we propose to apply spatial frequency as an effective contrast criterion to banding measurement.\n178. Specifically, given an image of size I W × I H , divided into N × N patches, where I W and I H denote the number of columns and rows respectively.\n179. The column (CF k ) and row (RF k ) frequencies of the image patches are given by where I(x, y) is the pixel value of the image patch.\n180. Then, the resulting spatial frequency of an N × N patch is computed as where k is the number of patches (1 ≤ k ≤ I W I H N 2 ).\n181. Since most banding regions are likely to have large contrast including edges and textures, which should be assigned greater weights than the smooth and blurred areas.\n182. Accordingly, we design a banding visibility transfer function to express spatial frequency masking as a function of the local textural feature.\n183. The final spatial frequency masking weight is calculated at each patch as where γ is the scaling constant factor chosen to tune the shape of the transfer function.\n184. We used γ = 1.5 in our implementation.\n185. 2) Building a Banding Metric: The visibility of banding artifacts depends on the combination of multiple visual mechanisms.\n186. In this paper, we propose a simple but effective product model for attribute integration at each predicted banding patch to obtain the entire banding map (BM): where P k denotes the predicted label of k-th patch and w k is the weight parameter that scales the visibility of measured contours, i.e., gradient magnitude of the high-frequency map, |HFM k (i, j) | at region (i, j).\n187. Furthermore, inspired by previous psychovisual findings that the QoE of observers is dominated by those regions having poor quality[14],[42], we thereby leverage the worst p% percentile visual pooling to calculate an average banding score from the generated BM, where p is set to 80 in this experiment.\n188. As a result, the perceptual score of the overall banding image is defined as where M is the total number of patches in image I.\n189. T p% denotes the index set of the top p% non-zero pixel-wise value contained in k-th patch of the BM.\n190. In this section, we first present the experimental protocol in detail and then evaluate the performance of the proposed method on two tasks, namely patch-level banding classification and banding image quality assessment.\n191. After that, the ablation study and cross-database validation are conducted to prove the robustness and effectiveness of the proposed method.\n192. Finally, we test the computational efficiency of our method.\n193. A. Experimental Protocol 1) Databases and Settings: We choose two databases to train and test the effectiveness of the proposed banding IQA method, which are the database released in[43]and our proposed BAND-2k database.\n194. The detail information of these two datasets can be found in TableI.\n195. The proposed model is implemented by PyTorch[13].\n196. Before training, we randomly split the training, validation, and testing set into 8:1:1 (as shown in TableII).\n197. We use the Adam optimizer with the initial learning rate set as 1e-4 and set the batch size as 32.\n198. The training process is stopped after 25 epochs.\n199. The resolution of each cropped patch is fixed to 235×235.\n200. All experiments on both the[43]database and the BAND-2k database are conducted repeatedly 10 times to obtain the mean performance.\n201. 2) Baseline Algorithms: We include a number of representative IQA algorithms in our evaluation as references to be compared against.\n202. These baseline methods include: • General FR IQA methods: We choose PSNR, SSIM[12], MS-SSIM[11], LPIPS[10]as baselines.\n203. These are the most commonly used FR IQA metrics in practical applications such as video coding, image enhancement, etc. • General NR IQA methods: BRISQUE[37], NIQE[36], NIMA[8], DBCNN[35], HyberIQA[9], and StairIQA[1].\n204. • Banding IQA methods: Considering that there exists few research on banding detection and quality assessment, we barely select the BBAND[42], CAMBI[41], VMAF BA[40]and DBI[43]metrics as comparisons.\n205. 3) Evaluation Criteria: To evaluate the IQA methods comprehensively, a total of seven evaluation indexes in two categories are adopted.\n206. For patch-level banding classification, we follow the common procedures as in[43]and utilize the area under the receiver operating characteristics (AUROC), the area under the precision-recall curve (AUPRC), and accuracy as the classification performance metrics.\n207. For banding image quality assessment, four mainstream metrics are selected as the evaluation criteria: Spearman rank-order correlation coefficient (SRCC) and Kendall rank-order correlation coefficient (KRCC) measure the prediction monotonicity, while Pearson linear correlation coefficient (PLCC) and root mean square error (RMSE) are calculated to assess prediction consistency.\n208. Considering the potential nonlinear mapping characteristics between the objective scores and the subjective scores, we perform score alignment by mapping the predicted value using the five-parameter logistic function before calculating PLCC and RMSE values[7].\n209. Since our goal is to develop an effective banding IQA approach, we regard the identification of banding patches as an important preceding process to achieve accurate banding quality prediction.\n210. However, there exist few methods that are designed for banding classification and nearly all IQA methods produce scalar values only while failing in classifying banding regions directly.\n211. Therefore, we adopt a thresholding step to convert the single quality value into binary classification results as[43]does.\n212. Concretely, a half-interval search algorithm[48]is employed to find the optimal threshold value that can generate the best classification result.\n213. Based on the above premise, TableIIIreports the experimental results on both the database from[43]and the BAND-2k database.\n214. We highlight the best results in boldface.\n215. As compared to other state-of-the-art IQA methods, our proposed method yields the best overall performance in terms of AUROC, AUPRC, and accuracy.\n216. It is shown that most general FR IQA and NR IQA models perform poorly on the patch-level banding classification task while performing fairly well on other IQA tasks[1],[9],[35], indicating that the current approaches are not sensitive to banding distortion.\n217. Benefiting from the powerful feature extraction ability of CNNs, our proposed method and the customized NR IQA models for banding artifacts detection (DBI[43]) reach a significant performance in the discrimination of false contours.\n218. However, the performance of banding IQA method BBAND, CAMBI, and VMAF BA is surprisingly poor compared with other methods, which shows their vulnerability in identifying local banding artifacts from texture regions and are not suitable for patch-level banding identification.\n219. In addition, we investigate the computational complexity in terms of execution time per image patch.\n220. It can be observed that except for those traditional FR IQA models, our method achieves comparable speed in patch-level banding classification, which determines the prediction efficiency of the subsequent image-level quality assessment, making it a favorable choice in time-constrained scenarios.\n221. Considering that there exist no image banding databases with subjective scores attached in the public domain, we merely compare the performance of the proposed method with the baseline approaches on the proposed BAND-2k database.\n222. The results are shown in TableIV, from which we can observe that our proposed method largely surpasses all baselines in terms of SRCC, KRCC, and PLCC except for the RMSE.\n223. Compared to the secondbest model, our method achieves 18.07%SRCC improvements, 24.91% KRCC improvements, and 18.01%PLCC improvements on the BAND-2k database.\n224. We also present scatter plots of predictions versus MOS for better visualization in Fig.8.\n225. Overall,  (a) PSNR, (b) SSIM[12], (c) MS-SSIM[11], (d) LPIPS[10], (e) VMAF BA[40], (f) BRISQUE[37], (g) NIQE[36], (h) NIMA[8], (i) DBCNN[35], (j) HyperIQA[9], (k) StairIQA[1], (l) BBAND[42], (m) CAMBI[41], (n) DBI[43], and (o) Ours.\n226. Fig.9. Visual comparisons of the banding map results.\n227. From top to bottom are banding images and their corresponding banding maps generated by BBAND[42], DBI[43], and our proposed method, respectively.\n228. The first five columns of images from left to right are from BAND-2k, while the rest images are from[43].\n229. the performance of traditional FR IQA models such as PSNR, SSIM, and MS-SSIM is remarkably inferior in banding images assessment and is uncorrelated with the MOS, which is consistent with the numerical results presented in TableIV.\n230. The reason is that PSNR and SSIM-based methods do perform not well on tiny, regional, and content-independent banding distortion while lacking the consideration for the mechanism of HVS.\n231. It is also worth mentioning that the deep CNN architecturebased methods (DBCNN[35], HyperIQA[9], and StairIQA[1]), despite performing well on LIVEC[6], KonIQ-10k[29], and other universal image quality evaluation databases[4],[5], underperformed our proposed model by a notable margin on the BAND-2k database.\n232. (2) the perception of banding artifacts is explicitly intensity-aware.\n233. These are the issues that the CNN-based approaches above do not take into account.\n234. To some extent, banding distortion is more like a kind of local distortion than common global distortion such as Gaussian noise, blur, and dither.\n235. This suggests that it is potentially valuable to integrate some local texture, edge, contrast, or other visibility-related features into quality prediction models when assessing banding images.\n236. Fortunately, with the help of the proposed scoring strategy, our method gains the ability to convert the area range of identified banding regions to indicative annoying scores.\n237. Surprisingly, the banding IQA methods BBAND[42], CAMBI[41], and VMAF BA[40], however, did not perform very well on the BAND-2k dataset.\n238. We infer that this is due to differences in the test environment.\n239. First, the source videos selected in CAMBI and VMAF BA are from the internal Netflix 4K catalogue while the source contents in BAND-2k are collected from the public streaming websites with different resolutions, leading to an uneven quality level.\n240. Second, the methods of artificially introducing banding distortion are different.\n241. 264 compression techniques are used to generate the banding artifacts, limiting the adaptive ability of the algorithm in other scenarios.\n242. For further investigation, we compare our method with other two methods, i.e., BBAND[42], DBI[43], and visually study the generated banding maps to verify their effectiveness in banding detection.\n243. The visualization results are shown in Fig.9, where seven representative banding images are selected for reference.\n244. As shown, the banding maps generated from BBAND are mostly disordered and have a lot of discontinuity while the banding maps generated by DBI are too vague to locate the exact location of banding artifacts, making it difficult to develop pixel-level debanding techniques.\n245. Overall, we can compendiously conclude that: (1) in comparison with BBAND and DBI, banding maps computed by our proposed method could provide an accurate, clear indication for banding artifacts; (2) in comparison with BBAND and DBI, the quality prediction of banding images guided by our method could achieve a high consistency with HVS.\n246. Moreover, to make a statistically meaningful comparison among different IQA methods, we further conduct the widely used F-test[2],[3]to assess the statistical significance of the proposed method.\n247. Based on the assumption that the model\'s prediction residuals follow the Gaussian distribution, the lefttailed F-test with a confidence level of 95% is performed on the residuals of every two IQA models.\n248. The results of significance tests on the BAND-2k database are shown in Fig.10.\n249. A value of \'1\' (colored in green) indicates that the model in the row is significantly better than the model in the column, while a value of \'0\' (colored in red) indicates that the model in the row is not significantly better than the model in the column.\n250. It is shown that our proposed method performs significantly better than other models, which is consistent with the observations from the above comparison experiments.\n251. In this section, we explore the effectiveness of our model\'s design philosophy.\n252. To verify the importance of the dual-branch (DB) architecture, the baseline variants use the same backbone as the proposed method, except that only a single branch (SB) is reserved.\n253. Then, we use the original banding images as the input while removing the frequency map generation module (SB-I).\n254. Besides, the high-frequency maps and low-frequency maps generation modules are retained respectively (SB-HFM and SB-LFM).\n255. It can be observed from Table V that SB-LFM achieved the worst results, since the low-frequency map filters out the highfrequency banding information to a certain extent, which reduces the ability of the model to identify false contours.\n256. SB-I performs significantly better than SB-HFM and SB-LFM, resulting from that it contains richer image information, but it is still inferior to our method, which demonstrates the effectiveness of the dualbranch scheme.\n257. To investigate the effect of the banding feature extraction, we further design two variants with different input combinations of frequency maps.\n258. First, the high-frequency map was taken as the input of both the CN N H and CN N L layers (DB-HFM).\n259. Then, we replace the inputs with the low-frequency maps (DB-LFM).\n260. As shown in TableV, the performance of DB-HFM and DB-LFM is far apart from our approach, which matches our hypothesis that the high-frequency texture information contained in HFM and the low-frequency background information contained in LFM are crucial to enhance the capacity of discernment for banding artifacts.\n261. Therefore, we may conclude that our model is the most suitable model among those compared variants in terms of both banding classification and IQA applications.\n262. Due to the effects of different compression techniques, shooting equipment, scenes, etc., the image content and banding distortions may vary significantly in practical applications.\n263. For the database[43], it only includes limited types of image sources and means of triggering banding distortion.\n264. As a result, we conduct a cross-database validation to verify the generalizability of the proposed model, wherein the database presented by[43]and BAND-2k are included.\n265. That is, we trained the model on one full database and report the test performance on the other.\n266. We mainly compare the proposed method with four learningbased models, i.e., DBCNN[35], HyperIQA[9], StairIQA[1], and DBI[43].\n267. Since MOS information is not provided in the database[43], which is an essential part of methods training, we condensed part of the experiments.\n268. TableVIand VII report the experimental results in terms of patch-level banding classification and image quality assessment.\n269. We can observe that our proposed method generalization between database[43]and BAND-2k was surprisingly good.\n270. Besides, it is worth noting that the performance of these methods trained on the BAND-2k has improved a little compared to the previous versions that were trained on the database[43], which further demonstrates the superiority of the proposed database BAND-2k.\n271. The efficiency of an image quality prediction model is of great importance in practical industrial deployments.\n272. Therefore, we measured the average running time of the compared IQA models, as shown in TableVIII.\n273. The experiments were performed in MATLAB R2021a and Python 3.7 under Windows 10 64bit system on a Lenovo laptop with Intel Core i5-9300HF CPU@2.4GHz,16GB RAM, and NVIDIA GTX 1660Ti 6G GPU.\n274. It can be observed that the proposed method achieves a reasonable running time among the FR, NR, and other bandingspecified IQA algorithms.\n275. Generally, the execution time of classical IQA algorithms is significantly less than learning-based methods.\n276. Simpler NSS-based models such as BRISQUE and NIQE still show competitive efficiency relative to CNN models while exhibiting inferior performance in banding image quality assessment.\n277. For CAMBI and VMAF BA , we use the officially launched software package, which is based on the stand-alone C library libvmaf and therefore surpasses other methods in speed.\n278. Moreover, unlike the general quality evaluation using regression to predict scores, the patch-wise prediction strategy that we adopted may increase the complexity.\n279. Note that although we deployed a more complex network structure, a nearly 10 times speedup has been seen when comparing DBI with our method since the sliding window mechanism[43]is removed.\n280. In this paper, we conduct a comprehensive exploration of banding images from both subjective and objective perspectives.\n281. Specifically, we construct the largest ecologically valid banding IQA database to date named BAND-2k database, which consists of 2,000 banding images generated by fifteen compression and quantization schemes, achieving several times larger in number and diversity than the existing banding dataset.\n282. The construction process of the database, including distortion content preparation, subjective test procedure, and the removal of outlying data, is described in detail in this paper.\n283. Relying on this database, we proposed a novel banding evaluator using the frequency characteristic of banding artifacts, which models the banding as high-frequency artifacts that contained in the low-frequency smoothing region.\n284. A dual-branch CNN is devised to extract hierarchical features to classify the banding regions, upon which we introduce the spatial frequency masking to refine and compute an overall banding score.\n285. Experimental results show that our proposed method outperforms the baseline algorithms significantly in patch-level banding classification and banding IQA tasks.\n286. We believe that our study will benefit further development, calibration, and benchmarking of banding IQA models.\n\n请严格按照JSON格式返回：{"labels": [0,1,0,...]}，不要包含任何其他文字。'}], 'model': 'qwen-plus', 'response_format': {'type': 'json_object'}}}
2025-10-14 20:51:53,597 - DEBUG - Sending HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
2025-10-14 20:51:53,598 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-10-14 20:51:53,598 - DEBUG - send_request_headers.complete
2025-10-14 20:51:53,598 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-10-14 20:51:53,598 - DEBUG - send_request_body.complete
2025-10-14 20:51:53,599 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-10-14 20:56:42,031 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'vary', b'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding'), (b'x-request-id', b'11f48dbe-4b6b-459b-9800-59f94b558ef4'), (b'x-dashscope-call-gateway', b'true'), (b'content-type', b'application/json'), (b'req-cost-time', b'288402'), (b'req-arrive-time', b'1760446303664'), (b'resp-start-time', b'1760446592066'), (b'x-envoy-upstream-service-time', b'288311'), (b'content-encoding', b'gzip'), (b'date', b'Tue, 14 Oct 2025 12:56:31 GMT'), (b'server', b'istio-envoy'), (b'transfer-encoding', b'chunked')])
2025-10-14 20:56:42,032 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-14 20:56:42,032 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-10-14 20:56:42,032 - DEBUG - receive_response_body.complete
2025-10-14 20:56:42,032 - DEBUG - response_closed.started
2025-10-14 20:56:42,033 - DEBUG - response_closed.complete
2025-10-14 20:56:42,033 - DEBUG - HTTP Response: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "200 OK" Headers({'vary': 'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding', 'x-request-id': '11f48dbe-4b6b-459b-9800-59f94b558ef4', 'x-dashscope-call-gateway': 'true', 'content-type': 'application/json', 'req-cost-time': '288402', 'req-arrive-time': '1760446303664', 'resp-start-time': '1760446592066', 'x-envoy-upstream-service-time': '288311', 'content-encoding': 'gzip', 'date': 'Tue, 14 Oct 2025 12:56:31 GMT', 'server': 'istio-envoy', 'transfer-encoding': 'chunked'})
2025-10-14 20:56:42,033 - DEBUG - request_id: 11f48dbe-4b6b-459b-9800-59f94b558ef4
2025-10-14 20:56:42,034 - DEBUG - API request completed in 288.45 seconds
2025-10-14 20:56:42,035 - DEBUG - Raw model response: {"labels": [1,0,0,1,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1
2025-10-14 20:56:42,035 - ERROR - JSON parsing error: Expecting ',' delimiter: line 1 column 9286 (char 9285)
2025-10-14 20:56:42,036 - ERROR - Problematic content: {"labels": [1,0,0,1,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1
2025-10-14 20:56:42,036 - INFO - Attempt 3 of 5
2025-10-14 20:56:42,038 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-db8f96ef-4881-4f74-862f-12eea6c7141e', 'json_data': {'messages': [{'role': 'system', 'content': '你是一个专门用于识别学术论文中数据集描述的助手。你需要判断每个输入的句子是否描述了研究中使用的数据集。请确保：\n    1. 只输出JSON格式的结果\n    2. 结果格式必须为 {"labels": [0,1,0,...]}\n    3. 不要输出任何其他文字或解释'}, {'role': 'user', 'content': '你是一位专业的学术论文数据集描述识别专家。请仔细分析下面这篇论文中的每个句子，判断它们是否描述了本研究特定构建的数据集信息。\n\n    论文标题: BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment\n\n    判断标准:\n    判断为1(数据集描述)的标准：\n    句子必须直接描述本研究特定构建的数据集,包括:\n    1. 该数据集的构建过程和方法\n    2. 该数据集的具体构成和规模\n    3. 该数据的来源和收集方式\n    4. 该数据的预处理步骤\n    5. 该数据集的可获取方式(如发布地址)\n    6. 或者其他直接或者间接描述了该数据集的句子\n\n    所有其他类型的句子均标记为0。\n    \n    请对每个句子进行分析，返回一个JSON格式的标签数组。格式要求：\n    1. 必须是有效的JSON格式\n    2. 只包含labels字段，值为0和1组成的数组\n    3. 数组长度必须与句子数量相同\n    4. 示例格式：{"labels": [0,1,0,1,...]}\n\n    需要判断的句子：\n\n    1. Abstract: Banding, also known as staircase-like contours, frequently occurs in flat areas of images/videos processed by the compression or quantization algorithms.\n2. As undesirable artifacts, banding destroys the original image structure, thus inevitably degrading users\' quality of experience (QoE).\n3. In this paper, we systematically investigate the banding image quality assessment (IQA) problem, aiming to detect the image banding artifacts and evaluate their perceptual visual quality.\n4. Considering that the existing image banding databases only contain limited content sources and banding generation methods, and lack perceptual quality labels (i.e.mean opinion scores), we first build the largest banding IQA database so far, named Banding Artifact Noticeable Database (BAND-2k), which consists of 2,000 banding images generated by 15 compression and quantization schemes.\n5. A total of 23 workers participated in the subjective IQA experiment, yielding over 214,000 patch-level banding class labels and 44,371 reliable image-level quality rating scores.\n6. Subsequently, we develop an effective no-reference (NR) banding evaluator for banding detection and quality assessment by leveraging frequency characteristics of banding artifacts.\n7. To be more specific, a dual convolutional neural network (CNN) is employed to concurrently learn the feature representation from the high-frequency and low-frequency maps, thereby enhancing the ability to discern banding artifacts.\n8. The quality score of a banding image is generated by pooling the banding detection maps masked by the spatial frequency filters.\n9. The experimental results demonstrate that our banding evaluator achieves a remarkably high accuracy in banding detection and also exhibits high SRCC and PLCC results with the perceptual quality labels, even without directly learning a regression model for banding quality evaluation.\n10. These findings unveil the strong correlations between the intensity of banding artifacts and the perceptual visual quality, thus validating the necessity of banding quality assessment.\n11. The BAND-2k database and the proposed banding evaluator will be available at https://github.com/zijianchen98/BAND-2k.\n12. R ECENT years have witnessed a rapid emergence of media streaming services and social platforms.\n13. YouTube, Netflix, and TikTok account for more than half of the world\'s video traffic.\n14. Improving the quality of images under limited encoding, Zijian Chen, Wei Sun, Jun Jia, Zicheng Zhang, Xiongkuo Min, and Guangtao Zhai are with the Institute of Image Communication and Information Processing, Shanghai Jiao Tong University, Shanghai 200240, China (e-mail: {zijian.chen,sunguwei, jiajun0302, zzc1998, minxiongkuo, zhaiguangtao}@sjtu.edu.cn).\n15. Fangfang Lu is with the College of Computer Science and Technology, Shanghai University of Electric Power, Shanghai 200290, China (email:lufangfang@shiep.edu.cn).\n16. Jing Liu is with the School of Electrical and Information Engineering, Tianjin University, Tianjin, China.(e-mail:jliu tju@tju.edu.cn).\n17. transmission bandwidth, and storage condition is a necessary prerequisite for meeting the quality of experience (QoE) of users.\n18. In the stages between image acquisition and display, an image may suffer from various types of degradation, while banding artifacts are a kind of false contour distortion that is quite perceptible to the human eye.\n19. Since the visual quality of image contents greatly affects the QoE of end-users, it is highly desirable to design an effective banding image quality assessment (IQA) method, which aims to automatically detect the traces of such false contours and predict the objective quality of banding images that can be used to develop pre-processing or postprocessing debanding algorithms and optimize the performance of streaming media application.\n20. Normally, banding artifacts take on the appearance of annual rings, radiation circles, halos, or geographical contour lines and especially exist in the background regions (e.g., sky, water, and wall surface), where the color transition is not smooth enough.\n21. Nearly all existing image or video encoders, including H.264/AVC[28], VP9[38], and H.265/HEVC[27]can introduce such artifacts more or less.\n22. Current banding IQA research can be divided into two categories: subjective quality assessment and objective quality assessment.\n23. The existing subjective banding IQA research[39],[41]-[43]mainly investigates the limited banding scenarios with internal-used and undisclosed databases while lacking the quality label and may be insufficiently generalizable to large-scale commercial applications.\n24. Meanwhile, general IQA methods aiming at common distortions are inapplicable for banding exacerbated images due to the essential differences between them.\n25. First, banding artifacts usually hold tiny, staircaselike, and regional structures, which can be regarded as a kind of high-frequency artifact in smooth areas, while general distortions occur obviously in the whole image and are globally uniform.\n26. Second, the perceptual severity of banding is quantified based on its fraction of coverage and intensity in an image, which is widely divergent from the design philosophy of many existing IQA approaches.\n27. As a result, it is challenging to design an effective banding IQA method.\n28. To address these limitations, we first conduct a comprehensive subjective study of banding exacerbated images and create the largest banding IQA database to date with reliable mean opinion scores (MOS) and patch-level banding labels.\n29. We also propose a novel no-reference banding evaluator for banding detection and quality assessment by leveraging the frequency characteristics of banding artifacts.\n30. First, due to the dissimilar peculiarities of banding and smooth regions, the same distortion in different regions, e.g., textual and pictorial regions, may lead to different visual perception of human beings.\n31. Considering that banding manifests as a high-frequency artifact that exists in the lowfrequency smooth region, we propose a dual-branch CNN, which takes the high-frequency map and low-frequency map as inputs simultaneously, to hierarchically incorporate different visual features from the first layer and the last layer of our Resnet-50 backbone, thus making the model learn more effective banding feature representation and achieving more accurate banding region discrimination.\n32. Furthermore, inspired by the previous studies[15],[16], spatial frequency extracts information consistent with the human visual system (HVS), which not only reflects the overall active level in an image but also intuitively quantifies the contrast information.\n33. In other words, the value of spatial frequency is large in smooth areas, while becoming small in areas with harsh contrast changes, i.e., banding areas.\n34. Based on this mechanism, we adopt a spatial frequency masking strategy to refine the detected banding map and then pool the masked banding detection map to obtain the image-level banding quality score.\n35. • We propose a novel no-reference banding evaluator for banding detection and quality assessment based on frequency characteristics of banding artifacts, which utilizes a dual-branch CNN model to extract hierarchical banding-related feature representation from the high-frequency maps and low-frequency maps simultaneously.\n36. • Experimental results show that the proposed banding evaluator achieves the best performance in banding detection and significantly surpasses baselines in terms of SRCC and PLCC in the banding IQA task, which demonstrates the effectiveness of the proposed model.\n37. The remainder of this paper is organized as follows.\n38. Section II provides an overview of related works, including the stateof-the-art banding databases and detection methods.\n39. Section III introduces the construction of the BAND-2k database and the subjective assessment study.\n40. Section IV proposes an effective no-reference banding evaluator for banding detection and quality assessment.\n41. Section V gives the experimental results and analysis.\n42. Section VI concludes this paper.\n43. In this section, we first provide an overview of the state-ofthe-art banding-related databases (TableI) and then review the banding detection and evaluation methods.\n44. The first banding artifact-relevant VQA database was proposed by Wang et al.[39], which consists of 21 stimuli with different quantization grades generated by VP9[38]from 7 clips of 1280 × 720 30fps video.\n45. Authors in[41]investigated the effect of encoding parameters and dithering on the visibility of banding.\n46. Nine 4k-10bit source clips from the existing Netflix catalogue between 1 and 5 seconds were used to generate banding distorted videos.\n47. Each source content was downsampled to appropriate resolutions (1080p, 2k, or 4k) with certain bit-depth and further compressed by libaom (an AV1 codec library) at QPs {12, 20, 32}.\n48. More recently, Kapoor et al.[43]constructed one of the first databases for data-driven image banding assessment models.\n49. This research included about 1,440 images shot from over 600 pristine HD videos with a resolution of 1920 × 1080.\n50. Six levels of bit-depth quantization in luminance and chrominance channels are introduced to obtain different intensities of banding.\n51. Meanwhile, the banding images were semi-automatically segmented and labeled into banded and non-banded to form a patch-level banding dataset, which allows for training machine learning-based and deep learning-based banding classification methods.\n52. However, to the best of our knowledge, thus far there still lacking a benchmarking dataset in the banding detection and the corresponding banding IQA domain.\n53. Researchers either resort to image/video quality datasets that do not aim at banding distortion or build a small, attribute-restricted, in-house dataset by themselves.\n54. This motivates us to construct a large-scale subjective assessment database focus on the perceived bandingaffected image quality.\n55. Early research on banding detection mainly focuses on false contour identification, which aims to find the wrong boundary rather than a "true" region edge in the image.\n56. Authors in[31]-[33]utilized monotonicity or non-monotonicity features of local support regions including the gradient, contrast, variance, and entropy information to measure the loss of low-amplitude detail caused by banding.\n57. However, these works ignored the perceptual characteristics of the human visual system (HVS) and thus did not perform a good correlation with subjective tests.\n58. Another banding detection strategy is conducted at the pixel-level estimation and segmentation.\n59. Bhagavathy et al.[30]proposed to identify banding artifacts by calculating the likelihood of pixel difference.\n60. Baugh et al.[34]measured the severity of banding based on the number of a group of connected pixels with the same color.\n61. Wang et al.[39]first detected uniform segments to find possible banding areas and further incorporated edge features (e.g.length and contrast) to capture false boundaries.\n62. Nevertheless, these kinds of methods are typically sensitive to edge noise and are computationally expensive, causing limited application in realtime scenarios.\n63. Towards addressing these problems, Tu et al.[42]presented a completely no-reference banding detection method, which combines various properties of HVS with a number of preprocessing steps to refine banding edge detection.\n64. Instead of regarding banding detection as a false edge detection problem, Tandon et al.[41]heuristically utilized the effect of contrast sensitivity function (CSF) on banding visibility and its dependence on spatial frequency.\n65. Based on this, Krasula et al.[40]further compared the banding annoyance with more commonly studied compression artifacts and proposed a banding-aware video quality metric.\n66. In recent years, deep learning approaches have prevailed in various VQA tasks.\n67. As the pioneering work, Kapoor et al.[43]developed an automated CNN-based banding detector for the first time, which is a simple two-stage algorithm and gives rise to devising other learning-based techniques.\n68. In this work, we build a large-scale banding database and propose a data-driven banding indicator that can generate pixelwise banding visibility maps with corresponding subjectively  consistent quality scores by combining human visual mechanisms and deep learning techniques.\n69. Subjective banding image quality assessment facilitates the development of automatic objective banding image and video quality models.\n70. We created the largest banding database in existence, denoted as the BAND-2k database, which consists of 2,000 banding distorted images and over 214,000 patch-level banding class labels.\n71. Then, a subjective experiment was conducted to obtain the mean opinion scores (MOS) of the BAND-2k database.\n72. The workflow of the banding database construction is shown in Fig.1.\n73. To build a content-rich and balanced database, we manually collected source videos including computer graphics (CG), usergenerated content (UGC), and professionally-generated content (PGC) from two popular media websites Bilibili.comand Youtube.com.\n74. Then, 885 clips with multiple spatial resolutions (i.e., 4096×2160, 3840×2160, 1920×1080) and frame rates (i.e., 60, 50, 30, 25) are chosen as candidate.\n75. Note that videos on the mentioned websites are firstly annotated by the community with assigned a number of favorites, views, and downloads.\n76. These statistics correlate with the content and quality of a video, which guides our choices to some extent.\n77. All videos selected on the website are released under an appropriate creative commons (CC) license that allows further editing and redistribution.\n78. After content selection, we further unified the format of all video clips, especially the spatial resolution and the pixel format, which avoid the effect of other facts on visual quality.\n79. Concretely, we first converted the frame rate of the original clips to 25fps, which is to reduce the storage pressure while ensuring the graphics quality.\n80. Considering the commonly used aspect ratio of the user interface and displays is 16:9, we cropped the partially unqualified videos rather than shrinking images unevenly.\n81. Then, we downsampled the trimmed spatial resolution 3840×2160 to a lower resolution -1920×1080 for the following subjective study.\n82. Before conducting the following experiments, we manually removed the videos that are either too dark or bright, overly blurry or colorful, which helps to obtain more reliable subjective assessments.\n83. To avoid redundancy and to make sure the diversity of selected contents, we also conducted attribute analysis studies on the selected images.\n84. Four metrics that correlated with human perception, i.e., contrast, brightness, sharpness, and colorfulness, are adopted as content diversity metrics.\n85. All video attributes are calculated on every 10 frames to reduce computational complexity, which are then averaged over frames of each video sequence.\n86. Fig.2shows the distribution of attributes extracted from the selected videos.\n87. • Contrast: The contrast metric is simply defined as the standard deviation of pixel gray-scale intensities[25].\n88. • Colorfulness: The colorfulness metric is measured by the R, G, and B components[25].\n89. We first compute two matrices of differences between channels rg = R -G and yb = 1 2 (R + G) -B.\n90. Then, the colorfulness metric can be calculated as µ 2 rg + µ 2 yb + σ 2 rg + σ 2 yb , where µ and σ are the mean and standard deviation of their corresponding terms, respectively.\n91. • Sharpness: The cumulative probability of blur detection (CPBD) metric[24]is used to measure the image sharpness, which estimates the probability of detecting blur at each edge.\n92. • Brightness: The brightness of an image is obtained directly from the pixel gray intensities in R, G, and B channels.\n93. Finally, the number of source videos are reduced to 873.\n94. Fig.3displays thumbnails for 30 selected representative video clips.\n95. To simulate authentic banding artifacts that exist in real viewing scenarios, we introduced four encoding/transcoding strategies including H.264/AVC[28], H.265/HEVC[27], VP9[38], and bit-depth manipulation[43]with fifteen quantization schemes in total.For H.264/AVC and H.265/HEVC, considering the range of their quantization parameter (QP in ffmpeg), we selected three typical QP values, namely, QP = {18, 33, 43}.\n96. This is because that coded video clips with a QP value smaller than 18 normally provide perceptual lossless quality, while coded video clips with a QP value larger than 43 will not be able to offer adequate quality, which may affect the subjective test of banding artifacts.\n97. Similarly, for the VP9 encoder, the QP values under our close inspection are chosen as {28, 45, 60}.\n98. Moreover, we applied the same quantization strategies as in[43], which introduces banding distortion by scaling bit-depth in luminance and chrominance channels.\n99. Here, the color coding scheme YCbCr4:2:0 is applied for maximum compatibility.\n100. To sum up, we generated fifteen levels of banding with different intensities and shapes to enhance the diversity of the database.\n101. The banding database is then built by extracting frames from the distorted video clips, resulting in 2,000 images with a resolution of 1920×1080.\n102. Fig.4shows the visualization results of banding exacerbated images.\n103. In the process of image patch labeling, we initially intend to extract image patches of size 235×235 from banding images directly and perform annotation operations.\n104. However, considering the theoretical number of patches, it will cost a lot of manpower and time to label patch by patch.\n105. Therefore, we first segmented the banding images roughly and label them into banded and non-banded regions.\n106. Then, labelled image patches are generated from these segmented and labelled images by a sliding window.\n107. Specifically, we followed the same demarcation of the banded and non-banded images in[43]that a patch is labelled as banded if it has more than 30% overlap with banded regions in the image.\n108. Eventually, a banding dataset containing 2,000 distorted images with 1920×1080 resolution and 214,324 labelled image patches of size 235×235 is built.\n109. To the best of our knowledge, it is the largest banding dataset in existence, which enables training various machine/deep learning based banding detection models and facilitates the development of image/video debanding techniques.\n110. TableIIreports the composition of labelled image patch dataset.\n111. It can be observed that the number of banded patches is a bit smaller than non-banded patches due to the fact that banding usually appears in smooth background areas.\n112. As shown in Fig.1, the subjective quality study contains four steps.\n113. In addition to preparing the experimental environment, subjects should pass the qualification test first to participate in the study.\n114. After the subjective rating, all resulting scores need to be analyzed and examined before generating the final mean opinion score (MOS).\n115. 1) Experimental Environment Setting: In this study, a total of 25 inexperienced subjects are gathered in a laboratory environment, where relevant experimental configuration must satisfy the following requirements: • Considering the viewing effect, desktops and laptops are allowed as displays.\n116. • The resolution of displays must be larger than or equal to 1920×1080 to show the images without spatial downsampling.\n117. • The viewing distance and optimal horizontal viewing angle are set as 1.9 times the height of the display and respectively.\n118. 500 recommendation[26].\n119. As a result, we used a 27-inch AOC Q27U2D monitor with a resolution of 2560×1440 for assessment with 25 subjects.\n120. Due to the large number of images to be assessed, we divided the dataset into ten sessions to avoid visual fatigue.\n121. Each session of tests took nearly 2 hours with a 30-minute break for each participant.\n122. 2) Qualification Test: Before starting the main experiment, subjects are required to pass a quiz to get the qualification of conducting follow-up experiments.\n123. Firstly, we manually selected 10 labeled banding exacerbated images beyond the database as training images to familiarize subjects with the operation interface and the goal of this subjective test.\n124. The quiz consists of two parts including banding classification and image-level quality rating.\n125. In banding classification, subjects were told to divide the test image into banded or non-banded.\n126. In image-level quality rating, subjects were instructed to focus on the coverage and intensity of banding areas, as well as the overall quality of images to get the final quality score.\n127. The rating scale is continuous from 0 to 100 while a higher value indicates more severe banding (i.e., more visible or occupying larger portion of the image).\n128. To make the quiz objective and fair, we take the banding images labeled by domain experts as the ground truth, while two existing banding metrics BBAND[42]and DBI[43]are used to determine the normal range of the rating scores.\n129. That is the quality scores rated by subjects should not exceed 20% of the above banding metrics.\n130. As a result, only 23 subjects with an accuracy above 80% in banding classification and subjective scores in the normal range were allowed to pass the quiz.\n131. Note that the steps for taking a quiz are the same as the main experiment, which aims to guarantee the consistency of results.\n132. 3) Formal Study: We adopted the single-stimulus (SS) method in this test.\n133. Ten golden images that have the acknowledged high quality or poor quality (assessed by BBAND[42]and DBI[43]) were added to each session for controlling the scoring deviations.\n134. Besides, 3 repeated images are randomly inserted into each session to ensure consistency of scores before and after subjects scoring.\n135. At last, 23 qualified subjects were asked to provide their opinions on the shuffled image groups.\n136. The resulting scores were collected and packed for further analysis.\n137. 4) Result Analysis: In total, 46,000 scores were collected by 23 qualified subjects in the main study phase.\n138. However, considering the qualification quiz cannot completely disallow those unreliable workers to muddle through to the main study while reliable subjects may also occasionally score odd values, which may be caused by the inter-individual differences in perceiving the quality of the unique characteristics of different contents.\n139. Thus, we further investigated the confidence of rating scores and removed outliers following the Grubbs\' test[23],[45].\n140. Concretely, let s = (s 1 , s 2 , . . ., s N ) denote a set of raw scores collected for one distorted image.\n141. The test statistics is defined as the maximum absolute deviation of a sample standard deviation (SD) unit from the sample mean, which is mathematically expressed as where s and σ s denote the mean and standard deviation of the sample, respectively.\n142. Then, given a significant level α, a sample is detected as an outlier if where t 2 α/(2N ),N -2 represents the upper critical value of the tdistribution with N -2 degrees of freedom and a significance level of α/(2N ).\n143. Empirically, we set the significance level α at 0.05.\n144. Then, a sample is identified as an outlier if its distance to the sample mean is larger than 2.5 times SD and is removed.\n145. Following the aforementioned steps, the total number of scores Fig.6.\n146. The overall architecture of the proposed method.\n147. Given a banding distorted image, it is first divided into patches.\n148. Then, the patch-level high-frequency map (HFM) and low-frequency map (LFM) are generated by Sobel operation and piece-wise smooth algorithm[19], respectively.\n149. After that, a dual-branch CNN (CNN H and CNN L ) is deployed to extract hierarchical features with different visual information and thus classify the patches into banded or non-banded.\n150. Lastly, a spatial frequency masking strategy is introduced to refine the banding map and calculate the image-level banding quality score.\n151. Note that the dual-branch networks do not share parameters.\n152. was reduced to 44,371, and MOS was created by averaging the scores for each image.\n153. Fig.5presents the histogram of MOSs over the entire database, showing a broad MOS distribution of banding images.\n154. In this section, we describe the architecture of the proposed banding evaluator in detail, as shown in Fig.6.\n155. As stated before, banding usually appear as high-frequency information in the smooth background, while humans perceive high-frequency texture regions and low-frequency plateau regions through different neural channels concurrently, and transfer the upper visual features into the cerebral cortex for final processing[17],[44].\n156. Inspired by this, we employ high-frequency maps (HFM) and low-frequency maps (LFM) as the deep learning network inputs, which represent the texture and structural information of the image respectively, to mimic the recognition mechanism of the human brain for better banding identification.\n157. High-frequency Maps.\n158. Since gradient has been widely used to represent edge information and has been confirmed beneficial to acquire high-frequency components with low computational cost[20]-[22], we apply the isotropic Sobel operator to each patch for enhancing the details of banding artifacts.\n159. Given an input patch I, the high-frequency map is calculated by where S x and S y are the horizontal and vertical isotropic Sobel operators, respectively." * " denotes the convolution operation.\n160. Low-frequency Maps.\n161. To maintain the principal content of the image and filter out the influence of high-frequency information, we use the piece-wise smooth algorithm[19]to generate the low-frequency map by minimizing a function for image approximation recovery: where L represents the low-frequency map, Ω and E denotes the image domain and edge set, respectively.\n162. P indicates the pixel and E dσ represents the total edge length.\n163. The coefficients α and β are positive regularization constants.\n164. An example of frequency maps is shown in Fig.7.\n165. To obtain the overall banding score, we first divide the banding image into 235×235 patches and apply CNN-based classification to each patch, resulting in a banding classification label for each patch, i.e., banded or non-banded.\n166. As shown in Fig.6, the proposed network consists of two parallel branches, namely CNN H and CNN L , which take the patch-level high-frequency map and low-frequency map as input, respectively.\n167. For each branch, we propose to use Resnet-50[18]as the backbone.\n168. Specifically, we incorporate the feature maps extracted from the first convolutional layer and the last layer of Resnet-50 as hierarchical visual features, which represent different visual information[46],[47]and can be used as predictive information to enhance the discrimination ability of the network for banding and non-banded regions.\n169. Afterward, the features extracted from two branches are concatenated first and reshaped into 128dimensional vectors through two fully-connected layers, which is further followed with the sigmoid activation function to output the final predicted label, namely banded or non-banded.\n170. Of note is that sharing parameters is extremely unfavorable for extracting low-and high-frequency features simultaneously, we thereby deploy two branches that work independently and do not share parameters.\n171. The loss function adopted here is binary cross entropy.\n172. With the trained CNN H and CNN L , each input patch is predicted to obtain a banding or non-banded label.\n173. To better guide the pre-processing and post-processing debanding algorithms, it is necessary to generate a quality score for the entire banding image.\n174. Since the visibility of edge is also affected by content, we further consider the effect of spatially varying content information on the local quality of human perception.\n175. As a consequence, we introduce the spatial frequency masking strategy to determine the weighting factor for the detected banding regions in each patch adaptively and thus obtain the image-level banding severity score while refining the visibility of banding artifacts.\n176. 1) Spatial Frequency Masking: The spatial frequency is defined as the activity level of an image, which establishes a filterbank based on the visual stimulus and is in accordance with HVS[16].\n177. In this paper, we propose to apply spatial frequency as an effective contrast criterion to banding measurement.\n178. Specifically, given an image of size I W × I H , divided into N × N patches, where I W and I H denote the number of columns and rows respectively.\n179. The column (CF k ) and row (RF k ) frequencies of the image patches are given by where I(x, y) is the pixel value of the image patch.\n180. Then, the resulting spatial frequency of an N × N patch is computed as where k is the number of patches (1 ≤ k ≤ I W I H N 2 ).\n181. Since most banding regions are likely to have large contrast including edges and textures, which should be assigned greater weights than the smooth and blurred areas.\n182. Accordingly, we design a banding visibility transfer function to express spatial frequency masking as a function of the local textural feature.\n183. The final spatial frequency masking weight is calculated at each patch as where γ is the scaling constant factor chosen to tune the shape of the transfer function.\n184. We used γ = 1.5 in our implementation.\n185. 2) Building a Banding Metric: The visibility of banding artifacts depends on the combination of multiple visual mechanisms.\n186. In this paper, we propose a simple but effective product model for attribute integration at each predicted banding patch to obtain the entire banding map (BM): where P k denotes the predicted label of k-th patch and w k is the weight parameter that scales the visibility of measured contours, i.e., gradient magnitude of the high-frequency map, |HFM k (i, j) | at region (i, j).\n187. Furthermore, inspired by previous psychovisual findings that the QoE of observers is dominated by those regions having poor quality[14],[42], we thereby leverage the worst p% percentile visual pooling to calculate an average banding score from the generated BM, where p is set to 80 in this experiment.\n188. As a result, the perceptual score of the overall banding image is defined as where M is the total number of patches in image I.\n189. T p% denotes the index set of the top p% non-zero pixel-wise value contained in k-th patch of the BM.\n190. In this section, we first present the experimental protocol in detail and then evaluate the performance of the proposed method on two tasks, namely patch-level banding classification and banding image quality assessment.\n191. After that, the ablation study and cross-database validation are conducted to prove the robustness and effectiveness of the proposed method.\n192. Finally, we test the computational efficiency of our method.\n193. A. Experimental Protocol 1) Databases and Settings: We choose two databases to train and test the effectiveness of the proposed banding IQA method, which are the database released in[43]and our proposed BAND-2k database.\n194. The detail information of these two datasets can be found in TableI.\n195. The proposed model is implemented by PyTorch[13].\n196. Before training, we randomly split the training, validation, and testing set into 8:1:1 (as shown in TableII).\n197. We use the Adam optimizer with the initial learning rate set as 1e-4 and set the batch size as 32.\n198. The training process is stopped after 25 epochs.\n199. The resolution of each cropped patch is fixed to 235×235.\n200. All experiments on both the[43]database and the BAND-2k database are conducted repeatedly 10 times to obtain the mean performance.\n201. 2) Baseline Algorithms: We include a number of representative IQA algorithms in our evaluation as references to be compared against.\n202. These baseline methods include: • General FR IQA methods: We choose PSNR, SSIM[12], MS-SSIM[11], LPIPS[10]as baselines.\n203. These are the most commonly used FR IQA metrics in practical applications such as video coding, image enhancement, etc. • General NR IQA methods: BRISQUE[37], NIQE[36], NIMA[8], DBCNN[35], HyberIQA[9], and StairIQA[1].\n204. • Banding IQA methods: Considering that there exists few research on banding detection and quality assessment, we barely select the BBAND[42], CAMBI[41], VMAF BA[40]and DBI[43]metrics as comparisons.\n205. 3) Evaluation Criteria: To evaluate the IQA methods comprehensively, a total of seven evaluation indexes in two categories are adopted.\n206. For patch-level banding classification, we follow the common procedures as in[43]and utilize the area under the receiver operating characteristics (AUROC), the area under the precision-recall curve (AUPRC), and accuracy as the classification performance metrics.\n207. For banding image quality assessment, four mainstream metrics are selected as the evaluation criteria: Spearman rank-order correlation coefficient (SRCC) and Kendall rank-order correlation coefficient (KRCC) measure the prediction monotonicity, while Pearson linear correlation coefficient (PLCC) and root mean square error (RMSE) are calculated to assess prediction consistency.\n208. Considering the potential nonlinear mapping characteristics between the objective scores and the subjective scores, we perform score alignment by mapping the predicted value using the five-parameter logistic function before calculating PLCC and RMSE values[7].\n209. Since our goal is to develop an effective banding IQA approach, we regard the identification of banding patches as an important preceding process to achieve accurate banding quality prediction.\n210. However, there exist few methods that are designed for banding classification and nearly all IQA methods produce scalar values only while failing in classifying banding regions directly.\n211. Therefore, we adopt a thresholding step to convert the single quality value into binary classification results as[43]does.\n212. Concretely, a half-interval search algorithm[48]is employed to find the optimal threshold value that can generate the best classification result.\n213. Based on the above premise, TableIIIreports the experimental results on both the database from[43]and the BAND-2k database.\n214. We highlight the best results in boldface.\n215. As compared to other state-of-the-art IQA methods, our proposed method yields the best overall performance in terms of AUROC, AUPRC, and accuracy.\n216. It is shown that most general FR IQA and NR IQA models perform poorly on the patch-level banding classification task while performing fairly well on other IQA tasks[1],[9],[35], indicating that the current approaches are not sensitive to banding distortion.\n217. Benefiting from the powerful feature extraction ability of CNNs, our proposed method and the customized NR IQA models for banding artifacts detection (DBI[43]) reach a significant performance in the discrimination of false contours.\n218. However, the performance of banding IQA method BBAND, CAMBI, and VMAF BA is surprisingly poor compared with other methods, which shows their vulnerability in identifying local banding artifacts from texture regions and are not suitable for patch-level banding identification.\n219. In addition, we investigate the computational complexity in terms of execution time per image patch.\n220. It can be observed that except for those traditional FR IQA models, our method achieves comparable speed in patch-level banding classification, which determines the prediction efficiency of the subsequent image-level quality assessment, making it a favorable choice in time-constrained scenarios.\n221. Considering that there exist no image banding databases with subjective scores attached in the public domain, we merely compare the performance of the proposed method with the baseline approaches on the proposed BAND-2k database.\n222. The results are shown in TableIV, from which we can observe that our proposed method largely surpasses all baselines in terms of SRCC, KRCC, and PLCC except for the RMSE.\n223. Compared to the secondbest model, our method achieves 18.07%SRCC improvements, 24.91% KRCC improvements, and 18.01%PLCC improvements on the BAND-2k database.\n224. We also present scatter plots of predictions versus MOS for better visualization in Fig.8.\n225. Overall,  (a) PSNR, (b) SSIM[12], (c) MS-SSIM[11], (d) LPIPS[10], (e) VMAF BA[40], (f) BRISQUE[37], (g) NIQE[36], (h) NIMA[8], (i) DBCNN[35], (j) HyperIQA[9], (k) StairIQA[1], (l) BBAND[42], (m) CAMBI[41], (n) DBI[43], and (o) Ours.\n226. Fig.9. Visual comparisons of the banding map results.\n227. From top to bottom are banding images and their corresponding banding maps generated by BBAND[42], DBI[43], and our proposed method, respectively.\n228. The first five columns of images from left to right are from BAND-2k, while the rest images are from[43].\n229. the performance of traditional FR IQA models such as PSNR, SSIM, and MS-SSIM is remarkably inferior in banding images assessment and is uncorrelated with the MOS, which is consistent with the numerical results presented in TableIV.\n230. The reason is that PSNR and SSIM-based methods do perform not well on tiny, regional, and content-independent banding distortion while lacking the consideration for the mechanism of HVS.\n231. It is also worth mentioning that the deep CNN architecturebased methods (DBCNN[35], HyperIQA[9], and StairIQA[1]), despite performing well on LIVEC[6], KonIQ-10k[29], and other universal image quality evaluation databases[4],[5], underperformed our proposed model by a notable margin on the BAND-2k database.\n232. (2) the perception of banding artifacts is explicitly intensity-aware.\n233. These are the issues that the CNN-based approaches above do not take into account.\n234. To some extent, banding distortion is more like a kind of local distortion than common global distortion such as Gaussian noise, blur, and dither.\n235. This suggests that it is potentially valuable to integrate some local texture, edge, contrast, or other visibility-related features into quality prediction models when assessing banding images.\n236. Fortunately, with the help of the proposed scoring strategy, our method gains the ability to convert the area range of identified banding regions to indicative annoying scores.\n237. Surprisingly, the banding IQA methods BBAND[42], CAMBI[41], and VMAF BA[40], however, did not perform very well on the BAND-2k dataset.\n238. We infer that this is due to differences in the test environment.\n239. First, the source videos selected in CAMBI and VMAF BA are from the internal Netflix 4K catalogue while the source contents in BAND-2k are collected from the public streaming websites with different resolutions, leading to an uneven quality level.\n240. Second, the methods of artificially introducing banding distortion are different.\n241. 264 compression techniques are used to generate the banding artifacts, limiting the adaptive ability of the algorithm in other scenarios.\n242. For further investigation, we compare our method with other two methods, i.e., BBAND[42], DBI[43], and visually study the generated banding maps to verify their effectiveness in banding detection.\n243. The visualization results are shown in Fig.9, where seven representative banding images are selected for reference.\n244. As shown, the banding maps generated from BBAND are mostly disordered and have a lot of discontinuity while the banding maps generated by DBI are too vague to locate the exact location of banding artifacts, making it difficult to develop pixel-level debanding techniques.\n245. Overall, we can compendiously conclude that: (1) in comparison with BBAND and DBI, banding maps computed by our proposed method could provide an accurate, clear indication for banding artifacts; (2) in comparison with BBAND and DBI, the quality prediction of banding images guided by our method could achieve a high consistency with HVS.\n246. Moreover, to make a statistically meaningful comparison among different IQA methods, we further conduct the widely used F-test[2],[3]to assess the statistical significance of the proposed method.\n247. Based on the assumption that the model\'s prediction residuals follow the Gaussian distribution, the lefttailed F-test with a confidence level of 95% is performed on the residuals of every two IQA models.\n248. The results of significance tests on the BAND-2k database are shown in Fig.10.\n249. A value of \'1\' (colored in green) indicates that the model in the row is significantly better than the model in the column, while a value of \'0\' (colored in red) indicates that the model in the row is not significantly better than the model in the column.\n250. It is shown that our proposed method performs significantly better than other models, which is consistent with the observations from the above comparison experiments.\n251. In this section, we explore the effectiveness of our model\'s design philosophy.\n252. To verify the importance of the dual-branch (DB) architecture, the baseline variants use the same backbone as the proposed method, except that only a single branch (SB) is reserved.\n253. Then, we use the original banding images as the input while removing the frequency map generation module (SB-I).\n254. Besides, the high-frequency maps and low-frequency maps generation modules are retained respectively (SB-HFM and SB-LFM).\n255. It can be observed from Table V that SB-LFM achieved the worst results, since the low-frequency map filters out the highfrequency banding information to a certain extent, which reduces the ability of the model to identify false contours.\n256. SB-I performs significantly better than SB-HFM and SB-LFM, resulting from that it contains richer image information, but it is still inferior to our method, which demonstrates the effectiveness of the dualbranch scheme.\n257. To investigate the effect of the banding feature extraction, we further design two variants with different input combinations of frequency maps.\n258. First, the high-frequency map was taken as the input of both the CN N H and CN N L layers (DB-HFM).\n259. Then, we replace the inputs with the low-frequency maps (DB-LFM).\n260. As shown in TableV, the performance of DB-HFM and DB-LFM is far apart from our approach, which matches our hypothesis that the high-frequency texture information contained in HFM and the low-frequency background information contained in LFM are crucial to enhance the capacity of discernment for banding artifacts.\n261. Therefore, we may conclude that our model is the most suitable model among those compared variants in terms of both banding classification and IQA applications.\n262. Due to the effects of different compression techniques, shooting equipment, scenes, etc., the image content and banding distortions may vary significantly in practical applications.\n263. For the database[43], it only includes limited types of image sources and means of triggering banding distortion.\n264. As a result, we conduct a cross-database validation to verify the generalizability of the proposed model, wherein the database presented by[43]and BAND-2k are included.\n265. That is, we trained the model on one full database and report the test performance on the other.\n266. We mainly compare the proposed method with four learningbased models, i.e., DBCNN[35], HyperIQA[9], StairIQA[1], and DBI[43].\n267. Since MOS information is not provided in the database[43], which is an essential part of methods training, we condensed part of the experiments.\n268. TableVIand VII report the experimental results in terms of patch-level banding classification and image quality assessment.\n269. We can observe that our proposed method generalization between database[43]and BAND-2k was surprisingly good.\n270. Besides, it is worth noting that the performance of these methods trained on the BAND-2k has improved a little compared to the previous versions that were trained on the database[43], which further demonstrates the superiority of the proposed database BAND-2k.\n271. The efficiency of an image quality prediction model is of great importance in practical industrial deployments.\n272. Therefore, we measured the average running time of the compared IQA models, as shown in TableVIII.\n273. The experiments were performed in MATLAB R2021a and Python 3.7 under Windows 10 64bit system on a Lenovo laptop with Intel Core i5-9300HF CPU@2.4GHz,16GB RAM, and NVIDIA GTX 1660Ti 6G GPU.\n274. It can be observed that the proposed method achieves a reasonable running time among the FR, NR, and other bandingspecified IQA algorithms.\n275. Generally, the execution time of classical IQA algorithms is significantly less than learning-based methods.\n276. Simpler NSS-based models such as BRISQUE and NIQE still show competitive efficiency relative to CNN models while exhibiting inferior performance in banding image quality assessment.\n277. For CAMBI and VMAF BA , we use the officially launched software package, which is based on the stand-alone C library libvmaf and therefore surpasses other methods in speed.\n278. Moreover, unlike the general quality evaluation using regression to predict scores, the patch-wise prediction strategy that we adopted may increase the complexity.\n279. Note that although we deployed a more complex network structure, a nearly 10 times speedup has been seen when comparing DBI with our method since the sliding window mechanism[43]is removed.\n280. In this paper, we conduct a comprehensive exploration of banding images from both subjective and objective perspectives.\n281. Specifically, we construct the largest ecologically valid banding IQA database to date named BAND-2k database, which consists of 2,000 banding images generated by fifteen compression and quantization schemes, achieving several times larger in number and diversity than the existing banding dataset.\n282. The construction process of the database, including distortion content preparation, subjective test procedure, and the removal of outlying data, is described in detail in this paper.\n283. Relying on this database, we proposed a novel banding evaluator using the frequency characteristic of banding artifacts, which models the banding as high-frequency artifacts that contained in the low-frequency smoothing region.\n284. A dual-branch CNN is devised to extract hierarchical features to classify the banding regions, upon which we introduce the spatial frequency masking to refine and compute an overall banding score.\n285. Experimental results show that our proposed method outperforms the baseline algorithms significantly in patch-level banding classification and banding IQA tasks.\n286. We believe that our study will benefit further development, calibration, and benchmarking of banding IQA models.\n\n请严格按照JSON格式返回：{"labels": [0,1,0,...]}，不要包含任何其他文字。'}], 'model': 'qwen-plus', 'response_format': {'type': 'json_object'}}}
2025-10-14 20:56:42,042 - DEBUG - Sending HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
2025-10-14 20:56:42,042 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-10-14 20:56:42,042 - DEBUG - send_request_headers.complete
2025-10-14 20:56:42,042 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-10-14 20:56:42,042 - DEBUG - send_request_body.complete
2025-10-14 20:56:42,042 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-10-14 21:01:30,599 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'vary', b'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding'), (b'x-request-id', b'3ce69e5b-5586-450e-b566-11cacf6925f8'), (b'x-dashscope-call-gateway', b'true'), (b'content-type', b'application/json'), (b'req-cost-time', b'288527'), (b'req-arrive-time', b'1760446592120'), (b'resp-start-time', b'1760446880647'), (b'x-envoy-upstream-service-time', b'288438'), (b'content-encoding', b'gzip'), (b'date', b'Tue, 14 Oct 2025 13:01:20 GMT'), (b'server', b'istio-envoy'), (b'transfer-encoding', b'chunked')])
2025-10-14 21:01:30,600 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-14 21:01:30,600 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-10-14 21:01:30,601 - DEBUG - receive_response_body.complete
2025-10-14 21:01:30,601 - DEBUG - response_closed.started
2025-10-14 21:01:30,601 - DEBUG - response_closed.complete
2025-10-14 21:01:30,601 - DEBUG - HTTP Response: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "200 OK" Headers({'vary': 'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding', 'x-request-id': '3ce69e5b-5586-450e-b566-11cacf6925f8', 'x-dashscope-call-gateway': 'true', 'content-type': 'application/json', 'req-cost-time': '288527', 'req-arrive-time': '1760446592120', 'resp-start-time': '1760446880647', 'x-envoy-upstream-service-time': '288438', 'content-encoding': 'gzip', 'date': 'Tue, 14 Oct 2025 13:01:20 GMT', 'server': 'istio-envoy', 'transfer-encoding': 'chunked'})
2025-10-14 21:01:30,602 - DEBUG - request_id: 3ce69e5b-5586-450e-b566-11cacf6925f8
2025-10-14 21:01:30,602 - DEBUG - API request completed in 288.57 seconds
2025-10-14 21:01:30,603 - DEBUG - Raw model response: {"labels": [1,0,0,1,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1
2025-10-14 21:01:30,604 - ERROR - JSON parsing error: Expecting ',' delimiter: line 1 column 9900 (char 9899)
2025-10-14 21:01:30,604 - ERROR - Problematic content: {"labels": [1,0,0,1,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1
2025-10-14 21:01:30,604 - INFO - Attempt 4 of 5
2025-10-14 21:01:30,606 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-bc80680a-1b57-401e-9592-bf321b993c5a', 'json_data': {'messages': [{'role': 'system', 'content': '你是一个专门用于识别学术论文中数据集描述的助手。你需要判断每个输入的句子是否描述了研究中使用的数据集。请确保：\n    1. 只输出JSON格式的结果\n    2. 结果格式必须为 {"labels": [0,1,0,...]}\n    3. 不要输出任何其他文字或解释'}, {'role': 'user', 'content': '你是一位专业的学术论文数据集描述识别专家。请仔细分析下面这篇论文中的每个句子，判断它们是否描述了本研究特定构建的数据集信息。\n\n    论文标题: BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment\n\n    判断标准:\n    判断为1(数据集描述)的标准：\n    句子必须直接描述本研究特定构建的数据集,包括:\n    1. 该数据集的构建过程和方法\n    2. 该数据集的具体构成和规模\n    3. 该数据的来源和收集方式\n    4. 该数据的预处理步骤\n    5. 该数据集的可获取方式(如发布地址)\n    6. 或者其他直接或者间接描述了该数据集的句子\n\n    所有其他类型的句子均标记为0。\n    \n    请对每个句子进行分析，返回一个JSON格式的标签数组。格式要求：\n    1. 必须是有效的JSON格式\n    2. 只包含labels字段，值为0和1组成的数组\n    3. 数组长度必须与句子数量相同\n    4. 示例格式：{"labels": [0,1,0,1,...]}\n\n    需要判断的句子：\n\n    1. Abstract: Banding, also known as staircase-like contours, frequently occurs in flat areas of images/videos processed by the compression or quantization algorithms.\n2. As undesirable artifacts, banding destroys the original image structure, thus inevitably degrading users\' quality of experience (QoE).\n3. In this paper, we systematically investigate the banding image quality assessment (IQA) problem, aiming to detect the image banding artifacts and evaluate their perceptual visual quality.\n4. Considering that the existing image banding databases only contain limited content sources and banding generation methods, and lack perceptual quality labels (i.e.mean opinion scores), we first build the largest banding IQA database so far, named Banding Artifact Noticeable Database (BAND-2k), which consists of 2,000 banding images generated by 15 compression and quantization schemes.\n5. A total of 23 workers participated in the subjective IQA experiment, yielding over 214,000 patch-level banding class labels and 44,371 reliable image-level quality rating scores.\n6. Subsequently, we develop an effective no-reference (NR) banding evaluator for banding detection and quality assessment by leveraging frequency characteristics of banding artifacts.\n7. To be more specific, a dual convolutional neural network (CNN) is employed to concurrently learn the feature representation from the high-frequency and low-frequency maps, thereby enhancing the ability to discern banding artifacts.\n8. The quality score of a banding image is generated by pooling the banding detection maps masked by the spatial frequency filters.\n9. The experimental results demonstrate that our banding evaluator achieves a remarkably high accuracy in banding detection and also exhibits high SRCC and PLCC results with the perceptual quality labels, even without directly learning a regression model for banding quality evaluation.\n10. These findings unveil the strong correlations between the intensity of banding artifacts and the perceptual visual quality, thus validating the necessity of banding quality assessment.\n11. The BAND-2k database and the proposed banding evaluator will be available at https://github.com/zijianchen98/BAND-2k.\n12. R ECENT years have witnessed a rapid emergence of media streaming services and social platforms.\n13. YouTube, Netflix, and TikTok account for more than half of the world\'s video traffic.\n14. Improving the quality of images under limited encoding, Zijian Chen, Wei Sun, Jun Jia, Zicheng Zhang, Xiongkuo Min, and Guangtao Zhai are with the Institute of Image Communication and Information Processing, Shanghai Jiao Tong University, Shanghai 200240, China (e-mail: {zijian.chen,sunguwei, jiajun0302, zzc1998, minxiongkuo, zhaiguangtao}@sjtu.edu.cn).\n15. Fangfang Lu is with the College of Computer Science and Technology, Shanghai University of Electric Power, Shanghai 200290, China (email:lufangfang@shiep.edu.cn).\n16. Jing Liu is with the School of Electrical and Information Engineering, Tianjin University, Tianjin, China.(e-mail:jliu tju@tju.edu.cn).\n17. transmission bandwidth, and storage condition is a necessary prerequisite for meeting the quality of experience (QoE) of users.\n18. In the stages between image acquisition and display, an image may suffer from various types of degradation, while banding artifacts are a kind of false contour distortion that is quite perceptible to the human eye.\n19. Since the visual quality of image contents greatly affects the QoE of end-users, it is highly desirable to design an effective banding image quality assessment (IQA) method, which aims to automatically detect the traces of such false contours and predict the objective quality of banding images that can be used to develop pre-processing or postprocessing debanding algorithms and optimize the performance of streaming media application.\n20. Normally, banding artifacts take on the appearance of annual rings, radiation circles, halos, or geographical contour lines and especially exist in the background regions (e.g., sky, water, and wall surface), where the color transition is not smooth enough.\n21. Nearly all existing image or video encoders, including H.264/AVC[28], VP9[38], and H.265/HEVC[27]can introduce such artifacts more or less.\n22. Current banding IQA research can be divided into two categories: subjective quality assessment and objective quality assessment.\n23. The existing subjective banding IQA research[39],[41]-[43]mainly investigates the limited banding scenarios with internal-used and undisclosed databases while lacking the quality label and may be insufficiently generalizable to large-scale commercial applications.\n24. Meanwhile, general IQA methods aiming at common distortions are inapplicable for banding exacerbated images due to the essential differences between them.\n25. First, banding artifacts usually hold tiny, staircaselike, and regional structures, which can be regarded as a kind of high-frequency artifact in smooth areas, while general distortions occur obviously in the whole image and are globally uniform.\n26. Second, the perceptual severity of banding is quantified based on its fraction of coverage and intensity in an image, which is widely divergent from the design philosophy of many existing IQA approaches.\n27. As a result, it is challenging to design an effective banding IQA method.\n28. To address these limitations, we first conduct a comprehensive subjective study of banding exacerbated images and create the largest banding IQA database to date with reliable mean opinion scores (MOS) and patch-level banding labels.\n29. We also propose a novel no-reference banding evaluator for banding detection and quality assessment by leveraging the frequency characteristics of banding artifacts.\n30. First, due to the dissimilar peculiarities of banding and smooth regions, the same distortion in different regions, e.g., textual and pictorial regions, may lead to different visual perception of human beings.\n31. Considering that banding manifests as a high-frequency artifact that exists in the lowfrequency smooth region, we propose a dual-branch CNN, which takes the high-frequency map and low-frequency map as inputs simultaneously, to hierarchically incorporate different visual features from the first layer and the last layer of our Resnet-50 backbone, thus making the model learn more effective banding feature representation and achieving more accurate banding region discrimination.\n32. Furthermore, inspired by the previous studies[15],[16], spatial frequency extracts information consistent with the human visual system (HVS), which not only reflects the overall active level in an image but also intuitively quantifies the contrast information.\n33. In other words, the value of spatial frequency is large in smooth areas, while becoming small in areas with harsh contrast changes, i.e., banding areas.\n34. Based on this mechanism, we adopt a spatial frequency masking strategy to refine the detected banding map and then pool the masked banding detection map to obtain the image-level banding quality score.\n35. • We propose a novel no-reference banding evaluator for banding detection and quality assessment based on frequency characteristics of banding artifacts, which utilizes a dual-branch CNN model to extract hierarchical banding-related feature representation from the high-frequency maps and low-frequency maps simultaneously.\n36. • Experimental results show that the proposed banding evaluator achieves the best performance in banding detection and significantly surpasses baselines in terms of SRCC and PLCC in the banding IQA task, which demonstrates the effectiveness of the proposed model.\n37. The remainder of this paper is organized as follows.\n38. Section II provides an overview of related works, including the stateof-the-art banding databases and detection methods.\n39. Section III introduces the construction of the BAND-2k database and the subjective assessment study.\n40. Section IV proposes an effective no-reference banding evaluator for banding detection and quality assessment.\n41. Section V gives the experimental results and analysis.\n42. Section VI concludes this paper.\n43. In this section, we first provide an overview of the state-ofthe-art banding-related databases (TableI) and then review the banding detection and evaluation methods.\n44. The first banding artifact-relevant VQA database was proposed by Wang et al.[39], which consists of 21 stimuli with different quantization grades generated by VP9[38]from 7 clips of 1280 × 720 30fps video.\n45. Authors in[41]investigated the effect of encoding parameters and dithering on the visibility of banding.\n46. Nine 4k-10bit source clips from the existing Netflix catalogue between 1 and 5 seconds were used to generate banding distorted videos.\n47. Each source content was downsampled to appropriate resolutions (1080p, 2k, or 4k) with certain bit-depth and further compressed by libaom (an AV1 codec library) at QPs {12, 20, 32}.\n48. More recently, Kapoor et al.[43]constructed one of the first databases for data-driven image banding assessment models.\n49. This research included about 1,440 images shot from over 600 pristine HD videos with a resolution of 1920 × 1080.\n50. Six levels of bit-depth quantization in luminance and chrominance channels are introduced to obtain different intensities of banding.\n51. Meanwhile, the banding images were semi-automatically segmented and labeled into banded and non-banded to form a patch-level banding dataset, which allows for training machine learning-based and deep learning-based banding classification methods.\n52. However, to the best of our knowledge, thus far there still lacking a benchmarking dataset in the banding detection and the corresponding banding IQA domain.\n53. Researchers either resort to image/video quality datasets that do not aim at banding distortion or build a small, attribute-restricted, in-house dataset by themselves.\n54. This motivates us to construct a large-scale subjective assessment database focus on the perceived bandingaffected image quality.\n55. Early research on banding detection mainly focuses on false contour identification, which aims to find the wrong boundary rather than a "true" region edge in the image.\n56. Authors in[31]-[33]utilized monotonicity or non-monotonicity features of local support regions including the gradient, contrast, variance, and entropy information to measure the loss of low-amplitude detail caused by banding.\n57. However, these works ignored the perceptual characteristics of the human visual system (HVS) and thus did not perform a good correlation with subjective tests.\n58. Another banding detection strategy is conducted at the pixel-level estimation and segmentation.\n59. Bhagavathy et al.[30]proposed to identify banding artifacts by calculating the likelihood of pixel difference.\n60. Baugh et al.[34]measured the severity of banding based on the number of a group of connected pixels with the same color.\n61. Wang et al.[39]first detected uniform segments to find possible banding areas and further incorporated edge features (e.g.length and contrast) to capture false boundaries.\n62. Nevertheless, these kinds of methods are typically sensitive to edge noise and are computationally expensive, causing limited application in realtime scenarios.\n63. Towards addressing these problems, Tu et al.[42]presented a completely no-reference banding detection method, which combines various properties of HVS with a number of preprocessing steps to refine banding edge detection.\n64. Instead of regarding banding detection as a false edge detection problem, Tandon et al.[41]heuristically utilized the effect of contrast sensitivity function (CSF) on banding visibility and its dependence on spatial frequency.\n65. Based on this, Krasula et al.[40]further compared the banding annoyance with more commonly studied compression artifacts and proposed a banding-aware video quality metric.\n66. In recent years, deep learning approaches have prevailed in various VQA tasks.\n67. As the pioneering work, Kapoor et al.[43]developed an automated CNN-based banding detector for the first time, which is a simple two-stage algorithm and gives rise to devising other learning-based techniques.\n68. In this work, we build a large-scale banding database and propose a data-driven banding indicator that can generate pixelwise banding visibility maps with corresponding subjectively  consistent quality scores by combining human visual mechanisms and deep learning techniques.\n69. Subjective banding image quality assessment facilitates the development of automatic objective banding image and video quality models.\n70. We created the largest banding database in existence, denoted as the BAND-2k database, which consists of 2,000 banding distorted images and over 214,000 patch-level banding class labels.\n71. Then, a subjective experiment was conducted to obtain the mean opinion scores (MOS) of the BAND-2k database.\n72. The workflow of the banding database construction is shown in Fig.1.\n73. To build a content-rich and balanced database, we manually collected source videos including computer graphics (CG), usergenerated content (UGC), and professionally-generated content (PGC) from two popular media websites Bilibili.comand Youtube.com.\n74. Then, 885 clips with multiple spatial resolutions (i.e., 4096×2160, 3840×2160, 1920×1080) and frame rates (i.e., 60, 50, 30, 25) are chosen as candidate.\n75. Note that videos on the mentioned websites are firstly annotated by the community with assigned a number of favorites, views, and downloads.\n76. These statistics correlate with the content and quality of a video, which guides our choices to some extent.\n77. All videos selected on the website are released under an appropriate creative commons (CC) license that allows further editing and redistribution.\n78. After content selection, we further unified the format of all video clips, especially the spatial resolution and the pixel format, which avoid the effect of other facts on visual quality.\n79. Concretely, we first converted the frame rate of the original clips to 25fps, which is to reduce the storage pressure while ensuring the graphics quality.\n80. Considering the commonly used aspect ratio of the user interface and displays is 16:9, we cropped the partially unqualified videos rather than shrinking images unevenly.\n81. Then, we downsampled the trimmed spatial resolution 3840×2160 to a lower resolution -1920×1080 for the following subjective study.\n82. Before conducting the following experiments, we manually removed the videos that are either too dark or bright, overly blurry or colorful, which helps to obtain more reliable subjective assessments.\n83. To avoid redundancy and to make sure the diversity of selected contents, we also conducted attribute analysis studies on the selected images.\n84. Four metrics that correlated with human perception, i.e., contrast, brightness, sharpness, and colorfulness, are adopted as content diversity metrics.\n85. All video attributes are calculated on every 10 frames to reduce computational complexity, which are then averaged over frames of each video sequence.\n86. Fig.2shows the distribution of attributes extracted from the selected videos.\n87. • Contrast: The contrast metric is simply defined as the standard deviation of pixel gray-scale intensities[25].\n88. • Colorfulness: The colorfulness metric is measured by the R, G, and B components[25].\n89. We first compute two matrices of differences between channels rg = R -G and yb = 1 2 (R + G) -B.\n90. Then, the colorfulness metric can be calculated as µ 2 rg + µ 2 yb + σ 2 rg + σ 2 yb , where µ and σ are the mean and standard deviation of their corresponding terms, respectively.\n91. • Sharpness: The cumulative probability of blur detection (CPBD) metric[24]is used to measure the image sharpness, which estimates the probability of detecting blur at each edge.\n92. • Brightness: The brightness of an image is obtained directly from the pixel gray intensities in R, G, and B channels.\n93. Finally, the number of source videos are reduced to 873.\n94. Fig.3displays thumbnails for 30 selected representative video clips.\n95. To simulate authentic banding artifacts that exist in real viewing scenarios, we introduced four encoding/transcoding strategies including H.264/AVC[28], H.265/HEVC[27], VP9[38], and bit-depth manipulation[43]with fifteen quantization schemes in total.For H.264/AVC and H.265/HEVC, considering the range of their quantization parameter (QP in ffmpeg), we selected three typical QP values, namely, QP = {18, 33, 43}.\n96. This is because that coded video clips with a QP value smaller than 18 normally provide perceptual lossless quality, while coded video clips with a QP value larger than 43 will not be able to offer adequate quality, which may affect the subjective test of banding artifacts.\n97. Similarly, for the VP9 encoder, the QP values under our close inspection are chosen as {28, 45, 60}.\n98. Moreover, we applied the same quantization strategies as in[43], which introduces banding distortion by scaling bit-depth in luminance and chrominance channels.\n99. Here, the color coding scheme YCbCr4:2:0 is applied for maximum compatibility.\n100. To sum up, we generated fifteen levels of banding with different intensities and shapes to enhance the diversity of the database.\n101. The banding database is then built by extracting frames from the distorted video clips, resulting in 2,000 images with a resolution of 1920×1080.\n102. Fig.4shows the visualization results of banding exacerbated images.\n103. In the process of image patch labeling, we initially intend to extract image patches of size 235×235 from banding images directly and perform annotation operations.\n104. However, considering the theoretical number of patches, it will cost a lot of manpower and time to label patch by patch.\n105. Therefore, we first segmented the banding images roughly and label them into banded and non-banded regions.\n106. Then, labelled image patches are generated from these segmented and labelled images by a sliding window.\n107. Specifically, we followed the same demarcation of the banded and non-banded images in[43]that a patch is labelled as banded if it has more than 30% overlap with banded regions in the image.\n108. Eventually, a banding dataset containing 2,000 distorted images with 1920×1080 resolution and 214,324 labelled image patches of size 235×235 is built.\n109. To the best of our knowledge, it is the largest banding dataset in existence, which enables training various machine/deep learning based banding detection models and facilitates the development of image/video debanding techniques.\n110. TableIIreports the composition of labelled image patch dataset.\n111. It can be observed that the number of banded patches is a bit smaller than non-banded patches due to the fact that banding usually appears in smooth background areas.\n112. As shown in Fig.1, the subjective quality study contains four steps.\n113. In addition to preparing the experimental environment, subjects should pass the qualification test first to participate in the study.\n114. After the subjective rating, all resulting scores need to be analyzed and examined before generating the final mean opinion score (MOS).\n115. 1) Experimental Environment Setting: In this study, a total of 25 inexperienced subjects are gathered in a laboratory environment, where relevant experimental configuration must satisfy the following requirements: • Considering the viewing effect, desktops and laptops are allowed as displays.\n116. • The resolution of displays must be larger than or equal to 1920×1080 to show the images without spatial downsampling.\n117. • The viewing distance and optimal horizontal viewing angle are set as 1.9 times the height of the display and respectively.\n118. 500 recommendation[26].\n119. As a result, we used a 27-inch AOC Q27U2D monitor with a resolution of 2560×1440 for assessment with 25 subjects.\n120. Due to the large number of images to be assessed, we divided the dataset into ten sessions to avoid visual fatigue.\n121. Each session of tests took nearly 2 hours with a 30-minute break for each participant.\n122. 2) Qualification Test: Before starting the main experiment, subjects are required to pass a quiz to get the qualification of conducting follow-up experiments.\n123. Firstly, we manually selected 10 labeled banding exacerbated images beyond the database as training images to familiarize subjects with the operation interface and the goal of this subjective test.\n124. The quiz consists of two parts including banding classification and image-level quality rating.\n125. In banding classification, subjects were told to divide the test image into banded or non-banded.\n126. In image-level quality rating, subjects were instructed to focus on the coverage and intensity of banding areas, as well as the overall quality of images to get the final quality score.\n127. The rating scale is continuous from 0 to 100 while a higher value indicates more severe banding (i.e., more visible or occupying larger portion of the image).\n128. To make the quiz objective and fair, we take the banding images labeled by domain experts as the ground truth, while two existing banding metrics BBAND[42]and DBI[43]are used to determine the normal range of the rating scores.\n129. That is the quality scores rated by subjects should not exceed 20% of the above banding metrics.\n130. As a result, only 23 subjects with an accuracy above 80% in banding classification and subjective scores in the normal range were allowed to pass the quiz.\n131. Note that the steps for taking a quiz are the same as the main experiment, which aims to guarantee the consistency of results.\n132. 3) Formal Study: We adopted the single-stimulus (SS) method in this test.\n133. Ten golden images that have the acknowledged high quality or poor quality (assessed by BBAND[42]and DBI[43]) were added to each session for controlling the scoring deviations.\n134. Besides, 3 repeated images are randomly inserted into each session to ensure consistency of scores before and after subjects scoring.\n135. At last, 23 qualified subjects were asked to provide their opinions on the shuffled image groups.\n136. The resulting scores were collected and packed for further analysis.\n137. 4) Result Analysis: In total, 46,000 scores were collected by 23 qualified subjects in the main study phase.\n138. However, considering the qualification quiz cannot completely disallow those unreliable workers to muddle through to the main study while reliable subjects may also occasionally score odd values, which may be caused by the inter-individual differences in perceiving the quality of the unique characteristics of different contents.\n139. Thus, we further investigated the confidence of rating scores and removed outliers following the Grubbs\' test[23],[45].\n140. Concretely, let s = (s 1 , s 2 , . . ., s N ) denote a set of raw scores collected for one distorted image.\n141. The test statistics is defined as the maximum absolute deviation of a sample standard deviation (SD) unit from the sample mean, which is mathematically expressed as where s and σ s denote the mean and standard deviation of the sample, respectively.\n142. Then, given a significant level α, a sample is detected as an outlier if where t 2 α/(2N ),N -2 represents the upper critical value of the tdistribution with N -2 degrees of freedom and a significance level of α/(2N ).\n143. Empirically, we set the significance level α at 0.05.\n144. Then, a sample is identified as an outlier if its distance to the sample mean is larger than 2.5 times SD and is removed.\n145. Following the aforementioned steps, the total number of scores Fig.6.\n146. The overall architecture of the proposed method.\n147. Given a banding distorted image, it is first divided into patches.\n148. Then, the patch-level high-frequency map (HFM) and low-frequency map (LFM) are generated by Sobel operation and piece-wise smooth algorithm[19], respectively.\n149. After that, a dual-branch CNN (CNN H and CNN L ) is deployed to extract hierarchical features with different visual information and thus classify the patches into banded or non-banded.\n150. Lastly, a spatial frequency masking strategy is introduced to refine the banding map and calculate the image-level banding quality score.\n151. Note that the dual-branch networks do not share parameters.\n152. was reduced to 44,371, and MOS was created by averaging the scores for each image.\n153. Fig.5presents the histogram of MOSs over the entire database, showing a broad MOS distribution of banding images.\n154. In this section, we describe the architecture of the proposed banding evaluator in detail, as shown in Fig.6.\n155. As stated before, banding usually appear as high-frequency information in the smooth background, while humans perceive high-frequency texture regions and low-frequency plateau regions through different neural channels concurrently, and transfer the upper visual features into the cerebral cortex for final processing[17],[44].\n156. Inspired by this, we employ high-frequency maps (HFM) and low-frequency maps (LFM) as the deep learning network inputs, which represent the texture and structural information of the image respectively, to mimic the recognition mechanism of the human brain for better banding identification.\n157. High-frequency Maps.\n158. Since gradient has been widely used to represent edge information and has been confirmed beneficial to acquire high-frequency components with low computational cost[20]-[22], we apply the isotropic Sobel operator to each patch for enhancing the details of banding artifacts.\n159. Given an input patch I, the high-frequency map is calculated by where S x and S y are the horizontal and vertical isotropic Sobel operators, respectively." * " denotes the convolution operation.\n160. Low-frequency Maps.\n161. To maintain the principal content of the image and filter out the influence of high-frequency information, we use the piece-wise smooth algorithm[19]to generate the low-frequency map by minimizing a function for image approximation recovery: where L represents the low-frequency map, Ω and E denotes the image domain and edge set, respectively.\n162. P indicates the pixel and E dσ represents the total edge length.\n163. The coefficients α and β are positive regularization constants.\n164. An example of frequency maps is shown in Fig.7.\n165. To obtain the overall banding score, we first divide the banding image into 235×235 patches and apply CNN-based classification to each patch, resulting in a banding classification label for each patch, i.e., banded or non-banded.\n166. As shown in Fig.6, the proposed network consists of two parallel branches, namely CNN H and CNN L , which take the patch-level high-frequency map and low-frequency map as input, respectively.\n167. For each branch, we propose to use Resnet-50[18]as the backbone.\n168. Specifically, we incorporate the feature maps extracted from the first convolutional layer and the last layer of Resnet-50 as hierarchical visual features, which represent different visual information[46],[47]and can be used as predictive information to enhance the discrimination ability of the network for banding and non-banded regions.\n169. Afterward, the features extracted from two branches are concatenated first and reshaped into 128dimensional vectors through two fully-connected layers, which is further followed with the sigmoid activation function to output the final predicted label, namely banded or non-banded.\n170. Of note is that sharing parameters is extremely unfavorable for extracting low-and high-frequency features simultaneously, we thereby deploy two branches that work independently and do not share parameters.\n171. The loss function adopted here is binary cross entropy.\n172. With the trained CNN H and CNN L , each input patch is predicted to obtain a banding or non-banded label.\n173. To better guide the pre-processing and post-processing debanding algorithms, it is necessary to generate a quality score for the entire banding image.\n174. Since the visibility of edge is also affected by content, we further consider the effect of spatially varying content information on the local quality of human perception.\n175. As a consequence, we introduce the spatial frequency masking strategy to determine the weighting factor for the detected banding regions in each patch adaptively and thus obtain the image-level banding severity score while refining the visibility of banding artifacts.\n176. 1) Spatial Frequency Masking: The spatial frequency is defined as the activity level of an image, which establishes a filterbank based on the visual stimulus and is in accordance with HVS[16].\n177. In this paper, we propose to apply spatial frequency as an effective contrast criterion to banding measurement.\n178. Specifically, given an image of size I W × I H , divided into N × N patches, where I W and I H denote the number of columns and rows respectively.\n179. The column (CF k ) and row (RF k ) frequencies of the image patches are given by where I(x, y) is the pixel value of the image patch.\n180. Then, the resulting spatial frequency of an N × N patch is computed as where k is the number of patches (1 ≤ k ≤ I W I H N 2 ).\n181. Since most banding regions are likely to have large contrast including edges and textures, which should be assigned greater weights than the smooth and blurred areas.\n182. Accordingly, we design a banding visibility transfer function to express spatial frequency masking as a function of the local textural feature.\n183. The final spatial frequency masking weight is calculated at each patch as where γ is the scaling constant factor chosen to tune the shape of the transfer function.\n184. We used γ = 1.5 in our implementation.\n185. 2) Building a Banding Metric: The visibility of banding artifacts depends on the combination of multiple visual mechanisms.\n186. In this paper, we propose a simple but effective product model for attribute integration at each predicted banding patch to obtain the entire banding map (BM): where P k denotes the predicted label of k-th patch and w k is the weight parameter that scales the visibility of measured contours, i.e., gradient magnitude of the high-frequency map, |HFM k (i, j) | at region (i, j).\n187. Furthermore, inspired by previous psychovisual findings that the QoE of observers is dominated by those regions having poor quality[14],[42], we thereby leverage the worst p% percentile visual pooling to calculate an average banding score from the generated BM, where p is set to 80 in this experiment.\n188. As a result, the perceptual score of the overall banding image is defined as where M is the total number of patches in image I.\n189. T p% denotes the index set of the top p% non-zero pixel-wise value contained in k-th patch of the BM.\n190. In this section, we first present the experimental protocol in detail and then evaluate the performance of the proposed method on two tasks, namely patch-level banding classification and banding image quality assessment.\n191. After that, the ablation study and cross-database validation are conducted to prove the robustness and effectiveness of the proposed method.\n192. Finally, we test the computational efficiency of our method.\n193. A. Experimental Protocol 1) Databases and Settings: We choose two databases to train and test the effectiveness of the proposed banding IQA method, which are the database released in[43]and our proposed BAND-2k database.\n194. The detail information of these two datasets can be found in TableI.\n195. The proposed model is implemented by PyTorch[13].\n196. Before training, we randomly split the training, validation, and testing set into 8:1:1 (as shown in TableII).\n197. We use the Adam optimizer with the initial learning rate set as 1e-4 and set the batch size as 32.\n198. The training process is stopped after 25 epochs.\n199. The resolution of each cropped patch is fixed to 235×235.\n200. All experiments on both the[43]database and the BAND-2k database are conducted repeatedly 10 times to obtain the mean performance.\n201. 2) Baseline Algorithms: We include a number of representative IQA algorithms in our evaluation as references to be compared against.\n202. These baseline methods include: • General FR IQA methods: We choose PSNR, SSIM[12], MS-SSIM[11], LPIPS[10]as baselines.\n203. These are the most commonly used FR IQA metrics in practical applications such as video coding, image enhancement, etc. • General NR IQA methods: BRISQUE[37], NIQE[36], NIMA[8], DBCNN[35], HyberIQA[9], and StairIQA[1].\n204. • Banding IQA methods: Considering that there exists few research on banding detection and quality assessment, we barely select the BBAND[42], CAMBI[41], VMAF BA[40]and DBI[43]metrics as comparisons.\n205. 3) Evaluation Criteria: To evaluate the IQA methods comprehensively, a total of seven evaluation indexes in two categories are adopted.\n206. For patch-level banding classification, we follow the common procedures as in[43]and utilize the area under the receiver operating characteristics (AUROC), the area under the precision-recall curve (AUPRC), and accuracy as the classification performance metrics.\n207. For banding image quality assessment, four mainstream metrics are selected as the evaluation criteria: Spearman rank-order correlation coefficient (SRCC) and Kendall rank-order correlation coefficient (KRCC) measure the prediction monotonicity, while Pearson linear correlation coefficient (PLCC) and root mean square error (RMSE) are calculated to assess prediction consistency.\n208. Considering the potential nonlinear mapping characteristics between the objective scores and the subjective scores, we perform score alignment by mapping the predicted value using the five-parameter logistic function before calculating PLCC and RMSE values[7].\n209. Since our goal is to develop an effective banding IQA approach, we regard the identification of banding patches as an important preceding process to achieve accurate banding quality prediction.\n210. However, there exist few methods that are designed for banding classification and nearly all IQA methods produce scalar values only while failing in classifying banding regions directly.\n211. Therefore, we adopt a thresholding step to convert the single quality value into binary classification results as[43]does.\n212. Concretely, a half-interval search algorithm[48]is employed to find the optimal threshold value that can generate the best classification result.\n213. Based on the above premise, TableIIIreports the experimental results on both the database from[43]and the BAND-2k database.\n214. We highlight the best results in boldface.\n215. As compared to other state-of-the-art IQA methods, our proposed method yields the best overall performance in terms of AUROC, AUPRC, and accuracy.\n216. It is shown that most general FR IQA and NR IQA models perform poorly on the patch-level banding classification task while performing fairly well on other IQA tasks[1],[9],[35], indicating that the current approaches are not sensitive to banding distortion.\n217. Benefiting from the powerful feature extraction ability of CNNs, our proposed method and the customized NR IQA models for banding artifacts detection (DBI[43]) reach a significant performance in the discrimination of false contours.\n218. However, the performance of banding IQA method BBAND, CAMBI, and VMAF BA is surprisingly poor compared with other methods, which shows their vulnerability in identifying local banding artifacts from texture regions and are not suitable for patch-level banding identification.\n219. In addition, we investigate the computational complexity in terms of execution time per image patch.\n220. It can be observed that except for those traditional FR IQA models, our method achieves comparable speed in patch-level banding classification, which determines the prediction efficiency of the subsequent image-level quality assessment, making it a favorable choice in time-constrained scenarios.\n221. Considering that there exist no image banding databases with subjective scores attached in the public domain, we merely compare the performance of the proposed method with the baseline approaches on the proposed BAND-2k database.\n222. The results are shown in TableIV, from which we can observe that our proposed method largely surpasses all baselines in terms of SRCC, KRCC, and PLCC except for the RMSE.\n223. Compared to the secondbest model, our method achieves 18.07%SRCC improvements, 24.91% KRCC improvements, and 18.01%PLCC improvements on the BAND-2k database.\n224. We also present scatter plots of predictions versus MOS for better visualization in Fig.8.\n225. Overall,  (a) PSNR, (b) SSIM[12], (c) MS-SSIM[11], (d) LPIPS[10], (e) VMAF BA[40], (f) BRISQUE[37], (g) NIQE[36], (h) NIMA[8], (i) DBCNN[35], (j) HyperIQA[9], (k) StairIQA[1], (l) BBAND[42], (m) CAMBI[41], (n) DBI[43], and (o) Ours.\n226. Fig.9. Visual comparisons of the banding map results.\n227. From top to bottom are banding images and their corresponding banding maps generated by BBAND[42], DBI[43], and our proposed method, respectively.\n228. The first five columns of images from left to right are from BAND-2k, while the rest images are from[43].\n229. the performance of traditional FR IQA models such as PSNR, SSIM, and MS-SSIM is remarkably inferior in banding images assessment and is uncorrelated with the MOS, which is consistent with the numerical results presented in TableIV.\n230. The reason is that PSNR and SSIM-based methods do perform not well on tiny, regional, and content-independent banding distortion while lacking the consideration for the mechanism of HVS.\n231. It is also worth mentioning that the deep CNN architecturebased methods (DBCNN[35], HyperIQA[9], and StairIQA[1]), despite performing well on LIVEC[6], KonIQ-10k[29], and other universal image quality evaluation databases[4],[5], underperformed our proposed model by a notable margin on the BAND-2k database.\n232. (2) the perception of banding artifacts is explicitly intensity-aware.\n233. These are the issues that the CNN-based approaches above do not take into account.\n234. To some extent, banding distortion is more like a kind of local distortion than common global distortion such as Gaussian noise, blur, and dither.\n235. This suggests that it is potentially valuable to integrate some local texture, edge, contrast, or other visibility-related features into quality prediction models when assessing banding images.\n236. Fortunately, with the help of the proposed scoring strategy, our method gains the ability to convert the area range of identified banding regions to indicative annoying scores.\n237. Surprisingly, the banding IQA methods BBAND[42], CAMBI[41], and VMAF BA[40], however, did not perform very well on the BAND-2k dataset.\n238. We infer that this is due to differences in the test environment.\n239. First, the source videos selected in CAMBI and VMAF BA are from the internal Netflix 4K catalogue while the source contents in BAND-2k are collected from the public streaming websites with different resolutions, leading to an uneven quality level.\n240. Second, the methods of artificially introducing banding distortion are different.\n241. 264 compression techniques are used to generate the banding artifacts, limiting the adaptive ability of the algorithm in other scenarios.\n242. For further investigation, we compare our method with other two methods, i.e., BBAND[42], DBI[43], and visually study the generated banding maps to verify their effectiveness in banding detection.\n243. The visualization results are shown in Fig.9, where seven representative banding images are selected for reference.\n244. As shown, the banding maps generated from BBAND are mostly disordered and have a lot of discontinuity while the banding maps generated by DBI are too vague to locate the exact location of banding artifacts, making it difficult to develop pixel-level debanding techniques.\n245. Overall, we can compendiously conclude that: (1) in comparison with BBAND and DBI, banding maps computed by our proposed method could provide an accurate, clear indication for banding artifacts; (2) in comparison with BBAND and DBI, the quality prediction of banding images guided by our method could achieve a high consistency with HVS.\n246. Moreover, to make a statistically meaningful comparison among different IQA methods, we further conduct the widely used F-test[2],[3]to assess the statistical significance of the proposed method.\n247. Based on the assumption that the model\'s prediction residuals follow the Gaussian distribution, the lefttailed F-test with a confidence level of 95% is performed on the residuals of every two IQA models.\n248. The results of significance tests on the BAND-2k database are shown in Fig.10.\n249. A value of \'1\' (colored in green) indicates that the model in the row is significantly better than the model in the column, while a value of \'0\' (colored in red) indicates that the model in the row is not significantly better than the model in the column.\n250. It is shown that our proposed method performs significantly better than other models, which is consistent with the observations from the above comparison experiments.\n251. In this section, we explore the effectiveness of our model\'s design philosophy.\n252. To verify the importance of the dual-branch (DB) architecture, the baseline variants use the same backbone as the proposed method, except that only a single branch (SB) is reserved.\n253. Then, we use the original banding images as the input while removing the frequency map generation module (SB-I).\n254. Besides, the high-frequency maps and low-frequency maps generation modules are retained respectively (SB-HFM and SB-LFM).\n255. It can be observed from Table V that SB-LFM achieved the worst results, since the low-frequency map filters out the highfrequency banding information to a certain extent, which reduces the ability of the model to identify false contours.\n256. SB-I performs significantly better than SB-HFM and SB-LFM, resulting from that it contains richer image information, but it is still inferior to our method, which demonstrates the effectiveness of the dualbranch scheme.\n257. To investigate the effect of the banding feature extraction, we further design two variants with different input combinations of frequency maps.\n258. First, the high-frequency map was taken as the input of both the CN N H and CN N L layers (DB-HFM).\n259. Then, we replace the inputs with the low-frequency maps (DB-LFM).\n260. As shown in TableV, the performance of DB-HFM and DB-LFM is far apart from our approach, which matches our hypothesis that the high-frequency texture information contained in HFM and the low-frequency background information contained in LFM are crucial to enhance the capacity of discernment for banding artifacts.\n261. Therefore, we may conclude that our model is the most suitable model among those compared variants in terms of both banding classification and IQA applications.\n262. Due to the effects of different compression techniques, shooting equipment, scenes, etc., the image content and banding distortions may vary significantly in practical applications.\n263. For the database[43], it only includes limited types of image sources and means of triggering banding distortion.\n264. As a result, we conduct a cross-database validation to verify the generalizability of the proposed model, wherein the database presented by[43]and BAND-2k are included.\n265. That is, we trained the model on one full database and report the test performance on the other.\n266. We mainly compare the proposed method with four learningbased models, i.e., DBCNN[35], HyperIQA[9], StairIQA[1], and DBI[43].\n267. Since MOS information is not provided in the database[43], which is an essential part of methods training, we condensed part of the experiments.\n268. TableVIand VII report the experimental results in terms of patch-level banding classification and image quality assessment.\n269. We can observe that our proposed method generalization between database[43]and BAND-2k was surprisingly good.\n270. Besides, it is worth noting that the performance of these methods trained on the BAND-2k has improved a little compared to the previous versions that were trained on the database[43], which further demonstrates the superiority of the proposed database BAND-2k.\n271. The efficiency of an image quality prediction model is of great importance in practical industrial deployments.\n272. Therefore, we measured the average running time of the compared IQA models, as shown in TableVIII.\n273. The experiments were performed in MATLAB R2021a and Python 3.7 under Windows 10 64bit system on a Lenovo laptop with Intel Core i5-9300HF CPU@2.4GHz,16GB RAM, and NVIDIA GTX 1660Ti 6G GPU.\n274. It can be observed that the proposed method achieves a reasonable running time among the FR, NR, and other bandingspecified IQA algorithms.\n275. Generally, the execution time of classical IQA algorithms is significantly less than learning-based methods.\n276. Simpler NSS-based models such as BRISQUE and NIQE still show competitive efficiency relative to CNN models while exhibiting inferior performance in banding image quality assessment.\n277. For CAMBI and VMAF BA , we use the officially launched software package, which is based on the stand-alone C library libvmaf and therefore surpasses other methods in speed.\n278. Moreover, unlike the general quality evaluation using regression to predict scores, the patch-wise prediction strategy that we adopted may increase the complexity.\n279. Note that although we deployed a more complex network structure, a nearly 10 times speedup has been seen when comparing DBI with our method since the sliding window mechanism[43]is removed.\n280. In this paper, we conduct a comprehensive exploration of banding images from both subjective and objective perspectives.\n281. Specifically, we construct the largest ecologically valid banding IQA database to date named BAND-2k database, which consists of 2,000 banding images generated by fifteen compression and quantization schemes, achieving several times larger in number and diversity than the existing banding dataset.\n282. The construction process of the database, including distortion content preparation, subjective test procedure, and the removal of outlying data, is described in detail in this paper.\n283. Relying on this database, we proposed a novel banding evaluator using the frequency characteristic of banding artifacts, which models the banding as high-frequency artifacts that contained in the low-frequency smoothing region.\n284. A dual-branch CNN is devised to extract hierarchical features to classify the banding regions, upon which we introduce the spatial frequency masking to refine and compute an overall banding score.\n285. Experimental results show that our proposed method outperforms the baseline algorithms significantly in patch-level banding classification and banding IQA tasks.\n286. We believe that our study will benefit further development, calibration, and benchmarking of banding IQA models.\n\n请严格按照JSON格式返回：{"labels": [0,1,0,...]}，不要包含任何其他文字。'}], 'model': 'qwen-plus', 'response_format': {'type': 'json_object'}}}
2025-10-14 21:01:30,609 - DEBUG - Sending HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
2025-10-14 21:01:30,609 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-10-14 21:01:30,609 - DEBUG - send_request_headers.complete
2025-10-14 21:01:30,609 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-10-14 21:01:30,610 - DEBUG - send_request_body.complete
2025-10-14 21:01:30,610 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-10-14 21:06:18,995 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'vary', b'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding'), (b'x-request-id', b'9e243568-f854-4b34-aa82-0aa14df629e7'), (b'x-dashscope-call-gateway', b'true'), (b'content-type', b'application/json'), (b'req-cost-time', b'288354'), (b'req-arrive-time', b'1760446880701'), (b'resp-start-time', b'1760447169056'), (b'x-envoy-upstream-service-time', b'288268'), (b'content-encoding', b'gzip'), (b'date', b'Tue, 14 Oct 2025 13:06:08 GMT'), (b'server', b'istio-envoy'), (b'transfer-encoding', b'chunked')])
2025-10-14 21:06:18,996 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-14 21:06:18,996 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-10-14 21:06:18,997 - DEBUG - receive_response_body.complete
2025-10-14 21:06:18,997 - DEBUG - response_closed.started
2025-10-14 21:06:18,997 - DEBUG - response_closed.complete
2025-10-14 21:06:18,997 - DEBUG - HTTP Response: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "200 OK" Headers({'vary': 'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding', 'x-request-id': '9e243568-f854-4b34-aa82-0aa14df629e7', 'x-dashscope-call-gateway': 'true', 'content-type': 'application/json', 'req-cost-time': '288354', 'req-arrive-time': '1760446880701', 'resp-start-time': '1760447169056', 'x-envoy-upstream-service-time': '288268', 'content-encoding': 'gzip', 'date': 'Tue, 14 Oct 2025 13:06:08 GMT', 'server': 'istio-envoy', 'transfer-encoding': 'chunked'})
2025-10-14 21:06:18,997 - DEBUG - request_id: 9e243568-f854-4b34-aa82-0aa14df629e7
2025-10-14 21:06:18,998 - DEBUG - API request completed in 288.39 seconds
2025-10-14 21:06:18,998 - DEBUG - Raw model response: {"labels": [1,1,1,1,1,0,0,0,0,0,1,0,0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1
2025-10-14 21:06:19,000 - ERROR - JSON parsing error: Expecting ',' delimiter: line 1 column 12386 (char 12385)
2025-10-14 21:06:19,000 - ERROR - Problematic content: {"labels": [1,1,1,1,1,0,0,0,0,0,1,0,0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1
2025-10-14 21:06:19,000 - INFO - Attempt 5 of 5
2025-10-14 21:06:19,002 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-b046e90d-0311-4db6-ba85-48be6c5065d3', 'json_data': {'messages': [{'role': 'system', 'content': '你是一个专门用于识别学术论文中数据集描述的助手。你需要判断每个输入的句子是否描述了研究中使用的数据集。请确保：\n    1. 只输出JSON格式的结果\n    2. 结果格式必须为 {"labels": [0,1,0,...]}\n    3. 不要输出任何其他文字或解释'}, {'role': 'user', 'content': '你是一位专业的学术论文数据集描述识别专家。请仔细分析下面这篇论文中的每个句子，判断它们是否描述了本研究特定构建的数据集信息。\n\n    论文标题: BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment\n\n    判断标准:\n    判断为1(数据集描述)的标准：\n    句子必须直接描述本研究特定构建的数据集,包括:\n    1. 该数据集的构建过程和方法\n    2. 该数据集的具体构成和规模\n    3. 该数据的来源和收集方式\n    4. 该数据的预处理步骤\n    5. 该数据集的可获取方式(如发布地址)\n    6. 或者其他直接或者间接描述了该数据集的句子\n\n    所有其他类型的句子均标记为0。\n    \n    请对每个句子进行分析，返回一个JSON格式的标签数组。格式要求：\n    1. 必须是有效的JSON格式\n    2. 只包含labels字段，值为0和1组成的数组\n    3. 数组长度必须与句子数量相同\n    4. 示例格式：{"labels": [0,1,0,1,...]}\n\n    需要判断的句子：\n\n    1. Abstract: Banding, also known as staircase-like contours, frequently occurs in flat areas of images/videos processed by the compression or quantization algorithms.\n2. As undesirable artifacts, banding destroys the original image structure, thus inevitably degrading users\' quality of experience (QoE).\n3. In this paper, we systematically investigate the banding image quality assessment (IQA) problem, aiming to detect the image banding artifacts and evaluate their perceptual visual quality.\n4. Considering that the existing image banding databases only contain limited content sources and banding generation methods, and lack perceptual quality labels (i.e.mean opinion scores), we first build the largest banding IQA database so far, named Banding Artifact Noticeable Database (BAND-2k), which consists of 2,000 banding images generated by 15 compression and quantization schemes.\n5. A total of 23 workers participated in the subjective IQA experiment, yielding over 214,000 patch-level banding class labels and 44,371 reliable image-level quality rating scores.\n6. Subsequently, we develop an effective no-reference (NR) banding evaluator for banding detection and quality assessment by leveraging frequency characteristics of banding artifacts.\n7. To be more specific, a dual convolutional neural network (CNN) is employed to concurrently learn the feature representation from the high-frequency and low-frequency maps, thereby enhancing the ability to discern banding artifacts.\n8. The quality score of a banding image is generated by pooling the banding detection maps masked by the spatial frequency filters.\n9. The experimental results demonstrate that our banding evaluator achieves a remarkably high accuracy in banding detection and also exhibits high SRCC and PLCC results with the perceptual quality labels, even without directly learning a regression model for banding quality evaluation.\n10. These findings unveil the strong correlations between the intensity of banding artifacts and the perceptual visual quality, thus validating the necessity of banding quality assessment.\n11. The BAND-2k database and the proposed banding evaluator will be available at https://github.com/zijianchen98/BAND-2k.\n12. R ECENT years have witnessed a rapid emergence of media streaming services and social platforms.\n13. YouTube, Netflix, and TikTok account for more than half of the world\'s video traffic.\n14. Improving the quality of images under limited encoding, Zijian Chen, Wei Sun, Jun Jia, Zicheng Zhang, Xiongkuo Min, and Guangtao Zhai are with the Institute of Image Communication and Information Processing, Shanghai Jiao Tong University, Shanghai 200240, China (e-mail: {zijian.chen,sunguwei, jiajun0302, zzc1998, minxiongkuo, zhaiguangtao}@sjtu.edu.cn).\n15. Fangfang Lu is with the College of Computer Science and Technology, Shanghai University of Electric Power, Shanghai 200290, China (email:lufangfang@shiep.edu.cn).\n16. Jing Liu is with the School of Electrical and Information Engineering, Tianjin University, Tianjin, China.(e-mail:jliu tju@tju.edu.cn).\n17. transmission bandwidth, and storage condition is a necessary prerequisite for meeting the quality of experience (QoE) of users.\n18. In the stages between image acquisition and display, an image may suffer from various types of degradation, while banding artifacts are a kind of false contour distortion that is quite perceptible to the human eye.\n19. Since the visual quality of image contents greatly affects the QoE of end-users, it is highly desirable to design an effective banding image quality assessment (IQA) method, which aims to automatically detect the traces of such false contours and predict the objective quality of banding images that can be used to develop pre-processing or postprocessing debanding algorithms and optimize the performance of streaming media application.\n20. Normally, banding artifacts take on the appearance of annual rings, radiation circles, halos, or geographical contour lines and especially exist in the background regions (e.g., sky, water, and wall surface), where the color transition is not smooth enough.\n21. Nearly all existing image or video encoders, including H.264/AVC[28], VP9[38], and H.265/HEVC[27]can introduce such artifacts more or less.\n22. Current banding IQA research can be divided into two categories: subjective quality assessment and objective quality assessment.\n23. The existing subjective banding IQA research[39],[41]-[43]mainly investigates the limited banding scenarios with internal-used and undisclosed databases while lacking the quality label and may be insufficiently generalizable to large-scale commercial applications.\n24. Meanwhile, general IQA methods aiming at common distortions are inapplicable for banding exacerbated images due to the essential differences between them.\n25. First, banding artifacts usually hold tiny, staircaselike, and regional structures, which can be regarded as a kind of high-frequency artifact in smooth areas, while general distortions occur obviously in the whole image and are globally uniform.\n26. Second, the perceptual severity of banding is quantified based on its fraction of coverage and intensity in an image, which is widely divergent from the design philosophy of many existing IQA approaches.\n27. As a result, it is challenging to design an effective banding IQA method.\n28. To address these limitations, we first conduct a comprehensive subjective study of banding exacerbated images and create the largest banding IQA database to date with reliable mean opinion scores (MOS) and patch-level banding labels.\n29. We also propose a novel no-reference banding evaluator for banding detection and quality assessment by leveraging the frequency characteristics of banding artifacts.\n30. First, due to the dissimilar peculiarities of banding and smooth regions, the same distortion in different regions, e.g., textual and pictorial regions, may lead to different visual perception of human beings.\n31. Considering that banding manifests as a high-frequency artifact that exists in the lowfrequency smooth region, we propose a dual-branch CNN, which takes the high-frequency map and low-frequency map as inputs simultaneously, to hierarchically incorporate different visual features from the first layer and the last layer of our Resnet-50 backbone, thus making the model learn more effective banding feature representation and achieving more accurate banding region discrimination.\n32. Furthermore, inspired by the previous studies[15],[16], spatial frequency extracts information consistent with the human visual system (HVS), which not only reflects the overall active level in an image but also intuitively quantifies the contrast information.\n33. In other words, the value of spatial frequency is large in smooth areas, while becoming small in areas with harsh contrast changes, i.e., banding areas.\n34. Based on this mechanism, we adopt a spatial frequency masking strategy to refine the detected banding map and then pool the masked banding detection map to obtain the image-level banding quality score.\n35. • We propose a novel no-reference banding evaluator for banding detection and quality assessment based on frequency characteristics of banding artifacts, which utilizes a dual-branch CNN model to extract hierarchical banding-related feature representation from the high-frequency maps and low-frequency maps simultaneously.\n36. • Experimental results show that the proposed banding evaluator achieves the best performance in banding detection and significantly surpasses baselines in terms of SRCC and PLCC in the banding IQA task, which demonstrates the effectiveness of the proposed model.\n37. The remainder of this paper is organized as follows.\n38. Section II provides an overview of related works, including the stateof-the-art banding databases and detection methods.\n39. Section III introduces the construction of the BAND-2k database and the subjective assessment study.\n40. Section IV proposes an effective no-reference banding evaluator for banding detection and quality assessment.\n41. Section V gives the experimental results and analysis.\n42. Section VI concludes this paper.\n43. In this section, we first provide an overview of the state-ofthe-art banding-related databases (TableI) and then review the banding detection and evaluation methods.\n44. The first banding artifact-relevant VQA database was proposed by Wang et al.[39], which consists of 21 stimuli with different quantization grades generated by VP9[38]from 7 clips of 1280 × 720 30fps video.\n45. Authors in[41]investigated the effect of encoding parameters and dithering on the visibility of banding.\n46. Nine 4k-10bit source clips from the existing Netflix catalogue between 1 and 5 seconds were used to generate banding distorted videos.\n47. Each source content was downsampled to appropriate resolutions (1080p, 2k, or 4k) with certain bit-depth and further compressed by libaom (an AV1 codec library) at QPs {12, 20, 32}.\n48. More recently, Kapoor et al.[43]constructed one of the first databases for data-driven image banding assessment models.\n49. This research included about 1,440 images shot from over 600 pristine HD videos with a resolution of 1920 × 1080.\n50. Six levels of bit-depth quantization in luminance and chrominance channels are introduced to obtain different intensities of banding.\n51. Meanwhile, the banding images were semi-automatically segmented and labeled into banded and non-banded to form a patch-level banding dataset, which allows for training machine learning-based and deep learning-based banding classification methods.\n52. However, to the best of our knowledge, thus far there still lacking a benchmarking dataset in the banding detection and the corresponding banding IQA domain.\n53. Researchers either resort to image/video quality datasets that do not aim at banding distortion or build a small, attribute-restricted, in-house dataset by themselves.\n54. This motivates us to construct a large-scale subjective assessment database focus on the perceived bandingaffected image quality.\n55. Early research on banding detection mainly focuses on false contour identification, which aims to find the wrong boundary rather than a "true" region edge in the image.\n56. Authors in[31]-[33]utilized monotonicity or non-monotonicity features of local support regions including the gradient, contrast, variance, and entropy information to measure the loss of low-amplitude detail caused by banding.\n57. However, these works ignored the perceptual characteristics of the human visual system (HVS) and thus did not perform a good correlation with subjective tests.\n58. Another banding detection strategy is conducted at the pixel-level estimation and segmentation.\n59. Bhagavathy et al.[30]proposed to identify banding artifacts by calculating the likelihood of pixel difference.\n60. Baugh et al.[34]measured the severity of banding based on the number of a group of connected pixels with the same color.\n61. Wang et al.[39]first detected uniform segments to find possible banding areas and further incorporated edge features (e.g.length and contrast) to capture false boundaries.\n62. Nevertheless, these kinds of methods are typically sensitive to edge noise and are computationally expensive, causing limited application in realtime scenarios.\n63. Towards addressing these problems, Tu et al.[42]presented a completely no-reference banding detection method, which combines various properties of HVS with a number of preprocessing steps to refine banding edge detection.\n64. Instead of regarding banding detection as a false edge detection problem, Tandon et al.[41]heuristically utilized the effect of contrast sensitivity function (CSF) on banding visibility and its dependence on spatial frequency.\n65. Based on this, Krasula et al.[40]further compared the banding annoyance with more commonly studied compression artifacts and proposed a banding-aware video quality metric.\n66. In recent years, deep learning approaches have prevailed in various VQA tasks.\n67. As the pioneering work, Kapoor et al.[43]developed an automated CNN-based banding detector for the first time, which is a simple two-stage algorithm and gives rise to devising other learning-based techniques.\n68. In this work, we build a large-scale banding database and propose a data-driven banding indicator that can generate pixelwise banding visibility maps with corresponding subjectively  consistent quality scores by combining human visual mechanisms and deep learning techniques.\n69. Subjective banding image quality assessment facilitates the development of automatic objective banding image and video quality models.\n70. We created the largest banding database in existence, denoted as the BAND-2k database, which consists of 2,000 banding distorted images and over 214,000 patch-level banding class labels.\n71. Then, a subjective experiment was conducted to obtain the mean opinion scores (MOS) of the BAND-2k database.\n72. The workflow of the banding database construction is shown in Fig.1.\n73. To build a content-rich and balanced database, we manually collected source videos including computer graphics (CG), usergenerated content (UGC), and professionally-generated content (PGC) from two popular media websites Bilibili.comand Youtube.com.\n74. Then, 885 clips with multiple spatial resolutions (i.e., 4096×2160, 3840×2160, 1920×1080) and frame rates (i.e., 60, 50, 30, 25) are chosen as candidate.\n75. Note that videos on the mentioned websites are firstly annotated by the community with assigned a number of favorites, views, and downloads.\n76. These statistics correlate with the content and quality of a video, which guides our choices to some extent.\n77. All videos selected on the website are released under an appropriate creative commons (CC) license that allows further editing and redistribution.\n78. After content selection, we further unified the format of all video clips, especially the spatial resolution and the pixel format, which avoid the effect of other facts on visual quality.\n79. Concretely, we first converted the frame rate of the original clips to 25fps, which is to reduce the storage pressure while ensuring the graphics quality.\n80. Considering the commonly used aspect ratio of the user interface and displays is 16:9, we cropped the partially unqualified videos rather than shrinking images unevenly.\n81. Then, we downsampled the trimmed spatial resolution 3840×2160 to a lower resolution -1920×1080 for the following subjective study.\n82. Before conducting the following experiments, we manually removed the videos that are either too dark or bright, overly blurry or colorful, which helps to obtain more reliable subjective assessments.\n83. To avoid redundancy and to make sure the diversity of selected contents, we also conducted attribute analysis studies on the selected images.\n84. Four metrics that correlated with human perception, i.e., contrast, brightness, sharpness, and colorfulness, are adopted as content diversity metrics.\n85. All video attributes are calculated on every 10 frames to reduce computational complexity, which are then averaged over frames of each video sequence.\n86. Fig.2shows the distribution of attributes extracted from the selected videos.\n87. • Contrast: The contrast metric is simply defined as the standard deviation of pixel gray-scale intensities[25].\n88. • Colorfulness: The colorfulness metric is measured by the R, G, and B components[25].\n89. We first compute two matrices of differences between channels rg = R -G and yb = 1 2 (R + G) -B.\n90. Then, the colorfulness metric can be calculated as µ 2 rg + µ 2 yb + σ 2 rg + σ 2 yb , where µ and σ are the mean and standard deviation of their corresponding terms, respectively.\n91. • Sharpness: The cumulative probability of blur detection (CPBD) metric[24]is used to measure the image sharpness, which estimates the probability of detecting blur at each edge.\n92. • Brightness: The brightness of an image is obtained directly from the pixel gray intensities in R, G, and B channels.\n93. Finally, the number of source videos are reduced to 873.\n94. Fig.3displays thumbnails for 30 selected representative video clips.\n95. To simulate authentic banding artifacts that exist in real viewing scenarios, we introduced four encoding/transcoding strategies including H.264/AVC[28], H.265/HEVC[27], VP9[38], and bit-depth manipulation[43]with fifteen quantization schemes in total.For H.264/AVC and H.265/HEVC, considering the range of their quantization parameter (QP in ffmpeg), we selected three typical QP values, namely, QP = {18, 33, 43}.\n96. This is because that coded video clips with a QP value smaller than 18 normally provide perceptual lossless quality, while coded video clips with a QP value larger than 43 will not be able to offer adequate quality, which may affect the subjective test of banding artifacts.\n97. Similarly, for the VP9 encoder, the QP values under our close inspection are chosen as {28, 45, 60}.\n98. Moreover, we applied the same quantization strategies as in[43], which introduces banding distortion by scaling bit-depth in luminance and chrominance channels.\n99. Here, the color coding scheme YCbCr4:2:0 is applied for maximum compatibility.\n100. To sum up, we generated fifteen levels of banding with different intensities and shapes to enhance the diversity of the database.\n101. The banding database is then built by extracting frames from the distorted video clips, resulting in 2,000 images with a resolution of 1920×1080.\n102. Fig.4shows the visualization results of banding exacerbated images.\n103. In the process of image patch labeling, we initially intend to extract image patches of size 235×235 from banding images directly and perform annotation operations.\n104. However, considering the theoretical number of patches, it will cost a lot of manpower and time to label patch by patch.\n105. Therefore, we first segmented the banding images roughly and label them into banded and non-banded regions.\n106. Then, labelled image patches are generated from these segmented and labelled images by a sliding window.\n107. Specifically, we followed the same demarcation of the banded and non-banded images in[43]that a patch is labelled as banded if it has more than 30% overlap with banded regions in the image.\n108. Eventually, a banding dataset containing 2,000 distorted images with 1920×1080 resolution and 214,324 labelled image patches of size 235×235 is built.\n109. To the best of our knowledge, it is the largest banding dataset in existence, which enables training various machine/deep learning based banding detection models and facilitates the development of image/video debanding techniques.\n110. TableIIreports the composition of labelled image patch dataset.\n111. It can be observed that the number of banded patches is a bit smaller than non-banded patches due to the fact that banding usually appears in smooth background areas.\n112. As shown in Fig.1, the subjective quality study contains four steps.\n113. In addition to preparing the experimental environment, subjects should pass the qualification test first to participate in the study.\n114. After the subjective rating, all resulting scores need to be analyzed and examined before generating the final mean opinion score (MOS).\n115. 1) Experimental Environment Setting: In this study, a total of 25 inexperienced subjects are gathered in a laboratory environment, where relevant experimental configuration must satisfy the following requirements: • Considering the viewing effect, desktops and laptops are allowed as displays.\n116. • The resolution of displays must be larger than or equal to 1920×1080 to show the images without spatial downsampling.\n117. • The viewing distance and optimal horizontal viewing angle are set as 1.9 times the height of the display and respectively.\n118. 500 recommendation[26].\n119. As a result, we used a 27-inch AOC Q27U2D monitor with a resolution of 2560×1440 for assessment with 25 subjects.\n120. Due to the large number of images to be assessed, we divided the dataset into ten sessions to avoid visual fatigue.\n121. Each session of tests took nearly 2 hours with a 30-minute break for each participant.\n122. 2) Qualification Test: Before starting the main experiment, subjects are required to pass a quiz to get the qualification of conducting follow-up experiments.\n123. Firstly, we manually selected 10 labeled banding exacerbated images beyond the database as training images to familiarize subjects with the operation interface and the goal of this subjective test.\n124. The quiz consists of two parts including banding classification and image-level quality rating.\n125. In banding classification, subjects were told to divide the test image into banded or non-banded.\n126. In image-level quality rating, subjects were instructed to focus on the coverage and intensity of banding areas, as well as the overall quality of images to get the final quality score.\n127. The rating scale is continuous from 0 to 100 while a higher value indicates more severe banding (i.e., more visible or occupying larger portion of the image).\n128. To make the quiz objective and fair, we take the banding images labeled by domain experts as the ground truth, while two existing banding metrics BBAND[42]and DBI[43]are used to determine the normal range of the rating scores.\n129. That is the quality scores rated by subjects should not exceed 20% of the above banding metrics.\n130. As a result, only 23 subjects with an accuracy above 80% in banding classification and subjective scores in the normal range were allowed to pass the quiz.\n131. Note that the steps for taking a quiz are the same as the main experiment, which aims to guarantee the consistency of results.\n132. 3) Formal Study: We adopted the single-stimulus (SS) method in this test.\n133. Ten golden images that have the acknowledged high quality or poor quality (assessed by BBAND[42]and DBI[43]) were added to each session for controlling the scoring deviations.\n134. Besides, 3 repeated images are randomly inserted into each session to ensure consistency of scores before and after subjects scoring.\n135. At last, 23 qualified subjects were asked to provide their opinions on the shuffled image groups.\n136. The resulting scores were collected and packed for further analysis.\n137. 4) Result Analysis: In total, 46,000 scores were collected by 23 qualified subjects in the main study phase.\n138. However, considering the qualification quiz cannot completely disallow those unreliable workers to muddle through to the main study while reliable subjects may also occasionally score odd values, which may be caused by the inter-individual differences in perceiving the quality of the unique characteristics of different contents.\n139. Thus, we further investigated the confidence of rating scores and removed outliers following the Grubbs\' test[23],[45].\n140. Concretely, let s = (s 1 , s 2 , . . ., s N ) denote a set of raw scores collected for one distorted image.\n141. The test statistics is defined as the maximum absolute deviation of a sample standard deviation (SD) unit from the sample mean, which is mathematically expressed as where s and σ s denote the mean and standard deviation of the sample, respectively.\n142. Then, given a significant level α, a sample is detected as an outlier if where t 2 α/(2N ),N -2 represents the upper critical value of the tdistribution with N -2 degrees of freedom and a significance level of α/(2N ).\n143. Empirically, we set the significance level α at 0.05.\n144. Then, a sample is identified as an outlier if its distance to the sample mean is larger than 2.5 times SD and is removed.\n145. Following the aforementioned steps, the total number of scores Fig.6.\n146. The overall architecture of the proposed method.\n147. Given a banding distorted image, it is first divided into patches.\n148. Then, the patch-level high-frequency map (HFM) and low-frequency map (LFM) are generated by Sobel operation and piece-wise smooth algorithm[19], respectively.\n149. After that, a dual-branch CNN (CNN H and CNN L ) is deployed to extract hierarchical features with different visual information and thus classify the patches into banded or non-banded.\n150. Lastly, a spatial frequency masking strategy is introduced to refine the banding map and calculate the image-level banding quality score.\n151. Note that the dual-branch networks do not share parameters.\n152. was reduced to 44,371, and MOS was created by averaging the scores for each image.\n153. Fig.5presents the histogram of MOSs over the entire database, showing a broad MOS distribution of banding images.\n154. In this section, we describe the architecture of the proposed banding evaluator in detail, as shown in Fig.6.\n155. As stated before, banding usually appear as high-frequency information in the smooth background, while humans perceive high-frequency texture regions and low-frequency plateau regions through different neural channels concurrently, and transfer the upper visual features into the cerebral cortex for final processing[17],[44].\n156. Inspired by this, we employ high-frequency maps (HFM) and low-frequency maps (LFM) as the deep learning network inputs, which represent the texture and structural information of the image respectively, to mimic the recognition mechanism of the human brain for better banding identification.\n157. High-frequency Maps.\n158. Since gradient has been widely used to represent edge information and has been confirmed beneficial to acquire high-frequency components with low computational cost[20]-[22], we apply the isotropic Sobel operator to each patch for enhancing the details of banding artifacts.\n159. Given an input patch I, the high-frequency map is calculated by where S x and S y are the horizontal and vertical isotropic Sobel operators, respectively." * " denotes the convolution operation.\n160. Low-frequency Maps.\n161. To maintain the principal content of the image and filter out the influence of high-frequency information, we use the piece-wise smooth algorithm[19]to generate the low-frequency map by minimizing a function for image approximation recovery: where L represents the low-frequency map, Ω and E denotes the image domain and edge set, respectively.\n162. P indicates the pixel and E dσ represents the total edge length.\n163. The coefficients α and β are positive regularization constants.\n164. An example of frequency maps is shown in Fig.7.\n165. To obtain the overall banding score, we first divide the banding image into 235×235 patches and apply CNN-based classification to each patch, resulting in a banding classification label for each patch, i.e., banded or non-banded.\n166. As shown in Fig.6, the proposed network consists of two parallel branches, namely CNN H and CNN L , which take the patch-level high-frequency map and low-frequency map as input, respectively.\n167. For each branch, we propose to use Resnet-50[18]as the backbone.\n168. Specifically, we incorporate the feature maps extracted from the first convolutional layer and the last layer of Resnet-50 as hierarchical visual features, which represent different visual information[46],[47]and can be used as predictive information to enhance the discrimination ability of the network for banding and non-banded regions.\n169. Afterward, the features extracted from two branches are concatenated first and reshaped into 128dimensional vectors through two fully-connected layers, which is further followed with the sigmoid activation function to output the final predicted label, namely banded or non-banded.\n170. Of note is that sharing parameters is extremely unfavorable for extracting low-and high-frequency features simultaneously, we thereby deploy two branches that work independently and do not share parameters.\n171. The loss function adopted here is binary cross entropy.\n172. With the trained CNN H and CNN L , each input patch is predicted to obtain a banding or non-banded label.\n173. To better guide the pre-processing and post-processing debanding algorithms, it is necessary to generate a quality score for the entire banding image.\n174. Since the visibility of edge is also affected by content, we further consider the effect of spatially varying content information on the local quality of human perception.\n175. As a consequence, we introduce the spatial frequency masking strategy to determine the weighting factor for the detected banding regions in each patch adaptively and thus obtain the image-level banding severity score while refining the visibility of banding artifacts.\n176. 1) Spatial Frequency Masking: The spatial frequency is defined as the activity level of an image, which establishes a filterbank based on the visual stimulus and is in accordance with HVS[16].\n177. In this paper, we propose to apply spatial frequency as an effective contrast criterion to banding measurement.\n178. Specifically, given an image of size I W × I H , divided into N × N patches, where I W and I H denote the number of columns and rows respectively.\n179. The column (CF k ) and row (RF k ) frequencies of the image patches are given by where I(x, y) is the pixel value of the image patch.\n180. Then, the resulting spatial frequency of an N × N patch is computed as where k is the number of patches (1 ≤ k ≤ I W I H N 2 ).\n181. Since most banding regions are likely to have large contrast including edges and textures, which should be assigned greater weights than the smooth and blurred areas.\n182. Accordingly, we design a banding visibility transfer function to express spatial frequency masking as a function of the local textural feature.\n183. The final spatial frequency masking weight is calculated at each patch as where γ is the scaling constant factor chosen to tune the shape of the transfer function.\n184. We used γ = 1.5 in our implementation.\n185. 2) Building a Banding Metric: The visibility of banding artifacts depends on the combination of multiple visual mechanisms.\n186. In this paper, we propose a simple but effective product model for attribute integration at each predicted banding patch to obtain the entire banding map (BM): where P k denotes the predicted label of k-th patch and w k is the weight parameter that scales the visibility of measured contours, i.e., gradient magnitude of the high-frequency map, |HFM k (i, j) | at region (i, j).\n187. Furthermore, inspired by previous psychovisual findings that the QoE of observers is dominated by those regions having poor quality[14],[42], we thereby leverage the worst p% percentile visual pooling to calculate an average banding score from the generated BM, where p is set to 80 in this experiment.\n188. As a result, the perceptual score of the overall banding image is defined as where M is the total number of patches in image I.\n189. T p% denotes the index set of the top p% non-zero pixel-wise value contained in k-th patch of the BM.\n190. In this section, we first present the experimental protocol in detail and then evaluate the performance of the proposed method on two tasks, namely patch-level banding classification and banding image quality assessment.\n191. After that, the ablation study and cross-database validation are conducted to prove the robustness and effectiveness of the proposed method.\n192. Finally, we test the computational efficiency of our method.\n193. A. Experimental Protocol 1) Databases and Settings: We choose two databases to train and test the effectiveness of the proposed banding IQA method, which are the database released in[43]and our proposed BAND-2k database.\n194. The detail information of these two datasets can be found in TableI.\n195. The proposed model is implemented by PyTorch[13].\n196. Before training, we randomly split the training, validation, and testing set into 8:1:1 (as shown in TableII).\n197. We use the Adam optimizer with the initial learning rate set as 1e-4 and set the batch size as 32.\n198. The training process is stopped after 25 epochs.\n199. The resolution of each cropped patch is fixed to 235×235.\n200. All experiments on both the[43]database and the BAND-2k database are conducted repeatedly 10 times to obtain the mean performance.\n201. 2) Baseline Algorithms: We include a number of representative IQA algorithms in our evaluation as references to be compared against.\n202. These baseline methods include: • General FR IQA methods: We choose PSNR, SSIM[12], MS-SSIM[11], LPIPS[10]as baselines.\n203. These are the most commonly used FR IQA metrics in practical applications such as video coding, image enhancement, etc. • General NR IQA methods: BRISQUE[37], NIQE[36], NIMA[8], DBCNN[35], HyberIQA[9], and StairIQA[1].\n204. • Banding IQA methods: Considering that there exists few research on banding detection and quality assessment, we barely select the BBAND[42], CAMBI[41], VMAF BA[40]and DBI[43]metrics as comparisons.\n205. 3) Evaluation Criteria: To evaluate the IQA methods comprehensively, a total of seven evaluation indexes in two categories are adopted.\n206. For patch-level banding classification, we follow the common procedures as in[43]and utilize the area under the receiver operating characteristics (AUROC), the area under the precision-recall curve (AUPRC), and accuracy as the classification performance metrics.\n207. For banding image quality assessment, four mainstream metrics are selected as the evaluation criteria: Spearman rank-order correlation coefficient (SRCC) and Kendall rank-order correlation coefficient (KRCC) measure the prediction monotonicity, while Pearson linear correlation coefficient (PLCC) and root mean square error (RMSE) are calculated to assess prediction consistency.\n208. Considering the potential nonlinear mapping characteristics between the objective scores and the subjective scores, we perform score alignment by mapping the predicted value using the five-parameter logistic function before calculating PLCC and RMSE values[7].\n209. Since our goal is to develop an effective banding IQA approach, we regard the identification of banding patches as an important preceding process to achieve accurate banding quality prediction.\n210. However, there exist few methods that are designed for banding classification and nearly all IQA methods produce scalar values only while failing in classifying banding regions directly.\n211. Therefore, we adopt a thresholding step to convert the single quality value into binary classification results as[43]does.\n212. Concretely, a half-interval search algorithm[48]is employed to find the optimal threshold value that can generate the best classification result.\n213. Based on the above premise, TableIIIreports the experimental results on both the database from[43]and the BAND-2k database.\n214. We highlight the best results in boldface.\n215. As compared to other state-of-the-art IQA methods, our proposed method yields the best overall performance in terms of AUROC, AUPRC, and accuracy.\n216. It is shown that most general FR IQA and NR IQA models perform poorly on the patch-level banding classification task while performing fairly well on other IQA tasks[1],[9],[35], indicating that the current approaches are not sensitive to banding distortion.\n217. Benefiting from the powerful feature extraction ability of CNNs, our proposed method and the customized NR IQA models for banding artifacts detection (DBI[43]) reach a significant performance in the discrimination of false contours.\n218. However, the performance of banding IQA method BBAND, CAMBI, and VMAF BA is surprisingly poor compared with other methods, which shows their vulnerability in identifying local banding artifacts from texture regions and are not suitable for patch-level banding identification.\n219. In addition, we investigate the computational complexity in terms of execution time per image patch.\n220. It can be observed that except for those traditional FR IQA models, our method achieves comparable speed in patch-level banding classification, which determines the prediction efficiency of the subsequent image-level quality assessment, making it a favorable choice in time-constrained scenarios.\n221. Considering that there exist no image banding databases with subjective scores attached in the public domain, we merely compare the performance of the proposed method with the baseline approaches on the proposed BAND-2k database.\n222. The results are shown in TableIV, from which we can observe that our proposed method largely surpasses all baselines in terms of SRCC, KRCC, and PLCC except for the RMSE.\n223. Compared to the secondbest model, our method achieves 18.07%SRCC improvements, 24.91% KRCC improvements, and 18.01%PLCC improvements on the BAND-2k database.\n224. We also present scatter plots of predictions versus MOS for better visualization in Fig.8.\n225. Overall,  (a) PSNR, (b) SSIM[12], (c) MS-SSIM[11], (d) LPIPS[10], (e) VMAF BA[40], (f) BRISQUE[37], (g) NIQE[36], (h) NIMA[8], (i) DBCNN[35], (j) HyperIQA[9], (k) StairIQA[1], (l) BBAND[42], (m) CAMBI[41], (n) DBI[43], and (o) Ours.\n226. Fig.9. Visual comparisons of the banding map results.\n227. From top to bottom are banding images and their corresponding banding maps generated by BBAND[42], DBI[43], and our proposed method, respectively.\n228. The first five columns of images from left to right are from BAND-2k, while the rest images are from[43].\n229. the performance of traditional FR IQA models such as PSNR, SSIM, and MS-SSIM is remarkably inferior in banding images assessment and is uncorrelated with the MOS, which is consistent with the numerical results presented in TableIV.\n230. The reason is that PSNR and SSIM-based methods do perform not well on tiny, regional, and content-independent banding distortion while lacking the consideration for the mechanism of HVS.\n231. It is also worth mentioning that the deep CNN architecturebased methods (DBCNN[35], HyperIQA[9], and StairIQA[1]), despite performing well on LIVEC[6], KonIQ-10k[29], and other universal image quality evaluation databases[4],[5], underperformed our proposed model by a notable margin on the BAND-2k database.\n232. (2) the perception of banding artifacts is explicitly intensity-aware.\n233. These are the issues that the CNN-based approaches above do not take into account.\n234. To some extent, banding distortion is more like a kind of local distortion than common global distortion such as Gaussian noise, blur, and dither.\n235. This suggests that it is potentially valuable to integrate some local texture, edge, contrast, or other visibility-related features into quality prediction models when assessing banding images.\n236. Fortunately, with the help of the proposed scoring strategy, our method gains the ability to convert the area range of identified banding regions to indicative annoying scores.\n237. Surprisingly, the banding IQA methods BBAND[42], CAMBI[41], and VMAF BA[40], however, did not perform very well on the BAND-2k dataset.\n238. We infer that this is due to differences in the test environment.\n239. First, the source videos selected in CAMBI and VMAF BA are from the internal Netflix 4K catalogue while the source contents in BAND-2k are collected from the public streaming websites with different resolutions, leading to an uneven quality level.\n240. Second, the methods of artificially introducing banding distortion are different.\n241. 264 compression techniques are used to generate the banding artifacts, limiting the adaptive ability of the algorithm in other scenarios.\n242. For further investigation, we compare our method with other two methods, i.e., BBAND[42], DBI[43], and visually study the generated banding maps to verify their effectiveness in banding detection.\n243. The visualization results are shown in Fig.9, where seven representative banding images are selected for reference.\n244. As shown, the banding maps generated from BBAND are mostly disordered and have a lot of discontinuity while the banding maps generated by DBI are too vague to locate the exact location of banding artifacts, making it difficult to develop pixel-level debanding techniques.\n245. Overall, we can compendiously conclude that: (1) in comparison with BBAND and DBI, banding maps computed by our proposed method could provide an accurate, clear indication for banding artifacts; (2) in comparison with BBAND and DBI, the quality prediction of banding images guided by our method could achieve a high consistency with HVS.\n246. Moreover, to make a statistically meaningful comparison among different IQA methods, we further conduct the widely used F-test[2],[3]to assess the statistical significance of the proposed method.\n247. Based on the assumption that the model\'s prediction residuals follow the Gaussian distribution, the lefttailed F-test with a confidence level of 95% is performed on the residuals of every two IQA models.\n248. The results of significance tests on the BAND-2k database are shown in Fig.10.\n249. A value of \'1\' (colored in green) indicates that the model in the row is significantly better than the model in the column, while a value of \'0\' (colored in red) indicates that the model in the row is not significantly better than the model in the column.\n250. It is shown that our proposed method performs significantly better than other models, which is consistent with the observations from the above comparison experiments.\n251. In this section, we explore the effectiveness of our model\'s design philosophy.\n252. To verify the importance of the dual-branch (DB) architecture, the baseline variants use the same backbone as the proposed method, except that only a single branch (SB) is reserved.\n253. Then, we use the original banding images as the input while removing the frequency map generation module (SB-I).\n254. Besides, the high-frequency maps and low-frequency maps generation modules are retained respectively (SB-HFM and SB-LFM).\n255. It can be observed from Table V that SB-LFM achieved the worst results, since the low-frequency map filters out the highfrequency banding information to a certain extent, which reduces the ability of the model to identify false contours.\n256. SB-I performs significantly better than SB-HFM and SB-LFM, resulting from that it contains richer image information, but it is still inferior to our method, which demonstrates the effectiveness of the dualbranch scheme.\n257. To investigate the effect of the banding feature extraction, we further design two variants with different input combinations of frequency maps.\n258. First, the high-frequency map was taken as the input of both the CN N H and CN N L layers (DB-HFM).\n259. Then, we replace the inputs with the low-frequency maps (DB-LFM).\n260. As shown in TableV, the performance of DB-HFM and DB-LFM is far apart from our approach, which matches our hypothesis that the high-frequency texture information contained in HFM and the low-frequency background information contained in LFM are crucial to enhance the capacity of discernment for banding artifacts.\n261. Therefore, we may conclude that our model is the most suitable model among those compared variants in terms of both banding classification and IQA applications.\n262. Due to the effects of different compression techniques, shooting equipment, scenes, etc., the image content and banding distortions may vary significantly in practical applications.\n263. For the database[43], it only includes limited types of image sources and means of triggering banding distortion.\n264. As a result, we conduct a cross-database validation to verify the generalizability of the proposed model, wherein the database presented by[43]and BAND-2k are included.\n265. That is, we trained the model on one full database and report the test performance on the other.\n266. We mainly compare the proposed method with four learningbased models, i.e., DBCNN[35], HyperIQA[9], StairIQA[1], and DBI[43].\n267. Since MOS information is not provided in the database[43], which is an essential part of methods training, we condensed part of the experiments.\n268. TableVIand VII report the experimental results in terms of patch-level banding classification and image quality assessment.\n269. We can observe that our proposed method generalization between database[43]and BAND-2k was surprisingly good.\n270. Besides, it is worth noting that the performance of these methods trained on the BAND-2k has improved a little compared to the previous versions that were trained on the database[43], which further demonstrates the superiority of the proposed database BAND-2k.\n271. The efficiency of an image quality prediction model is of great importance in practical industrial deployments.\n272. Therefore, we measured the average running time of the compared IQA models, as shown in TableVIII.\n273. The experiments were performed in MATLAB R2021a and Python 3.7 under Windows 10 64bit system on a Lenovo laptop with Intel Core i5-9300HF CPU@2.4GHz,16GB RAM, and NVIDIA GTX 1660Ti 6G GPU.\n274. It can be observed that the proposed method achieves a reasonable running time among the FR, NR, and other bandingspecified IQA algorithms.\n275. Generally, the execution time of classical IQA algorithms is significantly less than learning-based methods.\n276. Simpler NSS-based models such as BRISQUE and NIQE still show competitive efficiency relative to CNN models while exhibiting inferior performance in banding image quality assessment.\n277. For CAMBI and VMAF BA , we use the officially launched software package, which is based on the stand-alone C library libvmaf and therefore surpasses other methods in speed.\n278. Moreover, unlike the general quality evaluation using regression to predict scores, the patch-wise prediction strategy that we adopted may increase the complexity.\n279. Note that although we deployed a more complex network structure, a nearly 10 times speedup has been seen when comparing DBI with our method since the sliding window mechanism[43]is removed.\n280. In this paper, we conduct a comprehensive exploration of banding images from both subjective and objective perspectives.\n281. Specifically, we construct the largest ecologically valid banding IQA database to date named BAND-2k database, which consists of 2,000 banding images generated by fifteen compression and quantization schemes, achieving several times larger in number and diversity than the existing banding dataset.\n282. The construction process of the database, including distortion content preparation, subjective test procedure, and the removal of outlying data, is described in detail in this paper.\n283. Relying on this database, we proposed a novel banding evaluator using the frequency characteristic of banding artifacts, which models the banding as high-frequency artifacts that contained in the low-frequency smoothing region.\n284. A dual-branch CNN is devised to extract hierarchical features to classify the banding regions, upon which we introduce the spatial frequency masking to refine and compute an overall banding score.\n285. Experimental results show that our proposed method outperforms the baseline algorithms significantly in patch-level banding classification and banding IQA tasks.\n286. We believe that our study will benefit further development, calibration, and benchmarking of banding IQA models.\n\n请严格按照JSON格式返回：{"labels": [0,1,0,...]}，不要包含任何其他文字。'}], 'model': 'qwen-plus', 'response_format': {'type': 'json_object'}}}
2025-10-14 21:06:19,003 - DEBUG - Sending HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
2025-10-14 21:06:19,004 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-10-14 21:06:19,004 - DEBUG - send_request_headers.complete
2025-10-14 21:06:19,004 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-10-14 21:06:19,004 - DEBUG - send_request_body.complete
2025-10-14 21:06:19,004 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-10-14 21:11:07,347 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'vary', b'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding'), (b'x-request-id', b'88aaa40f-5ed2-4814-ac75-81d84ac23330'), (b'x-dashscope-call-gateway', b'true'), (b'content-type', b'application/json'), (b'req-cost-time', b'288314'), (b'req-arrive-time', b'1760447169107'), (b'resp-start-time', b'1760447457421'), (b'x-envoy-upstream-service-time', b'288225'), (b'content-encoding', b'gzip'), (b'date', b'Tue, 14 Oct 2025 13:10:56 GMT'), (b'server', b'istio-envoy'), (b'transfer-encoding', b'chunked')])
2025-10-14 21:11:07,348 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-14 21:11:07,348 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-10-14 21:11:07,349 - DEBUG - receive_response_body.complete
2025-10-14 21:11:07,349 - DEBUG - response_closed.started
2025-10-14 21:11:07,349 - DEBUG - response_closed.complete
2025-10-14 21:11:07,349 - DEBUG - HTTP Response: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "200 OK" Headers({'vary': 'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding', 'x-request-id': '88aaa40f-5ed2-4814-ac75-81d84ac23330', 'x-dashscope-call-gateway': 'true', 'content-type': 'application/json', 'req-cost-time': '288314', 'req-arrive-time': '1760447169107', 'resp-start-time': '1760447457421', 'x-envoy-upstream-service-time': '288225', 'content-encoding': 'gzip', 'date': 'Tue, 14 Oct 2025 13:10:56 GMT', 'server': 'istio-envoy', 'transfer-encoding': 'chunked'})
2025-10-14 21:11:07,349 - DEBUG - request_id: 88aaa40f-5ed2-4814-ac75-81d84ac23330
2025-10-14 21:11:07,350 - DEBUG - API request completed in 288.35 seconds
2025-10-14 21:11:07,350 - DEBUG - Raw model response: {"labels": [1,0,0,1,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1
2025-10-14 21:11:07,352 - ERROR - JSON parsing error: Expecting ',' delimiter: line 1 column 12454 (char 12453)
2025-10-14 21:11:07,352 - ERROR - Problematic content: {"labels": [1,0,0,1,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1
2025-10-14 21:11:07,352 - WARNING - No predictions obtained for paper: BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment
2025-10-14 21:11:07,352 - INFO - Evaluating paper 8/18: Comprehensive Named Entity Recognition on CORD_19 with Distant or Weak Supervision
2025-10-14 21:11:07,353 - INFO - Starting model prediction
2025-10-14 21:11:07,353 - INFO - Attempt 1 of 5
2025-10-14 21:11:07,354 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-f5949c50-8fe8-4d41-8cd0-92556408cc2c', 'json_data': {'messages': [{'role': 'system', 'content': '你是一个专门用于识别学术论文中数据集描述的助手。你需要判断每个输入的句子是否描述了研究中使用的数据集。请确保：\n    1. 只输出JSON格式的结果\n    2. 结果格式必须为 {"labels": [0,1,0,...]}\n    3. 不要输出任何其他文字或解释'}, {'role': 'user', 'content': '你是一位专业的学术论文数据集描述识别专家。请仔细分析下面这篇论文中的每个句子，判断它们是否描述了本研究特定构建的数据集信息。\n\n    论文标题: Comprehensive Named Entity Recognition on CORD_19 with Distant or Weak Supervision\n\n    判断标准:\n    判断为1(数据集描述)的标准：\n    句子必须直接描述本研究特定构建的数据集,包括:\n    1. 该数据集的构建过程和方法\n    2. 该数据集的具体构成和规模\n    3. 该数据的来源和收集方式\n    4. 该数据的预处理步骤\n    5. 该数据集的可获取方式(如发布地址)\n    6. 或者其他直接或者间接描述了该数据集的句子\n\n    所有其他类型的句子均标记为0。\n    \n    请对每个句子进行分析，返回一个JSON格式的标签数组。格式要求：\n    1. 必须是有效的JSON格式\n    2. 只包含labels字段，值为0和1组成的数组\n    3. 数组长度必须与句子数量相同\n    4. 示例格式：{"labels": [0,1,0,1,...]}\n\n    需要判断的句子：\n\n    1. Abstract: We created this CORD-NER dataset with comprehensive named entity recognition (NER) on the COVID-19 Open Research  Dataset Challenge (CORD-19) corpus (2020-03-13).\n2. This CORD-NER dataset covers 75 fine-grained entity types: In addition to the common biomedical entity types (e.g., genes, chemicals and diseases), it covers many new entity types related explicitly to the COVID-19 studies (e.g., coronaviruses, viral proteins, evolution, materials, substrates and immune responses), which may benefit research on COVID-19 related virus, spreading mechanisms, and potential vaccines.\n3. CORD-NER annotation is a combination of four sources with different NER methods.\n4. The quality of CORD-NER annotation surpasses SciSpacy (over 10% higher on the F1 score based on a sample set of documents), a fully supervised BioNER tool.\n5. Moreover, CORD-NER supports incrementally adding new documents as well as adding new entity types when needed by adding dozens of seeds as the input examples.\n6. We will constantly update CORD-NER based on the incremental updates of the CORD-19 corpus and the improvement of our system.\n7. Coronavirus disease 2019 (COVID-19) is an infectious disease caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2).\n8. The disease was first identified in 2019 in Wuhan, Central China, and has since spread globally, resulting in the 20192020 coronavirus pandemic.\n9. On March 16th, 2020, researchers and leaders from the Allen Institute for AI, Chan Zuckerberg Initiative (CZI), Georgetown University\'s Center for Security and Emerging Technology (CSET), Microsoft, and the National Library of Medicine (NLM) at the National Institutes of Health released the  Open Research Dataset (CORD-19)1of scholarly literature about COVID-19, SARS-CoV-2, and the coronavirus group.\n10. Named entity recognition (NER) is a fundamental step in text mining system development to facilitate COVID-19 studies.\n11. There is a critical need for NER methods that can quickly adapt to all the COVID-19 related new types without much human effort for training data annotation.\n12. We created this CORD-NER dataset2with comprehensive named entity annotation on theCORD-19 corpus (2020-03-13).\n13. This dataset covers 75 fine-grained named entity types.\n14. CORD-NER is automatically generated by combining the annotation results from four sources.\n15. In the following sections, we introduce the details of CORD-NER dataset construction.\n16. We also show some NER annotation results in this dataset.\n17. The input corpus is generated from the 29,500 documents in theCORD-19 corpus (2020-03-13).\n18. We first merge all the meta-data (all sources metadata 2020-03-13.csv) with their corresponding full-text papers.\n19. Then we create a tokenized corpus (CORD-NER-corpus.json)for further NER annotations.\n20. The input corpus is a combination of the "title", "abstract" and "full-text" from the CORD-19 corpus.\n21. We first conduct automatic phrase mining and tokenization on the input corpus using AutoPhrase(Shang et al., 2018a).\n22. Then we do a second round of tokenization with Spacy3on the phrase-replaced corpus.\n23. We found that keeping the AutoPhrase results will significantly improve the distantly-and weakly-supervised NER performance.\n24. CORD-NER annotation is a combination of four sources with different NER methods: 1.Pre-trained NER on 18 general entity types from Spacy using the model "en core web sm".\n25. 2.Pre-trained NER on 18 biomedical entity types from SciSpacy 4 using the models "en ner bionlp13cg md" and "en ner bc5cdr md".\n\n请严格按照JSON格式返回：{"labels": [0,1,0,...]}，不要包含任何其他文字。'}], 'model': 'qwen-plus', 'response_format': {'type': 'json_object'}}}
2025-10-14 21:11:07,355 - DEBUG - Sending HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
2025-10-14 21:11:07,355 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-10-14 21:11:07,356 - DEBUG - send_request_headers.complete
2025-10-14 21:11:07,356 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-10-14 21:11:07,356 - DEBUG - send_request_body.complete
2025-10-14 21:11:07,356 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-10-14 21:11:09,527 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'vary', b'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding'), (b'x-request-id', b'dfc6e157-3938-45e0-bc55-c1c6d1655240'), (b'x-dashscope-call-gateway', b'true'), (b'content-type', b'application/json'), (b'req-cost-time', b'2128'), (b'req-arrive-time', b'1760447457472'), (b'resp-start-time', b'1760447459601'), (b'x-envoy-upstream-service-time', b'2127'), (b'content-encoding', b'gzip'), (b'date', b'Tue, 14 Oct 2025 13:10:59 GMT'), (b'server', b'istio-envoy'), (b'transfer-encoding', b'chunked')])
2025-10-14 21:11:09,528 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-14 21:11:09,528 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-10-14 21:11:09,528 - DEBUG - receive_response_body.complete
2025-10-14 21:11:09,528 - DEBUG - response_closed.started
2025-10-14 21:11:09,528 - DEBUG - response_closed.complete
2025-10-14 21:11:09,528 - DEBUG - HTTP Response: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "200 OK" Headers({'vary': 'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding', 'x-request-id': 'dfc6e157-3938-45e0-bc55-c1c6d1655240', 'x-dashscope-call-gateway': 'true', 'content-type': 'application/json', 'req-cost-time': '2128', 'req-arrive-time': '1760447457472', 'resp-start-time': '1760447459601', 'x-envoy-upstream-service-time': '2127', 'content-encoding': 'gzip', 'date': 'Tue, 14 Oct 2025 13:10:59 GMT', 'server': 'istio-envoy', 'transfer-encoding': 'chunked'})
2025-10-14 21:11:09,528 - DEBUG - request_id: dfc6e157-3938-45e0-bc55-c1c6d1655240
2025-10-14 21:11:09,528 - DEBUG - API request completed in 2.18 seconds
2025-10-14 21:11:09,529 - DEBUG - Raw model response: {"labels": [1,1,1,0,1,1,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1]}
2025-10-14 21:11:09,529 - INFO - Successfully processed 25 labels
2025-10-14 21:11:09,538 - ERROR - Error processing paper Comprehensive Named Entity Recognition on CORD_19 with Distant or Weak Supervision: 'int' object has no attribute 'capitalize'
2025-10-14 21:11:09,538 - INFO - Evaluating paper 9/18: DCQA_ DOCUMENT_LEVEL CHART QUESTION ANSWERING TOWARDS COMPLEX REASONING AND COMMON_SENSE UNDERSTANDING
2025-10-14 21:11:09,538 - INFO - Starting model prediction
2025-10-14 21:11:09,538 - INFO - Attempt 1 of 5
2025-10-14 21:11:09,539 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-3aa665b6-ecf8-480c-a95e-5940d1a6a056', 'json_data': {'messages': [{'role': 'system', 'content': '你是一个专门用于识别学术论文中数据集描述的助手。你需要判断每个输入的句子是否描述了研究中使用的数据集。请确保：\n    1. 只输出JSON格式的结果\n    2. 结果格式必须为 {"labels": [0,1,0,...]}\n    3. 不要输出任何其他文字或解释'}, {'role': 'user', 'content': '你是一位专业的学术论文数据集描述识别专家。请仔细分析下面这篇论文中的每个句子，判断它们是否描述了本研究特定构建的数据集信息。\n\n    论文标题: DCQA_ DOCUMENT_LEVEL CHART QUESTION ANSWERING TOWARDS COMPLEX REASONING AND COMMON_SENSE UNDERSTANDING\n\n    判断标准:\n    判断为1(数据集描述)的标准：\n    句子必须直接描述本研究特定构建的数据集,包括:\n    1. 该数据集的构建过程和方法\n    2. 该数据集的具体构成和规模\n    3. 该数据的来源和收集方式\n    4. 该数据的预处理步骤\n    5. 该数据集的可获取方式(如发布地址)\n    6. 或者其他直接或者间接描述了该数据集的句子\n\n    所有其他类型的句子均标记为0。\n    \n    请对每个句子进行分析，返回一个JSON格式的标签数组。格式要求：\n    1. 必须是有效的JSON格式\n    2. 只包含labels字段，值为0和1组成的数组\n    3. 数组长度必须与句子数量相同\n    4. 示例格式：{"labels": [0,1,0,1,...]}\n\n    需要判断的句子：\n\n    1. Abstract: Visually-situated languages such as charts and plots are omnipresent in real-world documents.\n2. These graphical depictions are human-readable and are often analyzed in visuallyrich documents to address a variety of questions that necessitate complex reasoning and common-sense responses.\n3. Despite the growing number of datasets that aim to answer questions over charts, most only address this task in isolation, without considering the broader context of document-level question answering.\n4. Moreover, such datasets lack adequate common-sense reasoning information in their questions.\n5. In this work, we introduce a novel task named document-level chart question answering (DCQA).\n6. The goal of this task is to conduct document-level question answering, extracting charts or plots in the document via document layout analysis (DLA) first and subsequently performing chart question answering (CQA).\n7. The newly developed benchmark dataset comprises 50,010 synthetic documents integrating charts in a wide range of styles (6 styles in contrast to 3 for PlotQA and ChartQA) and includes 699,051 questions that demand a high degree of reasoning ability and common-sense understanding.\n8. Besides, we present the development of a potent question-answer generation engine that employs table data, a rich color set, and basic question templates to produce a vast array of reasoning question-answer pairs automatically.\n9. Based on DCQA, we devise an OCR-free transformer for document-level chartoriented understanding, capable of DLA and answering complex reasoning and common-sense questions over charts in an OCR-free manner.\n10. Our DCQA dataset is expected to foster research on understanding visualizations in documents, especially for scenarios that require complex reasoning for charts in the visually-rich document.\n11. We implement and evaluate a set of baselines, and our proposed method achieves comparable results.\n12. The emergence of visual language as a novel communicative tool, characterized by a tightly integrated interplay of visual and textual components, can be attributed to a confluence of factors, notably globalization, the growing intricacy of commerce and technology, and the convergence of lexicons from diverse fields that were once disparate[7].\n13. The prevalence of visually-situated language in various document types, such as academic, business, medical, and others, is markedly high[8][9][10].\n14. Gaining a comprehensive understanding of these graphical representations, such as charts and plots, is essential in extracting valuable and pragmatic insights from data[11].\n15. To conduct data analysis, individuals frequently pose intricate queries that require common-sense and arithmetic or logical operations pertaining to graphical representations.\n16. Answering such inquiries demands a substantial level of cognitive and reasoning exertion, as individuals are required to be aware of common sense and integrate numerous logical operations, including but not limited to retrieving entities, comparing trends, calculating averages, finding extremum, etc.Typically, the chart question answering (CQA) system[12]aims to generate the desired answer by taking a chart-question pair as input, constituting a fundamental function within the domain of intelligent document understanding (IDU)[13].\n17. Despite the CQA task has drawn ever-growing attention from visual question answering communities in recent years, existing datasets has encountered certain obstacles: (i) Notably, while charts constitute crucial components of documents, the majority of current datasets treat the CQA task solely at the question-answering level, without taking into account its significance as a document-level task.\n18. (ii) Questions generally prioritize reasoning or visual features, potentially losing sight of common sense information that individuals typically consider when posing questions, which is a misalignment with the typical questioning habits of individuals.\n19. (iii) The quantity of chart types, as exemplified by PlotQA and ChartQA datasets, is comparatively restricted (only three).\n20. Such a limited representation fails to capture the broad range of chart styles that are present in real-world documents.\n21. Furthermore, in real-world settings, users typically first identify the location of charts in documents before querying them.\n22. However, directly analyzing document layout poses challenges, as charts lack explicit annotations.\n23. Manually annotating datasets for document layout analysis related to charts is remarkably laborious and time-consuming.\n24. This motivates an automated system to generate annotations associated with charts, eliminating the need for costly labeled data collection.\n25. Such a system would locate charts in arbitrary documents without annotations and produce bounding boxes highlighting them, providing chart-specific layout information without human intervention.\n26. Additionally, certain baseline models rely on obtaining high-quality optical character recognition (OCR) outcomes to extract the data table structure from the chart image.\n27. Therefore, current models\' efficacy generally relies upon the accuracy of OCR results.\n28. Nevertheless, incorporating an OCR-dependent approach for CQA system poses significant challenges.\n29. For one thing, commercially available OCR techniques often exhibit limited adaptability in addressing diverse languages or changes in the domain, which are commonly encountered in the context of charts.\n30. Such limitations may impede the generalization abil-ity of these methods.\n31. For another, the occurrence of errors during the OCR process is unavoidable, and such erroneous outcomes have the potential to propagate to the CQA system, thereby adversely affecting subsequent processes[14].\n32. To alleviate above issues, we go beyond the traditional dataset by presenting a large-scale document-level chart question answering dataset (DCQA).\n33. DCQA comprises 50,010 synthetic documents and 699,051 question-answer pairs generated using our customized semantic-rich question-answer generation engine.\n34. The dataset includes questions that focus on vision, complex reasoning, and common-sense knowledge.\n35. Common-sense knowledge reasoning primarily involves evaluating the ability of CQA models to distinguish between legend labels and entity names belonging to specific parent classes, and subsequently performing reasoning operations based on this discriminative ability.\n36. Each document in the DCQA includes a chart, unrelated images and a descriptive caption related to the chart.\n37. The language used in the DCQA dataset is English.\n38. The chart types exhibit a diverse range of styles and can be broadly categorized into six major types, namely Bar chart, Line plot, Pie chart, Scatter plot, Box plot, and Mixed chart, each of which is further divided into subtypes, yielding a total of 30 chart subtypes.\n39. Figure1displays some examples from DCQA.\n40. More examples are provided in Appendix E.\n41. Drawing upon the DCQA dataset, we further devise a transformer-based OCR-free architecture to perform document layout analysis and chart question answering.\n42. Initially, we exploit swin transformer[15]as the vision backbone to extract visual features of the input document.\n43. Next, the extracted features are fed into the detection component to perform document layout analysis[16].\n44. Upon successfully identifying the chart image, we extract the relevant visual content from the chart, which is then utilized as input to the textual decoder for answer prediction.\n45. This novel OCR-free architecture provides a plug-and-play solution for performing chart question answering directly from the document.\n46. In a nutshell, our contributions are as follows: • We present a comprehensive and extensive documentlevel chart question answering dataset, DCQA, which features a wide range of chart styles and includes question-answer pairs that incorporate complex reasoning and common-sense knowledge.\n47. The dataset\'s scale and diversity make it a valuable resource for researchers interested in developing and evaluating chart question answering models.\n48. • We conceptualize chart question answering as a documentlevel task and propose a transformer-based OCR-free model to effectively address this task.\n49. • We perform comprehensive experiments and thorough analyses on DCQA, verifying the efficacy of our model.\n50. To date, only a limited number of datasets have been explicitly designed for chart question answering.\n51. These datasets include FigureQA[1], DVQA[2], LEAF-QA[3], LEAFQA++[4], PlotQA[5]and ChartQA[6].\n52. Despite consisting of a diverse set of synthetic charts, FigureQA suffers from a lack of specificity in terms of chart element labeling, utilizing only generic titles and color names.\n53. Furthermore, the questions are limited to a few template-based formats with binary "yes/no" answers.\n54. DVQA is limited to a single chart type, namely the Bar chart, and suffers from inadequate semantic relations between the textual elements.(e.g., bar and legend labels are randomly selected words) as well as restricted Y-axis value ranges.\n55. Numeric answers are primarily integers in both the train and test sets and share the same values.\n56. As with FigureQA, all bar plots in DVQA are artificially generated, and the questions are based on a small number of templates.\n57. Both LEAF-QA and its extended version, LEAFQA++, are not publicly available.\n58. Besides, they share a significant limitation: the absence of regression question-answering pairs.\n59. This is evident from the question templates described in their reference and the discrete answer set employed.\n60. Although PlotQA is currently the largest publicly available dataset for CQA, it is limited by imbalanced question distribution, as it is heavily weighted towards data-related questions and lacks an appropriate proportion of queries pertaining to the visual characteristics of chart elements, including color and shape.\n61. Regarding ChartQA, it is the pioneer dataset to compile realworld charts with a blend of human-created QA pairs and machine-generated QA pairs.\n62. However, despite its innovative contribution, ChartQA is characterized by a limited size and encompasses only three distinct plot types.\n63. Furthermore, the paucity of question-answer pairs (two) per chart undermines the potential for a comprehensive understanding of the underlying information conveyed by these visualizations.\n64. This work presents a novel and intricate CQA dataset, which diverges from prior datasets in several respects.\n65. Firstly, DCQA is introduced, which reformulates the CQA task by integrating document layout analysis and chart question answering.\n66. Secondly, in addition to visual and complex reasoning questions, DCQA incorporates common sense-aware questions.\n67. Last but not least, DCQA covers a broad range of chart types1.\n68. In this section, we describe the construction of DCQA from the following four aspects:(i) Data collection, (ii) Chart generation, (iii) Automatic QA pair generation engine, and (iv) Document generation.\n69. The general workflow of the DCQA generation process is shown in Figure3.\n70. A detailed generation procedure is provided in Appendix A.\n71. Given the variability of chart styles in real-world scenarios, integrating real-world sources and randomly generated data for producing charts can augment the models robustness and adaptability to various chart formats encountered in practical scenarios.\n72. Drawing upon this observation, charts included in our dataset was derived by two means: utilizing real-world sources and randomly generated data.\n73. The detailed process of data collection is shown in Appendix A.\n74. We exploit Pyecharts1, a Python visualization tool library based on the Echarts[21]charting library, to generate our charts.\n75. Our DCQA contains six different chart styles: bar chart, line chart, scatter plot, box plot, pie chart, and combination chart (line and bar).\n76. These chart styles can be further divided into 30 sub-types (As shown in Figure2).\n77. The color of chart elements is randomly picked up from a color set Johndecember, which covers a wide range of colors (595).\n78. Besides, the chart presents two distinct styles of background, namely dark and light, of which the former was not previously observed in any of the CQA datasets.\n79. Detailed chart information is provided in Appendix B.\n80. Since the generated charts are from disparate data sources and encompass a wide range of topics, engaging a cadre of individuals with diverse backgrounds, experiences, and expertise is necessary to craft questions about the corresponding charts.\n81. We have meticulously curated a corpus of 573 charts spanning six categories, comprising data extracted from real-world and randomly generated sources, which serve as paradigmatic instances from which questions can be formulated.\n82. We commission a cohort of post-graduate students affiliated with our academic institution, and employees from Huawei company , to generate ten distinct questions for each of the selected charts, with an emphasis on reasoning and common sense awareness.\n83. We have obtained 5730 questions.\n84. After an exhaustive process of meticulous meetings and indepth discussions, we have successfully distilled a total of 324 question templates from the original pool of 5730 questions.\n85. Out of these templates, 204 are specifically tailored for visual and numeric reasoning, while 120 templates are dedicated to common sense reasoning.\n86. Code will be publicly available at github .\n87. Table2displays the statistics of the dataset.\n88. Details in Appendix C.2.\n89. Visual and numeric reasoning: These kinds of questions necessitate combing visual elements understanding and numerical reasoning techniques (e.g., sum, multiple, average, etc.).\n90. Integrating visual and numerical reasoning inquiries can facilitate CQA systems\' comprehension of chart content, as it encourages the concurrent utilization of their visual and analytic faculties, thereby enabling them to engage in a more profound exploration of the underlying message conveyed by the data and achieve a more precise interpretation of chart figures.\n91. Examples of this type of question are presented in Figure1(a) and (b).\n92. Common sense reasoning: Questions of this type demand combining common sense knowledge and numerical operation.\n93. Common sense is able to serve as a facilitator for CQA systems to gain a more profound understanding of the real-life background and context reflected by the data, thus accurately inferring the meaning behind the data.\n94. Meanwhile, numerical reasoning skills can allow readers to fathom the underlying interconnections and relationships of the data and infer potential outcomes and trends.\n95. The combination of both proficiencies can profoundly equip CQA models with diverse conceptualizations of chart content and enable them to increase the usefulness of data in comprehensively examining and scrutinizing data, identifying patterns and trends, and making predictions and decisions.\n96. Examples of this type of question are presented in Figure1 (c).\n97. Construction of the hierarchical entity database: Common sense reasoning is a crucial aspect of DCQA, which primarily involves evaluating a CQA model\'s ability to discriminate between legend labels and entity names that belong to a specific parent class and then perform reasoning operations based on this discriminatory capacity.\n98. Therefore, a hierarchical entity database with a tree-structured architecture and a well-defined set of parent-child relationships is necessary to serve as a source for both entity names and legend labels.\n99. The construction of the hierarchical entity database is expounded upon in Appendix C.1.\n100. Categorization of question difficulty levels: Additionally, the entire set of question templates has been classified into five distinct levels delineated by their respective levels of complexity, namely, beginner, elementary, intermediate, ad-  vanced, and expert.\n101. The statistic of question levels is displayed in Table3.\n102. The difficulty levels of the question templates are manually annotated based on the following criteria: (1) Beginner includes questions about the overall nature of a chart image, such as whether it is horizontal or vertical or the number of columns it contains, as well as retrieval for the value of a specific chart element.\n103. (2) Elementary primarily involves questions carrying out some form of operation on all chart elements within a chart, such as determining the maximum, minimum, median, or mean value.\n104. (3) Intermediate refers to questions that involve applying specific operations to chart elements that satisfy predetermined criteria, including but not limited to identifying the maximum, minimum, median, mean, sum, or difference of chart elements based on their color, legend, or numerical value.\n105. (4) Advanced questions demand performing composite operations on chart elements that meet a specific property (building upon the operations mentioned for intermediate questions), such as finding the sum or difference of two maximum values after they have been identified.\n106. (5) Building upon advanced questions, questions that involve common sense will be categorized as expert-level.\n107. In this paper, answers are generated through an automated process based on a customized set of procedures.\n108. Specifically, a solution step is designed for each question template, with each step representing an atomic operation that is implemented using specific functions to achieve its intended functionality.\n109. When solving specific questions to generate answers, the designed solution steps are followed by invoking corresponding functions, resulting in answers for the respec- Upon completing the question-answer pair generation process, extra analysis is conducted on the distribution of every answer type.\n110. It is noted that the highest proportion of the answer type in the dataset is the binary classification yes or no.\n111. However, the ratio between yes and no is severely imbalanced, with a vastly larger number of no responses compared to yes.\n112. As a result, a post-processing adjustment is necessary to address this language bias and prevent the model from exploiting the answer distribution pattern to output the answer without paying attention to the visual content.\n113. The debiasing procedure can be concisely described as: Firstly, filter out all question templates with Yes/No answers.\n114. Secondly, count the number of Yes/No answers for each Yes/No question template in the dataset.\n115. For each Yes/No question template, determine whether the number of Yes answers exceeds the number of No answers or vice versa.\n116. For the question with a larger proportion of answers, adjust the values of the replaceable modules in the question to change the answer to the less frequent one, and iterate this process until the number of Yes and No answers for the template are equal or differ by only 1.\n117. Most of the Yes/No question templates can be balanced by modifying the answer using the above approach.\n118. However, a few questions cannot be balanced this way, and their answers are not significantly  imbalanced.\n119. Therefore, we do not handle such question templates in this paper for now.\n120. Before debiasing, the dataset\'s overall proportion of yes and no answers was 35.16% and 64.84%, respectively.\n121. After debiasing, the overall proportion of yes and no answers in the dataset became 49.26% and 50.74%, respectively.\n122. The effectiveness of the debiasing is compared in Figure4, where the noticeable changes in the answer distribution of "Yes" and "No" before and after debiasing demonstrate that the debiasing method employed in this study effectively addresses the answer imbalance in binary questions.\n123. In accordance with the methodology outlined in[22], we utilize LaTex to generate synthetic documents that include diverse multimedia elements.\n124. In addition to the chart image, the generated document also incorporates other visual content 2  and textual content produced by ChatGLM[23,24].\n125. org of the synthetic document beyond the mere inclusion of the chart image, which is more consistent with real-world documents.\n126. Detailed document information is provided in Appendix D.\n127. In this section, we provide a comprehensive analysis of the experimental results to establish the validity of the recently developed DCQA dataset and verify the excellent efficacy of our proposed TOT-Doctor model through a comparative evaluation against other baselines.\n128. We compare the TOT-Doctor with the classical approaches include LayoutLMv2[25], LayoutLMv3[26], Pix2Struct[9], and MATCHA[27].\n129. • LayoutLMv3[26]is a multi-modal Transformer framework without the vision backbone that leverages reconstructive objectives for cross-modal alignment learning, showcasing notable generality in the context of document vision tasks.\n130. • Pix2Struct[9]is an image-to-text model tailored for visual language understanding, which is pre-trained on visually-rich screenshots of web pages with screenshot parsing objective.\n131. • MATCHA[27]is a Pix2Struct-based model pre-trained for chart underlying structure understanding and mathematical reasoning.\n132. Our study employs accuracy the primary evaluation metric, wherein the assessment of the predicted answers correctness depends on the nature of the answer type.\n133. In the case of textual answers, such as binary responses, entities, and integers, the evaluation criterion mandates that the predicted answer should match the ground truth exactly.\n134. For numerical answers in the form of floating-point values, it is not always feasible to expect that the predicted answer will precisely match the correct answer.\n135. Therefore, we consider the answer correct if it falls within 5% of the expected value.\n136. This section primarily supplements experimental configurations, including parameters such as batch size and learning rate, and outlines the preprocessing steps undertaken to ensure a fair comparison for the extractive model.\n137. Table4shows the detailed experimental setup, in which CE refers to Cross Entropy.\n138. All models were verified every 5000 iterations during training.\n139. In our implementation of LayoutLMv2 and v3, we employ a multi-step learning rate schedule.\n140. More specifically, we gradually decrease the learning rate by a factor of 2 after each epoch of training.\n141. For other models, we use the cosine scheduler to adjust the learning rate, where the number of warm-up steps is set to 1000.\n142. The LayoutLM series employs a model-based extraction approach, which requires the system to select answers from the optical character recognition (OCR) results.\n143. To enable the model to perform tasks such as binary classification (e.g., Yes/No), we have implemented a simple yet effective solution: we add two special characters, "Yes" and "No", to the OCR results.\n144. TOT-Doctor consists of two main components, namely the document layout analysis and the chart question answering.\n145. The model parameters of TOT-Doctor are calculated and found to be as follows: the encoder of the Swin Transformer used in the document layout analysis has 74M parameters, while the detector component has 48M parameters.\n146. The encoder of the Swin Transformer used in the chart question answering phase has 74M parameters, and the BART has 127M parameters.\n147. Based on the fine-grained document element annotations present within the DCQA dataset introduced in this paper, encompassing various elements such as chart image, picture, textual content, list, caption, header, footer, and page number, the document layout analysis model of the proposed ToT-Doctor framework was trained.\n148. The conclusive results of the document layout analysis testing on the test set are presented in Table5.\n149. Notably, the detection accuracy for chart image reached an impressive 99.901%, exhibiting a near-complete precision in accurately identifying their respective spatial position.\n150. This achievement serves as a robust foundational prerequisite for facilitating subsequent stage of chart question answering task.\n151. The experiment results of our proposed TOT-Doctor and other baselines are displayed in Table6.\n152. Due to the incapacity of the baseline to perform document layout analysis, we add our document layout analysis framework to them before conduct chart question answering.\n153. Based on the data listed in Table6, it can be seen that TOT-Doctor consistently surpasses its counterparts concerning the accuracy in both the validation and test sets, corroborating the efficacy of TOT-Doctor.\n154. It is noteworthy that despite LayoutLMv2 and LayoutLMv3 being reliant on OCR for obtaining the answers, their performance continues to lag behind our OCR-free TOT-Doctor.\n155. This observation proves the robustness and OCR error mitigation capabilities of the TOT-Doctor proposed in this study.\n156. Furthermore, in comparison to the latest state-of-the-art (SOTA) pre-trained visual language understanding model Pix2Struct, TOT-Doctor demonstrates superior performance, achieving a significant improvement of approximately 21.171% on the development dataset, and a respectable enhancement of 20.796% on the test dataset, respectively.\n157. The outstanding performance of our TOT-Doctor model underscores the significance of integrating vision and language features in an OCR-free manner to address the questions posed in DCQA effectively.\n158. The DCQA dataset incorporates five distinct question levels.\n159. In order to better discern the effectiveness of our proposed TOT-Doctor and other baselines, we conduct a performance analysis on each question level.\n160. The results of our analysis are presented in Table7for reference.\n161. It is evident from the results that TOT-Doctor consistently outperforms other baselines in all five levels of questions.\n162. Notably, TOT-Doctor exhibits superior performance on intermediatelevel questions, demonstrating its efficacy in directly applying specific operations such as identifying maximum, minimum, median, mean, and sum to chart elements or analyzing the differences of chart elements based on their color, legend, or numerical value.\n163. However, when encountering the subdifficult advanced-level questions involving composite oper-ations and the most challenging expert-level questions that necessitate commonsense understanding, the performance of all baselines, including TOT-Doctor, considerably decreases compared to the other three more tractable question levels.\n164. This implies that the ability for complex reasoning and commonsense understanding still requires further improvement for analyzing complex documents.\n165. Fig.6: Performance comparison between common sense and numerical reasoning questions on DCQA test set.\n166. We conduct additional experiments to evaluate the performance of the TOT-Doctor and baseline models on different question types.\n167. As discussed before, DCQA comprises two primary question types: visual and numeric reasoning and commonsense reasoning.\n168. Figure6presents the results of all baselines for each question type on the test set.\n169. Our proposed TOT-Doctor outperforms other baselines significantly, particularly in numerical reasoning questions.\n170. To top it all off, TOT-Doctor demonstrates more proficiency in numerical reasoning compared to commonsense understanding.\n171. To further investigate our TOT-Doctor model, we assess the ability of the TOT-Doctor to generate different answer types.\n172. From Table8, we discover that except for LayoutLMv2, all other baselines perform better in answering Yes/No questions.\n173. We observe that LayoutLMv2 and LayoutLMv3 exhibit frustrating performance in generating numerical and string answers.\n174. We speculate that this is mainly because LayoutLMv2 and LayoutLMv3 are extractive models, which means that they cannot generate answers that have not appeared in the document.\n175. This precisely explains their poor performance on the DCQA dataset, where the answers are largely obtained through data reasoning and involve numerical values.\n176. It is noted that EasyOCR 3 is utilized as the OCR system for these two baselines, which deviates from the OCR employed in their original versions.\n177. Based on this observation, we posit that the accuracy of the OCR system may have contributed to the subpar performance of the models.\n178. Moreover, TOT-Doctor is well versed in generating answers in terms of numerical or string, which verifies the robustness of TOT-Doctor.\n179. However, overall, all baselines achieve abysmal accuracy in generating numerical and string answers, highlighting the significant challenge posed by document-level chart understanding, which calls for further research efforts.\n\n请严格按照JSON格式返回：{"labels": [0,1,0,...]}，不要包含任何其他文字。'}], 'model': 'qwen-plus', 'response_format': {'type': 'json_object'}}}
2025-10-14 21:11:09,540 - DEBUG - Sending HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
2025-10-14 21:11:09,540 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-10-14 21:11:09,541 - DEBUG - send_request_headers.complete
2025-10-14 21:11:09,541 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-10-14 21:11:09,541 - DEBUG - send_request_body.complete
2025-10-14 21:11:09,541 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-10-14 21:11:16,760 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'vary', b'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding'), (b'x-request-id', b'0bd10ed4-8e8e-487c-989e-f46a58b7f641'), (b'x-dashscope-call-gateway', b'true'), (b'content-type', b'application/json'), (b'req-cost-time', b'7176'), (b'req-arrive-time', b'1760447459657'), (b'resp-start-time', b'1760447466834'), (b'x-envoy-upstream-service-time', b'7129'), (b'content-encoding', b'gzip'), (b'date', b'Tue, 14 Oct 2025 13:11:06 GMT'), (b'server', b'istio-envoy'), (b'transfer-encoding', b'chunked')])
2025-10-14 21:11:16,761 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-14 21:11:16,762 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-10-14 21:11:16,762 - DEBUG - receive_response_body.complete
2025-10-14 21:11:16,763 - DEBUG - response_closed.started
2025-10-14 21:11:16,763 - DEBUG - response_closed.complete
2025-10-14 21:11:16,763 - DEBUG - HTTP Response: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "200 OK" Headers({'vary': 'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding', 'x-request-id': '0bd10ed4-8e8e-487c-989e-f46a58b7f641', 'x-dashscope-call-gateway': 'true', 'content-type': 'application/json', 'req-cost-time': '7176', 'req-arrive-time': '1760447459657', 'resp-start-time': '1760447466834', 'x-envoy-upstream-service-time': '7129', 'content-encoding': 'gzip', 'date': 'Tue, 14 Oct 2025 13:11:06 GMT', 'server': 'istio-envoy', 'transfer-encoding': 'chunked'})
2025-10-14 21:11:16,763 - DEBUG - request_id: 0bd10ed4-8e8e-487c-989e-f46a58b7f641
2025-10-14 21:11:16,764 - DEBUG - API request completed in 7.23 seconds
2025-10-14 21:11:16,764 - DEBUG - Raw model response: {"labels": [0,0,0,0,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,0,0,1,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}
2025-10-14 21:11:16,764 - INFO - Successfully processed 130 labels
2025-10-14 21:11:16,764 - ERROR - Label count mismatch for DCQA_ DOCUMENT_LEVEL CHART QUESTION ANSWERING TOWARDS COMPLEX REASONING AND COMMON_SENSE UNDERSTANDING
2025-10-14 21:11:16,764 - INFO - Evaluating paper 10/18: Debate Helps Supervise Unreliable Experts
2025-10-14 21:11:16,765 - INFO - Starting model prediction
2025-10-14 21:11:16,765 - INFO - Attempt 1 of 5
2025-10-14 21:11:16,766 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-cbae3fde-e3ac-4b17-b5e0-44d72d90eb29', 'json_data': {'messages': [{'role': 'system', 'content': '你是一个专门用于识别学术论文中数据集描述的助手。你需要判断每个输入的句子是否描述了研究中使用的数据集。请确保：\n    1. 只输出JSON格式的结果\n    2. 结果格式必须为 {"labels": [0,1,0,...]}\n    3. 不要输出任何其他文字或解释'}, {'role': 'user', 'content': '你是一位专业的学术论文数据集描述识别专家。请仔细分析下面这篇论文中的每个句子，判断它们是否描述了本研究特定构建的数据集信息。\n\n    论文标题: Debate Helps Supervise Unreliable Experts\n\n    判断标准:\n    判断为1(数据集描述)的标准：\n    句子必须直接描述本研究特定构建的数据集,包括:\n    1. 该数据集的构建过程和方法\n    2. 该数据集的具体构成和规模\n    3. 该数据的来源和收集方式\n    4. 该数据的预处理步骤\n    5. 该数据集的可获取方式(如发布地址)\n    6. 或者其他直接或者间接描述了该数据集的句子\n\n    所有其他类型的句子均标记为0。\n    \n    请对每个句子进行分析，返回一个JSON格式的标签数组。格式要求：\n    1. 必须是有效的JSON格式\n    2. 只包含labels字段，值为0和1组成的数组\n    3. 数组长度必须与句子数量相同\n    4. 示例格式：{"labels": [0,1,0,1,...]}\n\n    需要判断的句子：\n\n    1. Abstract: As AI systems are used to answer more difficult questions and potentially help create new knowledge, judging the truthfulness of their outputs becomes more difficult and more important.\n2. How can we supervise unreliable experts-which have access to the truth but may not accurately report it-to give answers that are systematically true and don\'t just superficially seem true, when the supervisor can\'t tell the difference between the two on their own?\n3. In this work, we show that debate between two unreliable experts can help a non-expert judge more reliably identify the truth.\n4. We collect a dataset of human-written debates on hard reading comprehension questions where the judge has not read the source passage, only ever seeing expert arguments and short quotes selectively revealed by \'expert\' debaters who have access to the passage.\n5. In our debates, one expert argues for the correct answer, and the other for an incorrect answer.\n6. Comparing debate to a baseline we call consultancy, where a single expert argues for only one answer which is correct half of the time, we find that debate performs significantly better, with 84% judge accuracy compared to consultancy\'s 74%.\n7. Debates are also more efficient, being 68% of the length of consultancies.\n8. By comparing human to AI debaters, we find evidence that with more skilled (in this case, human) debaters, the performance of debate goes up but the performance of consultancy goes down.\n9. Our error analysis also supports this trend, with 46% of errors in human debate attributable to mistakes by the honest debater (which should go away with increased skill); whereas 52% of errors in human consultancy are due to debaters obfuscating the relevant evidence from the judge (which should become worse with increased skill).\n10. Overall, these results show that debate is a promising approach for supervising increasingly capable but potentially unreliable AI systems.\n11. How can we tell if an AI system is telling the truth?\n12. Current language models trained to act as AI assistants, such asGPT-4 (OpenAI, 2023a)and Claude(Anthropic, 2023b,a) can correctly answer a wide variety of questions, construct coherent essays, and perform well on academic and professional exams(Hendrycks et al., 2020;OpenAI, 2023b).\n13. But the truthfulness of their responses is not robust: Such systems are prone to making false claims, giving misleading explanations about their reasoning Figure1: High-level summary of our experimental setup.\n14. We source hard reading comprehension questions from the QuALITY dataset(Pang et al., 2022)and incentivize human judges who can\'t read the passage to answer them correctly.\n15. Experts who have full access to the passage are allowed to reveal snippets of it (highlighted) in addition to free-text prose.\n16. In debate, the experts simultaneously defend their assigned option in their opening statements, and following rounds are sequential.In consultancy, the non-expert judge only interacts with one expert defending one option chosen at random.\n17. In both settings, the judge chooses when to end the session; sessions average at about 1,000 words total.(Turpin et al., 2023), and reinforcing the inferred opinions of their interlocutors(Perez et al., 2022;Bang et al., 2023;Borji, 2023).\n18. Language models have access to a vast array of information from their training data to draw on and synthesize-far beyond the knowledge of any individual human who might be involved in supervising them.\n19. As such, they could hold the potential to help us answer increasingly difficult questions or even create new knowledge that we otherwise couldn\'t.\n20. However, we expect that it will be increasingly hard to verify and supervise the truthfulness of their outputs in these cases.\n21. As language models become more capable and are used in more complex settings, it is likely that subtle mistakes, deceptive arguments, or selective use of evidence will become more difficult to spot.\n22. Making sure the information they provide is reliable requires effective methods for verifying the outputs of systems that know things we don\'t-a task known as scalable oversight(Amodei et al., 2016).\n23. Proposals for scalable oversight often involve leveraging the AI\'s abilities to help evaluators, for example with recursive reward modeling(Leike et al., 2018), model self-critique(Saunders et al., 2022), and debate(Irving et al., 2018).\n24. In debate-the focus of this work-two equally-capable expert debaters (e.g., AI systems) argue with each other over the answer to a question, each aiming to convince a non-expert (human) judge of their side.\n25. With an adversarial expert pointing out flaws in its arguments, neither debater will be able to get away with claims that its opponent can convincingly refute in the eyes of the judge.\n26. Training AI systems to win such debates should incentivize them not to make such claims in the first place.\n27. AsIrving et al. (2018)argue, this means that debate would incentivize an AI to tell the truth, as long as it is harder to lie than to refute a lie-i.e., the most successful strategies for debate lead judges to make good, informed decisions, rather than, for example, tricking them, confusing them, or prolonging the debate indefinitely.\n28. In this paper, we demonstrate for the first time that debate helps judges find truth on a realistic task, using debates on hard reading comprehension questions.\n29. To test this, we compare debate to a baseline we call consultancy, where the judge interacts with a single unreliable expert who has a 50/50 chance of arguing for the correct answer.\n30. By prompting the consultant to argue for the wrong answer half of the time, this baseline explicitly elicits dishonest behavior which may arise implicitly in Reinforcement Learning from Human Feedback (RLHF), as in cases, e.g., of sycophancy(Perez et al., 2022).\n31. To evaluate this with the strongest possible debaters, we collect and analyze a dataset of all-human debates, enlisting competitive debaters from the New York University debate team.\n32. A high-level overview of our setup is illustrated in Figure1.2For each debate, we pose a reading comprehension question from the QuALITY dataset(Pang et al., 2022)together with two answer choices (one correct, one incorrect), and allow the debaters-but not the judge-to read the story the question is about.\n33. The judge then interactively judges a debate on the question, where the debaters can back up their claims by selectively revealing short excerpts drawn from the story.\n34. Judge accuracy in these debates is 84%, compared to with 74% on consultancy (Section 4).\n35. Debate is also more efficient, being 68% of the length and requiring 61% as much ground-truth evidence, suggesting that it will be a more effective method than open-ended dialogue (cf.Bowman et al., 2022)for helping annotators efficiently supervise untrusted models that exceed their expertise.\n36. We also find that our judges are relatively calibrated overall on debates, though they struggle with overconfidence in the high-confidence regime (Figure5).\n37. While there are still cases when the judge of a debate gets the answer wrong, we find that the most common sources of error should be possible to mitigate with further judge training or stronger debaters.\n38. For example, in 33% of mistakes, the judge ended the session prematurely, either after only a single round or immediately after changing their preferred answer, giving the debaters no opportunity to refute the judge\'s final reasoning.\n39. In 46% of mistakes, the debater arguing for the correct answer missed an important argument or piece of evidence that they could have used (Section 5).\n40. We also include experimental results for AI debate, using GPT-4 as a debater (Section 4).\n41. In this setting, we find no difference between debate and consultancy.\n42. However, even if debate does not work better as an oversight method for current models, that may simply be because they have not yet reached human-level capabilities at deception (i.e., as a consultant) and argumentation (as a debater); it is also possible that we do not optimize GPT-4\'s prompt heavily enough to elicit such capabilities.\n43. It seems plausible that AI systems may soon be capable enough of argumentation and persuasion that debate will be important to incorporate into their training; in Section 8 we lay out an agenda for what this may look like, and what challenges will need to be solved to make this work.\n44. As we use AI systems in more difficult and complex settings, we will need stronger mechanisms to verify their arguments-ideally, methods which improve concordantly and at pace with the system\'s capabilities.\n45. Our results with human debaters demonstrate for the first time that debate, where equally-capable experts point out flaws with each other\'s arguments, can allow a non-expert judge to effectively determine the answers to questions they could not answer on their own.\n46. This suggests that debate may soon be important for effectively supervising models to truthfully answer hard questions.\n47. Source Material and Questions We draw the questions to be debated from the Question Answering with Long Input Texts, Yes!\n48. (QuALITY) dataset of reading comprehension questions(Pang et al., 2022).\n49. 4To focus on especially hard questions, we further restrict our results to questions that were marked by the untimed annotators as requiring more than one or two sentences of context to get correct (the idea being to avoid questions which could be easily resolved with a single quote from one of the debaters).\n50. Each question in QuALITY has four answers, one of which is correct; as our debates consider only two answer choices, we use the correct answer and the incorrect option that was labeled as the best distractor most often by the QuALITY dataset\'s untimed validation annotators.\n51. We only use the Project Gutenberg subset of QuALITY-hard, which consists of questions over public-domain science fiction short stories.\n52. Since the stories are entirely fictional, judges can almost never guess the answer on the basis of prior knowledge, and must rely on the information provided by the debaters.5.\n53. On average, the stories used for our debates have 27.7k characters, or 6.3k tokens using the CoreNLP tokenizer(Manning et al., 2014).\n54. For each turn, we use a character limit of ℓ  = 750 and a quote limit of ℓ  = 250, meaning that on average up to 1.8% of the story could be revealed in each round of the debate.\n55. Experimental Conditions While our main experimental results concern human debaters, we also test with AI debaters.\n56. As the AI debater, we use the version of GPT-4 with a 32,000-token context window available through the OpenAI API as gpt-4-32k.\n57. Prompts are provided in Appendix G.\n58. 6This gives us four experimental conditions: human debate, human consultancy, AI debate, and AI consultancy.\n59. We recruit 19 people to serve as both debaters and judges in our experiments.\n60. Our participants, all of whom were New York University employees during data collection, include 12 undergraduates on the NYU debate team, all with at least one year of experience in competitive debate; 6 members of the research team, three of whom have at least one year of experience with competitive debate; and one NYU Master\'s student with 6 years of experience studying Jewish legal reasoning and argumentation.\n61. After running initial pilots in Fall of 2022 to establish the debate protocol (see Appendix B), we collect debates according to the protocol defined in Section 2, with collection running from February to August of 2023.\n62. During collection, participants can log into our data collection platform to read stories or take turns in their debates at any time, but we also set aside specific times each week when we request that the debaters work, to facilitate near-synchronous debates.\n63. To avoid information leakage between debates, each participant is only allowed to judge one question about each story.\n64. After each debate is complete, all participants fill out a feedback survey with quantitative and qualitative observations which we use to help us analyze the results (see Appendix F).\n65. Participants cannot see the identities of the other participants in the debate until after filling out the feedback form.\n66. Data collection was not perfectly controlled between our four experimental conditions, as some components of our experimental design were developed part of the way through data collection: The consultancy baseline was only developed in June of 2023 and the AI debaters were only incorporated 4We ended up drawing 59% of our questions from the QuaLITY training set, which has 3 untimed validators per question, and 41% from the development set, which has 5 untimed validators per question.\n67. 5In our data, judge priors were between 45%-55% in 91% of debates, and between 35%-65% in 97% of debates.\n68. 6Because the QuALITY questions are drawn from public-domain texts available from Project Gutenberg, it is likely that the passages used in our experiments appear in GPT-4\'s training corpus.\n69. This does not pose a data contamination issue when using GPT-4 as a debater, since debaters are meant to be experts and are given full access to the text anyway.\n70. However, this does could pose issues for future work testing AI judging in our setting, since it might be difficult to guarantee that the models do not use prior knowledge of the story in their decisions, instead of relying on the debate.\n71. Quotes and characters per round measure how close debaters came to their character limits; quote totals are calculated only using new quoted material that hasn\'t been used yet in the debate.\n72. Bits/rd is the amount of information conveyed to the judge on average per round, calculated from the information gain between their final and initial judgment log 2 (  * , / * ,0 ),   is the judge\'s score as defined in Equation1, and ECE final is the expected calibration error of the judge\'s final judgments, calculated with a bin size of 10%. into the data collection platform in July.\n73. The set of debaters who participated in the experiment also varied over the course of these months.\n74. These factors were due to us trying to collect as much data as possible subject to practical limits on engineering capacity, annotator availability, and researcher foresight.\n75. We validate our analysis in Section 4 with partial controls in Appendix D.\n\n请严格按照JSON格式返回：{"labels": [0,1,0,...]}，不要包含任何其他文字。'}], 'model': 'qwen-plus', 'response_format': {'type': 'json_object'}}}
2025-10-14 21:11:16,769 - DEBUG - Sending HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
2025-10-14 21:11:16,769 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-10-14 21:11:16,769 - DEBUG - send_request_headers.complete
2025-10-14 21:11:16,769 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-10-14 21:11:16,769 - DEBUG - send_request_body.complete
2025-10-14 21:11:16,769 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-10-14 21:11:21,600 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'vary', b'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding'), (b'x-request-id', b'd32df855-c34f-4225-a410-96262167104c'), (b'x-dashscope-call-gateway', b'true'), (b'content-type', b'application/json'), (b'req-cost-time', b'4788'), (b'req-arrive-time', b'1760447466886'), (b'resp-start-time', b'1760447471674'), (b'x-envoy-upstream-service-time', b'4743'), (b'content-encoding', b'gzip'), (b'date', b'Tue, 14 Oct 2025 13:11:11 GMT'), (b'server', b'istio-envoy'), (b'transfer-encoding', b'chunked')])
2025-10-14 21:11:21,601 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-14 21:11:21,601 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-10-14 21:11:21,602 - DEBUG - receive_response_body.complete
2025-10-14 21:11:21,602 - DEBUG - response_closed.started
2025-10-14 21:11:21,602 - DEBUG - response_closed.complete
2025-10-14 21:11:21,602 - DEBUG - HTTP Response: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "200 OK" Headers({'vary': 'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding', 'x-request-id': 'd32df855-c34f-4225-a410-96262167104c', 'x-dashscope-call-gateway': 'true', 'content-type': 'application/json', 'req-cost-time': '4788', 'req-arrive-time': '1760447466886', 'resp-start-time': '1760447471674', 'x-envoy-upstream-service-time': '4743', 'content-encoding': 'gzip', 'date': 'Tue, 14 Oct 2025 13:11:11 GMT', 'server': 'istio-envoy', 'transfer-encoding': 'chunked'})
2025-10-14 21:11:21,602 - DEBUG - request_id: d32df855-c34f-4225-a410-96262167104c
2025-10-14 21:11:21,603 - DEBUG - API request completed in 4.84 seconds
2025-10-14 21:11:21,603 - DEBUG - Raw model response: {"labels": [0,0,0,1,1,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]}
2025-10-14 21:11:21,604 - INFO - Successfully processed 82 labels
2025-10-14 21:11:21,604 - ERROR - Label count mismatch for Debate Helps Supervise Unreliable Experts
2025-10-14 21:11:21,604 - INFO - Evaluating paper 11/18: Distilling Large Language Models for Matching Patients to Clinical Trials
2025-10-14 21:11:21,604 - INFO - Starting model prediction
2025-10-14 21:11:21,604 - INFO - Attempt 1 of 5
2025-10-14 21:11:21,606 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-6604dfca-cfba-4636-84a2-76490d312fd8', 'json_data': {'messages': [{'role': 'system', 'content': '你是一个专门用于识别学术论文中数据集描述的助手。你需要判断每个输入的句子是否描述了研究中使用的数据集。请确保：\n    1. 只输出JSON格式的结果\n    2. 结果格式必须为 {"labels": [0,1,0,...]}\n    3. 不要输出任何其他文字或解释'}, {'role': 'user', 'content': '你是一位专业的学术论文数据集描述识别专家。请仔细分析下面这篇论文中的每个句子，判断它们是否描述了本研究特定构建的数据集信息。\n\n    论文标题: Distilling Large Language Models for Matching Patients to Clinical Trials\n\n    判断标准:\n    判断为1(数据集描述)的标准：\n    句子必须直接描述本研究特定构建的数据集,包括:\n    1. 该数据集的构建过程和方法\n    2. 该数据集的具体构成和规模\n    3. 该数据的来源和收集方式\n    4. 该数据的预处理步骤\n    5. 该数据集的可获取方式(如发布地址)\n    6. 或者其他直接或者间接描述了该数据集的句子\n\n    所有其他类型的句子均标记为0。\n    \n    请对每个句子进行分析，返回一个JSON格式的标签数组。格式要求：\n    1. 必须是有效的JSON格式\n    2. 只包含labels字段，值为0和1组成的数组\n    3. 数组长度必须与句子数量相同\n    4. 示例格式：{"labels": [0,1,0,1,...]}\n\n    需要判断的句子：\n\n    1. Abstract: The recent success of large language models (LLMs) has paved the way for their adoption in the high-stakes domain of healthcare.\n2. Specifically, the application of LLMs in patient-trial matching, which involves assessing patient eligibility against clinical trial\'s nuanced inclusion and exclusion criteria, has shown promise.\n3. Recent research has shown that GPT-3.5, a widely recognized LLM developed by OpenAI, can outperform existing methods with minimal \'variable engineering\' by simply comparing clinical trial information against patient summaries.\n4. However, there are significant challenges associated with using closed-source proprietary LLMs like GPT-3.5 in practical healthcare applications, such as cost, privacy and reproducibility concerns.\n5. To address these issues, this study presents the first systematic examination of the efficacy of both proprietary (GPT-3.5, and GPT-4) and open-source LLMs (LLAMA 7B,13B, and 70B) for the task of patient-trial matching.\n6. Employing a multifaceted evaluation framework, we conducted extensive automated and human-centric assessments coupled with a detailed error analysis for each model.\n7. To enhance the adaptability of open-source LLMs, we have created a specialized synthetic dataset utilizing GPT-4, enabling effective fine-tuning under constrained data conditions.\n8. Our findings reveal that open-source LLMs, when fine-tuned on this limited and synthetic dataset, demonstrate performance parity with their proprietary counterparts.\n9. This presents a massive opportunity for their deployment in real-world healthcare applications.\n10. To foster further research and applications in this field, we release both the annotated evaluation dataset along with the fine-tuned LLM -Trial-LLAMA -for public use.\n11. Clinical trials represent both the most important and the most challenging aspect of medical advancements.\n12. These trials serve a dual function: first, as a conduit for patients to access potentially life-altering treatments, and second, as a mechanism for the iterative process of drug development and approval.However, a significant number of trials are beleaguered by extended timelines.\n13. Empirical data suggests that, on average, clinical trials take approximately twice as long as initially projected[22], with approximately 40% of trial sites failing to meet their enrollment targets[15].\n14. Apart from others, one of the major challenges in recruiting patients is matching them against suitable trials[5,20,6,26,12,28,19,29].\n15. The process of matching a patient to trials is challenging.\n16. It requires both the meticulous analysis of electronic health records (EHRs) and the contextual interpretation of this data against the backdrop of clinical trial criteria.\n17. This is particularly challenging because a majority of this data is stored in unstructured documents written in free text.\n18. Even the structured data is difficult to query due to the increasing complexity of inclusion and exclusion criteria.\n19. Automating this process can accelerate trials save healthcare providers\' time spent on manual chart reviews.\n20. Current approaches primarily rely on data extraction or classification pipelines[36,49,27].\n21. Nonetheless, these methods require extensive variable engineering, which frequently results in constrained contextual comprehension and limited scalability when dealing with intricate trial criteria.\n22. The emergence of Large Language Models (LLMs), such as Med-PaLM[37]and GPT-4[25], marks a paradigm shift in the domain of automated interpretation of patient health records.\n23. These models embody the cutting-edge in natural language processing (NLP), facilitating nuanced and context-aware analysis of complex medical data.\n24. Leveraging their capabilities, recent research has used these models for a variety of clinical information interpretation tasks, including patient matching[18].\n25. However, their deployment in healthcare settings presents challenges.\n26. One primary concern relates to the risk of Protected Health Information (PHI) leakage when using such models.\n27. Most healthcare organisations prefer on-premise infrastructure for tools that handle identified patient data.\n28. However, due to the cost and computational complexity associated with these models, they often remain in centralized cloud provider environments.\n29. These challenges can make LLMs prohibitive for widespread clinical application.\n30. Moreover, despite their effectiveness, advanced models are often characterized by opacity and proprietary restrictions, which further complicate their integration into healthcare systems subject to stringent regulatory constraints.\n31. In light of these considerations, there is a growing need for the development of open-source LLMs that can match the accuracy of their proprietary counterparts but at a significantly reduced cost.\n32. This also enables healthcare organizations to seamlessly integrate these technologies into their existing infrastructures, mitigating the risk of Protected Health Information (PHI) leaks.\n33. To the best of our knowledge, this study is the first comprehensive examination of the efficacy of open-source LLMs in this domain.\n34. • Our work thoroughly compares open-source LLMs and their proprietary counterparts for patienttrial matching.\n35. • We further explore and elucidate the impact of fine-tuning on various open-source LLMs for patient-trial matching.\n36. • We define the error taxonomy and thoroughly analyze the nature of errors made by the models on this task.\n37. • Along with the experimental details, we publicly release the evaluation dataset and the LLM trained based on LLAMA for patient-trial matching.\n38. We tested both proprietary (GPT-3.5, and GPT-4) and open-source LLMs (LLAMA-2 7B,13B, and 70B, referred to LLAMA hereafter) for the task of patient-trial matching.\n39. For GPT-3.5, we leveraged the Azure Open AI API, specifically gpt-35-turbo-16k-0613 as the model version.\n40. We set the temperature parameter to 0, aiming for deterministic outputs that would ensure consistency and repeatability in our experiments.\n41. This was coupled with a top p setting of 0.95, aligning with our goal to eliminate randomness in the model\'s response generation process.\n42. Additionally, we refrained from applying any frequency or repetition penalties, allowing the model\'s natural language generation capabilities to function without these constraints.\n43. For GPT-4, we employed a similar configuration with gpt-4-0613 as the model version.\n44. For LLAMA, we changed the configuration from Meta.\n45. We initially encountered challenges in aligning the standard versions of these models to produce outputs in the required format, particularly in the context of complex clinical trial criteria.\n46. To address this, we opted for specific versions tailored for chat applications, namely Llama-2-7b-chat-hf, Llama-2-13b-chat-hf, and Llama-2-70b-chat-hf.\n47. These versions offered a more flexible and adaptable framework for our needs.\n48. We adjusted the temperature setting to 0.4 for all LLAMA models, a decision informed by preliminary tests which indicated that a slightly higher temperature prevented the model from collapsing on certain trials where inclusion/exclusion criteria were not clearly defined.\n49. Maintaining the output format was particularly challenging when working with the base LLAMA models.\n50. Despite employing various techniques such as context-free grammar (CFG) to constrain the model\'s output, the results remained suboptimal.Consequently, the models were unable to generate structured output for complex clinical trials.\n51. To circumvent this, we adjusted the model\'s temperature to foster more exploratory behavior and executed five output generations iteratively till the output matched our JSON schema.\n52. This allowed us to generated structured output for majority of clinical trials even with base models.\n53. Compute Novelty Index: noveltyIndex ← 1 -score τ 6: if noveltyIndex > 0 then end for 10: end for 11: return C novel Each criterion in C final is then annotated with a gold-standard answer and corresponding evidences.\n54. These evidential references serve as a basis for language models to substantiate their answers.\n55. To gauge the faithfulness of various models in accurately citing these pieces of evidence, we calculate precision, recall, and F1 scores for each model.\n56. Additionally, we conduct a direct comparison of model performance at the criterion level to evaluate their relative effectiveness.\n57. Different from the metrics used in[18], we created two distinct aspects of Criterion-Level Accuracy (CLA), namely Explicit CLA and Implicit CLA, to holistically assess the model\'s performance.\n58. For Explicit CLA, our focus is on evaluating how accurately the model categorizes each criterion into the correct class, provided that the criterion has been previously identified as \'explicit\' in our manual annotation exercise.\n59. This evaluation primarily concerns criteria for which the necessary information for classification is clearly and directly stated in the patient documentation, leaving minimal room for interpretation or inference.\n60. The accuracy here reflects the model\'s ability to comprehend and correctly apply these straightforward, unambiguous data points.\n61. On the other hand, Implicit CLA tackles a more nuanced challenge: it assesses the model\'s performance on criteria deemed \'implicit\' by the annotators.\n62. These criteria involve situations where the required information is not explicitly stated but rather implied or inferred from the available data.\n63. This often requires connecting disparate pieces of information, understanding subtleties and nuances in the patient data, and making educated guesses based on the context.\n64. Calculating the Implicit CLA involves a thorough analysis of how well the model navigates these complexities and accurately classifies criteria based on less direct information.\n65. Both Explicit and Implicit CLAs are pivotal in understanding the model\'s overall capability to process and interpret clinical trial criteria.\n66. While Explicit CLA provides insight into the model\'s proficiency with clear-cut, straightforward tasks, Implicit CLA sheds light on its ability to handle ambiguity and complexity -crucial aspects in the realm of clinical data interpretation.\n67. For our dataset, we adopted the similar datasets as used in[18]that incorporate the SIGIR dataset[21]and both the 2021 and 2022 versions of the TREC CT cohorts[34,33], as shown in Table1.\n68. For each patient within these datasets, we extract 50 clinical trials, categorizing them into three distinct classifications: "eligible", "excluded", and "irrelevant".\n69. The categorization within the SIGIR dataset required a different approach, given its classification system.\n70. (a) "Will not refer to the trial": This class aligns with the \'irrelevant\' category in our study.\n71. (b) "Will refer to the trial": Corresponds to the \'eligible\' category.\n72. (c) "May refer to the trial": This class does not map directly to any of our predefined categories.\n73. Due to this non-conformity, we excluded all trial-patient combinations classified under (c) "May refer to the trial", to maintain consistency.\n74. To facilitate the fine-tuning of our models, we partition the dataset into a training and test set, adhering to an 80:20 ratio.\n75. This division is implemented along the patient axis to ensure no test patient record gets leaked into the training set.\n76. Prior to splitting, all datasets are combined and thoroughly shuffled.\n77. The specifics of the training and test sets are displayed in Table1.\n78. It is noteworthy that despite the large volume of records in the training set, they are not fully utilized for model training.\n79. Instead, the large size of this set provides with an easy mechanism to sample diverse training samples for fine-tuning while also allowing us to save on compute costs associated with evaluation a large number of model checkpoints.\n80. Evidently, as shown in Table1the sampled dataset is more diverse than the training set.\n81. It is known that the performance of a model improves with the volume of data it is exposed to[16].\n82. Nevertheless, the quality of data plays a pivotal role in determining the output\'s caliber.\n83. Multiple research works have demonstrated that while the fine-tuning performance of a model initially improves rapidly, it tends to reach a saturation point beyond a certain threshold of data exposure [?].\n84. This phenomenon is consistent with our findings with the fine-tuning of the LLAMA models of different sizes.\n85. As illustrated in Figure5b, the performance of all three LLAMA variants exhibits a significant initial leap with exposure to a small data subset, followed by a gradual enhancement as they are introduced to an increasing number of examples.\n86. Notably, the largest LLAMA variant swiftly surpasses the performance of GPT-3.5.\n87. For the assessment of model performance, we utilize the metric of overall criteria level accuracy, encompassing both implicit and explicit criteria.\n88. The process of fine-tuning LLMs presents both computational and methodological challenges, primarily due to the difficulty in providing a dense, multi-token signal that these models require for effective learning.\n89. While labeling for classification tasks typically involves single-token signals, enhancing model performance necessitates the provision of multi-token feedback, which is inherently more complex to curate due to its diversity and volume requirements.\n90. Despite these challenges, our experiments demonstrate that distillation techniques that have been used to enhance the dialogue capabilities of different models[46,43]can be used for the task of patient matching as well.\n91. This method significantly reduces the necessity for manually crafted examples, thereby streamlining the fine-tuning process and making it more affordable.\n\n请严格按照JSON格式返回：{"labels": [0,1,0,...]}，不要包含任何其他文字。'}], 'model': 'qwen-plus', 'response_format': {'type': 'json_object'}}}
2025-10-14 21:11:21,608 - DEBUG - Sending HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
2025-10-14 21:11:21,608 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-10-14 21:11:21,608 - DEBUG - send_request_headers.complete
2025-10-14 21:11:21,608 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-10-14 21:11:21,608 - DEBUG - send_request_body.complete
2025-10-14 21:11:21,608 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-10-14 21:11:28,389 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'vary', b'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding'), (b'x-request-id', b'2017b48e-676e-4ccf-9068-c52d42f65736'), (b'x-dashscope-call-gateway', b'true'), (b'content-type', b'application/json'), (b'req-cost-time', b'6738'), (b'req-arrive-time', b'1760447471724'), (b'resp-start-time', b'1760447478463'), (b'x-envoy-upstream-service-time', b'6694'), (b'content-encoding', b'gzip'), (b'date', b'Tue, 14 Oct 2025 13:11:18 GMT'), (b'server', b'istio-envoy'), (b'transfer-encoding', b'chunked')])
2025-10-14 21:11:28,390 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-14 21:11:28,390 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-10-14 21:11:28,390 - DEBUG - receive_response_body.complete
2025-10-14 21:11:28,391 - DEBUG - response_closed.started
2025-10-14 21:11:28,391 - DEBUG - response_closed.complete
2025-10-14 21:11:28,391 - DEBUG - HTTP Response: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "200 OK" Headers({'vary': 'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding', 'x-request-id': '2017b48e-676e-4ccf-9068-c52d42f65736', 'x-dashscope-call-gateway': 'true', 'content-type': 'application/json', 'req-cost-time': '6738', 'req-arrive-time': '1760447471724', 'resp-start-time': '1760447478463', 'x-envoy-upstream-service-time': '6694', 'content-encoding': 'gzip', 'date': 'Tue, 14 Oct 2025 13:11:18 GMT', 'server': 'istio-envoy', 'transfer-encoding': 'chunked'})
2025-10-14 21:11:28,391 - DEBUG - request_id: 2017b48e-676e-4ccf-9068-c52d42f65736
2025-10-14 21:11:28,392 - DEBUG - API request completed in 6.79 seconds
2025-10-14 21:11:28,392 - DEBUG - Raw model response: {"labels": [0,0,0,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}
2025-10-14 21:11:28,392 - INFO - Successfully processed 91 labels
2025-10-14 21:11:28,401 - ERROR - Error processing paper Distilling Large Language Models for Matching Patients to Clinical Trials: 'int' object has no attribute 'capitalize'
2025-10-14 21:11:28,401 - INFO - Evaluating paper 12/18: ECMD_ An Event_Centric Multisensory Driving Dataset for SLAM
2025-10-14 21:11:28,401 - INFO - Starting model prediction
2025-10-14 21:11:28,401 - INFO - Attempt 1 of 5
2025-10-14 21:11:28,401 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-1f0191f1-cc5d-467a-ab25-b036578ff68f', 'json_data': {'messages': [{'role': 'system', 'content': '你是一个专门用于识别学术论文中数据集描述的助手。你需要判断每个输入的句子是否描述了研究中使用的数据集。请确保：\n    1. 只输出JSON格式的结果\n    2. 结果格式必须为 {"labels": [0,1,0,...]}\n    3. 不要输出任何其他文字或解释'}, {'role': 'user', 'content': '你是一位专业的学术论文数据集描述识别专家。请仔细分析下面这篇论文中的每个句子，判断它们是否描述了本研究特定构建的数据集信息。\n\n    论文标题: ECMD_ An Event_Centric Multisensory Driving Dataset for SLAM\n\n    判断标准:\n    判断为1(数据集描述)的标准：\n    句子必须直接描述本研究特定构建的数据集,包括:\n    1. 该数据集的构建过程和方法\n    2. 该数据集的具体构成和规模\n    3. 该数据的来源和收集方式\n    4. 该数据的预处理步骤\n    5. 该数据集的可获取方式(如发布地址)\n    6. 或者其他直接或者间接描述了该数据集的句子\n\n    所有其他类型的句子均标记为0。\n    \n    请对每个句子进行分析，返回一个JSON格式的标签数组。格式要求：\n    1. 必须是有效的JSON格式\n    2. 只包含labels字段，值为0和1组成的数组\n    3. 数组长度必须与句子数量相同\n    4. 示例格式：{"labels": [0,1,0,1,...]}\n\n    需要判断的句子：\n\n    1. Abstract: Leveraging multiple sensors enhances complex environmental perception and increases resilience to varying luminance conditions and high-speed motion patterns, achieving precise localization and mapping.\n2. This paper proposes, ECMD, an event-centric multisensory dataset containing 81 sequences and covering over 200 km of various challenging driving scenarios including high-speed motion, repetitive scenarios, dynamic objects, etc. ECMD provides data from two sets of stereo event cameras with different resolutions (640×480, 346×260), stereo industrial cameras, an infrared camera, a top-installed mechanical LiDAR with two slanted LiDARs, two consumer-level GNSS receivers, and an onboard IMU.\n3. Meanwhile, the ground-truth of the vehicle was obtained using a centimeter-level high-accuracy GNSS-RTK/INS navigation system.\n4. All sensors are well-calibrated and temporally synchronized at the hardware level, with recording data simultaneously.\n5. We additionally evaluate several state-of-the-art SLAM algorithms for benchmarking visual and LiDAR SLAM and identifying their limitations.\n6. The dataset is available at https://arclab-hku.github.io/ecmd/.\n7. V ISUAL and LiDAR simultaneous localization and map- ping (SLAM) achieved notable progress within driving scenarios in recent years.\n8. However, they encounter the challenging task of operating robustly under heterogeneous environments, such as varying lighting conditions, lowtexture scenarios, repetitive structures, diverse motion patterns, dense dynamic objects, etc. Utilizing novel sensors and integrating multiple sensors can provide a comprehensive perception and enhance the robustness of the entire system[1]-[3].\n9. These motivate us to develop a dataset that integrates novel sensors under realistic and complex driving scenarios, thereby promoting SLAM research.\n10. Event cameras have low latency (µs-level) and high dynamic range (140 dB compared to 60 dB with standard cameras) properties, which offers great opportunities for visual (VO) and visual-inertial odometry (VIO) in rough terrain, aggressive motions, and high dynamic range (HDR)[4].\n11. Unlike traditional frame-based cameras that directly capture fixed-rate intensity frames, event cameras are motionactivated sensors that capture pixel-wise intensity differences asynchronously in continuous streams.\n12. However, the widespread commercialization and implementation of event cameras in robotics are still early due to the expensive cost.\n13. In addition, event cameras confront challenges during rapid vibrations and ego-motion, as these conditions generate a substantial quantity of events, leading to intensive computations.\n14. Conversely, in cases where minimal relative motion between the event camera and the scene exists, such as under static states, they only provide limited information or even introduce noise[5].\n15. Therefore, we embark on this research effort to explore the inquiry: Are event cameras ready for autonomous driving?\n16. There exist several stereo event-based driving datasets that are worth mentioning and exploring.\n17. MVSEC[6]was the first stereo event-based driving dataset proposed for evaluating the localization performance.\n18. While MVSEC employs the low resolution of DAVIS346 which limits the feature detection for accurate localization.\n19. DSEC[7]offers stereo event streams with a high resolution of 0.31 Megapixels(MP).\n20. However, this dataset focuses on computer vision tasks segmentation, depth estimation, optical flow estimation, etc., which is not specifically designed for VO/VIO/SLAM domains.\n21. MA-VIED[8]propose a largescale driving dataset under standard urban scenarios and race track-like loops.\n22. The ground-truth trajectory relies on GNSS-RTK, which only ensures high accuracy in open-sky environments and fails to provide high accuracy in GNSSdenied scenarios such as tunnels or densely street areas.\n23. [9]focuses on collecting both stereo event data and stereo intensity images under indoor and urban driving scenes with the ground-truth of GNSS-RTK/INS.\n24. Their sequences do not encompass extremely high-speed or repetitive scenarios that could be challenging to VO/VIO/SLAM algorithms.\n25. To address the above drawbacks, we propose ECMD, a dataset procured from diverse challenging driving scenarios with a comprehensive suite of sensors for benchmarking various VO/VIO/SLAM algorithms.\n26. To the best of our knowledge, this is the first event-based SLAM dataset specifically focused on densely urbanized driving scenarios.\n27. The contributions of our work can be summarized as follows: 1) Our sensor platform consists of various novel sensors shown in Fig.1, including two sets of stereo event cameras with distinct resolutions (640×480, 346×260), an infrared camera, stereo industrial cameras, three mechanical LiDARs (including two slanted LiDARs), a high-quality inertial measurement unit (IMU), and three global navigation satellite system (GNSS) receivers.\n28. 2) ECMD collects 81 sequences covering over 200 kilometers of trajectories in various driving scenarios, including dense streets, urban, tunnels, highways, bridges, and suburbs.\n29. These sequences are recorded under daylight and nighttime, providing challenging situations for Visual and LiDAR SLAM, e.g., dynamic objects, highspeed motion, repetitive scenarios, and HDR scenes.\n30. Meanwhile, we evaluate existing state-of-the-art visual and LiDAR SLAM algorithms with various sensor modalities on our datasets.\n31. Moreover, our dataset and benchmark results are released publicly available on our website.\n32. The remainder of the paper is organized as follows: Section II introduces the related works.\n33. Section III presents the sensor setup and sensor calibration.\n34. Section IV intro-duces the dataset overview.\n35. Section V demonstrates the dataset application.\n36. Section VI introduces known issues.\n37. The conclusion is given in Section VII.II.RELATED WORKS Currently, several event-based datasets combined with various sensors have been released for VO/VIO/SLAM domains, utilizing handheld devices or a variety of robotics platforms.\n38. DAVIS240C[10], TUM-VIE[11], VECtor[12], and HKU-dataset1were collected by handheld / headmounted devices under indoor environments.\n39. M2DGR[13]utilizes ground robots to collect a multi-sensor dataset with an event camera under large-scale scenes, while the event streams exhibit large noises.\n40. FusionPortable[14]proposes multi-sensor campus-scene datasets with stereo event cameras on diverse platforms (handheld, quadruped robot, and UGV).\n41. Moreover, there exist specialized event-based datasets such as UZH-FPV[15]and GRIFFIN[16], which are targeted for flying robots.\n42. Moreover, a number of event-based datasets are published under large-scale driving scenarios for computer vision.\n43. These autopilot datasets offer more realistic and challenging conditions, including high-speed scenarios, repetitive situations, and HDR scenes compared to datasets collected from handheld devices.\n44. The first dataset catering to driving recordings using an event camera is DDD17[17], as well as the follow-up DDD20[18], for studying the end-to-end driving application incorporating diverse vehicle control data.\n45. [22]published their event-based datasets for the computer vision task of object classification, image reconstruction, and vision place recognition in driving scenarios.\n46. MVSEC[6]is a pioneering cross-modal dataset with stereo event and image cameras, as well as LiDAR.\n47. However, a limitation of MVSEC resides in the utilization of low-resolution event cameras (346×260) with a compact baseline of 10 cm, coupled with the imprecision of the ground-truth derived from GNSS or LiDAR-SLAM.\n48. DSEC[7]proposed an event-based dataset whose scenarios are similar to KITTI[23], providing higher resolution stereo event (640×480) and image, LiDAR, and IMU under various illumination conditions.\n49. M3ED[24]encompasses high-resolution event cameras (1280×720) and covers three different robotics platforms: driving, flight, and legged robot.\n50. However, both DSEC and M3ED datasets are primarily utilized for computer vision fields, such as optical flow estimation, segmentation, and disparity estimation, rather than specifically for localization or mapping problems.\n51. Besides, they do not provide sufficient challenges for SLAM, as the majority of these datasets were collected in rural or suburban areas with relatively low-lying structures, light traffic, and less dynamic objects.\n52. ViViD++[25]focuses on diverse vision sensors for handheld and driving platforms, including event, thermal, and standard cameras.\n53. MA-VIED[8]proposes a comprehensive driving dataset that encompasses race track-like loops, maneuvers, and standard driving scenarios.\n54. However, both of these datasets exclusively offer monocular data for each camera type, thereby precluding the possibility of conducting stereo visual SLAM.\n55. [9]introduces a stereo visual localization dataset that exploits both the high-resolution event and standard cameras under indoor and urban scenarios.\n56. TableI. summarizes the differences between our ECMD and other event-based datasets under autonomous driving scenarios.\n57. Compared to other datasets, our ECMD offers several advantages: (i) Capture diverse visual data format (RGB image, event stream, and infrared image) from multiple types of vision sensors in varying luminance conditions and urbanized scenarios; (ii) 1kHz-rate event streams from different resolution event cameras empower in-depth exploration of event-based perception; (iii) Based on our previous work[26], three LiDARs, including two slanted LiDARs, are employed to collect high-rising building structures for LiDAR point cloud maps generation; (iv) We employ a centimeter-level localization system, GNSS-RTK/INS, as ground-truth, enabling a comprehensive evaluation of various SLAM algorithms.\n58. The data collection platform is shown in Fig.1.\n59. Our sensor suite consists of a multi-camera setup (event camera, industrial camera, and infrared camera) equipped with three LiDARs, high-quality IMU, three GNSS receivers, and GNSS-RTK/INS systems.\n60. The specific specifications of each sensor are presented in TableII.\n61. An Intel NUC (i7-1260P, 32GB RAM) and an industrial computer (i7-10610U, 32GB RAM) are used to run sensor drivers, and record data into ROS bags on the Ubuntu system.\n62. 1) Visual Sensors: Two sets of stereo event cameras with different resolutions, DAVIS436 (346×260) and DVXplorer (640×480), are configured at a baseline of 30 cm respectively.\n63. DAVIS346 produces asynchronous events and intensity frames.\n64. In contrast, DVXplorer exclusively generates events, while its resolution surpasses that of DAVIS346, enabling the provision of more intricate scene information.\n65. Each event camera is equipped with additional infrared filters to mitigate interference from LiDAR.\n66. Two FLIR BFLY-U3-23S3C industrial cameras with a resolution of 1920×1200 are used to capture RGB images at 20 Hz in fixed exposure mode.\n67. Forward-facing stereo industrial cameras are installed with a baseline of 30 cm, ensuring fairness by maintaining consistency with the baseline of the stereo event cameras.\n68. Hikrobot MV-CI003-GL-N6 infrared camera collects thermal frames at 20 Hz, encompassing a response band of 8-14µm and equipped with a 6.3mm focal length lens.\n69. 2) Mechanical LiDAR: We configure three mechanical LiDARs including two slanted LiDARs to collect accurate point clouds of surrounding environments.\n70. Velodyne HDL-32E is positioned on the top of the vehicle to capture the surroundings horizontally.\n71. Two slanted LiDARs, Lslidar C16 and Velodyne VLP-16, are mounted on the left and right sides of the sensor kit, respectively.\n72. This configuration facilitates the thorough recording of architectural particulars relevant to high-rising buildings in urbanized areas and all LiDAR data are collected at 10 Hz.\n73. 3) GNSS-RTK/INS Sensor: A tactical-level Xsens-MTI-30 IMU is employed to collect the raw acceleration and angular velocity at 400 Hz.\n74. The accurate ground-truth of localization is furnished by a centimeter-level GNSS-RTK/INS navigation system, further details can be found in Section IV-B1.\n75. We use a Precision Time Protocol (PTP)[29]device to synchronize the clocks of various data collection devices across the sensor network.\n76. The PTP ensures time accuracy within nanoseconds.\n77. The synchronization device acquires the NMEA[30]output and pulse-per-second (PPS) signal from a u-blox M8T GNSS receiver to align the ROS time of the onboard computers with the GPS time.\n78. This enables sensors such as cameras, LiDAR, and IMU to record timestamps based on the synchronized GPS time.\n79. Moreover, to achieve time synchronization between different event cameras, the DAVIS346 on the rightmost side is configured as the master device and transmits trigger signal pulses to the remaining slave event cameras sequentially from left to right via external cables.\n80. To calibrate the IMU, we position it on a level surface for a duration of three hours and record the raw measurements.\n81. Utilizing the Kalibr toolbox, we can accurately calibrate the random walk and Gaussian white noise of the IMU.\n82. 2) Industrial Cameras Calibration: For industrial cameras, we move the sensor platform against the 9×7 checkerboard in the XYZ-axis and collect the sequence of RGB images and IMU.\n83. Then intrinsics calibration of industrial cameras is achieved by Kalibr toolbox[31], where the pinhole and radial-tangential camera models are adopted.\n84. 3) Event Cameras Calibration: For event cameras, DAVIS346 can produce fixed-rated frames, enabling imagebased calibration, while DVXplorer merely produces asynchronous event streams.\n85. Therefore, E2Calib[32][33] is used to achieve image reconstruction from event streams.\n86. 4) Infrared Camera Calibration: Due to infrared cameras solely capturing the temperature rather than the intensity difference, we design a distinct 9×7 checkerboard to make the pattern detectable for infrared cameras.\n87. As shown in Fig.3(a), the checkerboard intervals are affixed with aluminum materials, and then using a heating plate to raise the temperature of the checkerboard.\n88. Since the superior thermal dissipation of aluminum compared to plastic, a temperature contrast emerges between the two materials, enabling infrared cameras to distinctly capture the lattice shape of the checkerboard, as in Fig.3(b).\n89. With the special infrared image of the checkerboard, intrinsic can be calibrated by Kalibr.\n90. After completing intrinsics calibration, we move the sensor suite in front of checkerboards along the XYZ-RPY-axis and collect data simultaneously.\n91. Subsequently, the extrinsics and the temporal offset between all cameras and IMU could be estimated using Kalibr.\n92. For the calibration of mechanical LiDAR, LI-Init[34]is capable of achieving temporal and spatial calibration for LiDAR and IMU without checkerboards or extra devices in Fig.4.\n93. We rotate and move the device around the XYZ-axis to ensure sufficient excitation until the data accumulation is completed, thus we acquire the extrinsic transformation between LiDAR and IMU.\n94. Our dataset encompasses a wide range of driving scenes, including urban streets, urban roads, tunnels, highways, bridges, and suburban roads.\n95. We have specifically focused on scenarios where visual SLAM algorithms encounter difficulties.\n96. These scenarios involve high-speed motion (up to 110 km/h), limited texture, as well as difficult glare conditions in both daytime and nighttime driving.\n97. We also targeted situations where LiDAR SLAM encounters limitations, such as long corridors or areas with sparse geometric structures.\n98. The complete dataset is partitioned into 81 sequences to facilitate researchers in evaluating their algorithms.\n99. Each sequence has an approximate duration of 120 seconds.\n100. Additionally, we have retained a few sequences with long duration, lasting approximately 34 minutes, specifically for the evaluation of loop closure in large-scale environments and loop closure scenarios.\n101. The summary of sequence types can be found in TableIII. A. Scenarios 1) Dense Urban Street: This scenario focuses on lowspeed vehicles, around 30km/h, proceeding on highly urbanized areas and urban canyons in Hong Kong with multiple light conditions.\n102. The streets are narrow at 10m in width and buildings on both sides of the scene are dense.\n103. Meanwhile, the presence of congested traffic and dynamic crowds may produce the degradation of visual or LiDAR localization, such as Dense street day easy b.\n104. To evaluate the loop closure performance of SLAM, we remarkably recorded sequences Dense street difficult circle and Dense street difficult loop where our vehicle was circling in repeated routes.\n105. 2) Urban Road: This type of scenario records the vehicle traveling at an approximate speed of 60km/h on an expressway in Hong Kong with multiple weather conditions.\n106. Compared to the Dense Urban Street scenario, Urban Road sequences travel through Hong Kong city at a higher speed, while the buildings are not as tightly packed on either side and the road is more spacious with four lanes.\n107. Despite the absence of pedestrians on the road, the scene still includes vehicles overtaking, paralleling, and other situations where the relative motion is not consistent with the absolute motion.\n108. The aforementioned discrepancy might pose a challenge for the VIO or LIO system.\n109. Moreover, the sequence comprises the vehicle traveling during nighttime in rainy conditions.\n110. We record trajectories in rainy situations under nighttime like Urban road night difficult rainy a which are commonly faced in practical driving scenarios, whereas they are not present in previous datasets.\n111. 3) Tunnel: Tunnel scenarios commence with a high-speed vehicle on an open-sky highway, entering an enclosed tunnel without satellite reception.\n112. Inside the tunnel, GNSS positioning is unreliable since the satellite signal is completely blocked.\n113. Meanwhile, the scenario represents a typical and challenging scene for VIO and LIO systems due to the repetitive and texture-less environments for vision sensors and LiDAR.\n114. The sequence collections end after the vehicle exits the tunnel and continues to proceed on the highway for twenty seconds.\n115. 4) Highway: The scenario involves vehicles traveling at speeds up to 100km/h on low-texture highways both during the day and night, with sparse buildings alongside the road.\n116. High speeds, rapid changes in vehicle speed, repetitive visual scenes, and low-texture environments present significant challenges for autonomous driving.\n117. Meanwhile, the vibration of the vehicle body at high-speed motion amplifies the random walk and Gaussian white noise of IMU, thereby diminishing its reliability.\n118. 5) Bridge: The motion pattern of vehicles in bridge scenarios resembles that of highways, with vehicles traveling in a straight line at high speed along the bridge.\n119. However, this scene differs as there are no buildings on either side of the bridge, only the sea surrounds it.\n120. Bridges present scenes with limited texture, and the feature information within these scenes tends to be monotonous and repetitive, which further exacerbates the challenge of achieving accurate localization.\n121. 6) Suburban Road: Suburban road scenarios present complex natural environments characterized by winding and rugged roads, steep slopes, and narrow lanes.\n122. The vehicle navigates the serpentine mountain roads at a moderate speed (approximately 50km/h), with significant altitude changes.\n123. The abundant texture information in the mountain road scene facilitates visual algorithms to extract stable features and construct effective constraints.\n124. 1) Ground-truth Poses: We obtained the ground-truth positioning from the NovAtel SPAN-CPT[28], a highperformance GNSS RTK/INS integrated navigation system.\n125. The ground-truth of most existing event-based driving datasets are derived from LiDAR-SLAM[6][24], GPS/GNSS[6], GNSS-RTK[7][25][24].\n126. The ground-truth derived from LiDAR-SLAM relies on the estimation of vehicle trajectories using LiDAR SLAM which only provides relative trajectories.\n127. It is difficult to quantify the accuracy of ground-truth pose, and errors may even exceed ten meters in some cases.\n128. The complex environment or the equipment malfunctions may disrupt the satellite reception of GPS/GNSS, thus relying solely on GPS/GNSS for ground-truth pose may lead to significant drift.\n129. The GNSS-RTK device can only provide centimeter-level accuracy in the open sky[26]In contrast, our SPAN-CPT can provide continuous high accuracy aided by the internal fiber-optic gyroscopes under high-rise buildings, tunnels, and other environments with weak satellite signals.\n130. Furthermore, we post-process the ground-truth positioning from SPAN-CPT using the state-ofthe-art NovAtel Inertial Explorer[28]software to maximize the accuracy of the trajectory.\n131. For the GNSS positioning benchmark, we provide the WGS84 coordinate data for comparison.\n132. For the evaluation of SLAM algorithms, we provide the tools2to transform the ground-truth data from the WGS84 coordinates to the local frame/ENU frame based on the original points.\n133. 2) LiDAR Point Cloud Maps Generation: Utilizing the ground-truth pose for each frame in conjunction with their corresponding LiDAR point clouds, we accumulate these point clouds to construct a highly accurate LiDAR point cloud map to depict the TsingMa Bridge in Fig.6.\n134. The map encompasses rich spatial information, providing a detailed 3D reconstruction of the bridge and its surrounding areas.\n135. As shown in TableIV., we evaluate the performance of VINS-MONO[35], ORB-SLAM3[36], and ESVIO[37]Fig.6.\n136. The vehicle poses ground-truth on Google map with the LiDAR point cloud maps of Tsing Ma bridge.across various scenes and lighting conditions on our dataset.\n137. The accuracy is quantified using mean position error (MPE, %), which aligns the estimated trajectory with ground-truth through 6-DOF transformation (in SE3) computed by the tool[38].\n138. For the VINS-Mono, we evaluate it separately using RGB images and infrared images.\n139. Due to the resolution provided by industrial cameras being higher in contrast to the infrared camera, we achieve superior performance when utilizing RGB images.\n140. The ORB-SLAM3 often fails to robustly track features during high-speed vehicle movements, potentially resulting in the tracking thread restarts.\n141. The ESVIO leverages the complementary advantages of event streams and RGB images, allowing it to handle the lack of texture in RGB images under broad illumination conditions to achieve higher accuracy.\n142. Fig.7compares event, RGB images, and infrared images under different lighting conditions.\n143. The RGB images offer rich texture under regular luminance scenes in contrast to events and infrared images offer comparatively limited information, e.g., the infrared image struggles to accurately discern traffic left-turn symbol on the ground.\n144. Conversely, the RGB image may lose numerous environmental features under the conditions of low light or over-exposure.\n145. The infrared camera can capture infrared radiation beyond the visible spectrum and event cameras can detect pixel-level intensity changes at low latency.\n146. Both the event camera and the infrared camera are more resilient in external varying lighting conditions, providing effective visibility compared to the industrial camera, e.g., in nighttime scenes, event cameras can capture road signs, and the infrared camera can clearly capture the surrounding bushes.\n147. TableV. demonstrates the performance of LIO-SAM[39], LVI-SAM[40], Fast-LIO2[41], Point-LIO[42]across various scenes on our dataset.\n148. We use the same criteria introduced in Section V-A to evaluate the localization accuracy.\n149. Due to the tilt-mounted LiDAR setups (see Fig.1), we are able to acquire point clouds of towering buildings situated on both sides of the street.\n150. This installation approach compensated for the lack of vertical point clouds compared to the horizontally mounted LiDAR.\n151. In Fig.8, red point clouds are generated from a horizontally mounted LiDAR while white and green point clouds are generated from tilt-mounted LiDARs.\n152. We evaluate the performance of LOAM[43]using three different LiDARs (center, left, and right)in Dense street day medium circle a sequences.\n153. The MPE of LOAM using center LiDAR is 1.02%, compared to 8.67% using the left LiDAR and 2.00% using the right LiDAR.\n154. LOAM using the left LiDAR exhibits significant drift since it initially captures minimal point cloud information.\n155. Although the LOAM merely using tilt-mounted LiDAR produces less accurate results compared to the center LiDAR, multi-LiDAR fusion can integrate complementary information, thereby improving localization accuracy and constructing more precise point cloud maps.\n156. Meanwhile, tunnel scenes present challenges for LiDAR SLAM.\n157. We capture three consecutive frames of LiDAR point clouds at two-second intervals in Fig.9.\n158. It is evident that these LiDAR point clouds exhibit high similarity in the tunnel environment, potentially resulting in degradation phenomena and inaccurate state estimation.\n159. Due to space limitations, we positioned LiDAR closer to the event cameras.\n160. As a consequence, the infrared wavelengths emitted by LiDAR directly impinge on the photoreceptor of event cameras, resulting in continuous disturbances and flickering in the captured images and event streams.\n161. To address this issue, we implement infrared filters on event cameras to counteract the effect.\n162. However, this intervention led to a compromise, resulting in a degradation of the quality of the recorded event data.\n163. During the night or low illumination scenarios, we observed that when event cameras were directly toward a glowing light source, such as street lights or store lighting, event streams would exhibit persistent flickering and produce artifacts around the light source.\n164. This could potentially lead to a distorted view of the observed object.\n165. We postulate this phenomenon is related to the inherent principle of event cameras, and presently, there is no known solution to address this issue.\n166. In this paper, we propose an event-centric autonomous driving dataset generated with multiple sensors across various scenarios for developing SLAM algorithms.\n167. All sensors undergo meticulous calibration and are temporally synchronized at the hardware level.\n168. We employ the GNSS-RTK/INS navigation system, which provides centimeter-level accuracy, to acquire precise ground-truth of the vehicle.\n169. Furthermore, we conduct the evaluation of various state-of-the-art visual and LiDAR SLAM algorithms while identifying their constraints.\n170. We hope this dataset could contribute to the development of visual and LiDAR SLAM.\n\n请严格按照JSON格式返回：{"labels": [0,1,0,...]}，不要包含任何其他文字。'}], 'model': 'qwen-plus', 'response_format': {'type': 'json_object'}}}
2025-10-14 21:11:28,402 - DEBUG - Sending HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
2025-10-14 21:11:28,402 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-10-14 21:11:28,402 - DEBUG - send_request_headers.complete
2025-10-14 21:11:28,402 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-10-14 21:11:28,402 - DEBUG - send_request_body.complete
2025-10-14 21:11:28,402 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-10-14 21:11:34,953 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'vary', b'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding'), (b'x-request-id', b'a4582560-9491-44cc-b78f-4a66452b8210'), (b'x-dashscope-call-gateway', b'true'), (b'content-type', b'application/json'), (b'req-cost-time', b'6503'), (b'req-arrive-time', b'1760447478519'), (b'resp-start-time', b'1760447485023'), (b'x-envoy-upstream-service-time', b'6457'), (b'content-encoding', b'gzip'), (b'date', b'Tue, 14 Oct 2025 13:11:25 GMT'), (b'server', b'istio-envoy'), (b'transfer-encoding', b'chunked')])
2025-10-14 21:11:34,954 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-14 21:11:34,954 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-10-14 21:11:34,955 - DEBUG - receive_response_body.complete
2025-10-14 21:11:34,955 - DEBUG - response_closed.started
2025-10-14 21:11:34,955 - DEBUG - response_closed.complete
2025-10-14 21:11:34,955 - DEBUG - HTTP Response: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "200 OK" Headers({'vary': 'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding', 'x-request-id': 'a4582560-9491-44cc-b78f-4a66452b8210', 'x-dashscope-call-gateway': 'true', 'content-type': 'application/json', 'req-cost-time': '6503', 'req-arrive-time': '1760447478519', 'resp-start-time': '1760447485023', 'x-envoy-upstream-service-time': '6457', 'content-encoding': 'gzip', 'date': 'Tue, 14 Oct 2025 13:11:25 GMT', 'server': 'istio-envoy', 'transfer-encoding': 'chunked'})
2025-10-14 21:11:34,955 - DEBUG - request_id: a4582560-9491-44cc-b78f-4a66452b8210
2025-10-14 21:11:34,956 - DEBUG - API request completed in 6.56 seconds
2025-10-14 21:11:34,956 - DEBUG - Raw model response: {"labels": [0,1,1,1,0,1,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]}
2025-10-14 21:11:34,956 - INFO - Successfully processed 121 labels
2025-10-14 21:11:34,957 - ERROR - Label count mismatch for ECMD_ An Event_Centric Multisensory Driving Dataset for SLAM
2025-10-14 21:11:34,957 - INFO - Evaluating paper 13/18: Inline Citation Classification using Peripheral Context and Time_evolving Augmentation
2025-10-14 21:11:34,957 - INFO - Starting model prediction
2025-10-14 21:11:34,957 - INFO - Attempt 1 of 5
2025-10-14 21:11:34,958 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-7899f13f-31b7-4a18-8ef8-cc18eaf6953c', 'json_data': {'messages': [{'role': 'system', 'content': '你是一个专门用于识别学术论文中数据集描述的助手。你需要判断每个输入的句子是否描述了研究中使用的数据集。请确保：\n    1. 只输出JSON格式的结果\n    2. 结果格式必须为 {"labels": [0,1,0,...]}\n    3. 不要输出任何其他文字或解释'}, {'role': 'user', 'content': '你是一位专业的学术论文数据集描述识别专家。请仔细分析下面这篇论文中的每个句子，判断它们是否描述了本研究特定构建的数据集信息。\n\n    论文标题: Inline Citation Classification using Peripheral Context and Time_evolving Augmentation\n\n    判断标准:\n    判断为1(数据集描述)的标准：\n    句子必须直接描述本研究特定构建的数据集,包括:\n    1. 该数据集的构建过程和方法\n    2. 该数据集的具体构成和规模\n    3. 该数据的来源和收集方式\n    4. 该数据的预处理步骤\n    5. 该数据集的可获取方式(如发布地址)\n    6. 或者其他直接或者间接描述了该数据集的句子\n\n    所有其他类型的句子均标记为0。\n    \n    请对每个句子进行分析，返回一个JSON格式的标签数组。格式要求：\n    1. 必须是有效的JSON格式\n    2. 只包含labels字段，值为0和1组成的数组\n    3. 数组长度必须与句子数量相同\n    4. 示例格式：{"labels": [0,1,0,1,...]}\n\n    需要判断的句子：\n\n    1. Abstract: Citation plays a pivotal role in determining the associations among research articles.\n2. It portrays essential information in indicative, supportive, or contrastive studies.\n3. The task of inline citation classification aids in extrapolating these relationships; However, existing studies are still immature and demand further scrutiny.\n4. Current datasets and methods used for inline citation classification only use citation-marked sentences constraining the model to turn a blind eye to domain knowledge and neighboring contextual sentences.\n5. In this paper, we propose a new dataset, named 3Cext, which along with the cited sentences, provides discourse information using the vicinal sentences to analyze the contrasting and entailing relationships as well as domain information.\n6. We propose PeriCite, a Transformer-based deep neural network that fuses peripheral sentences and domain knowledge.\n7. Our model achieves the state-of-the-art on the 3Cext dataset by +0.09 F1 against the best baseline.\n8. We conduct extensive ablations to analyze the efficacy of the proposed dataset and model fusion methods.\n9. For the past several decades, there has been an interest in citation analysis for research evaluation.\n10. Researchers have emphasized the necessity for new methodologies that take into account various components of citing sentences.\n11. A wellknown qualitative technique for assessing the scientific influence is to analyze the sentence in which the research article is mentioned to ascertain the purpose behind the citation.\n12. The context of the citation, or the text in which the cited document is mentioned, has proven to be an effective indicator of the citation\'s intent[25].\n13. Measuring the scientific impact of research articles requires a fundamental understanding of citation intent.\n14. A great way to gauge the significance of a scientific publication is to determine why citations are made in one\'s work and how significant they are.\n15. Previous methods for citation context categorization used a range of annotation techniques with low-to-high granularity.\n16. Comparing the earlier systems is extremely difficult due to the absence of standardized methodologies and annotation schemes.\n17. The 3C shared task[12,13]used a piece of the Academic Citation Typing (ACT) dataset to categorize the reference anchor into \'function\' or \'purpose\' by looking at the citing sentence or the text that contains the citation[19].\n18. Only quantitative elements are considered in traditional citation analysis based solely on the citation count.\n19. One of the biggest obstacles to citation context analysis for citation identification is that there is no multidisciplinary dataset and that there isn\'t any medium to fine-grained schemes that adequately represent the function and its influence[8].\n20. To address this challenge, Kunnath et al.[12]provided a unified task, called the 3C Shared Task, to compare several citation classification approaches on the same dataset to address the shortcomings of citation context categorization.\n21. The main distinction in the second iteration of this task[13]was that the subtasks contained full-text datasets.\n22. However, even with the full text, the metadata associated with the citation sentence was not adequate to understand the reasoning for the citation.\n23. To alleviate the above limitations, we propose a new dataset, named 3Cext, and a new model, named PeriCite that combines the advantages of augmentation and peripheral context.\n24. Experiments show that the cited sentences heavily rely on the peripheral context to strengthen an argument by contrasting or entailing information.\n25. Our main contributions are as follows 1.We extend the 3C dataset[13]-3Cext, which, along with the cited sentence, adds more discourse information by providing contrasting and entailing information using the peripheral sentences.\n26. 2. We propose a novel model, PeriCite, which uses spatial fusion and crosstext attention to attend to contextual information for the peripheral sentences and time-evolving augmentation to counter class imbalance during the training time.\n27. 3. We also compare our proposed model against various baselines and show the efficacy of the module along with ablation studies and error analysis.\n28. We also compare our proposed model against various baselines and show the efficacy of the module along with ablation studies and error analysis.\n29. In this section, we discuss our proposed 3Cext dataset in detail.\n30. Kunnath et al.[12]introduced the ACT dataset, with annotations for 11, 233 citations annotated by 883 authors.\n31. The cited label was masked with "#AUTHOR TAG" denoting the position of the cited object.\n32. Additionally, the 3C dataset contained full text and the label denoting the class of a particle citation (c.f.Table2).\n33. In our work, we extend the 3C dataset to house more discourse information to explain better why a citation is present in a sentence.\n34. Our intuition is that the cited sentences mostly either entail or contrast the adjoining sentences.\n35. To capture the peripheral sentences, we extract the full-text files corresponding to the COREIDs (unique paper ID) in our dataset to follow through on this discovery.\n\n请严格按照JSON格式返回：{"labels": [0,1,0,...]}，不要包含任何其他文字。'}], 'model': 'qwen-plus', 'response_format': {'type': 'json_object'}}}
2025-10-14 21:11:34,960 - DEBUG - Sending HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
2025-10-14 21:11:34,960 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-10-14 21:11:34,960 - DEBUG - send_request_headers.complete
2025-10-14 21:11:34,960 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-10-14 21:11:34,960 - DEBUG - send_request_body.complete
2025-10-14 21:11:34,960 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-10-14 21:11:38,260 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'vary', b'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding'), (b'x-request-id', b'4dc7d010-2490-4e3a-90bf-96d583781eb9'), (b'x-dashscope-call-gateway', b'true'), (b'content-type', b'application/json'), (b'req-cost-time', b'3251'), (b'req-arrive-time', b'1760447485080'), (b'resp-start-time', b'1760447488332'), (b'x-envoy-upstream-service-time', b'3249'), (b'content-encoding', b'gzip'), (b'date', b'Tue, 14 Oct 2025 13:11:28 GMT'), (b'server', b'istio-envoy'), (b'transfer-encoding', b'chunked')])
2025-10-14 21:11:38,260 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-14 21:11:38,261 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-10-14 21:11:38,261 - DEBUG - receive_response_body.complete
2025-10-14 21:11:38,261 - DEBUG - response_closed.started
2025-10-14 21:11:38,261 - DEBUG - response_closed.complete
2025-10-14 21:11:38,261 - DEBUG - HTTP Response: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "200 OK" Headers({'vary': 'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding', 'x-request-id': '4dc7d010-2490-4e3a-90bf-96d583781eb9', 'x-dashscope-call-gateway': 'true', 'content-type': 'application/json', 'req-cost-time': '3251', 'req-arrive-time': '1760447485080', 'resp-start-time': '1760447488332', 'x-envoy-upstream-service-time': '3249', 'content-encoding': 'gzip', 'date': 'Tue, 14 Oct 2025 13:11:28 GMT', 'server': 'istio-envoy', 'transfer-encoding': 'chunked'})
2025-10-14 21:11:38,262 - DEBUG - request_id: 4dc7d010-2490-4e3a-90bf-96d583781eb9
2025-10-14 21:11:38,263 - DEBUG - API request completed in 3.31 seconds
2025-10-14 21:11:38,263 - DEBUG - Raw model response: {"labels": [0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,1,0,0,0,1,0,1]}
2025-10-14 21:11:38,263 - INFO - Successfully processed 35 labels
2025-10-14 21:11:38,271 - ERROR - Error processing paper Inline Citation Classification using Peripheral Context and Time_evolving Augmentation: 'int' object has no attribute 'capitalize'
2025-10-14 21:11:38,271 - INFO - Evaluating paper 14/18: llm_japanese_dataset v0_ Construction of Japanese Chat Dataset for Large Language Models and its Methodology
2025-10-14 21:11:38,271 - INFO - Starting model prediction
2025-10-14 21:11:38,271 - INFO - Attempt 1 of 5
2025-10-14 21:11:38,271 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-d705bac8-f93d-4e1e-9601-ae6eb1dc07dc', 'json_data': {'messages': [{'role': 'system', 'content': '你是一个专门用于识别学术论文中数据集描述的助手。你需要判断每个输入的句子是否描述了研究中使用的数据集。请确保：\n    1. 只输出JSON格式的结果\n    2. 结果格式必须为 {"labels": [0,1,0,...]}\n    3. 不要输出任何其他文字或解释'}, {'role': 'user', 'content': '你是一位专业的学术论文数据集描述识别专家。请仔细分析下面这篇论文中的每个句子，判断它们是否描述了本研究特定构建的数据集信息。\n\n    论文标题: llm_japanese_dataset v0_ Construction of Japanese Chat Dataset for Large Language Models and its Methodology\n\n    判断标准:\n    判断为1(数据集描述)的标准：\n    句子必须直接描述本研究特定构建的数据集,包括:\n    1. 该数据集的构建过程和方法\n    2. 该数据集的具体构成和规模\n    3. 该数据的来源和收集方式\n    4. 该数据的预处理步骤\n    5. 该数据集的可获取方式(如发布地址)\n    6. 或者其他直接或者间接描述了该数据集的句子\n\n    所有其他类型的句子均标记为0。\n    \n    请对每个句子进行分析，返回一个JSON格式的标签数组。格式要求：\n    1. 必须是有效的JSON格式\n    2. 只包含labels字段，值为0和1组成的数组\n    3. 数组长度必须与句子数量相同\n    4. 示例格式：{"labels": [0,1,0,1,...]}\n\n    需要判断的句子：\n\n    1. Abstract: This study constructed a Japanese chat dataset for tuning large language models (LLMs), which consist of about 8.4 million records.\n2. Recently, LLMs have been developed and gaining popularity.\n3. However, high-performing LLMs are usually mainly for English.\n4. There are two ways to support languages other than English by those LLMs: constructing LLMs from scratch or tuning existing models.\n5. However, in both ways, datasets are necessary parts.\n6. In this study, we focused on supporting Japanese in those LLMs and making a dataset for training or tuning LLMs in Japanese.\n7. The dataset we constructed consisted of various tasks, such as translation and knowledge tasks.\n8. In our experiment, we tuned an existing LLM using our dataset and evaluated the performance qualitatively.\n9. The results suggest that our dataset is possibly beneficial for LLMs.However, we also revealed some difficulties in constructing LLMs in languages other than English.\n10. 2 Dataset Construction: izumi-lab/llm-japanese-dataset v0 In this study, we created a Japanese chat dataset.\n11. The dataset 1 contains 8,393,726 data points.\n12. In the following, we describe the details of datasets and their creation process.\n13. Large language models (LLMs) have recently achieved remarkable progress in performance and generalization.\n14. Specifically, Transformer-based LLMs such as BERT[3]and the GPT series[17,18,1]have demonstrated high-performance thanks to their pre-training.\n15. Furthermore, models that have evolved from these, such as Chat-GPT[14]and GPT4[15], have gained popularity for their remarkable performance.\n16. Other models such asBard [6], LLaMA[24], Dolly[2], Bloom[21], and Vicuna[26]have also emerged.\n17. Masanori HIRANO, Masahiro SUZUKI, and Hiroki SAKAJI The University of Tokyo, 7-3-1 Hongo, Bunkyo, Tokyo 113-8656 Japan, e-mail: research@mhirano.jp,b2019msuzuki@socsim.org,sakaji@sys.t.utokyo.ac.jpSome of those models are already provided to consumers as a web service.\n18. Moreover, via API, those models and services are also now available for sub-parts of web services, and many spin-off services are emerging.\n19. However, despite the prosperity of language models, there are still challenges in handling diverse prompts, including prompts written in languages other than English.\n20. For example, Alpaca[23]dataset has been proposed due to the incompleteness of LLaMA\'s response.\n21. However, the dataset of Alpaca is only available in English, and the incompleteness pointed out by Alpaca has not been filled yet in the other languages.\n22. Moreover, LLaMA has difficultness to respond appropriately to some prompts in languages other than English.\n23. Considering these challenges, it is necessary to enhance models\' performances in languages other than English.\n24. However, it is not a good idea to study a specific model in terms of performance improvements in the other language.\n25. Moreover, model development is still ongoing and very competitive, and the situation is changing dramatically recently.\n26. It is also easy to assume that newer models with better performance will emerge in a few months or even 1-2 months.\n27. Therefore, enhancing datasets that support model training may be more useful than focusing on specific models.\n28. This approach may also lower the barrier to adapting new models to languages other than English.\n29. Therefore, this study constructed a new chat dataset in Japanese for LLM training, which contains approximately 8.4 million data points, and demonstrated the performance of the dataset qualitatively.\n30. The dataset and trained models are opensourced and publicly available.\n31. The details are as follows: • Dataset: https://huggingface.co/datasets/izumi-lab/llm-japanesedataset • Trained Models (LLaMA 1 epoch): https://huggingface.co/izumi-lab/llama-13b-japanese-lora-v0-1ep.\n32. The more details are explained in the following.\n33. Moreover, data expansion and additional model training are planned as future tasks.\n34. The format of the chat data used for model training is shown below.\n35. In the description of the dataset later, we will omit some of the introductory parts and line breaks.\n36. Below is an instruction that describes a task, paired with an input that provides further context.\n37. Write a response that appropriately completes the request.\n38. Note that, in the following examples, the underlined sentences are originally written in Japanese.\n39. We utilized the aforementioned ParaNatCom[25]to create tasks related to our research paper.\n40. The license for the dataset is CC BY 4.0, and the size of the created dataset is 1,732.\n41. ### Instruction: Please make a title from the abstract of the paper.\n42. 1  ### Input: Superthin nanostructures, particularly with atomic-level thicknesses, typically display unique optical properties because of their exceptional light-matter interactions.\n43. ### Instruction: Please rephrase the following Japanese into easy Japanese.\n44. 1 ### Input: Bill has no sense of adventure at all.\n45. 1  ### Response: Bill has no desire to do anything dangerous.\n46. In addition, we incorporated Japanese-translated versions of existing publicly available chat datasets.\n47. The following datasets were included: • Japanese-Alpaca-LoRA17: A translation of the Alpaca[23]dataset into Japanese.\n48. The license is Apache License 2.0.\n49. • databricks-dolly-15k-ja18: A Japanese-translated version of the dataset used for training Dolly[2].\n50. The license is CC BY-SA 3.0.\n51. The dataset size is 15,015.\n52. This study used LoRA[7]as a method to fine-tune LLMs without significant performance degradations.\n53. It is because building LLMs from scratch requires a massive amount of computational resources.\n54. Furthermore, LLMs with a large number of parameters require GPU resources not only for pre-training but also for fine-tuning.\n55. On the other hand, LoRA updates only small parts of LLM parameters.\n56. Therefore, LoRA is a feasible option for us to evaluate the benefits of our dataset.\n57. The main parameters used in the experiment are shown below.\n58. • Base model: LLaMA 13B[24]• Learning rate: 3e-4 We used PEFT[12]and DeepSpeed ZeRO 2[19]for the implementation.\n59. This tuned model is publicly available at https://huggingface.co/izumilab/llama-13b-japanese-lora-v0-1ep.\n60. In order to increase the reproducibility of the evaluation experiment, the temperature parameter for prompt generation was set to 0.0.\n61. Below are some qualitative comparisons we conducted to assess performance.\n62. The phone rings.\n63. When the call is received, the person receiving the call should receive the call.\n64. 1   Response Example(5)### Input: What are the three major festivals in Kyoto?\n65. )### Output(LLaMA): What are the three major festivals in Kyoto?\n66. What are the three major festivals in Kyoto?\n67. What are the three major festivals in Kyoto?\n\n请严格按照JSON格式返回：{"labels": [0,1,0,...]}，不要包含任何其他文字。'}], 'model': 'qwen-plus', 'response_format': {'type': 'json_object'}}}
2025-10-14 21:11:38,271 - DEBUG - Sending HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
2025-10-14 21:11:38,272 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-10-14 21:11:38,272 - DEBUG - send_request_headers.complete
2025-10-14 21:11:38,272 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-10-14 21:11:38,272 - DEBUG - send_request_body.complete
2025-10-14 21:11:38,272 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-10-14 21:11:43,927 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'vary', b'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding'), (b'x-request-id', b'3efceed9-c45f-4a74-adc7-8acf4b2d9db6'), (b'x-dashscope-call-gateway', b'true'), (b'content-type', b'application/json'), (b'req-cost-time', b'5612'), (b'req-arrive-time', b'1760447488390'), (b'resp-start-time', b'1760447494002'), (b'x-envoy-upstream-service-time', b'5610'), (b'content-encoding', b'gzip'), (b'date', b'Tue, 14 Oct 2025 13:11:33 GMT'), (b'server', b'istio-envoy'), (b'transfer-encoding', b'chunked')])
2025-10-14 21:11:43,928 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-14 21:11:43,928 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-10-14 21:11:43,928 - DEBUG - receive_response_body.complete
2025-10-14 21:11:43,928 - DEBUG - response_closed.started
2025-10-14 21:11:43,929 - DEBUG - response_closed.complete
2025-10-14 21:11:43,929 - DEBUG - HTTP Response: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "200 OK" Headers({'vary': 'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding', 'x-request-id': '3efceed9-c45f-4a74-adc7-8acf4b2d9db6', 'x-dashscope-call-gateway': 'true', 'content-type': 'application/json', 'req-cost-time': '5612', 'req-arrive-time': '1760447488390', 'resp-start-time': '1760447494002', 'x-envoy-upstream-service-time': '5610', 'content-encoding': 'gzip', 'date': 'Tue, 14 Oct 2025 13:11:33 GMT', 'server': 'istio-envoy', 'transfer-encoding': 'chunked'})
2025-10-14 21:11:43,929 - DEBUG - request_id: 3efceed9-c45f-4a74-adc7-8acf4b2d9db6
2025-10-14 21:11:43,930 - DEBUG - API request completed in 5.66 seconds
2025-10-14 21:11:43,930 - DEBUG - Raw model response: {"labels": [1,0,0,0,0,1,1,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}
2025-10-14 21:11:43,930 - INFO - Successfully processed 64 labels
2025-10-14 21:11:43,930 - ERROR - Label count mismatch for llm_japanese_dataset v0_ Construction of Japanese Chat Dataset for Large Language Models and its Methodology
2025-10-14 21:11:43,930 - INFO - Evaluating paper 15/18: Re 3 Dial_ Retrieve_ Reorganize and Rescale Conversations for Long_Turn Open_Domain Dialogue Pre_training
2025-10-14 21:11:43,931 - INFO - Starting model prediction
2025-10-14 21:11:43,931 - INFO - Attempt 1 of 5
2025-10-14 21:11:43,932 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-d98fef7f-b549-4e98-a283-c1ad536785b8', 'json_data': {'messages': [{'role': 'system', 'content': '你是一个专门用于识别学术论文中数据集描述的助手。你需要判断每个输入的句子是否描述了研究中使用的数据集。请确保：\n    1. 只输出JSON格式的结果\n    2. 结果格式必须为 {"labels": [0,1,0,...]}\n    3. 不要输出任何其他文字或解释'}, {'role': 'user', 'content': '你是一位专业的学术论文数据集描述识别专家。请仔细分析下面这篇论文中的每个句子，判断它们是否描述了本研究特定构建的数据集信息。\n\n    论文标题: Re 3 Dial_ Retrieve_ Reorganize and Rescale Conversations for Long_Turn Open_Domain Dialogue Pre_training\n\n    判断标准:\n    判断为1(数据集描述)的标准：\n    句子必须直接描述本研究特定构建的数据集,包括:\n    1. 该数据集的构建过程和方法\n    2. 该数据集的具体构成和规模\n    3. 该数据的来源和收集方式\n    4. 该数据的预处理步骤\n    5. 该数据集的可获取方式(如发布地址)\n    6. 或者其他直接或者间接描述了该数据集的句子\n\n    所有其他类型的句子均标记为0。\n    \n    请对每个句子进行分析，返回一个JSON格式的标签数组。格式要求：\n    1. 必须是有效的JSON格式\n    2. 只包含labels字段，值为0和1组成的数组\n    3. 数组长度必须与句子数量相同\n    4. 示例格式：{"labels": [0,1,0,1,...]}\n\n    需要判断的句子：\n\n    1. Abstract: Pre-training on large-scale open-domain dialogue data can substantially improve the performance of dialogue models.\n2. However, the pre-trained dialogue model\'s ability to utilize long-range context is limited due to the scarcity of long-turn dialogue sessions.\n3. Most dialogues in existing pre-training corpora contain fewer than three turns of dialogue.\n4. To alleviate this issue, we propose the Retrieve, Reorganize and Rescale framework (Re 3 Dial), which can automatically construct billion-scale long-turn dialogues by reorganizing existing short-turn ones.\n5. Given a short-turn session, Re 3 Dial first employs a session retriever to retrieve coherent consecutive sessions.\n6. To this end, we train the retriever to capture semantic and discourse relations within multi-turn dialogues through contrastive training.\n7. Next, Re 3 Dial samples a session from retrieved results following a diversity sampling strategy, which is designed to penalize repetitive or generic sessions.\n8. A longer session is then derived by concatenating the original session and the sampled session.\n9. By repeating the above process, Re 3 Dial can yield a coherent long-turn dialogue.\n10. Extensive experiments on multiple multi-turn dialogue benchmarks demonstrate that Re 3 Dial significantly improves the dialogue model\'s ability to utilize long-range context and thus generate more sensible and informative responses.\n11. Finally, we build a toolkit for efficiently rescaling conversations with Re 3 Dial, which enables us to construct a corpus containing 1B Chinese dialogue sessions with 11.3 turns on average (5× longer than the original corpus).\n12. Our retriever model, code, and data is publicly available at https://github.com/thu-coai/Re3Dial.\n13. Building intelligent open-domain dialogue systems that can generate coherent and engaging multi-turn dialogues with humans has been one of the long-Parents really expect too much from their children in this society, and such children are 100% mentally unhealthy even if they achieve success in the future.\n14. There is no need for children to start working so hard from such a young age.standing goals in AI.\n15. Recently, a variety of largescale open-domain pre-trained dialogue models have dramatically promoted this progress(Roller et al., 2020;Zhou et al., 2021;Shuster et al., 2022b).\n16. And a critical ingredient to the success of these models is the pre-training dialogue corpus.\n17. However, while existing dialogue pre-training corpus collects millions to billions of dialogues from public social media, e.g., Reddit for English(Roller et al., 2020)and Weibo for Chinese(Zhou et al., 2021), long-turn dialogues are highly scarce.\n18. More specifically, based on the publicly reported data statistics shown in Figure1(a), most dialogues in existing pre-training corpora only have less than three turns.\n19. The lack of large-scale long-turn di-alogue data restricts dialogue models from deriving more advanced abilities to utilize long-range context for modeling multi-turn dialogues during pre-training(Xu et al., 2021(Xu et al., , 2022b)).\n20. In this paper, we focus on answering the following research question: Can we automatically build a billionscale long-turn dialogue corpus by reorganizing existing short-turn dialogues?\n21. Our basic idea is to construct a long-turn dialogue via recursively retrieving and selecting one consecutive session from the existing dialogue corpus.\n22. Despite the simplicity of this idea, we still face several challenges to make the constructed corpus effective in enhancing long-turn dialogue pre-training.\n23. First, the selected session should be coherent with the query session.\n24. Otherwise, it will introduce noisy utterances without long-range dependency or break the conversation flow(Liu et al., 2021), which may impact the performance of dialogue models.\n25. Second, our in-depth analysis reveals that the retrieved sessions tend to be biased to be relevant but semantically repetitive with the query or overly generic (e.g., "A: Haha, it\'s so cute.B: Haha! LMAO.") due to both the data bias in the dialogue corpus(Zhou et al., 2021;Lee et al., 2021;Li et al., 2015;Liu et al., 2018)and the model bias of the retriever(Thakur et al., 2021).\n26. These biases significantly lower the diversity and informativeness of the reorganized long-turn dialogues.\n27. To tackle the above challenges, we propose the Retrieve, Reorganize and Rescale framework (Re 3 Dial), which employs an Unsupervised Dense Session Retriever (UDSR) to retrieve coherent short-turn dialogues and reorganize them into a long-turn one.\n28. We train UDSR through contrastive learning by taking consecutive dialogue segments from the same dialogue as positive pairs and those from different dialogues as negative pairs.\n29. To avoid overly retrieving semantically repetitive or generic sessions, we propose a diversity sampling strategy, effectively improving the diversity and informativeness of the reorganized long-turn dialogues.\n30. We verify the effectiveness of Re 3 Dial on three Chinese multi-turn open-domain dialogue benchmarks.\n31. Extensive experiments demonstrate that Re 3 Dial consistently and significantly enhances the dialogue model\'s ability to utilize long-range context, leading to more sensible and informative responses in multi-turn dialogue.\n32. Finally, we develop a toolkit for efficiently rescaling conversations with Re 3 Dial, which enables us to construct a corpus containing 1B Chinese dialogue sessions with 11.3 turns on average (5× longer than that of the original EVA corpus).\n33. We will make our retriever model, toolkit, and data public.\n34. We believe our work provides new opportunities in long-turn dialogue pre-training to the research community.\n35. • We propose to train a dense session retriever on massive unlabeled plain dialogue data with contrastive learning to capture the global semantic and discourse relations within multiturn dialogues.\n36. We also propose the diversity sampling strategy to improve the diversity and informativeness of the automatically constructed corpus.\n37. In the past few years, large-scale pre-training has greatly promoted the progress of the NLP community(Brown et al., 2020).\n38. Recently, large-scale pre-training has also become the mainstream approach to building open-domain dialogue models, both in English(Zhang et al., 2019;Roller et al., 2020;Thoppilan et al., 2022)and Chinese(Bao et al., 2020;Zhou et al., 2021;Gu et al., 2022;Wen et al., 2022).\n39. Through pre-training on massive dialogue data crawled from public social media, these models exhibit strong conversational ability, significantly outperforming traditional non-pre-trained dialogue models.\n40. However, the scarcity of longturn dialogues in the pre-training corpus hinders these models from deriving a better ability to utilize long-range context for modeling multi-turn dialogues during pre-training.\n41. To alleviate this issue, we study how to automatically and efficiently build a large-scale long-turn dialogue corpus based on the existing short-turn dialogue corpus.\n42. We train UDSR on a subset of the EVA pretraining corpus(Zhou et al., 2021), which contains 1,000,000/49,000/1,000 examples for the train/validation/test split.\n43. More details of data processing are provided in Appendix A.1.\n44. We adopt BERT-base(Devlin et al., 2018)as the encoder backbone.\n45. The parameters of E q and E c are not shared according to our preliminary experiments.\n46. Settings We consider three general scenarios where Re Benchmarks We conduct evaluations on three widely-adopted Chinese open-domain multi-turn dialogue benchmarks, including KdConv(Zhou et al., 2020), DuLeMon(Xu et al., 2022b), and NaturalConv(Wang et al., 2021), each has 16~20 turns on average.\n47. Data statistics are shown in Table9.\n48. Metrics We adopt the following automatic metrics for evaluation.\n49. PPL zero-shot measures the perplexity on the test set without fine-tuning on the downstream training sets.\n50. PPL measures the perplexity on the test set after fine-tuning.\n51. BLEU-N measures the precision of the n-gram overlap between generated and ground-truth responses(Papineni et al., 2002)after fine-tuning.\n52. ROUGE-L measures the recall of the n-gram overlap between generated and ground-truth responses(Lin, 2004)after fine-tuning.\n53. Distinct-N measures the percentage of the unique n-grams over all the generated n-grams after fine-tuning(Li et al., 2015).\n54. Table2shows the automatic evaluation results.\n55. In the zero-shot setting, Re  46.25 on DuLeMon, compared to the original baseline\'s performance of 48.79.\n56. This indicates a better ability in multi-turn dialogue modeling.\n57. Moreover, beyond benefiting zero-shot performance, Re 3 Dial can also significantly improve the model\'s performance after fine-tuning on sizable crowdsourcing high-quality long-turn datasets.\n58. Specifically, the Re 3 Dial-trained model achieves better perplexity, BLEU, and ROUGE scores, while showing an improved or comparable generation diversity.\n59. In summary, these results demonstrate that Re 3 Dial provides a well-generalized data foundation in the era of large-scale dialogue pre-training.\n60. We conduct a pair-wise human evaluation to study the models\' performance when provided with dialogue contexts of different lengths.\n61. We first randomly sample 100 long-turn contexts (consisting of at least six turns) from DuLeMon as the Longturn test set.\n62. We then extract the last utterances from these contexts to form the Short-turn test set.\n63. We hence obtain 400 generated responses from the two models.\n64. For each pair of responses (one by the Re 3 Dial-trained model and the other by the Original-trained model), three annotators are hired to give a preference in sensibleness and informativeness, respectively.\n65. Sensibleness mea- sures whether the response is relevant and consistent with the context.\n66. Informativeness measures whether the response is informative given the context.\n67. We adopt majority voting to make final decisions among three annotators.\n68. Effect of Retriever We compare different approaches to retrieve dialogue sessions and evaluate the final dialogue model performance.\n69. We try Random sampling, a term-based retriever BM25, and a state-of-the-art dense retriever Contriever.\n70. Table3presents the results.\n71. All baselines bring fewer improvements or even inversely hurt model performance, especially zero-shot performance in the further pre-training setting.\n72. In contrast, using the retriever in Re 3 Dial achieves consistent and significant improvements across different benchmarks and pre-training settings.\n73. To gain a deeper understanding of the effectiveness of different retrievers in capturing global semantic and discourse relations within multi-turn dialogues, we propose to evaluate the retriever using individual tests in different aspects(Ribeiro et al., 2020).\n74. To this end, we first construct positive pairs following the strategy illustrated in Section 3.1 and introduce perturbations to create negative pairs.\n75. We then compute the retriever\'s accuracy in discriminating between positive and negative pairs, expecting it assigns a higher score to positive pairs.\n76. Our evaluation focuses on three aspects: Irrelevance, Local Relevance, and Discourse Incoherence.\n77. For example, to create a locally relevant negative pair, we keep one utterance from the positive session unchanged while replacing the other utterances with a randomly sampled session.\n78. More details can be found in Appendix E.\n79. The results shown in Table4Overall, these results indicate that automatically building long-turn dialogues to enhance pretraining is non-trivial.Simply improving dialogue turns is insufficient.\n80. It is important to retrieve coherent sessions based on both global semantic relevance and discourse coherence within multi-turn dialogues rather than relying solely on word overlap or semantic similarity.\n81. Otherwise, it will introduce unexpected noise or biases and lead to slightly improved or even decreased model performance.\n82. To further investigate the influence of the proposed diversity sampling strategy in Re 3 Dial, we conduct an ablation study.\n83. As shown in Table5, the dialogue-level and corpus-level weights reduce the bias towards repetitive and generic sessions and improve the diversity and the informativeness of the constructed corpus as expected.\n84. Finally, both of them contribute to the pre-trained dialogue model\'s performance.\n85. To manifest the benefits of Re 3 Dial, we visualize the distribution of PPL zero-shot on samples with varying numbers of dialogue context turns.\n86. Specifically, we first (2) Although other retrieval baselines also exhibit a sharper decreasing trend in perplexity compared to the Original-trained model, they generally yield higher perplexity.\n87. This implies that while these baselines enhance the utilization of long-range context, they capture fewer long-range dependencies compared to Re 3 Dial and may even exhibit inferior performance when the local context is more effectively utilized.\n88. While Re 3 Dial aims to construct a long-turn dialogue pre-training corpus to enhance the utilization of long-range context, there is another line of work that focuses on compressing long contexts into short contexts.\n89. We hence additionally conduct experiments on a retrieval-based baseline and a summarization-based baseline for long-term context modeling and compare them with Re 3 Dial.\n90. We introduce an additional summarization model to summarize long-term context into short sentences.\n91. We try two summarization models: (1) Pegasus-523M(Zhang et al., 2020): It is a widely-adopted encoder-decoder model specifically pre-trained and fine-tuned for text summarization.\n92. (2) ChatGLM-66B(Zeng et al., 2022): It is a widely-adopted instruction-tuned large language model.\n93. We report the average PPL zero-shot over three multi-turn dialogue benchmarks.\n94. From the results shown in Table6, we observe that Re 3 Dial significantly outperforms all baselines in long-turn dialogue benchmarks.\n95. Moreover, augmenting the dialogue model with a context summarization model or a retriever shows less improvement or inversely hurts model performance in several cases.\n96. On the one hand, the two-stage framework suffers from error propagation due to the introduced summarization model or the retriever.\n97. For example, both the summarization model and the retriever may lose important information in the original context.\n98. Moreover, the summarization model could also suffer from hallucination problems(Maynez et al., 2020), thereby introducing new noises.\n99. In contrast, Re 3 Dial keeps the original long-turn context unchanged and thus does not lead to information loss or introduce new noises.\n100. On the other hand, we conjecture that augmenting dialogue models with the context summarization model requires further training on summarizationbased dialogue datasets(Xu et al., 2022a).\n101. In contrast, Re 3 Dial does not require collecting additional training datasets and greatly improves the model performance.\n102. As shown in Table7, the Original-trained model mainly focuses on local context and tends to generate more generic responses (e.g., "I think the same" in responding to the preceding utterance, "they thought it was too risky").\n103. In contrast, the Re 3 Dial-trained dialogue model generates words related to the long-range context (e.g., "fashion designer" which has been mentioned nine turns prior), Original: Well, actually I think the same.\n104. Re 3 Dial: I think it would be a good idea to find another experienced fashion designer , which will help you to achieve your dream.\n105. Table7: Generated responses from the model pretrained on Re 3 Dial and Original corpus (translated from Chinese to English).\n106. We highlight the generated spans that are related to long-range context. leading to a more sensible and specific response.\n107. To show the efficiency of constructing large-scale long-turn dialogue data with Re 3 Dial and allow researchers to explore Re 3 Dial easily, we finally release Re 3 Dial-1B, an improved corpus based on the original EVA corpus that contains 1B sessions with 11.3 turns on average (5× longer than that of the original EVA corpus).\n108. The whole pipeline costs about five days with 32 V100 32G GPUs.\n\n请严格按照JSON格式返回：{"labels": [0,1,0,...]}，不要包含任何其他文字。'}], 'model': 'qwen-plus', 'response_format': {'type': 'json_object'}}}
2025-10-14 21:11:43,935 - DEBUG - Sending HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
2025-10-14 21:11:43,935 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-10-14 21:11:43,935 - DEBUG - send_request_headers.complete
2025-10-14 21:11:43,935 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-10-14 21:11:43,935 - DEBUG - send_request_body.complete
2025-10-14 21:11:43,935 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-10-14 21:11:50,512 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'vary', b'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding'), (b'x-request-id', b'64136208-2052-46b1-b4c2-170835fe4930'), (b'x-dashscope-call-gateway', b'true'), (b'content-type', b'application/json'), (b'req-cost-time', b'6533'), (b'req-arrive-time', b'1760447494053'), (b'resp-start-time', b'1760447500587'), (b'x-envoy-upstream-service-time', b'6486'), (b'content-encoding', b'gzip'), (b'date', b'Tue, 14 Oct 2025 13:11:40 GMT'), (b'server', b'istio-envoy'), (b'transfer-encoding', b'chunked')])
2025-10-14 21:11:50,512 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-14 21:11:50,512 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-10-14 21:11:50,512 - DEBUG - receive_response_body.complete
2025-10-14 21:11:50,512 - DEBUG - response_closed.started
2025-10-14 21:11:50,512 - DEBUG - response_closed.complete
2025-10-14 21:11:50,513 - DEBUG - HTTP Response: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "200 OK" Headers({'vary': 'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding', 'x-request-id': '64136208-2052-46b1-b4c2-170835fe4930', 'x-dashscope-call-gateway': 'true', 'content-type': 'application/json', 'req-cost-time': '6533', 'req-arrive-time': '1760447494053', 'resp-start-time': '1760447500587', 'x-envoy-upstream-service-time': '6486', 'content-encoding': 'gzip', 'date': 'Tue, 14 Oct 2025 13:11:40 GMT', 'server': 'istio-envoy', 'transfer-encoding': 'chunked'})
2025-10-14 21:11:50,513 - DEBUG - request_id: 64136208-2052-46b1-b4c2-170835fe4930
2025-10-14 21:11:50,513 - DEBUG - API request completed in 6.58 seconds
2025-10-14 21:11:50,513 - DEBUG - Raw model response: {"labels": [1,0,0,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,1,1,0,0,0,0,0,1,1,1,0,1,1,1,1,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}
2025-10-14 21:11:50,513 - INFO - Successfully processed 112 labels
2025-10-14 21:11:50,513 - ERROR - Label count mismatch for Re 3 Dial_ Retrieve_ Reorganize and Rescale Conversations for Long_Turn Open_Domain Dialogue Pre_training
2025-10-14 21:11:50,513 - INFO - Evaluating paper 16/18: Syn2Real_ Forgery Classification via Unsupervised Domain Adaptation
2025-10-14 21:11:50,513 - INFO - Starting model prediction
2025-10-14 21:11:50,513 - INFO - Attempt 1 of 5
2025-10-14 21:11:50,514 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-957b0186-be86-4bdb-a057-d2794a0a28c9', 'json_data': {'messages': [{'role': 'system', 'content': '你是一个专门用于识别学术论文中数据集描述的助手。你需要判断每个输入的句子是否描述了研究中使用的数据集。请确保：\n    1. 只输出JSON格式的结果\n    2. 结果格式必须为 {"labels": [0,1,0,...]}\n    3. 不要输出任何其他文字或解释'}, {'role': 'user', 'content': '你是一位专业的学术论文数据集描述识别专家。请仔细分析下面这篇论文中的每个句子，判断它们是否描述了本研究特定构建的数据集信息。\n\n    论文标题: Syn2Real_ Forgery Classification via Unsupervised Domain Adaptation\n\n    判断标准:\n    判断为1(数据集描述)的标准：\n    句子必须直接描述本研究特定构建的数据集,包括:\n    1. 该数据集的构建过程和方法\n    2. 该数据集的具体构成和规模\n    3. 该数据的来源和收集方式\n    4. 该数据的预处理步骤\n    5. 该数据集的可获取方式(如发布地址)\n    6. 或者其他直接或者间接描述了该数据集的句子\n\n    所有其他类型的句子均标记为0。\n    \n    请对每个句子进行分析，返回一个JSON格式的标签数组。格式要求：\n    1. 必须是有效的JSON格式\n    2. 只包含labels字段，值为0和1组成的数组\n    3. 数组长度必须与句子数量相同\n    4. 示例格式：{"labels": [0,1,0,1,...]}\n\n    需要判断的句子：\n\n    1. Abstract: In recent years, image manipulation is becoming increasingly more accessible, yielding more natural-looking images, owing to the modern tools in image processing and computer vision techniques.\n2. The task of the identification of forged images has become very challenging.\n3. Amongst different types of forgeries, the cases of Copy-Move forgery are increasing manifold, due to the difficulties involved to detect this tampering.\n4. To tackle such problems, publicly available datasets are insufficient.\n5. In this paper, we propose to create a synthetic forged dataset using deep semantic image inpainting and copy-move forgery algorithm.\n6. However, models trained on these datasets have a significant drop in performance when tested on more realistic data.\n7. To alleviate this problem, we use unsupervised domain adaptation networks to detect copy-move forgery in new domains by mapping the feature space from our synthetically generated dataset.\n8. Furthermore, we improvised the F1 score on CA-SIA and CoMoFoD dataset to 80.3% and 78.8%, respectively.\n9. Our approach can be helpful in those cases where the classification of data is unavailable.\n10. With the advancement of new image editing technologies, there is a sharp increase in the number of forgery cases.\n11. While sophisticated image editing tools are meant to enhance the quality of images, they are misused to create forged images for nefarious purposes.\n12. These images look so natural that it is difficult to tell with naked eyes whether they have been tampered or are they authentic.\n13. [10]There are diverse ways of forging images, of which Copy-Move, Splicing, Retouching, and Resampling forgeries are the most common ones.\n14. Copy-Move Forgery (CMF) is a type of passive image forgery technique in which a section of an image is copied and pasted within the same image.\n15. Many post-image processing operations such as rescaling, affine transformations, resizing, and blurring are applied to the copied region.\n16. As the source and target image remains the same, the photometric characteristics of the image remain mostly invariable.\n17. Thus, the detection becomes even more difficult.\n18. For instance, in contrast to CMF, splicing forgery is a composition of two images.\n19. A section is cut from an image and pasted on another image.\n20. As a result, there is an edge discrepancy that makes the detection of splicing forgery relatively easier.\n21. Image tampering can have significant effects in various domains.\n22. For instance, in medical imaging, the images are procured with the utmost sensitivity and is a tiresome process.\n23. There can be ulterior economical motives for tampering these confidential and sophisticated images.\n24. Consequently, it could misguide the patients about their illnesses and injuries.\n25. In the field of education, students can tamper their documents with online available software tools.\n26. The significant impact of image tampering can happen in the socio-political area, as manipulated images can affect the perception of a large group of people.\n27. Many magazines and newspaper editors tamper the images in such a way that they can change the semantic meaning of the image.\n28. There have been several traditional approaches for forgery detection that include mostly block-based and keypoint feature extraction[7,16,21]and matching procedures.\n29. Nowadays, deep learning approaches[20,1,17]have been proposed to counterattack the problem of image forgery.\n30. However, most of the approaches are based on supervised learning.\n31. When there are a lot of labeled examples, then it is easy to train the model via supervised learning.\n32. To counter the problems of training data, we generally surrogate the training data by including the dataset from adjacent modality or use synthetic imagery.\n33. When the same model is evaluated on these datasets, it results in a significant drop in the performance.\n34. It happens due to the shift in style, content, or appearance distribution between various datasets.\n35. In these cases, domain adaptation is needed to learn the distribution shift.\n36. In this work, we show that manipulations in images across different domains can be detected via domain adaptation.\n37. We leverage the power of Convolutional Neural Figure1.\n38. The first and second column shows the example of target domain dataset (CASIA and CoMoFoD respectively).\n39. Subsequent column shows the generated synthetic data from COCO dataset using semantic inpainting and copy-move forgery algorithm.\n40. First row is authentic image of each category and second row is forged image.networks (CNNs) to perceive the distinguishable features of authentic and tampered images.\n41. We tackle the problem of performance drop by incorporating the feature space alignment between our synthetic generated datasets and datasets that are publicly available.\n42. We generate the synthetic dataset using Edge-connect semantic inpainting and CMF algorithm.\n43. Contributions Our main contributions in the paper are summarized as follows: 1) The primary task is to classify images as forged or authentic, for which we employ Unsupervised Domain Adaptation (DA), due to the difference in content and style between our source and target dataset, 2) As the publicly available datasets are small, we generate a new dataset comprising of 80,000 images using deep semantic inpainting and copy-move forgery algorithms on COCO[6]dataset, and 3) We explore two Unsupervised DA methods to adapt the features from source dataset to target dataset, such that the variation between the domains is minimized.\n44. The paper is organized as follows: Section II describes the traditional and deep learning solutions that evolved over the years for forgery classification and a review of domain adaptation methods.\n45. Section III describes our methodology in detail that involves dataset generation, Unsupervised DA, and final architectures used for training.\n46. After that, in Section IV, we evaluate the performance of our architecture on CASIA[2]and CoMoFoD[13]dataset.\n47. Section V discusses the conclusion and future directions of our work.\n48. We applied two methods to generate the dataset.\n49. The inclusion of any one of them shows an increase in performance.\n50. Semantic Inpainting helps the model to learn edge discrepancies when the objects are removed.\n51. Copy-Move tampered images improve the focus of the network to recognize similar patches.\n52. We evaluated our architecture on CASIA V2 and CoMo-FoD datasets.\n53. In our case, the source domain constitutes of COCO CMF and semantic inpainted images, and, target domain comprises CASIA V2 and CoMoFoD datasets.\n54. Exhaustive experiments were done using AlexNet[5]and VGG-7[11]for feature extraction.\n55. These datasets are explained briefly in the following sections: It contains 12,614 images in total, of which 7,497 are authentic, and 5,123 are forged images.\n56. The resolution of images ranges from 240 x 160 to 900 x 600.\n57. The tampered images have been applied to post-processing operations and saved in JPEG and TIFF formats.\n58. Out of these 5,123 tampered images, 3,274 images are copy-move, and 1,788 are splicing.\n59. The number of authentic images presents, respectively, for forged images, are 1,701.\n60. Henceforth, our total dataset size comes out to be of 4,975 images.\n61. This dataset contains 400 images, 200 authentic, and 200 forged.\n62. It contains only copy-move forgery cases in PNG format.\n63. The dimension of images in this dataset is 512 x 512.\n64. Various distortions such as translation, rotation, and scaling are applied to tampered images.\n65. We explored diverse color spaces to get a sense of the behavior of CMF images in different color spaces.\n66. Using Alexnet for feature extraction and DANN for domain adaptation, we varied the number of CMF images across RGB and YCrCb color space for the CASIA dataset.\n67. Chrominance component of YCrCb illuminates the identical regions in images with the same luminosity.\n68. It helps the deep networks to visualize copy-pasted regions in images.\n69. In DANN, we used categorical cross-entropy as loss function and Adam optimizer with learning rate 0.001.\n70. The DDC network is trained using Stochastic Gradient Descent optimizer, with a momentum value of 0.9, and learning rate value of 0.0001.\n71. At the time of training, we initially used only CMF images for unsupervised domain adaptation.\n72. Then, we included semantic inpainted images to study the effects of edge discrepancy in recognizing forged images.\n73. There are no labels used at the time of training.\n74. For the target domain, images are passed with a domain label attached to it, and the source domain has a class label also assigned to it.\n75. The source model adapts the weights to classify target images with the same features into a particular category.\n76. During testing, the target images are passed through the source classifier model, whose weights are now adapted to features specific to target data.\n77. 80% of the data used for training, and then, 20% used for testing.\n78. As CoMoFoD contains only 200 images, all the images were used to learn the discriminative features, as well as for evaluation.\n79. We used classification accuracy, precision, recall, and F1-score as performance metrics to evaluate our architectures.\n80. Precision is expressed as the number of true positives divided by the sum of true and false positives.\n81. The recall is defined as the ratio of true positives by true positives and false negatives.\n82. F1-score is the harmonic mean of recall and precision score.\n83. We will now discuss the results summarized in Table1, 2 and 3.\n84. We trained our architecture on source dataset and evaluated it on target dataset.\n85. From Table As the number of images increased, the results improved for domain adaptation.\n86. Due to complex post-processing operations, YCrCb space was unable to localize same tampered regions.\n87. As RGB color space performed better, therefore, for our future training of domain adaptation algorithms, we chose RGB images for source and target domains.\n88. In starting, we only used CMF images for unsupervised DA.\n89. DANN and DDC were able to minimize the distance between the two datasets distributions, but using only CMF images makes the network biased towards objects resembling the same feature characteristics.\n90. To analyze the contribution of the amount of CMF images for domain adaptation, we examined each time with an increment of 10,000 images.\n91. We saw that just by increasing CMF images, there are no noteworthy changes in the accuracy and F1-score on the target domain.\n92. In cases where the copied and background region are the same, e.g., grass, then the model is unable to distinguish the image as authentic or tampered.\n93. To alleviate this problem, we incorporated semantic inpainted images to learn the edge discriminative features.\n94. Itmodel to learn the dissimilarities near the edges of the images are copy-pasted.\n95. As the target domain contains CMF images, increasing the distribution of semantic images beyond 10,000 images leads to drop in performance.\n96. Table2shows the effect of utilizing both semantic inpainted and copy-move tampered images.\n97. In contrast to contemporary networks such as Inception[12]and ResNet[4], we used AlexNet and VGG-7 as our base models, because, these networks have a huge number of parameters and due to limited amount of target domain images, the model doesn\'t generalize well.\n98. COCO → CASIA: In CASIA, there are 3979 images used for training purposes and 996 images for evaluation.\n99. With DDC, we saw a sudden jump by including semantic inpainted images.\n100. Using DANN, we achieved the best score, when the highest number of images were used.\n101. As there is a large number of images available at the time of training, we can see from Table3that DANN+VGG-7 achieves the highest recall and F1-score.\n102. COCO → CoMoFoD: CoMoFoD dataset is very small.\n103. Due to the presence of 200 images only, we trained and evaluated on the whole dataset.\n104. With the increase in the number of images in the source domain, accuracy and F1 score decreased in DDC, and, insignificant increase using DANN.\n105. As the dataset was small, we can see from Table3that DDC+ MMD with Alexnet as base model performed better compared to VGG-7.\n106. VGG-7 has a huge number of parameters that can\'t be optimized; hence, they performed poorly at the test time.\n107. To compare with previous work, we analyzed our results with BusterNet architecture.\n108. They mainly took into account of CASIA CMF and CoMoFoD dataset.\n109. Other works, mainly used all images of CASIA dataset, not explicitly for CMF images.\n110. In BusterNet, they created and trained on 1 lakh images for supervised training, and then evaluated on these datasets.\n111. In CoMoFoD, they used 200 images as ours, but, in CASIA, they took only 1356 CMFD images into account compared to 4975 of ours.\n112. Our approach improves the accuracy by 5-6% in the case of CASIA and 27-28% in the case of CoMoFoD.\n113. Table3shows the performance comparison between ours and BusterNet.\n114. Whereas Buster-Net has used pixel-wise annotations to learn the class of images, we have not used any label at the time of training.\n115. In our case, as the data distribution is too much imbalanced, precision and recall score plays a significant role.\n116. We can see that our precision score is not in the comparable range of recall scores.\n117. It is due to the reason, as we have less num-ber of positive class images in contrast to the negative class.\n118. As we look into the denominator of precision and recall, in the first case, the denominator is the sum of true plus false positives.\n119. Now, we have too many images in a false class.\n120. It attributes to a large number of false positives.\n121. Whereas in the recall, the denominator is the sum of true positives plus false negatives.\n122. The false-negative number is less as the number of images in the correct class is fewer.\n\n请严格按照JSON格式返回：{"labels": [0,1,0,...]}，不要包含任何其他文字。'}], 'model': 'qwen-plus', 'response_format': {'type': 'json_object'}}}
2025-10-14 21:11:50,515 - DEBUG - Sending HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
2025-10-14 21:11:50,515 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-10-14 21:11:50,515 - DEBUG - send_request_headers.complete
2025-10-14 21:11:50,515 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-10-14 21:11:50,515 - DEBUG - send_request_body.complete
2025-10-14 21:11:50,515 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-10-14 21:11:56,205 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'vary', b'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding'), (b'x-request-id', b'ac6f4805-1596-415c-abe1-a1c7fb95d6d1'), (b'x-dashscope-call-gateway', b'true'), (b'content-type', b'application/json'), (b'req-cost-time', b'5645'), (b'req-arrive-time', b'1760447500634'), (b'resp-start-time', b'1760447506280'), (b'x-envoy-upstream-service-time', b'5602'), (b'content-encoding', b'gzip'), (b'date', b'Tue, 14 Oct 2025 13:11:46 GMT'), (b'server', b'istio-envoy'), (b'transfer-encoding', b'chunked')])
2025-10-14 21:11:56,206 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-14 21:11:56,206 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-10-14 21:11:56,207 - DEBUG - receive_response_body.complete
2025-10-14 21:11:56,207 - DEBUG - response_closed.started
2025-10-14 21:11:56,207 - DEBUG - response_closed.complete
2025-10-14 21:11:56,207 - DEBUG - HTTP Response: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "200 OK" Headers({'vary': 'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding', 'x-request-id': 'ac6f4805-1596-415c-abe1-a1c7fb95d6d1', 'x-dashscope-call-gateway': 'true', 'content-type': 'application/json', 'req-cost-time': '5645', 'req-arrive-time': '1760447500634', 'resp-start-time': '1760447506280', 'x-envoy-upstream-service-time': '5602', 'content-encoding': 'gzip', 'date': 'Tue, 14 Oct 2025 13:11:46 GMT', 'server': 'istio-envoy', 'transfer-encoding': 'chunked'})
2025-10-14 21:11:56,207 - DEBUG - request_id: ac6f4805-1596-415c-abe1-a1c7fb95d6d1
2025-10-14 21:11:56,208 - DEBUG - API request completed in 5.69 seconds
2025-10-14 21:11:56,208 - DEBUG - Raw model response: {"labels": [0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}
2025-10-14 21:11:56,209 - INFO - Successfully processed 106 labels
2025-10-14 21:11:56,209 - ERROR - Label count mismatch for Syn2Real_ Forgery Classification via Unsupervised Domain Adaptation
2025-10-14 21:11:56,209 - INFO - Evaluating paper 17/18: Topic Shift Detection in Chinese Dialogues_ Corpus and Benchmark
2025-10-14 21:11:56,209 - INFO - Starting model prediction
2025-10-14 21:11:56,209 - INFO - Attempt 1 of 5
2025-10-14 21:11:56,211 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-6bb6e627-1752-47d4-8b74-44e34b2ed903', 'json_data': {'messages': [{'role': 'system', 'content': '你是一个专门用于识别学术论文中数据集描述的助手。你需要判断每个输入的句子是否描述了研究中使用的数据集。请确保：\n    1. 只输出JSON格式的结果\n    2. 结果格式必须为 {"labels": [0,1,0,...]}\n    3. 不要输出任何其他文字或解释'}, {'role': 'user', 'content': '你是一位专业的学术论文数据集描述识别专家。请仔细分析下面这篇论文中的每个句子，判断它们是否描述了本研究特定构建的数据集信息。\n\n    论文标题: Topic Shift Detection in Chinese Dialogues_ Corpus and Benchmark\n\n    判断标准:\n    判断为1(数据集描述)的标准：\n    句子必须直接描述本研究特定构建的数据集,包括:\n    1. 该数据集的构建过程和方法\n    2. 该数据集的具体构成和规模\n    3. 该数据的来源和收集方式\n    4. 该数据的预处理步骤\n    5. 该数据集的可获取方式(如发布地址)\n    6. 或者其他直接或者间接描述了该数据集的句子\n\n    所有其他类型的句子均标记为0。\n    \n    请对每个句子进行分析，返回一个JSON格式的标签数组。格式要求：\n    1. 必须是有效的JSON格式\n    2. 只包含labels字段，值为0和1组成的数组\n    3. 数组长度必须与句子数量相同\n    4. 示例格式：{"labels": [0,1,0,1,...]}\n\n    需要判断的句子：\n\n    1. Abstract: Dialogue topic shift detection is to detect whether an ongoing topic has shifted or should shift in a dialogue, which can be divided into two categories, i.e., response-known task and response-unknown task.\n2. Currently, only a few investigated the latter, because it is still a challenge to predict the topic shift without the response information.\n3. In this paper, we first annotate a Chinese Natural Topic Dialogue (CNTD) corpus consisting of 1308 dialogues to fill the gap in the Chinese natural conversation topic corpus.\n4. And then we focus on the response-unknown task and propose a teacher-student framework based on hierarchical contrastive learning to predict the topic shift without the response.\n5. Specifically, the response at high-level teacher-student is introduced to build the contrastive learning between the response and the context, while the label contrastive learning is constructed at low-level student.\n6. The experimental results on our Chinese CNTD and English TIAGE show the effectiveness of our proposed model.\n7. Dialogue topic shift detection is to detect whether a dialogue\'s utterance has shifted in the topic, which can help the dialog system to change the topic and guide the dialogue actively.\n8. Although dialog topic shift detection is a new task, it has become a hotspot due to its remarkable benefit to many downstream tasks, such as response generation[1]and reading comprehension[2,3], and can help those real-time applications produce on-topic or topic-shift responses which perform well in dialogue scenarios[4,5,6].\n9. The task of dialogue topic shift detection can be divided into two lines, i.e., response-known task and response-unknown task, as shown in Fig.1.\n10. The former can gain the response information and obtain a better result, while the latter is the opposite.\n11. Moreover, both of them are not accessible to future information.\n12. This is the biggest difference from the task of text topic segmentation, in which all the basic utterances are visible to each other.\n13. That is, those existing topic segmentation models cannot be applied to dialogue topic shift detection since it depends on the response and its subsequent utterances heavily.\n14. Therefore, it is more difficult to discern differences between utterances in the task of dialogue topic shift detection.\n15. Due to the absence of future utterances, dialogue topic shift detection is still a challenging task.\n16. In this paper, we focus on the response-unknown task of topic shift detection in Chinese dialogues.\n17. There are two issues in the response-unknown task of topic shift detection in Chinese dialogues, i.e., lack of annotated corpus in Chinese and how to predict the response.\n18. Fig.1.\n19. Two lines of dialogue topic shift detection tasks to detect whether it exists topic shift between the utterances ui-1 and ui, where the response-known task (left) can use the response ui, while the response-unknown task (right) can be regarded as topic shift prediction without the response ui.\n20. There are only a few publicly dialogue topic shift corpus available and most of them are provided for the segmentation task, which does not satisfy natural conversation.\n21. Xie et al.[7]provided a detailed definition of the dialogue topic shift detection task, and annotated an English dialogue topics corpus TIAGE.\n22. Although it can fill the gap in the corpus of English conversation topics, its scale is still too small.\n23. In Chinese, Xu et al.[8]annotated a Chinese dialogue topic corpus.\n24. However, due to its small size and poor quality, this is detrimental to the further research and development of Chinese dialogue topic shift tasks.\n25. To fill the gap in the Chinese natural dialogue topic corpus, we first annotated a Chinese Natural Topic Dialogue (CNTD) corpus which consists of 1308 dialogues with high quality.\n26. Xie et al.[7]also established a benchmark for this response-unknown task based on the T5 model[9]and this benchmark only used the context to predict topic shift and performed poorly due to the lack of the response information.\n27. Thus, it is more challenging to predict the topic shift in natural dialogue without useful response information.\n28. The teacher-student framework has been used widely to obtain information that is not available to the model[1].\n29. To solve the issue of the lack of response information, we propose a teacher-student framework to introduce the response information.\n30. The teacher can obtain the response information, and the student can learn the response information from the teacher through knowledge distillation.\n31. To facilitate knowledge transfer, the student mimics the teacher on every layer instead of just the top layer, which alleviates the delayed supervised signal problem using hierarchical semantic information in the teacher[10].\n32. Besides, we construct hierarchical contrastive learning in which we consider the teacher-student as high-level and the student as low-level.\n33. At high-level, we build an information simulation loss between the context and the response to improve the semantic information of the student model with more reliable predictive information.\n34. At low-level, we design a semantic coherence-aware loss to better distinguish the different shift cases and produce more reliable prediction results.\n35. Finally, the experimental results on our Chinese CNTD and the English TIAGE show that our proposed model outperforms the baselines.\n36. The contributions of this paper are as follows.\n37. -We introduce hierarchical contrastive learning to further improve performance.\n38. -The experimental results both on the CNTD and TIAGE datasets show that our model outperforms the baselines.\n39. Previous studies explored the dialogue topic tasks and published the annotated topic dialogue corpus.\n40. For English, Xie et al.[7]annotated the TIAGE consisting of 500 dialogues with 7861 turns based on PersonaChat[11].\n41. Xu et al.[8]built a dataset including 711 dialogues by joining dialogues from existing multi-turn dialogue datasets: MultiWOZ Corpus[12], and Stanford Dialog Dataset[13].\n42. Both corpora are either small or limited to a particular domain, and neither applies to the study of the natural dialogue domain.\n43. For Chinese, Xu et al.[8]annotated a dataset including 505 phone records of customer service on banking consultation.\n44. However, this corpus is likewise restricted to a few specialized domains while natural dialogues are more complicated.\n45. Natural dialogues have a range of topic shift scenarios, unrestricted topics, and more free colloquialisms in the utterances.\n46. The above corpus is insufficient to fill the gap in the Chinese natural dialogue topic corpus.\n47. The existing corpus of Chinese dialogue topic detection[8]is small and does not satisfy natural conversation.\n48. Although the English dialogue topic corpora can be converted into Chinese by machine translation, they lack natural conversation colloquiality and are small in size.\n49. Therefore, we annotate a Chinese dialogue topic detection corpus CNTD based on NaturalConv dataset[20].\n50. In this section, we show our annotation guidelines and outline the reasons for our selection of corpus sources, as well as the manual annotation procedure and data statistics.\n51. We also analyze the topic shift distribution in CNTD.\n52. Each dialogue in our corpus has a piece of news as a base document, which is not available in other corpus and can be used as additional information for further research and expansion.\n53. The news is from six domains, which brings our conversations closer to natural dialogue.\n54. Besides, the speakers in our corpus are not restricted in any way, which also makes it closer to natural dialogues.\n55. In addition, we annotated the fine-grained dialogues topics, refer to Section 3.2.\n56. Fine-grained labels are beneficial to promote further research on dialogue topics.\n57. Compared with the existing Chinese topic corpus annotated by Xu et al.[8], the dialogues in our corpus do not have meaningless and repetitive turns.\n58. Also, the corpus is more than twice the size of the other corpus.\n59. In addition, the news in the corpus can be studied as additional information for the dialogues.\n60. Following the annotation guidelines in TIAGE[7], we distinguish each dialogue turns whether changed the topic compared with the context.\n61. -Commenting on the previous context: The response is a comment on what is said by the speaker previously; -Question answering: The response is an answer to the question that comes from the speaker previously; -Developing the dialogue to sub-topics: The response develops to a sub-topic compared to the context; -Introducing a relevant but different topic: The response introduces a relevant but different topic compared to the context; -Completely changing the topic: The response completely changes the topic compared to the context.\n62. Among them, we uniformly identify the two cases of greeting and farewell specific to CNTD as the topic shift.\n63. We chose the NaturalConv dataset[20]as the source corpus, which contains about 400K utterances and 19.9K dialogues in multiple domains.\n64. It is designed to collect a multi-turn document grounded dialogue dataset with scenario and naturalness properties of dialogue.\n65. We consider NaturalConv as a promising dataset for dialogue topic detection for the following reasons: 1) NaturalConv is much closer to human-like dialogue with the natural property, including a full and natural setting such as scenario assumption, free topic extension, greetings, etc.; 2) NaturalConv contains about 400K utterances and 19.9K dialogues in multiple domains; 3) The average turn number of this corpus is 20, and longer dialogue contexts tend to exhibit a flow with more topics; 4) The corpus has almost no restrictions or assumptions about the speakers, e.g., no explicit goal is proposed[21].\n66. We have three annotators for coarse-grained annotations and two for fine-grained annotations.\n67. Both annotations are divided into three stages as follows.\n68. Co-annotation Stage First, for coarse-grained annotations, we draw a total of 100 dialogues from each domain of the NaturalConv dataset proportionally for a total of 2014 dialogue turns.\n69. In this stage, three annotators are asked to discuss every 20 dialogues they annotated, and each annotator is asked to give a reason for the annotation during the discussion.\n70. Finally, the Kappa value of all annotators for coarse-grained annotations at this stage is 0.7426.\n71. In addition, we annotated the fine-grained information based on the results of the complete coarse-grained annotations.\n72. Two annotators annotated the same 150 dialogues and discussed them several times for consistency.\n73. Finally, the kappa value of all annotators for fine-grained annotations at this stage is 0.9032.\n74. These kappa values confirm that our annotators already have sufficient annotation capabilities for independent annotation, as well as the high quality of our corpus.\n75. Independent-annotation Stage We ensured the quality of each annotator\'s annotation and judging criteria before starting the second phase of annotation.\n76. For both granularity annotations, we randomly assign the dialogues drawn from each domain to each annotator for independent annotation.\n77. At this stage, we annotate 1208 dialogues for coarse-grained annotations and 1158 dialogues for fine-grained annotations.\n78. Semi-automatic Rechecking Stage Finally, we use a semi-automatic rechecking process to ensure that the corpus is still of high quality.\n79. On the one hand, we automatically format the dialogues with annotations to detect formatting problems caused by manual annotation.\n80. On the other hand, we automatically match the related news to each dialogue and check that the topic attributes are consistent with the dialogue to rule out any possible errors.\n81. Due to the limited time, we randomly select 1308 dialogues from the Natural-Conv dataset and annotate them with four annotators.\n82. Finally, we construct a Chinese natural topic dialogues corpus containing 26K dialogue turns.\n83. As shown in Table2, we randomly split them into 1041 train, 134 validation, and 133 test dialogues respectively, according to the percentage of different categories.\n84. In addition, we show the details of CNTD in Table3, which shows that our corpus has enough topics and long turns which is suitable for dialogue topic detection.\n85. Finally, there are the statistics of our fine-grained labels, as shown in Table4.\n86. We count the number of dialogues with different numbers of topics, as shown in Fig.2.\n87. On another side, we count the distribution of topic shift signals in dialogues, shown in Fig.3.\n88. We can see there are a total of 21 turns and three peaks of topic shift signals, which occur in 2 nd , 4 th , and 18 th turns, respectively.\n89. The reason is that the dialogue in our corpus usually starts with a greeting and   ends with a farewell, which leads to more topic shifts at the beginning and end of the dialogues.\n90. In addition, the NaturalConv corpus gives a piece of news as the base document of the dialogue, so there are more frequent transitions from news to derived topics, leading to the third highest peak in 4 th turn.\n91. However, we think this is consistent with a natural dialogue scenario because people often talk about recent news after daily greetings.\n92. Based on the train/validation/test dataset of CNTD we partitioned in Table2and previous work on TIAGE[7], we extract (context, response) pairs from each dialogue as input and the label of response as a target for the responseunknown task.\n93. In our experiments, every utterance except the first utterance of the dialogue can be considered as a response.\n94. As for evaluation, we report Precision (P), Recall (R), and Micro-F1 scores.\n95. We use BERT as an encoder and fine-tune it during training.\n96. For both the TIAGE and CNTD corpus, all pre-trained model parameters are set to default values.\n97. We conduct our experiments on NVIDIA GeForce GTX 1080 Ti and NVIDIA GeForce GTX 3090 with batch sizes of 2 and 6 for both CNTD and TIAGE, with the initial learning rates of 2e-5.\n98. And we set the epochs of training to 20, and the dropout to 0.5.\n99. For the pre-trained models in the experiment, we apply BERT-base-Chinese and MT5-base to obtain the semantic representation of the dialogues in CNTD, and we apply BERT-base-uncased and T5-base to obtain the semantic representation of the dialogues in TIAGE.\n100. Dialogue topic shift detection is a new task and there is no complex model available, besides a simple T5[7]that can be considered as the SOTA model.\n101. Since we employ BERT as our encoder and the T5 model is used in TIAGE, we use the pre-trained models of T5[9]and BERT[27]as baselines.\n102. For BERT, we For T5, we also connect utterances in the context and classify the undecidable predicted results to the \'not a topic shift\' category.\n103. Table5shows the performance comparison between our model and the baselines, in which TS denotes our teacher-student model without the hierarchical comparative learning (HCL) and Ours denotes our final model, i.e., the addition of SCL on the student side based on the addition of ISL on both the teacher and student sides.\n104. It can be found that on CNTD, our model achieves a good improvement and improves both precision and recall in comparison with the baselines.\n105. Although T5 does not perform poorly on recall, its precision is inadequate in comparison with BERT, and it is clear that T5 is not effective in predicting topics.\n106. In contrast, TS improved by 1.0 in Micro-F1 in comparison with BERT, which confirms that the teacher-student framework is effective in introducing response information.\n107. As well, Ours improved by 4.0 in micro-F1 in comparison with TS, and also showed significant improvement in P and R, which fully demonstrates that our HCL can improve the model\'s ability to discriminate between different topic situations.\n108. In particular, our model improves on CNTD by 5.0 in comparison with the best baseline BERT, which shows the effectiveness of our proposed model.\n109. To verify the effectiveness of the components used in our model, we conduct ablation studies on CTND, and the experimental results are shown in Table6.\n110. If we remove ISL on the teacher side (-ISL S ) or the student side (-ISL T ), the performance of the model decreased by 1.5 and 1.3 on the Micro-F1 value, respectively, with the largest decrease after removing the ISL on the student side.\n111. Although -ISL T has the highest precision in predicting topics and lower error probability than Ours and -ISL S .\n112. However, it can be seen that adding ISL at both the teacher and student sides can better improve the correct prediction rate.\n113. Moreover, if we remove ISL both on the teacher and student side (-ISL T S ), it achieves a similar performance on Micro-F1, in comparison with -ISL S and -ISL T .\n114. However, it achieves the highest precision (58.8%).\n115. If we remove SCL (-SCL) or HCL (-HCL) from our model, the Micro-F1 value of the models -SCL and -HCL drop from 53.9 to 52.4 (-1.5) and 49.9 (-4.0), respectively.\n116. These results show that our Semantic Conherent-aware Loss(SCL), and Hierarchical Contrastive Learning(HCL) are effective for this task, especially HCL.\n117. In addition, we explore the performance of the dialogues with different numbers of topics to analyze our model in comparison with BERT, as shown in Table7.\n118. It can be found that our model has a better performance than BERT on dialogues with fewer topics.\n119. Our model gets at least a 6% improvement in topic shift prediction on dialogues with 2 to 5 topics and obtains above-average performance.\n120. And when the number of topics increases to 9, the performance improves because the conversation length is still about 20 and the topics shift more significantly.\n121. In Table8, we also investigate the recall of the topic shift detection for various topic turns.\n122. Our model is improved for varying degrees across topic turns, with the most significant improvements in turns 7-9.\n123. Even in long topic shift cases, our model can obtain an effective boost.\n124. However, the performance of our model inevitably decreases compared to short topic shift cases.\n125. When there are fewer topic turns, the topic shift situation is simpler, so it is easier to determine.\n126. When the length of turns becomes longer and the situation becomes complicated, the topic of long turns has more information so it is easier to identify.\n127. As shown in Table9, it can be found that our model also achieves a good improvement on English TIAGE.\n128. et al. we obtain the best performance on both recall and Micro-F1 values, especially on micro-F1 with a 5.8% improvement over T5.\n129. This proves that our model achieves the best performance both in English and Chinese.\n130. We also conducted a case study.\n131. The prediction made by our model, the BERT model on the instance, and the manual labels are shown in Table10.\n132. "etc., belonging to the questionanswering scenario.\n133. "etc. belonging to the commenting on the previous context scenario, our model or BERT cannot accurately predict the topic shift in this scenario.\n134. This shows that detecting the topic shifts in natural dialogue is still challenging.\n135. We further analyze the errors of the prediction produced in our experiments.\n136. Specifically, we analyzed the example to explore whether the error in the results of this example is prevalent in other dialogues.\n137. From Table10, we can find that the wrong predictions at 14 th and 18 th turn.\n138. "as \'topic shift\'.\n139. We counted the appearance of many errors, and the errors are mainly divided into two categories.\n140. One is for the "Introducing a relevant but different topic" type of utterance.\n141. It was predicted that no topic shift occurred due to the lack of information about the future of the conversation.\n142. The other is the "commenting on the previous context" category.\n143. Since this type of response does not affect the integrity of the previous topic, it is mostly predicted to be a topic shift.\n144. Table10.\n145. The results of BERT, Ours, and Human of different turns where "1" indicates that a topic shift has occurred and "0" indicates the opposite.\n146. We omit the lines with all 0.\n\n请严格按照JSON格式返回：{"labels": [0,1,0,...]}，不要包含任何其他文字。'}], 'model': 'qwen-plus', 'response_format': {'type': 'json_object'}}}
2025-10-14 21:11:56,215 - DEBUG - Sending HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
2025-10-14 21:11:56,215 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-10-14 21:11:56,216 - DEBUG - send_request_headers.complete
2025-10-14 21:11:56,216 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-10-14 21:11:56,216 - DEBUG - send_request_body.complete
2025-10-14 21:11:56,216 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-10-14 21:12:02,793 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'vary', b'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding'), (b'x-request-id', b'9fd308c2-6ec6-4b6e-aafa-041698a02680'), (b'x-dashscope-call-gateway', b'true'), (b'content-type', b'application/json'), (b'req-cost-time', b'6534'), (b'req-arrive-time', b'1760447506335'), (b'resp-start-time', b'1760447512869'), (b'x-envoy-upstream-service-time', b'6489'), (b'content-encoding', b'gzip'), (b'date', b'Tue, 14 Oct 2025 13:11:52 GMT'), (b'server', b'istio-envoy'), (b'transfer-encoding', b'chunked')])
2025-10-14 21:12:02,794 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-14 21:12:02,794 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-10-14 21:12:02,795 - DEBUG - receive_response_body.complete
2025-10-14 21:12:02,795 - DEBUG - response_closed.started
2025-10-14 21:12:02,795 - DEBUG - response_closed.complete
2025-10-14 21:12:02,795 - DEBUG - HTTP Response: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "200 OK" Headers({'vary': 'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding', 'x-request-id': '9fd308c2-6ec6-4b6e-aafa-041698a02680', 'x-dashscope-call-gateway': 'true', 'content-type': 'application/json', 'req-cost-time': '6534', 'req-arrive-time': '1760447506335', 'resp-start-time': '1760447512869', 'x-envoy-upstream-service-time': '6489', 'content-encoding': 'gzip', 'date': 'Tue, 14 Oct 2025 13:11:52 GMT', 'server': 'istio-envoy', 'transfer-encoding': 'chunked'})
2025-10-14 21:12:02,795 - DEBUG - request_id: 9fd308c2-6ec6-4b6e-aafa-041698a02680
2025-10-14 21:12:02,796 - DEBUG - API request completed in 6.59 seconds
2025-10-14 21:12:02,796 - DEBUG - Raw model response: {"labels": [0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]}
2025-10-14 21:12:02,796 - INFO - Successfully processed 110 labels
2025-10-14 21:12:02,797 - ERROR - Label count mismatch for Topic Shift Detection in Chinese Dialogues_ Corpus and Benchmark
2025-10-14 21:12:02,797 - INFO - Evaluating paper 18/18: Towards Equitable Representation in Text_to_Image Synthesis Models with the Cross_Cultural Understanding Benchmark _CCUB_ Dataset
2025-10-14 21:12:02,797 - INFO - Starting model prediction
2025-10-14 21:12:02,797 - INFO - Attempt 1 of 5
2025-10-14 21:12:02,799 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-958e5662-8857-4f33-9dfb-c4c78714d93b', 'json_data': {'messages': [{'role': 'system', 'content': '你是一个专门用于识别学术论文中数据集描述的助手。你需要判断每个输入的句子是否描述了研究中使用的数据集。请确保：\n    1. 只输出JSON格式的结果\n    2. 结果格式必须为 {"labels": [0,1,0,...]}\n    3. 不要输出任何其他文字或解释'}, {'role': 'user', 'content': '你是一位专业的学术论文数据集描述识别专家。请仔细分析下面这篇论文中的每个句子，判断它们是否描述了本研究特定构建的数据集信息。\n\n    论文标题: Towards Equitable Representation in Text_to_Image Synthesis Models with the Cross_Cultural Understanding Benchmark _CCUB_ Dataset\n\n    判断标准:\n    判断为1(数据集描述)的标准：\n    句子必须直接描述本研究特定构建的数据集,包括:\n    1. 该数据集的构建过程和方法\n    2. 该数据集的具体构成和规模\n    3. 该数据的来源和收集方式\n    4. 该数据的预处理步骤\n    5. 该数据集的可获取方式(如发布地址)\n    6. 或者其他直接或者间接描述了该数据集的句子\n\n    所有其他类型的句子均标记为0。\n    \n    请对每个句子进行分析，返回一个JSON格式的标签数组。格式要求：\n    1. 必须是有效的JSON格式\n    2. 只包含labels字段，值为0和1组成的数组\n    3. 数组长度必须与句子数量相同\n    4. 示例格式：{"labels": [0,1,0,1,...]}\n\n    需要判断的句子：\n\n    1. Abstract: It has been shown that accurate representation in media improves the well-being of the people who consume it.\n2. By contrast, inaccurate representations can negatively affect viewers and lead to harmful perceptions of other cultures.\n3. To achieve inclusive representation in generated images, we propose a culturally-aware priming approach for text-to-image synthesis using a small but culturally curated dataset that we collected, known here as Cross-Cultural Understanding Benchmark (CCUB) Dataset, to fight the bias prevalent in giant datasets.\n4. Our proposed approach is comprised of two fine-tuning techniques: (1) Adding visual context via fine-tuning a pre-trained text-to-image synthesis model, Stable Diffusion, on the CCUB text-image pairs, and (2) Adding semantic context via automated prompt engineering using the finetuned large language model, GPT-3, trained on our CCUB culturally-aware text data.\n5. CCUB dataset is curated and our approach is evaluated by people who have a personal relationship with that particular culture.\n6. Our experiments indicate that priming using both text and image is effective in improving the cultural relevance and decreasing the offensiveness of generated images while maintaining quality.\n7. † indicates corresponding authors.\n8. In media, studies repeatedly show that representation affects the well-being of its viewers[Shaw, 2010;Caswell et al., 2017;Elbaba, 2019].\n9. Representation can positively affect viewers by providing them with role models that they identify with, but it can also negatively affect viewers by creating harmful, stereotypical understandings of people and culture[Castañeda, 2018].\n10. When people are accurately represented in media, it allows people to properly understand cultures without harmful stereo- types forming[Dixon and Linz, 2000;Mastro and Greenberg, 2000].\n11. Despite the benefits of representation, many media generating Artificial Intelligence (AI) models show poor representation in their results[Ntoutsi et al., 2020].\n12. Many of these issues stem from their large training datasets which are gathered by crawling the Internet without filtering supervision and contain malign stereotypes and ethnic slurs among other problematic content[Birhane et al., 2021].\n13. As AI models are increasingly used to create and aid in the production of visual content, it is important that the models have a true understanding of culture such that it can give accurate and proper representation leading to well-being rewards for its consumers.\n14. In this paper, we aim to address such a representation issue in image generation and introduce a new task of culturally-aware image synthesis: generating visual content within a cultural context that is both accurate and inoffensive.\n15. Our overarching goal is to improve the well-being of consumers of the AI generated images with particular attention to those consumers from underrepresented groups.\n16. Specifically, we formulate the culturally-aware text-to-image synthesis task to take an additional input of a country name to specify a cultural context in addition to language description.\n17. It was found that large datasets such as the LAION-5B[Schuhmann et al., 2021]used to train many text-toimage synthesis models such as Stable Diffusion[Rombach et al., 2021]are Anglo-centric and Euro-centric[Birhane et al., 2021]as shown in the top row of Figure1.\n18. As a consequence, these powerful models may generate culturally offensive images due to misrepresentation during training.\n19. Our research question is, how can effective existing text-to-image models be improved to become more culturally representative and thus less offensive?\n20. It may be infeasible to vet billions of training examples for accurate cultural content.\n21. We hypothesize that a small dataset that is veritably representative of a culture can be used to prime pre-trained textto-image models to guide the model towards more culturally accurate content creation.\n22. To verify the hypothesis, we collected a dataset of image and caption pairs for 8 cultures.\n23. For each culture, data was collected by a few people who are native of that culture as they are the people who properly understand it and are most affected by its misrepresentations.\n24. We call this the Cross-Cultural Understanding Benchmark (CCUB) dataset which comprises of 100-200 images each with a manually written caption as shown in Figure2.\n25. We propose two techniques for enhancing the text-toimage pipelines using CCUB.\n26. First, we fine-tune a text-toimage synthesis model, Stable Diffusion, on the CCUB textimage pairs to generate images tailored for a given cultural context.\n27. We evaluate our approach\'s two components individually as well as combined against the baseline of simply specifying the culture in the text prompt.\n28. Our evaluation was performed by native people of each country.\n29. Our survey results based on 2,244 image comparisions conducted by 72 participants from 5 countries indicate that our proposed approach is both less offensive and more cultural relevant than simply adding the country name as a suffix to the prompt.\n30. Our contributions are as follows: 1.\n31. Following the definition of culture in[Halpern, 1955]and [Key and Comrie, 2021], nine categories are used to represent cultural elements in our dataset: food & drink, clothing, artwork, dance and music, religion, architecture, people, city and nature.\n32. The categories are further divided into traditional and modern to reflect a characteristic of the culture that culture changes over time.\n33. Our CCUB image are collected based on the nine cultural categories.\n34. For collection, we recruited cultural experts who confidently know this culture well or belong to it.\n35. Cultural experts are asked to collect 10-20 relevant images containing different objects for each cultural category.\n36. The images were collected either from Creative Commons licensed images from Google searches or the collectors own photographs.\n37. Cultural experts were also asked to select images with common or culturally representative items.\n38. Each image in the CCUB dataset is also captioned by cultural experts forming paired image-text data.\n39. Cultural experts were asked to focus on the general and specific items in each cultural image, rather than adding captions to subtle components of the image.\n40. The captions accurately express cultural contents in English as opposed to large datasets such asLAION [Schuhmann et al., 2021]which are scraped from the internet and not vetted for cultural accuracy.\n41. We produced surveys to evaluate the effectiveness of our two proposed techniques for culturally-aware text-to-image synthesis and compare them to a baseline of simply appending the culture to the prompt, e.g., "A family eating dinner , China." and using an existing text-to-image model.\n42. In setting up our study, we consider a comparative structure between images: the baseline image versus another image from our results.\n43. The setup of a single question in our survey was as follows: Given two images, the participant selects which image best fits three given comparative properties.\n44. The properties analyzed were: (1) Text and Image Alignment: Participants are given a text prompt and consider which of the two images is more similar to the prompt; (2) Cultural Alignment: Participants decide which of the two images is a better representation of the country\'s culture; and (3) Offensiveness: Participants consider which of the two images is more offensive to them.\n45. The participants for the study were selected based on whether they had a personal understanding of the culture for which the images in the survey were generated.\n46. Participants were recruited among university students, friends, and family members of the authors.\n47. It was ensured that the participants would not be able to discern the approaches used to generate the compared images by randomizing the order of questions and images in the survey.\n\n请严格按照JSON格式返回：{"labels": [0,1,0,...]}，不要包含任何其他文字。'}], 'model': 'qwen-plus', 'response_format': {'type': 'json_object'}}}
2025-10-14 21:12:02,800 - DEBUG - Sending HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
2025-10-14 21:12:02,800 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-10-14 21:12:02,800 - DEBUG - send_request_headers.complete
2025-10-14 21:12:02,800 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-10-14 21:12:02,800 - DEBUG - send_request_body.complete
2025-10-14 21:12:02,800 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-10-14 21:12:06,927 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'vary', b'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding'), (b'x-request-id', b'1cd0a050-f2c5-4a2a-b886-12828dc13534'), (b'x-dashscope-call-gateway', b'true'), (b'content-type', b'application/json'), (b'req-cost-time', b'4080'), (b'req-arrive-time', b'1760447512922'), (b'resp-start-time', b'1760447517003'), (b'x-envoy-upstream-service-time', b'4077'), (b'content-encoding', b'gzip'), (b'date', b'Tue, 14 Oct 2025 13:11:56 GMT'), (b'server', b'istio-envoy'), (b'transfer-encoding', b'chunked')])
2025-10-14 21:12:06,928 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-14 21:12:06,928 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-10-14 21:12:06,928 - DEBUG - receive_response_body.complete
2025-10-14 21:12:06,928 - DEBUG - response_closed.started
2025-10-14 21:12:06,928 - DEBUG - response_closed.complete
2025-10-14 21:12:06,929 - DEBUG - HTTP Response: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "200 OK" Headers({'vary': 'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding', 'x-request-id': '1cd0a050-f2c5-4a2a-b886-12828dc13534', 'x-dashscope-call-gateway': 'true', 'content-type': 'application/json', 'req-cost-time': '4080', 'req-arrive-time': '1760447512922', 'resp-start-time': '1760447517003', 'x-envoy-upstream-service-time': '4077', 'content-encoding': 'gzip', 'date': 'Tue, 14 Oct 2025 13:11:56 GMT', 'server': 'istio-envoy', 'transfer-encoding': 'chunked'})
2025-10-14 21:12:06,929 - DEBUG - request_id: 1cd0a050-f2c5-4a2a-b886-12828dc13534
2025-10-14 21:12:06,930 - DEBUG - API request completed in 4.13 seconds
2025-10-14 21:12:06,930 - DEBUG - Raw model response: {"labels": [0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,1,0,0,0,0,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0]}
2025-10-14 21:12:06,930 - INFO - Successfully processed 47 labels
2025-10-14 21:12:06,947 - ERROR - Error processing paper Towards Equitable Representation in Text_to_Image Synthesis Models with the Cross_Cultural Understanding Benchmark _CCUB_ Dataset: 'int' object has no attribute 'capitalize'
2025-10-14 21:12:06,947 - ERROR - No valid results obtained from any paper
2025-10-14 21:46:00,186 - INFO - Initializing ResearchDatasetEvaluator
2025-10-14 21:46:00,205 - INFO - Starting data loading process
2025-10-14 21:46:00,207 - DEBUG - Processing paper: A Multilingual Multi_Target Dataset for Stance Detection
2025-10-14 21:46:00,207 - DEBUG - Processed 139 sentences for paper A Multilingual Multi_Target Dataset for Stance Detection
2025-10-14 21:46:00,207 - DEBUG - Processing paper: Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation
2025-10-14 21:46:00,207 - DEBUG - Processed 256 sentences for paper Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation
2025-10-14 21:46:00,207 - DEBUG - Processing paper: Amazon_M2_ A Multilingual Multi_locale Shopping Session Dataset for Recommendation and Text Generation
2025-10-14 21:46:00,207 - DEBUG - Processed 121 sentences for paper Amazon_M2_ A Multilingual Multi_locale Shopping Session Dataset for Recommendation and Text Generation
2025-10-14 21:46:00,207 - DEBUG - Processing paper: AntM 2 C_ A Large Scale Dataset For Multi_Scenario Multi_Modal CTR Prediction
2025-10-14 21:46:00,207 - DEBUG - Processed 182 sentences for paper AntM 2 C_ A Large Scale Dataset For Multi_Scenario Multi_Modal CTR Prediction
2025-10-14 21:46:00,207 - DEBUG - Processing paper: AU_AIR_ A Multi_modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance
2025-10-14 21:46:00,207 - DEBUG - Processed 170 sentences for paper AU_AIR_ A Multi_modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance
2025-10-14 21:46:00,207 - DEBUG - Processing paper: AV_NeRF_ Learning Neural Fields for Real_World Audio_Visual Scene Synthesis
2025-10-14 21:46:00,207 - DEBUG - Processed 67 sentences for paper AV_NeRF_ Learning Neural Fields for Real_World Audio_Visual Scene Synthesis
2025-10-14 21:46:00,207 - DEBUG - Processing paper: BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment
2025-10-14 21:46:00,207 - DEBUG - Processed 286 sentences for paper BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment
2025-10-14 21:46:00,207 - DEBUG - Processing paper: Comprehensive Named Entity Recognition on CORD_19 with Distant or Weak Supervision
2025-10-14 21:46:00,207 - DEBUG - Processed 25 sentences for paper Comprehensive Named Entity Recognition on CORD_19 with Distant or Weak Supervision
2025-10-14 21:46:00,207 - DEBUG - Processing paper: DCQA_ DOCUMENT_LEVEL CHART QUESTION ANSWERING TOWARDS COMPLEX REASONING AND COMMON_SENSE UNDERSTANDING
2025-10-14 21:46:00,207 - DEBUG - Processed 179 sentences for paper DCQA_ DOCUMENT_LEVEL CHART QUESTION ANSWERING TOWARDS COMPLEX REASONING AND COMMON_SENSE UNDERSTANDING
2025-10-14 21:46:00,207 - DEBUG - Processing paper: Debate Helps Supervise Unreliable Experts
2025-10-14 21:46:00,207 - DEBUG - Processed 75 sentences for paper Debate Helps Supervise Unreliable Experts
2025-10-14 21:46:00,207 - DEBUG - Processing paper: Distilling Large Language Models for Matching Patients to Clinical Trials
2025-10-14 21:46:00,207 - DEBUG - Processed 91 sentences for paper Distilling Large Language Models for Matching Patients to Clinical Trials
2025-10-14 21:46:00,207 - DEBUG - Processing paper: ECMD_ An Event_Centric Multisensory Driving Dataset for SLAM
2025-10-14 21:46:00,207 - DEBUG - Processed 170 sentences for paper ECMD_ An Event_Centric Multisensory Driving Dataset for SLAM
2025-10-14 21:46:00,207 - DEBUG - Processing paper: Inline Citation Classification using Peripheral Context and Time_evolving Augmentation
2025-10-14 21:46:00,207 - DEBUG - Processed 35 sentences for paper Inline Citation Classification using Peripheral Context and Time_evolving Augmentation
2025-10-14 21:46:00,207 - DEBUG - Processing paper: llm_japanese_dataset v0_ Construction of Japanese Chat Dataset for Large Language Models and its Methodology
2025-10-14 21:46:00,207 - DEBUG - Processed 67 sentences for paper llm_japanese_dataset v0_ Construction of Japanese Chat Dataset for Large Language Models and its Methodology
2025-10-14 21:46:00,207 - DEBUG - Processing paper: Re 3 Dial_ Retrieve_ Reorganize and Rescale Conversations for Long_Turn Open_Domain Dialogue Pre_training
2025-10-14 21:46:00,207 - DEBUG - Processed 108 sentences for paper Re 3 Dial_ Retrieve_ Reorganize and Rescale Conversations for Long_Turn Open_Domain Dialogue Pre_training
2025-10-14 21:46:00,207 - DEBUG - Processing paper: Syn2Real_ Forgery Classification via Unsupervised Domain Adaptation
2025-10-14 21:46:00,207 - DEBUG - Processed 122 sentences for paper Syn2Real_ Forgery Classification via Unsupervised Domain Adaptation
2025-10-14 21:46:00,207 - DEBUG - Processing paper: Topic Shift Detection in Chinese Dialogues_ Corpus and Benchmark
2025-10-14 21:46:00,207 - DEBUG - Processed 146 sentences for paper Topic Shift Detection in Chinese Dialogues_ Corpus and Benchmark
2025-10-14 21:46:00,207 - DEBUG - Processing paper: Towards Equitable Representation in Text_to_Image Synthesis Models with the Cross_Cultural Understanding Benchmark _CCUB_ Dataset
2025-10-14 21:46:00,207 - DEBUG - Processed 47 sentences for paper Towards Equitable Representation in Text_to_Image Synthesis Models with the Cross_Cultural Understanding Benchmark _CCUB_ Dataset
2025-10-14 21:46:00,207 - INFO - Starting evaluation of 18 papers
2025-10-14 21:46:00,207 - INFO - Evaluating paper 1/18: A Multilingual Multi_Target Dataset for Stance Detection
2025-10-14 21:46:00,207 - INFO - Starting model prediction
2025-10-14 21:46:00,207 - INFO - Attempt 1 of 5
2025-10-14 21:46:00,265 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-e7e76c28-6daa-4ceb-a8ba-78c4f78a62d2', 'json_data': {'messages': [{'role': 'system', 'content': '你是一个专门用于识别学术论文中数据集描述的助手。你需要判断每个输入的句子是否描述了研究中使用的数据集。请确保：\n    1. 只输出JSON格式的结果\n    2. 结果格式必须为 {"labels": [0,1,0,...]}\n    3. 不要输出任何其他文字或解释'}, {'role': 'user', 'content': '你是一位专业的学术论文数据集描述识别专家。请仔细分析下面这篇论文中的每个句子，判断它们是否描述了本研究特定构建的数据集信息。\n\n    论文标题: A Multilingual Multi_Target Dataset for Stance Detection\n\n    判断标准:\n    判断为1(数据集描述)的标准：\n    句子必须直接描述本研究特定构建的数据集,包括:\n    1. 该数据集的构建过程和方法\n    2. 该数据集的具体构成和规模\n    3. 该数据的来源和收集方式\n    4. 该数据的预处理步骤\n    5. 该数据集的可获取方式(如发布地址)\n    6. 或者其他直接或者间接描述了该数据集的句子\n\n    所有其他类型的句子均标记为0。\n    \n    请对每个句子进行分析，返回一个JSON格式的标签数组。格式要求：\n    1. 必须是有效的JSON格式\n    2. 只包含labels字段，值为0和1组成的数组\n    3. 数组长度必须与句子数量相同\n    4. 示例格式：{"labels": [0,1,0,1,...]}\n\n    需要判断的句子：\n\n    1. Abstract: We extract a large-scale stance detection dataset from comments written by candidates of elections in Switzerland.\n2. The dataset consists of German, French and Italian text, allowing for a cross-lingual evaluation of stance detection.\n3. It contains 67 000 comments on more than 150 political issues (targets).\n4. Unlike stance detection models that have specific target issues, we use the dataset to train a single model on all the issues.\n5. To make learning across targets possible, we prepend to each instance a natural question that represents the target (e.g."Do you support X?").\n6. Baseline results from multilingual BERT show that zero-shot crosslingual and cross-target transfer of stance detection is moderately successful with this approach.\n7. In recent years many datasets have been created for the task of automated stance detection, advancing natural language understanding systems for political science, opinion research and other application areas.\n8. Typically, such benchmarks(Mohammad et al., 2016a)are composed of short pieces of text commenting on politicians or public issues and are manually annotated with their stance towards a target entity (e.g.Climate Change, or Trump).\n9. However, they are limited in scope on multiple levels(Küçük and Can, 2020).\n10. First of all, it is questionable how well current stance detection methods perform in a crosslingual setting, as the multilingual datasets avail-able today are relatively small, and specific to a single target(Taulé et al., 2017(Taulé et al., , 2018)).\n11. Furthermore, specific models tend to be developed for each single target or pair of targets(Sobhani et al., 2017).\n12. Concerns have been raised that cross-target performance is often considerably lower than fully supervised performance(Küçük and Can, 2020).\n13. In this paper we propose a much larger dataset that combines multilinguality and a multitude of topics and targets.\n14. X-stance comprises more than 150 questions about Swiss politics and more than 67k answers given by candidates running for political office in Switzerland.\n15. Questions are available in four languages: English, Swiss Standard German, French, and Italian.\n16. The language of a comment depends on the candidate\'s region of origin.\n17. We have extracted the data from the voting advice application Smartvote.\n18. Candidates respond to questions mainly in categorical form (yes / rather yes / rather no / no).\n19. They can also submit a freetext comment to justify or explain their categorical answer.\n20. An example is given in Figure1.\n21. We transform the dataset into a stance detection task by interpreting the question as a naturallanguage representation of the target, and the commentary as the input to be classified.\n22. The dataset is split into a multilingual training set and into several test sets to evaluate zeroshot cross-lingual and cross-target transfer.\n23. To provide a baseline, we fine-tune a multilingual BERT model(Devlin et al., 2019)on X-stance.\n24. We show that the baseline accuracy is comparable to previous stance detection benchmarks while leaving ample room for improvement.\n25. In addition, the model can generalize to a degree both crosslingually and in a cross-target setting.\n26. We have made the dataset and the code for reproducing the baseline models publicly available.\n27. Figure1: Example of a question and two answers in the X-stance dataset.\n28. The answers were submitted by electoral candidates on a voting advice website.\n29. The author of the German comment was in favor of the issue; the author of the French comment against.\n30. Both authors use comments to explain their respective stance.\n31. Provenance We downloaded the questions and answers via the Smartvote API 2 .\n32. The downloaded data cover 175 communal, cantonal and national elections between 2011 and 2020.\n33. All candidates in an election who participate in Smartvote are asked the same set of questions, but 2 https://smartvote.chdepending on the locale they see translated versions of the questions.\n34. They can answer each question with either \'yes\', \'rather yes\', \'rather no\', or \'no\'.\n35. They can supplement each answer with a comment of at most 500 characters.\n36. The questions asked on Smartvote have been edited by a team of political scientists.\n37. They are intended to cover a broad range of political issues relevant at the time of the election.\n38. A detailed documentation of the design of Smartvote and the editing process of the questions is provided byThurman and Gasser (2009).\n39. Preprocessing We merged the two labels on each pole into a single label: \'yes\' and \'rather yes\' were combined into \'favor\'; \'rather no\', or \'no\' into \'against\'.\n40. This improves the consistency of the data and the comparability to previous stance detection datasets.\n41. We did not further preprocess the text of the comments.\n42. Language Identification As the API does not provide the language of comments, we employed a language identifier to automatically annotate this information.\n43. We used the langdetect library(Shuyo, 2010).\n44. For each responder we classified all the comments jointly, assuming that responders did not switch code during the answering of the questionnaire.\n45. We applied the identifier in a two-step approach.\n46. In the first run we allowed the identifier to output all 55 languages that it supports out of the box, plus Romansh, the fourth official language in Switzerland3.\n47. We found that no Romansh comments were detected and that all unexpected outputs were misclassifications of German, French or Italian comments.\n48. We further concluded that little or no Swiss German comments are in the dataset; otherwise, some of them would have manifested themselves via misclassifications (e.g. as Dutch).\n49. In the second run, drawing from these conclusions, we restricted the identifier\'s set of choices to English, French, German and Italian.\n50. Filtering We pre-filtered the questions and answers to improve the quality of the dataset.\n51. In the right column the model encounters unseen answers to unseen questions within an unseen topic.\n52. The two test sets in parentheses are too small for a significant evaluation. questions and corresponding answers pertaining to national elections were included.\n53. In the context of communal and cantonal elections, candidates have answered both local questions and a subset of the national questions.\n54. Of those elections, we only considered answers to the questions that also had been asked in a national election.\n55. They were only used to augment the training set while the validation and test sets were restricted to answers from national elections.\n56. We discarded the fewer than 20 comments classified as English.\n57. Furthermore, we discarded instances that met any of the following conditions: • Question is not a closed question or does not address a clearly defined political issue.\n58. • No comment was submitted by the candidate or the comment is shorter than 50 characters.\n59. • Comment starts with "but" or a similar indicator that the comment is not self-contained.\n60. • Comment contains a URL.\n61. In total, a fifth of the comments were filtered out.\n62. Topics The questions have been organized by the Smartvote editors into categories (such as "Economy").\n63. We further consolidated the predefined categories into 12 broad topics (Table1).\n64. Compliance The dataset is shared under a CC BY-NC 4.0 license.\n65. Copyright remains with www.smartvote.ch.\n66. Given the sensitive nature of the data, we increase the anonymity of the data by hashing the respondents\' IDs.\n67. No personal attributes of the respondents are included in the dataset.\n68. We provide a data statement(Bender and Friedman, 2018)in Appendix B.\n69. We held out the topics "Healthcare" and "Political System" from the training data and created a separate cross-topic test set that contains the questions and answers related to those topics.\n70. Furthermore, in order to test cross-question generalization performance within previously seen topics, we manually selected 16 held-out questions that are distributed over the remaining 10 topics.\n71. We selected the held-out questions manually because we wanted to make sure that they are truly unseen and that no paraphrases of the questions are found in the training set.\n72. We designated Italian as a test-only language, since relatively few comments have been written in Italian.\n73. From the remaining German and French data we randomly selected a percentage of respondents as validation or as test respondents.\n74. As a result we received one training set, one validation set and four test sets.\n75. The sizes of the sets are listed in Table 2.\n76. We did not consider test sets that are cross-lingual and cross-target at the same time, as they would have been too small to yield significant results.\n77. We evaluate four baselines to obtain an impression of the difficulty of the task.\n78. The first pair of baselines uses the most frequent class in the training set for prediction.\n79. Specifically, the global majority class baseline predicts the most frequent class across all training targets while the target-wise majority class baseline predicts the class that is most frequent for a given target question.\n80. The latter can only be applied to the intra-target test sets.\n81. As a second baseline, we train a fastText bag-ofwords linear classifier(Joulin et al., 2017).\n82. For each comment, we select the translation of the question that matches its language, and concatenate it to the comment.\n83. We tokenize the text using the Europarl preprocessing tools(Koehn, 2005).\n84. The \'against\' class was slightly upsampled in the training data so that the classes are balanced when summing over all questions and topics.\n85. We use the standard settings provided by the fastText library.\n86. The word vectors were set to a size of 300.\n87. We do not initialize them with pre-trained multilingual embeddings since preliminary experiments did not show a beneficial effect.\n88. As our main baseline model we fine-tune multilingual BERT (M-BERT) on the task(Devlin et al., 2019)which has been pre-trained jointly in 104 languages 5 and has established itself as a state of the art for various multilingual tasks(Wu and Dredze, 2019;Pires et al., 2019).\n89. Within the field of stance detection, BERT can outperform both feature-based and other neural approaches in a monolingual English setting(Ghosh et al., 2019).\n90. Architecture In the context of BERT we interpret the X-stance task as sequence pair classification inspired by natural language inference tasks(Bowman et al., 2015).\n91. We follow the procedure outlined byDevlin et al. (2019)for such tasks.\n92. We designate the question as segment A and the comment as segment B.\n93. The two segments are separated with the special token [SEP], and the special token [CLS] is prepended to the sequence.\n94. The final hidden state corresponding to [CLS] is then classified by a linear layer.\n95. We fine-tune the full model with a cross-entropy loss, using the AllenNLP library(Gardner et al., 2018)as a basis for our implementation.\n96. Training As above, we balanced out the number of classes in the training set.\n97. We use a batch size of 16 and a maximum sequence length of 512 subwords, and performed a grid search over the following hyperparameters based on the validation accuracy: • Learning rate: 5e-5, 3e-5, 2e-5 No Italian samples were seen during training, making this a case of zero-shot cross-lingual transfer.\n98. The scores are reported as the macro-average of the F1scores for \'favor\' and for \'against\'.\n99. The grid search was repeated independently for every variant that we test in the following subsections.\n100. Furthermore, the standard recommendations for fine-tuning BERT were used: Adam with β 1 = 0.9 and β 2 = 0.999; an L2 weight decay of 0.01; a learning rate warmup over the first 10% of the steps; and a linear decay of the learning rate.\n101. A dropout probability of 0.1 was set on all layers.\n102. Results Table3shows the results for the crosslingual setting.\n103. M-BERT performs consistently better than the previous baselines.\n104. Even the zeroshot performance in Italian, while significantly lower than the supervised scores, is much better than the target-wise majority class baseline.\n105. Results for the cross-target setting are given in Table4.\n106. Similar to the cross-lingual setting, model performance drops in the cross-target setting, but M-BERT remains the strongest baseline and easily surpasses the majority class baselines.\n107. Furthermore, the cross-question score of M-BERT is slightly lower than the cross-topic score.\n108. The default setup preserves horizontal language consistency in that the language of the questions always corresponds to the language of the comments.\n109. For example, the Italian test instances are combined with the Italian version of the questions, even though during training the model has only ever seen the German and French version of them.\n110. An alternative concept is vertical language consistency, whereby the questions are consistently presented in one language, regardless of the comment.\n111. To test whether horizontal or vertical consistency is more helpful, we train and evaluate M-BERT on a dataset variant where all questions are in their English version.\n112. We chose English as a lingua franca because it had the largest share of data during the pre-training of M-BERT.\n113. Cross-topic Results are shown in Table5.\n114. While the effect is negligible in most settings, cross-lingual performance increases when all questions are in English.\n115. In order to rule out that only the questions or only the comments are necessary to optimally solve the task, we conduct some additional experiments: • Only use a single segment containing the comment, removing the questions from the training and test data (missing questions).\n116. • Only use the question and remove the comment (missing comments).\n117. In both cases the performance decreases across all evaluation settings (Table5).\n118. The loss in performance is much higher when comments are missing, indicating that the comments contain the most important information about stance.\n119. As can be expected, the score achieved without comments is only slightly different from the target-wise majority class baseline.\n120. But there is also a loss in performance when the questions are missing, which underlines the importance of pairing both pieces of text.\n121. The effect of missing questions is especially strong in the supervised and cross-lingual settings.\n122. To illustrate this, we provide in TableA8some examples of comments that occur with multiple different targets in the training set.\n123. Those examples can explain why the target can be essential for disambiguating a stance detection problem.\n124. On the other hand, the effect of omitting the questions is less pronounced in the cross-target settings.\n125. The above single-segment experiments tell us that both the comment and the question provide crucial information.\n126. But it is possible that the M-BERT model, even though trained on both segments, mainly looks at a single segment at test time.\n127. To rule this out, we probe the model with randomized data at test time: • Test the model on versions of the test sets where the comments remain in place but the questions are shuffled randomly (random questions).\n128. We make sure that the random questions come from the same test set and language as the original questions.\n129. • Keep the questions in place and randomize the comments (random comments).\n130. Again we shuffle the comments only within test set boundaries.\n131. The results in Table5show that the performance of the model decreases in both cases, confirming that it learns to take into account both segments.\n132. 4.6 How Important are Spelled-Out Targets?\n133. Finally we test whether the target really needs to be represented by natural language (e.g."Do you support X?").\n134. An alternative is to represent the target with a trainable embedding instead.\n135. In order to fit target embeddings smoothly into our architecture, we represent each target type with a different reserved symbol from the M-BERT vocabulary.\n136. Segment A is then set to this symbol instead of a natural language question.\n137. The results for this experiment are listed in the bottom row of Table5.\n138. An M-BERT model that learns target embeddings instead of encoding a question performs clearly worse in the supervised and cross-lingual settings.\n139. From this we conclude that spelled-out natural language questions provide important linguistic detail that can help in stance detection.\n\n请严格按照JSON格式返回：{"labels": [0,1,0,...]}，不要包含任何其他文字。'}], 'model': 'qwen-plus', 'response_format': {'type': 'json_object'}}}
2025-10-14 21:46:00,271 - DEBUG - Sending HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
2025-10-14 21:46:00,272 - DEBUG - connect_tcp.started host='dashscope.aliyuncs.com' port=443 local_address=None timeout=5.0 socket_options=None
2025-10-14 21:46:00,275 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7fd2c3dd1040>
2025-10-14 21:46:00,275 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x7fd216373640> server_hostname='dashscope.aliyuncs.com' timeout=5.0
2025-10-14 21:46:00,350 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7fd215a7ef40>
2025-10-14 21:46:00,351 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-10-14 21:46:00,351 - DEBUG - send_request_headers.complete
2025-10-14 21:46:00,351 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-10-14 21:46:00,351 - DEBUG - send_request_body.complete
2025-10-14 21:46:00,351 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-10-14 21:46:06,778 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'vary', b'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding'), (b'x-request-id', b'67085eac-e6db-497b-a9de-db5b46920cbf'), (b'x-dashscope-call-gateway', b'true'), (b'content-type', b'application/json'), (b'req-cost-time', b'6394'), (b'req-arrive-time', b'1760449551484'), (b'resp-start-time', b'1760449557878'), (b'x-envoy-upstream-service-time', b'6361'), (b'set-cookie', b'acw_tc=67085eac-e6db-497b-a9de-db5b46920cbf0240504bee2a9e0969a59bcd309d8aa5;path=/;HttpOnly;Max-Age=1800'), (b'content-encoding', b'gzip'), (b'date', b'Tue, 14 Oct 2025 13:45:57 GMT'), (b'server', b'istio-envoy'), (b'transfer-encoding', b'chunked')])
2025-10-14 21:46:06,780 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-14 21:46:06,780 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-10-14 21:46:06,780 - DEBUG - receive_response_body.complete
2025-10-14 21:46:06,781 - DEBUG - response_closed.started
2025-10-14 21:46:06,781 - DEBUG - response_closed.complete
2025-10-14 21:46:06,781 - DEBUG - HTTP Response: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "200 OK" Headers({'vary': 'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding', 'x-request-id': '67085eac-e6db-497b-a9de-db5b46920cbf', 'x-dashscope-call-gateway': 'true', 'content-type': 'application/json', 'req-cost-time': '6394', 'req-arrive-time': '1760449551484', 'resp-start-time': '1760449557878', 'x-envoy-upstream-service-time': '6361', 'set-cookie': 'acw_tc=67085eac-e6db-497b-a9de-db5b46920cbf0240504bee2a9e0969a59bcd309d8aa5;path=/;HttpOnly;Max-Age=1800', 'content-encoding': 'gzip', 'date': 'Tue, 14 Oct 2025 13:45:57 GMT', 'server': 'istio-envoy', 'transfer-encoding': 'chunked'})
2025-10-14 21:46:06,781 - DEBUG - request_id: 67085eac-e6db-497b-a9de-db5b46920cbf
2025-10-14 21:46:06,788 - DEBUG - API request completed in 6.58 seconds
2025-10-14 21:46:06,788 - DEBUG - Raw model response: {"labels": [1,1,1,0,1,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]}
2025-10-14 21:46:06,788 - INFO - Successfully processed 94 labels
2025-10-14 21:46:06,788 - ERROR - Label count mismatch for A Multilingual Multi_Target Dataset for Stance Detection
2025-10-14 21:46:06,788 - INFO - Evaluating paper 2/18: Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation
2025-10-14 21:46:06,788 - INFO - Starting model prediction
2025-10-14 21:46:06,788 - INFO - Attempt 1 of 5
2025-10-14 21:46:06,789 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-3c37ed3c-9a20-484c-a6b7-2c179bff9322', 'json_data': {'messages': [{'role': 'system', 'content': '你是一个专门用于识别学术论文中数据集描述的助手。你需要判断每个输入的句子是否描述了研究中使用的数据集。请确保：\n    1. 只输出JSON格式的结果\n    2. 结果格式必须为 {"labels": [0,1,0,...]}\n    3. 不要输出任何其他文字或解释'}, {'role': 'user', 'content': '你是一位专业的学术论文数据集描述识别专家。请仔细分析下面这篇论文中的每个句子，判断它们是否描述了本研究特定构建的数据集信息。\n\n    论文标题: Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation\n\n    判断标准:\n    判断为1(数据集描述)的标准：\n    句子必须直接描述本研究特定构建的数据集,包括:\n    1. 该数据集的构建过程和方法\n    2. 该数据集的具体构成和规模\n    3. 该数据的来源和收集方式\n    4. 该数据的预处理步骤\n    5. 该数据集的可获取方式(如发布地址)\n    6. 或者其他直接或者间接描述了该数据集的句子\n\n    所有其他类型的句子均标记为0。\n    \n    请对每个句子进行分析，返回一个JSON格式的标签数组。格式要求：\n    1. 必须是有效的JSON格式\n    2. 只包含labels字段，值为0和1组成的数组\n    3. 数组长度必须与句子数量相同\n    4. 示例格式：{"labels": [0,1,0,1,...]}\n\n    需要判断的句子：\n\n    1. Abstract: Digital humans have witnessed extensive applications in various domains, necessitating related quality assessment studies.\n2. However, there is a lack of comprehensive digital human quality assessment (DHQA) databases.\n3. To address this gap, we propose SJTU-H3D, a subjective quality assessment database specifically designed for full-body digital humans.\n4. It comprises 40 high-quality reference digital humans and 1,120 labeled distorted counterparts generated with seven types of distortions.\n5. The SJTU-H3D database can serve as a benchmark for DHQA research, allowing evaluation and refinement of processing algorithms.\n6. Further, we propose a zero-shot DHQA approach that focuses on no-reference (NR) scenarios to ensure generalization capabilities while mitigating database bias.\n7. Our method leverages semantic and distortion features extracted from projections, as well as geometry features derived from the mesh structure of digital humans.\n8. Specifically, we employ the Contrastive Language-Image Pre-training (CLIP) model to measure semantic affinity and incorporate the Naturalness Image Quality Evaluator (NIQE) model to capture low-level distortion information.\n9. Additionally, we utilize dihedral angles as geometry descriptors to extract mesh features.\n10. By aggregating these measures, we introduce the Digital Human Quality Index (DHQI), which demonstrates significant improvements in zeroshot performance.\n11. The DHQI can also serve as a robust baseline for DHQA tasks, facilitating advancements in the field.\n12. The database and the code are available at https://github.com/zzc-1998/SJTU-H3D.\n13. Fig.1.Motovation of our works.\n14. Unlike 2D images/videos, collection of 3D digital humans is more difficult and expensive.\n15. Therefore there is a lack of subjective databases for 3D digital humans currently.\n16. To tackle this issue, we propose the first perceptual quality assessment database for full-body digital humans (SJTU-H3D).\n17. Furthermore, in contrast to the numerous large-scale image/video quality assessment (I/VQA) databases that facilitate data-driven methodologies, supervised methods can be easily bothered by the bias of the limited number of DHQA databases, affecting generalization ability.\n18. Thus we propose a zero-shot no-reference quality assessment method to address this concern. humans.\n19. Regrettably, acquisition of digital human models is a laborious and costly process compared with 2D media such as images and videos, requiring specialized three-dimensional (3D) scanning devices and professional post-production, which makes it quite difficult to carry out digital human quality assessment (DHQA) databases.\n20. Therefore, few works about subjective DHQA have been carried out in the literature.\n21. Then the absence of large-scale subjective experiments for assessing the visual quality of digital humans further hinders progress in this domain.\n22. Therefore, in this paper, we propose a comprehensive subjective quality assessment database (SJTU-H3D) targeted at digital humans, aiming to address this research gap and contribute to the advancement of DHQA.\n23. The SJTU-H3D database introduced in this study comprises 40 high-quality reference digital humans, represented by textured meshes in full-body format, and the database includes 1,120 distorted digital humans that have been generated using seven different types of distortions.\n24. The perceptual mean opinion scores (MOSs) of these distorted digital humans are collected through a meticulously controlled subjective experiment.\n25. Notably, the SJTU-H3D database is the first large-scale database specifically designed for digital human quality assessment (DHQA) that focuses on full-body representations.\n26. The primary objective of this database is to advance the research and development of DHQA within the scientific community.\n27. Furthermore, it serves as an ideal platform for evaluating and refining various processing algorithms, including but not being limited to denoising and compression techniques.\n28. By providing a comprehensive database consisting of high-quality reference models and distorted counterparts, the proposed SJTU-H3D database offers researchers and practitioners an opportunity to explore and enhance their DHQA methodologies.\n29. The availability of such a resource is expected to significantly contribute to the growth and advancement of the DHQA research community.\n30. During recent years, data-driven image and video quality assessment (I/VQA) approaches[2],[3],[4],[5]have garnered significant attention and have demonstrated remarkable performance in various application domains.\n31. The success of these approaches can be partly attributed to the availability of largescale I/VQA databases such as the SPAQ database (containing 11,125 labeled images)[6]and the LSVQ database (comprising up to 38,811 annotated videos)[7].\n32. These databases have also contributed to ensuring the generalization capability and robustness of data-driven methods.\n33. However, in the realm of DHQA research, the availability of suitable perceptual quality assessment databases is limited.\n34. With the exception of the proposed SJTU-H3D database, only one perceptual quality assessment database, DHHQA[8], focusing solely on digital human heads rather than full-body representations, exists.\n35. This scarcity of databases makes it challenging to develop datadriven DHQA methods and ensure their generalization ability in practical scenarios.\n36. Hence, this challenge serves as a motivation for us to devise a zero-shot DHQA method that does not necessitate training on labeled DHQA databases.\n37. To cater to most practical applications where pristine references may not be readily available, our focus is only on no-reference (NR) methods.\n38. To extract both semantic and distortion features for evaluating the visual quality of digital humans, we employ projection rendering techniques.\n39. From a semantic perspective, we utilize the Contrastive Language-Image Pre-training (CLIP) model[9]to measure the correlation between the input projections and quality-related texts.\n40. Our hypothesis is that high-quality digital human projections should exhibit a strong correlation with positive quality-related texts and a weak correlation with negative ones.\n41. To determine the quality levels of the input projections, we design several positive-negative text pairs.\n42. The semantic affinity quality measure is then derived by computing the difference in affinity between positive and negative texts.\n43. However, CLIP operates on low-resolution images, which limits its ability to capture low-level distortion information.\n44. To address this limitation, we incorporate the completely blind Naturalness Image Quality Evaluator (NIQE)[10]to extract low-level quality representations from the raw resolution.\n45. To further enhance the accuracy of quality prediction, we also extract features from the mesh modality.\n46. For robustness and effectiveness, we choose the dihedral angle as the geometry descriptor, as it has been widely recognized for effectively capturing geometric features relevant to visual quality[11],[12],[13],[14]and its values are confined within the range of [0, π].\n47. By analyzing the changing tendency of dihedral angles corresponding to geometry compression and simplification levels, we average-pool the dihedral angles to derive the geometry loss quality measure.\n48. Finally, all three quality measures (semantic affinity quality measure, spatial naturalness quality measure, and geometry loss quality measure) are aggregated using a sum function to form the proposed Digital Human Quality Index (DHQI).\n49. Experimental results demonstrate that DHQI significantly improves zero-shot performance and even achieves competitiveness with supervised methods.\n50. In summary, our contributions are as follows: • We propose the first large-scale full-body DHQA database, SJTU-H3D, which consists of 40 high-quality digital humans represented by textured meshes and 1,120 distorted digital humans generated by 7 types of distortions.\n51. 40 human subjects are invited and a total of 44,800 ratings are collected to gather the mean opinion scores (MOSs) for 1,120 distorted digital humans.\n52. In this section, we give a brief introduction to the development of 3D model quality assessment (3DQA) and noreference image quality assessment (NR-IQA) methods.\n53. A. 3DQA Development 1) 3DQA Databases: Early subjective 3D quality assessment (3DQA) databases primarily employ colorless point clouds and are relatively small in scale[20],[21],[22].\n54. However, recent efforts have been directed towards addressing the challenge of assessing visual quality in colored 3D models, resulting in the development of substantial 3DQA databases[20],[21],[22],[15],[16],[17],[18],[19].\n55. A detailed comparison between these databases and the proposed database is presented in TableI.\n56. From the table, it is evident that the recent 3DQA databases, with the exception of DHHQA, encompass general 3D objects and do not specifically focus on 3D digital humans.\n57. Although the DHHQA database comprises real human heads, it neglects the consideration of the body part.\n58. This highlights the significance of the proposed SJTU-H3D database.\n59. 2) 3DQA Methods: In the field of 3D quality assessment (3DQA), metrics can be broadly categorized into model-based and projection-based methods.\n60. Model-based methods[23],[24],[11],[25],[26],[27],[28],[29],[30],[31]involve extracting features directly from the 3D model, which offers the advantage of being viewpoint-invariant and relatively straightforward.\n61. However, due to the inherent complexity of 3D models, these methods can be computationally expensive and time-consuming.\n62. On the other hand, projection-based methods[15],[32],[14],[33],[34]infer the visual quality of a 3D model based on its corresponding projections.\n63. These methods leverage mature and successful 2D media analysis tools, which often lead to excellent performance.\n64. However, projection-based methods are highly dependent on the selection of viewpoints and can be susceptible to instability when subjected to various rendering setups.\n65. More recently,[36], which utilizes natural scene statistics (NSS) in the spatial domain to analyze image quality.\n66. CPBD[37]estimates blur levels by computing the cumulative probability of blur detection.\n67. BMPRI[38]predicts image quality by generating multiple pseudo-reference images obtained through further degradation of the distorted image and comparing their similarities.\n68. NFERM[39]investigates image quality using the free energy principle.\n69. Deep learning-based IQA methods have gained momentum with the advancement of deep neural networks.\n70. DBCNN[40]consists of two streams of deep neural networks to address both synthetic and authentic distortions.\n71. HyperIQA[41]employs a self-adaptive hyper network to handle challenges arising from distortion diversity and content variation in IQA tasks.\n72. MUSIQ[42]utilizes a multi-scale image quality transformer to represent image quality at different levels of granularity.\n73. StairIQA[43]hierarchically integrates features extracted from intermediate layers to leverage low-level and high-level visual information.\n74. 2) Zero-shot NR-IQA: Zero-shot IQA methods, also known as opinion-unaware methods, have emerged, which do not rely on training on subjective-rated quality assessment databases and can operate on unseen images.\n75. The earliest zero-shot NR-IQA methods are NIQE[10]and IL-NIQE[44].\n76. NIQE extracts handcrafted natural scene statistics (NSS) features from raw-resolution images and quantifies naturalness quality by computing the Multivariate Gaussian (MVG) distance to high-quality images.\n77. IL-NIQE enhances the feature set by incorporating additional quality-aware features, including gradient features, log Gabor filter responses, and color statistics.\n78. In this section, we mainly present the construction details of the proposed SJTU-H3D database, which includes reference collection, reference characterization, distortion generation, and subjective experiment.\n79. In order to ensure the visual quality and content diversity of the reference 3D digital humans, a manual selection process is conducted to choose all reference digital humans from the HumanAlloy1, a wonderful platform that provides high-quality 3D humans.\n80. A total of 40 digital humans are purchased and collected for this study.\n81. These digital humans are represented as textured meshes, with texture resolutions of 2048×2048.\n82. Fig.2illustrates the rendered projections of the selected digital humans, and Table II provides detailed information regarding the number of vertices and faces for each model.\n83. The primary objective of our study is to curate a database that exhibits high diversity and generality while minimizing biases associated with the selection of source models.\n84. 1) Geometry Information: In the domain of image quality assessment (IQA), the analysis of spatial information often involves computing the standard deviation of the Sobel-filtered image.\n85. Motivated by this concept, we propose a novel approach to quantify the geometry information by utilizing the standard deviation of the dihedral angles in a mesh.\n86. The dihedral angle is a fundamental metric employed in computer graphics and geometric modeling to characterize the shape and curvature of meshes[11],[12], thus drawing a parallel to the Sobel-filtering process in image analysis.\n87. It denotes the angle between two neighboring faces that share an edge within the mesh, providing valuable insights into the smoothness or sharpness of the surface.\n88. Specifically, the geometry information can be obtained as: where GI represents the geometry information and std(•) stands for the standard deviation function.\n89. By leveraging the standard deviation of dihedral angles, we aim to capture and assess the geometric characteristics of the mesh, enabling a more comprehensive evaluation of its structure and shape.\n90. 2) Colorfulness: To evaluate the color characteristics, we focus solely on the texture map.\n91. Following the common color calculation process[45],[46], we first convert the texture from RGB channels to LAB channels and combine the standard deviation of A and B channels, which can be mathematically expressed as: where CF represents the colorfulness measure, A and B denote the corresponding color channels of the texture.\n92. 3) Characterization Visualization: We apply the extracted geometry information and colorfulness measure to the collection of 40 reference digital humans.\n93. The results are visualized in Fig.3.\n94. The analysis demonstrates that the selected reference 3D digital humans exhibit a wide spectrum of geometry information and colorfulness.\n95. Notably, model #24 positioned in the top-right corner showcases intricate geometry details and vibrant colorfulness.\n96. In contrast, model #15 portrays simpler geometry information and relatively subdued colorfulness.\n97. The proposed measures thoroughly capture the distinctiveness of 3D digital humans concerning their geometry and color characteristics.\n98. It is important to emphasize that these measures are directly computed from the underlying model files, thereby ensuring their stability and viewpoint invariance.\n99. To account for the common sources of distortion, we incorporate distortions arising from both the generation process and the transmission process.\n100. During the generation process, we consider geometry noise resulting from erroneous scanning procedures, as well as color noise introduced by cameras.\n101. Furthermore, compression and simplification techniques are widely employed during the transmission process.\n102. Hence, these factors are also taken into consideration in our assessment.\n103. By considering the full range of distortion sources, we aim to provide a comprehensive evaluation of the quality of 3D digital humans.\n104. Therefore, to degrade the quality of the reference 3D digital humans, we apply seven types of distortions and the specific settings for each distortion type are listed in TableIII.\n105. We manually select the distortion parameters to cover most visual quality range and the details are illustrated as follows: • Geometry Noise (GN): Gaussian noise with standard deviations σ g of 0.05, 0.1, 0.15, and 0.2 is added to the vertices\' geometry coordinates of the digital humans.\n106. In accordance with the recommended procedure outlined in[15],[16], passive watching is chosen over interactive watching for the subjective experiment to mitigate potential viewing bias.\n107. The 3D digital humans are rendered into video sequences for exhibition purposes.\n108. The open3d library is utilized to generate the projections[52].\n109. The rendering window is configured with a resolution of 1080 × 1920.\n110. To capture the video frames, a horizontal and a vertical circle are employed as the predefined camera paths.\n111. Each 3D digital human is captured at one frame every 3 degree, resulting in a total of 240 frames (360 × 2 ÷ 3).\n112. These frames are then compiled into an 8-second video with a framerate of 30 frames per second.\n113. This approach ensures that the viewers can effectively perceive the significant quality information.\n114. The rendering process is depicted in Fig.5.\n115. 2) Experiment Process: A total of 40 human subjects, comprising 20 males and 20 females, are recruited to participate in the subjective experiment.\n116. Prior to the experiment, a training session is conducted, wherein additional videos generated using the same aforementioned process are presented to familiarize the subjects with the tasks.\n117. The rating process takes place within a well-controlled laboratory environment, maintaining a normal level of illumination.\n118. The viewers are seated at a distance of twice the screen height.\n119. The videos are displayed on an iMac monitor capable of supporting resolutions up to 4096×2304.\n120. The order of video presentations is randomized.\n121. To facilitate the evaluation process, a double stimuli strategy is employed, where the reference and distorted videos are simultaneously displayed on the screen.\n122. The rating interface is excited in Fig.5and the quality score ranges from 0 to 5.\n123. In order to mitigate viewer fatigue, the entire experiment is divided into 20 sessions, with each session featuring 56 digital humans.\n124. Ultimately, a total of 44,800 subjective ratings (1, 120 × 40) are collected.\n125. 3) Subjective Data Analysis: After the subjective experiment, we calculate the z-scores from the raw ratings as follows: where and N i is the number of digital humans judged by subject i.\n126. 500-13[53]standard, ratings from unreliable subjects are excluded from the analysis.\n127. The corresponding z-scores are linearly rescaled to the range of [0, 5].\n128. Finally, the mean opinion scores (MOSs) are computed by averaging the rescaled z-scores.\n129. Fig.6illustrates the distribution of MOSs and the corresponding probability distributions for different distortion types.\n130. Interestingly, the probability distributions reveal that visual quality is less sensitive to varying levels of FS distortions compared to other distortion types.\n131. Even when reducing the face numbers to a ratio of 0.05 (only about 2k faces are preserved), the visual quality score remains higher than other distortions with similar levels.\n132. This observation indicates that visual quality is relatively resilient to FS distortions, implying that the reduction in face complexity may not significantly impact the perceived quality.\n133. In this section, we introduce the three indexes that make up the whole proposed digital human quality index (DHQI), which includes the text-prompted semantic affinity quality measure, spatial naturalness quality measure, and geometry loss quality measure.\n134. These three indexes are then aligned and aggregated into the proposed DHQI quality index.\n135. The framework is exhibited in Fig.7.\n136. We acquire the cube-like projection set of the given digital human as follows: P = ψ(DH), where P represents the set of the 6 rendered projections and ψ(•) stands for the rendering process.\n137. Such rendering process has been employed in the popular point cloud compression standard MPEG VPCC[54]and many other 3DQA works[15],[34].\n138. The projections are utilized as the input information for the text-prompted semantic affinity and spatial naturalness measure.\n139. To assess the perception of quality related to semantic content, specifically evaluating the quality of contents and the ability to discern semantic distortions, we design the text-prompted semantic affinity quality measure.\n140. Inspired by CLIP[9]-based quality assessment tasks[55],[56], we hold the hypothesis that the projections of the high-quality digital humans should have higher affinity with positive qualityrelated descriptions (e.g.good, perfect) and lower affinity with negative quality-related descriptions (e.g.bad, distorted).\n141. 1) Text Prompt Format: In accordance with the official recommendation provided by CLIP[9]and drawing from established practices, our text prompts are designed as a concatenation of three components: a prefix, a description, and a suffix.\n142. To be more precise, the text prompt T corresponding to the raw description D is defined as: T = "a" + D + "projection of 3d human model",(5)where the suffix "projection of 3d human model" is specifically designed to fit the task of DHQA.\n143. This carefully chosen suffix can encourage the CLIP model to prioritize and focus its attention on the detection and evaluation of content-aware distortions that may arise in the context of 3D digital humans.\n144. 2) Description Selection: We have identified descriptions pertaining to quality assessment that encompass broad evaluation aspects to ensure robustness.\n145. In this study, the general quality-related descriptions employed comprise the contrasting pairs of high quality ↔ low quality, good ↔ bad, and perfect ↔ distorted.\n146. The utilization of the high quality ↔ low quality as well as the good ↔ bad text pair assists in directing the attention of the CLIP model towards general subjective impressions.\n147. Conversely, the perfect ↔ distorted pair compels the CLIP model to prioritize the existence of distortions.\n148. 3) Affinity Difference Computation: Given the input image I and text T , the senmantic affinity can be calculated with the assistance of CLIP as: where E I and E T stand for the image and text encoders of CLIP, F I and F T represent the CLIP-encoded features, and A(I, T ) indicates the affinity between the input image and text.\n149. Afterward, the computation of zero-shot quality affinity can be derived from the aforementioned selected descriptions by calculating the disparity between the probabilities assigned to positive and negative textual inputs:\n150. A(P k , T ), where the averaged affinity to the given text T , denoted by A(P, T ), is calculated by CLIP across the six projections P.\n151. In this context, T i + and T i -refer to the positive and negative text descriptions, respectively, from the i-th text pair.\n152. The variable N T represents the total number of text pairs.\n153. Furthermore, A dif f signifies the cumulative difference between the averaged positive and negative affinity.\n154. The sigmoid remapping technique is then used to map the raw difference scores A dif f obtained from perceptual quality evaluation into a range of [0, 1].\n155. This remapping is done based on the guidance provided by the Video Quality Experts Group (VQEG)[57].\n156. Apart from evaluating semantic affinity, we incorporate the use of NIQE (Naturalness Image Quality Evaluator[10]) as a blind quality evaluator to assess the spatial naturalness of the digital humans.\n157. The purpose of employing NIQE is to identify and quantify common low-level distortions encountered in practical digital humans, including Gaussian noise, blur, and JPEG compression artifacts.\n158. By incorporating NIQE alongside semantic affinity evaluation, we aim to complement the assessment of high-level information with an evaluation of low-level technical quality.\n159. The NIQE index operates by quantifying the disparity between the characteristics of the input image features and the anticipated distribution of features observed in "high-quality" images, which are derived from a diverse set of pristine natural images.\n160. Since the raw NIQE scores and the raw affinity difference scores are on different scales, it is necessary to normalize the NIQE scores to facilitate meaningful comparison.\n161. To achieve this, we divide the NIQE scores by a constant value, denoted as c 1 , which effectively restricts the majority of NIQE scores to the range of [0,1].\n162. Consequently, the spatial naturalness quality measure can be computed as follows: where N (P k ) denotes the NIQE value for the k-th projection, N (P) represents the average NIQE value across the 6 projections, and Q N stands for the spatial naturalness quality measure.\n163. It\'s worth noting that the NIQE scores are inversely correlated with quality and the negative sign is incorporated into the sigmoid function, allowing for a consistent interpretation and alignment of the NIQE scores with the quality evaluation framework.\n164. The aforementioned measures are applied to projections, specifically the image modality.\n165. In order to enhance the model\'s understanding of digital humans, it is proposed to directly extract features from the mesh modality to capture the loss in geometry with respect to visual quality.\n166. 1) Descriptor Selection: Various geometry attributes have been utilized to describe the quality-related geometric characteristics of meshes[14], including curvature, dihedral angle, face angle, face area, etc.For the purpose of preserving stability and improving the robustness of the proposed zeroshot method, the dihedral angle is selected as the geometry descriptor for the following reasons: a) Extensive evidence supports the effectiveness of the dihedral angle in describing geometric features relevant to visual quality[11],[12],[13],[14].b).\n167. b) Unlike other geometry attributes, the dihedral angle is invariant to scale.\n168. Its values are confined within the range of [0, π], thereby contributing to its robustness.\n169. The dihedral angle is the angle between two adjacent faces, which can be calculated as the dot product of corresponding normal vectors: where θj π indicates the scaled dihedral angle corresponding to the j-th edge of the mesh, Θ indicates the set of the scaled dihedral angle values, n j1 and n j2 stand for the normal vectors of the two adjacent faces whose co-edge is the j-th edge.\n170. 2) Quality Correlation with Dihedral Angle: Lossy mesh compression and simplification techniques can potentially diminish a mesh\'s structural details, resulting in a smoother and simpler surface representation.\n171. In such cases, the faces comprising the smoother and simpler surface tend to exhibit dihedral angles that approach π, leading to an inherent inclination for larger dihedral angles.\n172. To substantiate this observation, we present the tendencies of the mean values of the dihedral angles in Fig.8, from which we can find a consistent upward trend in dihedral angle means as compression/simplification levels increase.\n173. Therefore, the mean values of the dihedral angle can be generally taken as an indicator of geometry detail loss caused by compression/simplification.\n174. Then geometry loss quality measure can be calculated as: where Q G represents the geometry loss quality measure, Θ indicates the mean value of the dihedral angles, and the negative sign is added to the sigmoid function due to the positive correlation between the dihedral angles\' mean values and compression/simplification levels.\n175. In order to develop a reliable zero-shot perceptual quality index, we adopt a direct aggregation approach wherein we sum up the scale-aligned scores of various indices without performing any fine-tuning processes.\n176. Considering that the Q A , Q N , and Q G have undergone sigmoid rescaling, all three measures are bounded within the range of [0, 1].\n177. Consequently, we define the comprehensive unified DHQI (digital human quality index) as follows: where Q DHQI indicates the final quality values for the digital humans.\n178. V. EXPERIMENT A.\n179. Validation Setup 1) Benchmark Databases: In addition to the proposed SJTU-H3D database, we have incorporated the digital human quality assessment (DHHQA) database[8]as an additional resource for benchmark validation.\n180. The DHHQA database comprises a total of 55 scanned digital human heads that serve as reference samples, along with 1,540 labeled distorted digital human heads.\n181. These distorted samples have been intentionally degraded through the introduction of noise and compression/simplification.\n182. 2) k-fold Cross-Validation: To ensure robust evaluation, we adopt a k-fold cross-validation strategy.\n183. This approach involves dividing the database into k equally sized folds.\n184. The model is then trained on k-1 of these folds and subsequently tested on the remaining fold.\n185. This process is repeated k times, with each fold being used as the test set once.\n186. By averaging the performance across these k iterations, we obtain a more reliable estimate of the model\'s effectiveness, minimizing the impact of random variations.\n187. For both the SJTU-H3D and DHHQA databases, we have selected a value of k = 5 to conduct the k-fold cross-validation, ensuring a balanced evaluation across multiple subsets.\n188. It\'s worth mentioning that there is no content overlap between the training and testing folds.\n189. To facilitate a direct and fair comparison between zeroshot and supervised methods, we validate their performance in the following way.\n190. Zero-shot methods are directly applied to the testing folds, as they do not require any training.\n191. The performance is then averaged across the testing folds and reported as the final performance.\n192. On the other hand, supervised methods undergo training on the training folds and are subsequently tested on the testing folds.\n193. Similar to zero-shot methods, the average performance is calculated and reported as the final performance.\n194. Adopting this methodology enables a direct and unbiased comparison of the performance between zero-shot and supervised methods, insights into their respective strengths and limitations.\n195. 3) Implemetation Details: The cube-like projection process described in Section IV-A is conducted with the assistance of open3d[52]library with a resolution of 1080P.\n196. The white background is cropped out.\n197. The projections are downsampled to 224×224 as the input of the CLIP[9]image encoder.\n198. The ViT-B-32[64]backbone with LAION-2B[65]pretrained weights is utilized as the CLIP model.\n199. To fit the DHHQA database, we replace the suffix "projection of 3d human model" as described in Equation5with "projection of 3d human face".\n200. The scale parameter c 1 constant described in Section IV-C is set as 100.\n201. The supervised training of the proposed DHQA method is conducted with the Support Vector Regression (SVR) model with RBF kernel.\n202. The official source code is used for the competitors and default parameters are maintained.\n203. The default 5-fold crossvalidation is strictly followed for the competitors to make the comparison fair.\n204. In addition, the predicted scores of all the methods are followed by a five-parameter logistic regression to map the scores to the MOS scale.\n205. The competitors\' selection is conducted to ensure high diversity, which includes the zero-shot FR methods, zero-shot NR methods, and the supervised NR methods.\n206. 1) Zero-shot FR Methods: We consider several classical projection-based FR methods: PSNR, SSIM[58], MS-SSIM[59], and GMSD[60].\n207. These methods are applied to the six perpendicular projections, and the resulting scores are averaged and recorded.\n208. Additionally, we incorporate three popular point-based FR metrics proposed by MPEG: PSNR p2po[61], PSNR p2pl[62], and PSNR yuv[63].\n209. For the purpose of validation, we convert the digital human models into point clouds.\n210. Furthermore, we utilize G-LPIPS*[19], which is a projection-based FR metric modified from LPIPS[66]and is designed for textured meshes.\n211. The official pretrained weights are employed for this metric.\n212. 2) Zero-shot NR Methods: These methods comprise CPBD[37], pretrained BRISQUE*[36], NIQE[10], and IL-NIQE[44].\n213. 3) Supervised NR Methods: These methods encompass handcrafted approaches such as BRISQUE[36], NFERM[39], and BMPRI[38], which are supervised using the Support Vector Regression (SVR) model.\n214. Additionally, we include deep learning-based methods, namely DBCNN[40], Hyper-IQA[41], MUSIQ[42], and StaiIQA[43], which have been retrained for our evaluation.\n215. The overall performance on the SJTU-H3D and DHHQA databases are exhibited in TableIV, from which we can draw several conclusions.\n216. 1) Zero-shot Performance: a) Among all the zero-shot methods compared on the SJTU-H3D database, the DHQI method demonstrates superior performance and outperforms them all.\n217. b) Nevertheless, the FR metrics that exhibit the highest performance on the DHHQA database, namely MS-SSIM & GMSD, suffer significant performance degradation when applied to the SJTU-H3D database.\n218. c) In contrast, all the competing zero-shot NR methods consistently exhibit lower performance compared to the proposed DHQI method.\n219. The reason for this disparity lies in the focus of these methods on addressing low-level distortions, which restricts their ability to effectively capture and model high-level semantic quality representations.\n220. By leveraging the semantic affinity quality measure, the DHQI method can enhance the performance of zero-shot NR approaches even further.\n221. 2) Supervised Performance: Due to the significant advancements achieved by deep neural networks, deep learning-based methods such as HyperIQA and MUSIQ have demonstrated superior performance compared to traditional handcrafted methods.\n222. Despite this, the proposed DHQI method, which is solely supervised by Support Vector Regression (SVR) model, achieves the top-ranking performance on the SJTU-H3D database.\n223. One notable advantage of the proposed supervised DHQI index is its cost-effectiveness in terms of time and computational resources.\n224. b) The point-based methods proposed by MPEG exhibit high sensitivity to noise-related distortions.\n225. This can be attributed to the direct impact of geometry and color noise on the point-level quality characteristics.\n226. Additionally, the PSNR yuv metric demonstrates a strong discriminative ability in distinguishing quality differences within CN, PC, UMC, and TD distortions.\n227. c) The zeroshot NR methods NIQE and IL-NIQE show competitive performance for UMC, TD, and TC distortions.\n228. This can be attributed to the fact that UMC and TD distortions introduce blurring effects to digital human projections, which aligns with the strengths of these methods.\n229. d) FS distortion proves to be the most challenging distortion to evaluate.\n230. This is due to the fact that the MOS distribution for FS distortion tends to be more centered, as shown in Fig.6, indicating a more fine-grained quality level that is less distinctive.\n231. FS distortion primarily causes digital humans to exhibit more geometric characteristics, which may lead to small differences in NSS reflected by the projections and result in the poor performance of NIQE and IL-NIQE.\n232. In this section, we present an analysis of the effects of different quality measures: Q A , Q N , and Q G , on the experimental performance.\n233. The combinations of these quality measures are tested, and the results are summarized in TableVI.\n234. Throughout the experiments, we maintain the default experimental setup.\n235. Table VI clearly demonstrates that among the single quality measures, Q A achieves the highest performance.\n236. This finding indicates a strong correlation between quality-aware semantic affinity and the visual quality of digital humans.\n237. It suggests that considering the quality of semantic representations is crucial for accurately assessing the visual fidelity of digital human models.\n238. Furthermore, excluding any of the three quality measures leads to a drop in performance compared to utilizing all quality measures together.\n239. This observation implies that each quality measure contributes significantly to the final results.\n240. The effectiveness of the proposed framework is thereby validated by the consistent performance improvements achieved when all quality measures are incorporated.\n241. To further analyze the performance of the proposed method, we conduct the statistical test in this section.\n242. We follow the same experiment setup as in[67]and compare the difference between the predicted quality scores with the subjective ratings.\n243. All possible pairs of models are tested and the results are listed in Fig.9.\n244. Our method demonstrates remarkable superiority over 12 zero-shot methods and 5 supervised methods when compared on the SJTU-H3D database.\n245. On the DHHQA database, our method exhibits substantial outperformance compared to 9 zero-shot methods and 3 supervised methods.\n246. The increasing applications of digital humans across various domains have highlighted the need for comprehensive A black/white block means the row method is statistically worse/better than the column one.\n247. A gray block means the row method and the column method are statistically indistinguishable.\n248. The methods are denoted by the same index as in TableIV. quality assessment studies.\n249. However, the limited availability of comprehensive digital human quality assessment (DHQA) databases has posed challenges in this area.\n250. To address this gap, we have introduced the SJTU-H3D subjective quality assessment database, specifically designed for full-body digital humans.\n251. This database consists of 40 high-quality reference digital humans and 1,120 labeled distorted counterparts created with seven types of distortions.\n252. Nonetheless, the scarcity of suitable DHQA databases remains a hindrance to the development of data-driven methods.\n253. To overcome this limitation and enhance generalization capabilities, we propose a zero-shot DHQA approach that focuses on no-reference (NR) scenarios.\n254. Our approach leverages semantic and distortion features obtained from projections, as well as geometry features derived from the mesh structure of digital humans.\n255. The proposed DHQI not only serves as a robust baseline for DHQA tasks but also facilitates advancements in the field.\n256. We hope our work can contribute to the establishment of effective evaluation frameworks and methodologies for digital humans, enabling their widespread application in diverse domains.\n\n请严格按照JSON格式返回：{"labels": [0,1,0,...]}，不要包含任何其他文字。'}], 'model': 'qwen-plus', 'response_format': {'type': 'json_object'}}}
2025-10-14 21:46:06,790 - DEBUG - Sending HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
2025-10-14 21:46:06,790 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-10-14 21:46:06,791 - DEBUG - send_request_headers.complete
2025-10-14 21:46:06,791 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-10-14 21:46:06,791 - DEBUG - send_request_body.complete
2025-10-14 21:46:06,791 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-10-14 21:46:13,735 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'vary', b'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding'), (b'x-request-id', b'47a71aa5-8128-45f4-82b9-e12a3468a367'), (b'x-dashscope-call-gateway', b'true'), (b'content-type', b'application/json'), (b'req-cost-time', b'6909'), (b'req-arrive-time', b'1760449557927'), (b'resp-start-time', b'1760449564836'), (b'x-envoy-upstream-service-time', b'6868'), (b'content-encoding', b'gzip'), (b'date', b'Tue, 14 Oct 2025 13:46:04 GMT'), (b'server', b'istio-envoy'), (b'transfer-encoding', b'chunked')])
2025-10-14 21:46:13,735 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-14 21:46:13,736 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-10-14 21:46:13,736 - DEBUG - receive_response_body.complete
2025-10-14 21:46:13,736 - DEBUG - response_closed.started
2025-10-14 21:46:13,736 - DEBUG - response_closed.complete
2025-10-14 21:46:13,736 - DEBUG - HTTP Response: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "200 OK" Headers({'vary': 'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding', 'x-request-id': '47a71aa5-8128-45f4-82b9-e12a3468a367', 'x-dashscope-call-gateway': 'true', 'content-type': 'application/json', 'req-cost-time': '6909', 'req-arrive-time': '1760449557927', 'resp-start-time': '1760449564836', 'x-envoy-upstream-service-time': '6868', 'content-encoding': 'gzip', 'date': 'Tue, 14 Oct 2025 13:46:04 GMT', 'server': 'istio-envoy', 'transfer-encoding': 'chunked'})
2025-10-14 21:46:13,736 - DEBUG - request_id: 47a71aa5-8128-45f4-82b9-e12a3468a367
2025-10-14 21:46:13,737 - DEBUG - API request completed in 6.95 seconds
2025-10-14 21:46:13,737 - DEBUG - Raw model response: {"labels": [1,1,1,1,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,1,1,1,1,1,1,1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}
2025-10-14 21:46:13,737 - INFO - Successfully processed 141 labels
2025-10-14 21:46:13,737 - ERROR - Label count mismatch for Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation
2025-10-14 21:46:13,737 - INFO - Evaluating paper 3/18: Amazon_M2_ A Multilingual Multi_locale Shopping Session Dataset for Recommendation and Text Generation
2025-10-14 21:46:13,737 - INFO - Starting model prediction
2025-10-14 21:46:13,737 - INFO - Attempt 1 of 5
2025-10-14 21:46:13,737 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-eca41b40-2419-4ab1-a88a-00a8c92e66f8', 'json_data': {'messages': [{'role': 'system', 'content': '你是一个专门用于识别学术论文中数据集描述的助手。你需要判断每个输入的句子是否描述了研究中使用的数据集。请确保：\n    1. 只输出JSON格式的结果\n    2. 结果格式必须为 {"labels": [0,1,0,...]}\n    3. 不要输出任何其他文字或解释'}, {'role': 'user', 'content': '你是一位专业的学术论文数据集描述识别专家。请仔细分析下面这篇论文中的每个句子，判断它们是否描述了本研究特定构建的数据集信息。\n\n    论文标题: Amazon_M2_ A Multilingual Multi_locale Shopping Session Dataset for Recommendation and Text Generation\n\n    判断标准:\n    判断为1(数据集描述)的标准：\n    句子必须直接描述本研究特定构建的数据集,包括:\n    1. 该数据集的构建过程和方法\n    2. 该数据集的具体构成和规模\n    3. 该数据的来源和收集方式\n    4. 该数据的预处理步骤\n    5. 该数据集的可获取方式(如发布地址)\n    6. 或者其他直接或者间接描述了该数据集的句子\n\n    所有其他类型的句子均标记为0。\n    \n    请对每个句子进行分析，返回一个JSON格式的标签数组。格式要求：\n    1. 必须是有效的JSON格式\n    2. 只包含labels字段，值为0和1组成的数组\n    3. 数组长度必须与句子数量相同\n    4. 示例格式：{"labels": [0,1,0,1,...]}\n\n    需要判断的句子：\n\n    1. Abstract: Modeling customer shopping intentions is a crucial task for e-commerce, as it directly impacts user experience and engagement.\n2. Thus, accurately understanding customer preferences is essential for providing personalized recommendations.\n3. Session-based recommendation, which utilizes customer session data to predict their next interaction, has become increasingly popular.\n4. However, existing session datasets have limitations in terms of item attributes, user diversity, and dataset scale.\n5. As a result, they cannot comprehensively capture the spectrum of user behaviors and preferences.\n6. To bridge this gap, we present the Amazon Multilingual Multilocale Shopping Session Dataset, namely Amazon-M2.\n7. It is the first multilingual dataset consisting of millions of user sessions from six different locales, where the major languages of products are English, German, Japanese, French, Italian, and Spanish.\n8. Remarkably, the dataset can help us enhance personalization and understanding of user preferences, which can benefit various existing tasks as well as enable new tasks.\n9. To test the potential of the dataset, we introduce three tasks in this work: (1) next-product recommendation, (2) next-product recommendation with domain shifts, and (3) next-product title generation.\n10. With the above tasks, we benchmark a range of algorithms on our proposed dataset, drawing new insights for further research and practice.\n11. In addition, based on the proposed dataset and tasks, we hosted a competition in the KDD CUP 2023 2 and have attracted thousands of users and submissions.\n12. The winning solutions and the associated workshop can be accessed at our website https://kddcup23.github.io/.* Equal contribution. 2https://www.aicrowd.com/challenges/amazon-kdd-cup-23-multilingual-recommendationchallenge37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks.\n13. In the era of information explosion, recommender systems have become a prevalent tool for understanding user preferences and reducing information overload[1,2,3,4,5,6].\n14. Traditionally, the majority of recommendation algorithms focus on understanding long-term user interests through utilizing user-profiles and behavioral records.\n15. However, they tend to overlook the user\'s current purpose which often has a dominant impact on user\'s next behavior.\n16. Besides, many recommendation algorithms require access to user profiles[7,8,9], which can be incomplete or even missing in real-world situations especially when users are browsing in an incognito mode.\n17. In these cases, only the most recent user interactions in the current session can be utilized for understanding their preferences.\n18. Consequently, the session-based recommendation has emerged as an effective solution for modeling user\'s short-term interest, focusing on a user\'s most recent interactions within the current session to predict the next product.\n19. Over the past few years, the session-based recommendation has gained significant attention and has prompted the development of numerous models[10,11,12,13,14,15,16,17].\n20. A critical ingredient for evaluating the efficacy of these methods is the session dataset.\n21. While numerous session datasets[18,19,20,11,21]have been carefully curated to meet the requirements of modeling user intent and are extensively employed for evaluating session-based recommender systems, they have several drawbacks.\n22. First, existing datasets only provide limited product attributes, resulting in incomplete product information and obscuring studies that leverage attribute information to advance the recommendation.\n23. Second, the user diversity within these datasets is limited and may not adequately represent the diversity of user-profiles and behaviors.\n24. Consequently, it can result in biased or less accurate recommendations, as the models may not capture the full range of customer preferences.\n25. Third, the dataset scale, particularly in terms of the product set, is limited, which falls short of reflecting real-world recommendation scenarios with vast product and user bases.\n26. To break the aforementioned limitations, we introduce the Amazon Multilingual Multi-Locale Shopping Session Dataset, namely Amazon-M2, a large dataset of anonymized user sessions with their interacted products collected from multiple language sources at Amazon.\n27. Specifically, the dataset contains samples constructed from real user session data, where each sample contains a list of user-engaged products in chronological order.\n28. In addition, we provide a table of product attributes, which contains all the interacted products with their associated attributes such as title, brand, color, etc. Modeling such session data can help us better understand customers\' shopping intentions, which is also the main focus of e-commerce.\n29. Particularly, the proposed dataset exhibits the following characteristics that make it unique from existing session datasets.\n30. (a) Rich semantic attributes: Amazon-M2 includes rich product attributes (categorical, textual, and numerical attributes) as product features including title, price, brand, description, etc.These attributes provide a great opportunity to accurately comprehend the user\'s interests.\n31. To our best knowledge, it is the first session dataset to provide textual features.\n32. (b) Large scale: Amazon-M2 is a large-scale dataset with millions of user sessions and products, while existing datasets only contain tens of thousands of products.\n33. (c) Multiple locales: Amazon-M2 collected data from diverse sources, i.e., six different locales including the United Kingdom, Japan, Italian, Spanish, French, and Germany.\n34. (d) Multiple languages: Given the included locales, Amazon-M2 is special for its multilingual property.\n35. Particularly, six different languages (English, Japanese, Italian, Spanish, French, and German) are provided.\n36. It enables us to leverage recent advances such as language models[22,23,24]to model different languages in user sessions.\n37. By utilizing this dataset, we can perform diverse downstream tasks for evaluating relevant algorithms in recommendation and text generation.\n38. Here, we focus on three different tasks, consisting of (1) next-product recommendation, (2) next-product recommendation with domain shifts, and (3) nextproduct title generation.\n39. The first task is the classic session-based recommendation which requires models to predict the ID of the next product, where the training dataset and test dataset are from the same domain.\n40. The second task is similar to the first task but requires the models to pre-train on the large dataset from large locales and transfer the knowledge to make predictions on downstream datasets from different domains (i.e., underrepresented locales).\n41. The third task is a novel task proposed by us, which asks models to predict the title of the next product which has never been shown in the training set.\n42. Based on these tasks, we benchmark representative baselines along with simple heuristic methods.\n43. Our empirical observations suggest that the representative baselines fail to outperform simple heuristic methods in certain evaluation metrics in these new settings.\n44. Therefore, we believe that Amazon-M2 can inspire novel solutions for session-based recommendation and enable new opportunities for tasks that revolve around large language models and recommender systems.\n45. denote a dictionary of unique products that appeared in the sessions, and each product is associated with some attributes.\n46. Designed for session-based recommendation, Amazon-M2 is a large-scale dataset composed of customer shopping sessions with interacted products.\n47. Specifically, the dataset consists of two components: (1) user sessions where each session is a list of product IDs interacted by the current user (Figure1a), and (2) a table of products with each row representing the attributes of one product (Figure1b).\n48. Particularly, the user sessions come from six different locales, i.e., the United Kingdom (UK), Japan (JP), German (DE), Spain (ES), Italian (IT), and France (FR).\n49. Given its multi-locale nature, the dataset is also multilingual: the textual attributes (e.g., title and description) of the products in the user sessions are in multiple languages, namely, English, Italian, French, Germany, and Spanish.\n50. Based on this dataset, we construct the training/test dataset for each task.\n51. A summary of our session dataset is given in Table2.\n52. It includes the number of sessions, the number of interactions, the number of products, and the average session length for six different locales.\n53. We can find that UK, DE, and JP have approximately 10 times the number of sessions/products compared to ES, FR, and IT.\n54. More details about the collection process of the dataset can be found in Appendix B.\n55. Comparison with Existing Datasets.\n56. We summarize the differences between existing session datasets (especially from session-based recommendation) and Amazon-M2 in Table1.\n57. First of all, Amazon-M2 is the first dataset to provide textural information while other datasets majorly focus on the product ID information or categorical attributes.\n58. Without comprehensive product attributes, the recommendation models may struggle to capture the nuanced preferences of customers.\n59. Second, existing datasets only provide sessions from a single locale (or country) which limits their user diversity.\n60. Consequently, it may lead to biased or less accurate recommendations, as the models may not capture the full range of customer preferences.\n61. By contrast, our proposed Amazon-M2 is collected from multiple locales and is multilingual in nature.\n62. Third, our proposed Amazon-M2 provides a large number of user sessions and is on a much larger product scale, which can better reflect real-world recommendation scenarios with huge product bases.\n63. In this section, we offer a comprehensive analysis of the Amazon-M2 dataset to uncover valuable insights.\n64. Our analysis covers several essential perspectives: long-tail phenomenon, product overlap between locales, session lengths, repeat pattern, and collaborative filtering pattern.\n65. Corresponding codes can be found here.\n66. Long-tail phenomenon[26,27]is a significant challenge in the session recommendation domain.\n67. It refers to the situation where only a handful of products enjoy high popularity, while the majority of products receive only a limited number of interactions.\n68. To investigate the presence of the long-tail phenomenon in Amazon-M2 dataset, we analyze the distribution of product frequencies, as depicted  in Figure2a.\n69. The results clearly demonstrate the existence of a long-tail distribution, where the head of the distribution represents popular items and the tail consists of less popular ones.\n70. Furthermore, we observe that the long-tail phenomenon is also evident within each individual locale.\n71. For detailed experimental results regarding this phenomenon in each locale, please refer to Appendix B.2.\n72. The long-tail distribution makes it difficult to effectively recommend less popular products, as a small number of popular items dominate the recommendations.\n73. Product overlap ratio between locales is the proportion of the same products shared by different locales.\n74. A large number of overlapping products indicates a better transferability potential when transferring the knowledge from one locale to the other.\n75. For example, cross-domain recommendation algorithms like[28]can then be successfully applied, which directly transfers the learned embedding of the overlapping products from popular locales to the underrepresented locales.\n76. We then examine product overlap between locales in Amazon-M2 with the product overlap ratio.\n77. |Na| , where N a and N b correspond to the products set of locale a and b, respectively.\n78. In Figure2bwe use a heatmap to show the overlap ratio, where x and y axes stand for locale a and b, respectively.\n79. (2) Considering the product overlap ratio between large locales and underrepresented locales, i.e., ES, FR, and IT, we can see a large product overlapping, indicating products in the underrepresented domain also appear in the large locales.\n80. Particularly, the overlap ratio between small locales and DE can reach around 0.4.\n81. Thus, it has the potential to facilitate knowledge transfer from large locales and areas to underrepresented regions.\n82. Notably, despite the existence of overlapping products between different locales, there still remains a large proportion of distinguished products in each locale, indicating the difficulty of transferability with distribution shift.\n83. Moreover, the multilingual property, where the product textual description from different locales is in different languages, also induces to distribution shift issue.\n84. Such a multilingual issue is a long-established topic in the NLP domain.\n85. For instance,[29,30,31]point out morphology disparity, tokenization differences, and negative transfer in the multilingual scenario, leading to distribution shift.\n86. Session length is an important factor in the session recommendation domain.\n87. Typically, a longer session length may lead to the interest shift challenge problem[15]with difficulties in capturing multiple user interests in one single session.\n88. Most existing algorithms[13,16]show a better performance on the shorter sessions while failing down on the longer ones.\n89. As shown in Figure2c.\n90. We can observe that the session length also exhibits a long-tail distribution: most sessions are short while only few sessions are with a length larger than 100.\n91. Repeat pattern[32,33,34,35]is also an important user pattern, which refers to the phenomenon that a user repeatedly engages the same products multiple times in a single session.\n92. The presence of repeat patterns in recommender systems can potentially result in the system offering more familiar products that match users\' existing preferences, which may lead to a less diverse and potentially less satisfying user experience.\n93. On the other hand, the repeat pattern is also an important property utilized in the graph-based session recommendation algorithms[13,17,36,37].\n94. Typically, those graph-based algorithms construct a session graph where each node represents a product and each edge indicates two products interacted by the user consecutively.\n95. Complicated session graphs with different structure patterns can be built when sessions exhibit evident repeat patterns.\n96. In Figure2d, we report the proportion of sessions with repeat patterns for the six locales and we can observe that there are around 35% sessions with repeat patterns across different locales.\n97. Furthermore, we examine the number of repeat products in those sessions with repeat patterns and report results on the distribution of repeated products in Figure2e.\n98. We make two observations: (1) the number of repeated products varies on different sessions; and (2) the number of repeated products in a session also follows the long-tail distribution where most sessions only appear with a few repeated products.\n99. Collaborative filtering pattern.\n100. Collaborative filtering is a widely used technique that generates recommended products based on the behavior of other similar users.\n101. It is generally utilized as an important data argumentation technique to alleviate the data sparsity issue, especially for short sessions[38,39].\n102. Since Amazon-M2 encompasses a much larger product set than existing datasets, we investigate whether collaborative filtering techniques can potentially operate in this challenging data environment.\n103. Specifically, we utilize the session collaborative filtering algorithm, Session-KNN (SKNN)[11], to identify sessions that are similar to the target user\'s current session.\n104. The similarity score of SKNN can be calculated in the following steps.\n105. First, for a particular session s, we first determines a set of its most similar sessions N (s) ⊆ S with the cosine similarity sim(s, s j ) = |s ∩ s j |/ |s||s j |.\n106. • SRGNN[13]is the first to employ GNN layer to capture user interest in the current session.\n107. • CORE[41]ensures that sessions and items are in the same representation space via encoding the session embedding as a linear combination of item embeddings.\n108. • MGS[42]incorporates product attribute information to construct a mirror graph, aiming to learn better preference understanding via combining session graph and mirror graph.\n109. Notably, MGS can only adapt categorical attributes.\n110. Therefore, we discretize the price attribute as the input feature.\n111. In addition, we include a simple yet effective method, Popularity, by simply recommending all users the most popular products.\n112. We utilize Mean Reciprocal Rank@K (MRR@100) and Recall@100 to evaluate various recommendation algorithms.\n113. More results on NDCG@100 metric can be found in Appendix C.\n114. Corresponding codes can be found here.\n115. Results & Observations.\n116. The experiment results across different locales can be found in Table3.\n117. We can observe that the popularity heuristic generally outperforms all the deep models with respect to both MRR and Recall.\n118. The only exception is that CORE achieves better performance on Recall.\n119. On one hand, the success of the popularity heuristic indicates that the product popularity is a strong bias for this dataset.\n120. On the other hand, it indicates that the large product set in Amazon-M2 poses great challenges in developing effective recommendation algorithms.\n121. Thus, more advanced recommendation strategies are needed for handling the challenging Amazon-M2 dataset.\n\n请严格按照JSON格式返回：{"labels": [0,1,0,...]}，不要包含任何其他文字。'}], 'model': 'qwen-plus', 'response_format': {'type': 'json_object'}}}
2025-10-14 21:46:13,738 - DEBUG - Sending HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
2025-10-14 21:46:13,738 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-10-14 21:46:13,738 - DEBUG - send_request_headers.complete
2025-10-14 21:46:13,738 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-10-14 21:46:13,738 - DEBUG - send_request_body.complete
2025-10-14 21:46:13,738 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-10-14 21:46:20,214 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'vary', b'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding'), (b'x-request-id', b'1d83c5ae-3652-4ca4-bbd7-b2f662d8d63d'), (b'x-dashscope-call-gateway', b'true'), (b'content-type', b'application/json'), (b'req-cost-time', b'6444'), (b'req-arrive-time', b'1760449564871'), (b'resp-start-time', b'1760449571316'), (b'x-envoy-upstream-service-time', b'6411'), (b'content-encoding', b'gzip'), (b'date', b'Tue, 14 Oct 2025 13:46:10 GMT'), (b'server', b'istio-envoy'), (b'transfer-encoding', b'chunked')])
2025-10-14 21:46:20,215 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-14 21:46:20,215 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-10-14 21:46:20,215 - DEBUG - receive_response_body.complete
2025-10-14 21:46:20,215 - DEBUG - response_closed.started
2025-10-14 21:46:20,216 - DEBUG - response_closed.complete
2025-10-14 21:46:20,216 - DEBUG - HTTP Response: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "200 OK" Headers({'vary': 'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding', 'x-request-id': '1d83c5ae-3652-4ca4-bbd7-b2f662d8d63d', 'x-dashscope-call-gateway': 'true', 'content-type': 'application/json', 'req-cost-time': '6444', 'req-arrive-time': '1760449564871', 'resp-start-time': '1760449571316', 'x-envoy-upstream-service-time': '6411', 'content-encoding': 'gzip', 'date': 'Tue, 14 Oct 2025 13:46:10 GMT', 'server': 'istio-envoy', 'transfer-encoding': 'chunked'})
2025-10-14 21:46:20,216 - DEBUG - request_id: 1d83c5ae-3652-4ca4-bbd7-b2f662d8d63d
2025-10-14 21:46:20,217 - DEBUG - API request completed in 6.48 seconds
2025-10-14 21:46:20,217 - DEBUG - Raw model response: {"labels": [1,0,0,0,0,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}
2025-10-14 21:46:20,217 - INFO - Successfully processed 105 labels
2025-10-14 21:46:20,217 - ERROR - Label count mismatch for Amazon_M2_ A Multilingual Multi_locale Shopping Session Dataset for Recommendation and Text Generation
2025-10-14 21:46:20,217 - INFO - Evaluating paper 4/18: AntM 2 C_ A Large Scale Dataset For Multi_Scenario Multi_Modal CTR Prediction
2025-10-14 21:46:20,218 - INFO - Starting model prediction
2025-10-14 21:46:20,218 - INFO - Attempt 1 of 5
2025-10-14 21:46:20,219 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-de70024d-cc71-49e2-a9ca-5f6f5fe44490', 'json_data': {'messages': [{'role': 'system', 'content': '你是一个专门用于识别学术论文中数据集描述的助手。你需要判断每个输入的句子是否描述了研究中使用的数据集。请确保：\n    1. 只输出JSON格式的结果\n    2. 结果格式必须为 {"labels": [0,1,0,...]}\n    3. 不要输出任何其他文字或解释'}, {'role': 'user', 'content': '你是一位专业的学术论文数据集描述识别专家。请仔细分析下面这篇论文中的每个句子，判断它们是否描述了本研究特定构建的数据集信息。\n\n    论文标题: AntM 2 C_ A Large Scale Dataset For Multi_Scenario Multi_Modal CTR Prediction\n\n    判断标准:\n    判断为1(数据集描述)的标准：\n    句子必须直接描述本研究特定构建的数据集,包括:\n    1. 该数据集的构建过程和方法\n    2. 该数据集的具体构成和规模\n    3. 该数据的来源和收集方式\n    4. 该数据的预处理步骤\n    5. 该数据集的可获取方式(如发布地址)\n    6. 或者其他直接或者间接描述了该数据集的句子\n\n    所有其他类型的句子均标记为0。\n    \n    请对每个句子进行分析，返回一个JSON格式的标签数组。格式要求：\n    1. 必须是有效的JSON格式\n    2. 只包含labels字段，值为0和1组成的数组\n    3. 数组长度必须与句子数量相同\n    4. 示例格式：{"labels": [0,1,0,1,...]}\n\n    需要判断的句子：\n\n    1. Abstract: Click-through rate (CTR) prediction is a crucial issue in recommendation systems, directly impacting user experience and platform revenue.\n2. In recent years, CTR has garnered attention from both industry and academia, leading to the emergence of various public CTR datasets.\n3. However, existing CTR datasets primarily suffer from the following limitations.\n4. Firstly, users generally click different types of items from multiple scenarios, and modeling the CTR from multiple scenarios can provide a more comprehensive understanding of users and share knowledge between different scenarios.\n5. Existing datasets only include CTR data for the same type of items from a single scenario.\n6. Secondly, multi-modal features are essential in multi-scenario CTR prediction as they effectively address the issue of inconsistent ID encoding between different scenarios.\n7. The existing datasets are based on ID features and lack multi-modal features.\n8. Third, a large-scale CTR dataset can provide a more reliable and comprehensive evaluation of complex models, fully reflecting the performance differences between models.\n9. While the scale of existing datasets is around 100 million, which is relatively small compared to the real-world industrial CTR prediction.\n10. To address these limitations, we propose AntM 2 C, a Multi-Scenario Multi-Modal CTR dataset based on real industrial data from the Alipay platform.\n11. Specifically, AntM 2 C possesses the following characteristics: 1) It covers CTR data of 5 different types of items from Alipay, providing insights into the preferences of users for different items, including advertisements, vouchers, mini-programs, contents, and videos.\n12. 2) Apart from ID-based features, AntM 2 C also provides 2 multi-modal features, raw text and image features, which can effectively establish connections between items with different IDs.\n13. 3) AntM 2 C provides 1 billion CTR data with 200 features, including:\n14. Click-through rate (CTR) prediction plays a significant role in various domains, including online advertising, search engines, and recommendation systems.\n15. CTR prediction refers to the task of estimating the probability that a user will click on a given item.\n16. It is essential for optimizing ad revenue, enhancing user experience, and improving engagement.\n17. One of the challenging issues in CTR prediction lies in the faithful evaluation of the model.\n18. Public CTR datasets provide a standardized and benchmarked environment for evaluating the performance of different CTR models.\n19. This enables researchers to compare the effectiveness of different models and identify the most suitable ones for specific applications.\n20. However, in order to meet the constantly growing demands of users, the current CTR scenarios and items are becoming increasingly diverse, and the amount of CTR data is also increasing.\n21. For example, in Alipay, CTR occurs in the consumer coupons at marketing campaigns, videos on the tab3 page, and mini-programs after a search.\n22. As a result, the existing CTR datasets suffer from the following limitations.\n23. Firstly, in real-world industrial CTR prediction, users generally click various types of items from different business scenarios, reflecting their preferences for different items.\n24. For example, on Alipay, a user may browse a video about coffee on the Tab3 page, then click on a coffee coupon during a marketing campaign, and finally use the Alipay search to click a coffee ordering mini-program to place an order.\n25. Jointly modeling this multi-scenario CTR data can provide a more comprehensive understanding of user preferences, and the knowledge across scenarios can be shared to improve the CTR performance in each scenario.\n26. However, existing CTR datasets have a limited range of item types and generally originate from the same business scenario, which fails to capture the multi-scenario preferences of users.\n27. For example, Criteo1and Avazu2only involve CTR data for advertisements.\n28. As e-commerce platforms, both Amazon3and AliExpress4provide CTR data for their e-commerce items.\n29. Tenrec[14]focuses more on video and article recommendations.\n30. Secondly, multi-modal features can address the issue of inconsistent IDs for similar items in different business scenarios and effectively establish a bridge between different scenarios.\n31. For example, a video about coffee and a coffee coupon have different IDs in different business scenarios.\n32. Directly using ID features cannot perceive the relationship between these two items.\n33. Multi-modal features inherently carry semantic meaning and can better compensate for the inconsistency of ID features across different domains.\n34. Additionally, with the rise of large language models (LLMs), combining LLMs with CTR prediction has become an emerging research field.\n35. Existing CTR datasets are based on ID features and lack abundant multi-modal features, resulting in the CTR model being unable to test the performance in multi-scenarios and multi-modal settings.\n36. Furthermore, large-scale datasets can reliably and comprehensively reflect the performance of CTR models, while also highlighting the differences between CTR models.\n37. The existing datasets are typically at the scale of 100 million, which is insufficient to further validate the capabilities in larger-scale industrial scenarios.\n38. To address the aforementioned challenges, we propose the AntM 2 C dataset, a large-scale multi-scenario multi-modal dataset for CTR prediction.\n39. Compared with existing CTR datasets, AntM 2 C has the following advantages: • Diverse business scenarios and item types: AntM 2 C contains different types of items from five typical business scenarios on the Alipay platform, including advertisements, vouchers, mini-programs, contents, and videos.\n40. Each business scenario has a unique data distribution.\n41. The abundant intersecting users and similar items between scenarios enable a more comprehensive evaluation for multi-scenario CTR modeling.\n42. • Multi-modal feature system: AntM 2 C not only includes ID features but also provides rich multi-modal features such as text and image, which can establish connections between similar items across scenarios and provide better evaluation for multi-modal CTR models.\n43. • Largest data scale: AntM 2 C comprises 200 million users and 6 million items, with a total of 1 billion samples 5 .\n44. The average number of interactions per user is above 50.\n45. To the best of our knowledge, AntM 2 C is the largest public CTR dataset in terms of scale, which can provide comprehensive and reliable CTR evaluation results.\n46. • Comprehensive benchmark: Based on AntM 2 C, three typical CTR tasks have been built, including multi-scenario modeling, cold-start modeling, and multi-modal modeling.\n47. Benchmark evaluation results based on state-of-the-art models are also provided.\n48. The rest of the paper is organized as follows.\n49. In Section 2, we briefly review some related works about public CTR datasets.\n50. In Section 3, we give a detailed introduction to the dataset collection and data analysis.\n51. In Section 4, we conduct empirical studies with baseline CTR methods on different CTR tasks.\n52. The existing public CTR datasets can be roughly divided into two categories: single-scenario and multi-scenario.\n53. Both have been widely adopted by the evaluation of CTR methods.\n54. The Criteo dataset is one of the publicly available datasets for CTR prediction.\n55. It contains over 45 million records of user interactions with advertisements, including features such as click-through rates, impression rates, and user demographics.\n56. Similar to the Criteo dataset, the Avazu dataset contains over 40 million records of user interactions with mobile advertisements.\n57. It includes features such as device information, app category, and user demographics.\n58. One of the main limitations of the Criteo and Avazu dataset is they only include CTR data for advertisements and cannot be used to evaluate CTR for other business scenarios or types of items.\n59. Additionally, the datasets do not provide text information about the advertisement or user, which can limit the scope of the multi-modal modeling.\n60. The AliExpress is a dataset gathered from real-world traffic logs of the search system in AliExpress.\n61. This dataset is collected from 5 countries: Russia, Spain, French, Netherlands, and America, which can be seen as 5 scenarios.\n62. It can be used to develop and evaluate CTR prediction models for e-commerce platforms.\n63. The Tenrec dataset is a multipurpose dataset for CTR prediction where click data was collected from two scenarios: articles and videos.\n64. Although the above datasets cover different scenarios, the items within these scenarios are similar.\n65. The AliExpress dataset only consists of ecommerce items, and Tenrec involves videos and articles that only reflect the personal interests of users in the entertainment and cultural aspects.\n66. Additionally, similar to single-scenario datasets, both  of these datasets lack textual modal information and only provide features such as IDs.\n67. This limitation restricts the application of multi-modal modeling.\n68. AntM 2 C\'s data is collected from Alipay, a leading platform for payments and digital services.\n69. In order to meet the growing demands of users, Alipay recommends various types of items from different business scenarios to users.\n70. 3.1.1Scenarios.\n71. AntM 2 C collects CTR data in five scenarios on Alipay, and there are differences in the types of items in each scenario.\n72. As shown in Figure1, the CTR prediction occurs in multiple scenarios, including services and content on search, vouchers on marketing, videos on Tab3 page, and advertisements on the membership page.\n73. In the search scenario, when a user enters search words, several relevant mini-apps of services or content are displayed for the user to click on.\n74. Marketing scenarios recommend some consumer vouchers, and users click the coupons they are willing to use.\n75. On the Tab3 page, the recommended items are primarily short videos, and users will click to watch the videos they are interested in.\n76. On the membership page, users may click on some online advertisements.\n77. In conclusion, AntM 2 C includes various types of items from different business scenarios.\n78. 2,we will show that there are differences in the data distribution of these different scenarios.\n79. The rich and diverse items provide a more comprehensive evaluation for CTR prediction.\n80. 3.1.2Data Sampling.\n81. AntM 2 C collects 9-day (from 20230709 to 20230717) CTR samples from the above-mentioned five scenarios and then filters out 1 billion samples of relatively high-activity users who have a total click count ≥ 30 across all scenarios.\n82. In the first stage of open sourcing, we randomly sampled 10 million data from these 1 billion samples, and their statistical properties are shown in Table1.\n83. We will open all 1 billion data in the subsequent stage.\n84. For the purpose of protecting user privacy, we do not explicitly indicate Table1: Data statistics of AntM 2 C.\n85. To protect user privacy, AntM 2 C anonymizes the scenario names as A-E.\n86. The click rate is calculated by dividing the number of clicks by the number of exposures.\n87. Since negative sampling is applied to the samples, the click rate may be higher than the actual value.A-E).\n88. The horizontal axis represents the number of frequencies for users/items, while the vertical axis represents the number of users/items at that frequency.\n89. It can be observed that, in terms of item distribution, all scenarios exhibit a long-tail distribution, with 80% of the sample appearing less than 5 frequencies.\n90. This long-tail distribution is consistent with real-world situations.\n91. As for user distribution, there are differences between scenarios.\n92. In scenario B, the distribution of user frequency has two peaks, one at less than 5 times and the other around 50 times.\n93. After the frequency is greater than 50, the number of users decreases as the frequency increases.\n94. In other scenarios, the exposure frequency of users follows a long-tail distribution similar to that of items, where more exposure frequency leads to fewer users.\n95. Due to the overlapping users between scenarios, the long-tail distribution of users in multiple scenarios becomes a normal distribution in the global samples.\n96. Most users have an exposure frequency of around 50.\n97. Overall, the distribution of items and users in AntM 2 C reflects CTR prediction in practice.\n98. The feature system of AntM 2 C, as shown in Table3, includes ID features of users and items, as well as raw text features.\n99. The user features consist of static profile features6and user sequence features.\n100. The static profile features include basic user attributes such as gender, age, occupation, etc.The sequence features provide the user\'s recent activities on Alipay, including clicked mini-apps, searched services, purchased items, etc.\n101. As mentioned in Section 3.1.3,these user features have been desensitized and encrypted for the purpose of user privacy protection and appear in the dataset in an encrypted ID format, making it impossible to reconstruct the original user features.\n102. In addition to the ID-based features, AntM 2 C also includes the raw text of user search entities to provide multi-modal evaluation.1.\n103. It should be noted that there are a large number of negative samples in the actual online logs (samples that were exposed but not clicked on).\n104. To address this issue, negative sampling was performed which resulted in a higher click-through rate in the AntM 2 C dataset compared to that in the actual online logs.\n105. In this section, we describe the applications of AntM 2 C in several CTR prediction tasks.\n106. We briefly introduce each task and report the results of some baseline methods.\n107. We select the commonly used AUC (Area Under the Curve) as the metrics for all experiments.\n108. The baseline methods and evaluation results in the experiment provide a demo of using AntM 2 C.\n109. More baselines and evaluations will continue to be updated in future work.\n110. Multi-scenario CTR prediction is a common issue in industrial recommendation systems.\n111. It builds a unified model by leveraging CTR data from multiple scenarios.\n112. The knowledge sharing between scenarios enables the multi-scenario model to achieve better performance compared to single-scene modeling.\n113. We conduct an evaluation on multi-scenario CTR prediction using different baseline methods based on the 5 scenarios in the AntM 2 C dataset.\n114. 3. The text features will be used for multi-modal evaluation (see in Section 4.3).\n115. We mainly choose the multitask methods as the baseline methods for multi-scenario CTR prediction.\n116. We treat the CTR estimation for each scenario as a task and share the knowledge among the scenarios at the bottom layer, with each scenario\'s CTR score output at the tower layer.\n117. The baseline methods and hyperparameter settings are as follows: • DNN: The DNN is trained on a mixture of samples from all scenarios without tasks, serving as the baseline for multiscenario CTR prediction.\n118. The DNN consists of three layers with 128, 32, and 2 units, respectively.\n119. • Shared Bottom[10]: Shared bottom is the most fundamental model in multi-task learning, where the knowledge is shared among the tasks at the bottom layer.\n120. Each task has its own independent tower layer and outputs the corresponding CTR score7.\n121. • MMoE[7]: Based on the shared bottom, MMOE introduces multiple expert networks, each specialized in predicting a specific task, sharing a common input layer.\n122. Additionally, MMOE adds a gating network that assigns different weights to each expert based on the input data to determine their influence on predicting the output for a specific task.\n123. • PLE[12]: Based on MMOE, PLE further designs task-specific experts for each task, while retaining the shared expert.\n124. This structure allows the model to better learn the differences and correlations among tasks.\n125. We set the number of experts in PLE to be the same as MMOE, with each of the five scenarios having its own specific expert and one globally shared expert 7 .\n126. All baseline methods utilized the Adam[5]optimizer with a learning rate of 1e-3 for parameter optimization.\n127. The models were trained for 5 epochs with a batch size of 512.5shows the evaluation results of different baseline methods on multi-scenario CTR prediction, from which we can draw the following conclusions.\n128. Firstly, compared to the DNN model that trains all data together without considering scenario characteristics, all multi-task models achieve better performance.\n129. This demonstrates that in AntM 2 C, there are differences and commonalities between scenarios, and simply mixing training data will not achieve the best results.\n130. Secondly, the CTR performance varies across each scenario, indicating different levels of difficulty between scenarios.\n131. For example, in scenario B, where there is a large amount of data, the AUC is generally above 0.93, while in scenario D, the AUC is only around 0.68.\n132. The diverse business scenarios and items in AntM 2 C enable a more comprehensive and diverse evaluation of CTR.\n133. Finally, the expert-structured MMOE and PLE outperform the shared bottom model, demonstrating that refined model design can enhance the performance on AntM 2 C.\n134. AntM 2 C is capable of reflecting the differences between different models.\n135. The cold-start problem is a challenging issue in recommendation systems.\n136. Training high-quality CTR models using sparse user-item interaction data is a challenging task.\n137. Cold-start primarily involves two aspects: users and items.\n138. As shown in Figure2, the AntM 2 C dataset exhibits a natural long-tail distribution in both users and items.\n139. Therefore, we conduct a comprehensive evaluation of coldstart baseline methods based on AntM 2 C dataset.\n140. In cold-start CTR prediction, we split the dataset based on time, using data before 20230717 as the training set and data on 20230717 as the validation and test sets.\n141. Based on this data division, we simulated two common cold-start problems in practice: few-shot and zero-shot.\n142. • Few-shot: users and items that appear in the training set with a count greater than 0 and less than9, meaning there is only a small amount of training data for these users and items.\n143. • Zero-shot: users and items that have never appeared in the training set, indicating that either the user is visiting the scenario for the first time or the item has been launched and added to the scenario on the first day.\n144. Table6shows the data distribution of the test set under cold-start CTR evaluation.\n145. By using this dataset division, we can comprehensively evaluate and compare the performance of CTR models on few-shot and zero-shot samples.\n146. For few-shot samples, we can observe the model\'s performance with only a small amount of training data and evaluate the model\'s generalization ability.\n147. For zero-shot samples, we can evaluate the model\'s recommendation ability on samples that it has never seen before.\n148. The key issue in cold-start modeling is how to learn user preferences and embeddings of users and items with limited data.\n149. In recent years, meta-learning-based cold-start methods have become state-of-the-art methods.\n150. We selected several representative methods with publicly available code as our baseline models.\n151. • DropoutNet[13]: The DropoutNet is a popular cold-start method which applies dropout to control input, and exploits the average representations of interacted items/users to enhance the embeddings of users/items.\n152. • MAML[2]: The MAML algorithm is a popular meta-learning approach that aims to enable fast adaptation to new tasks with limited data.\n153. MAML learns a good initialization of model parameters that can be effectively adapted to new tasks quickly.\n154. We treat each user and item as a task in MAML, and conduct meta-training on warm items.\n155. Then we perform meta-testing on cold-start items.\n156. • MeLU[6]: The MeLU algorithm is the first to apply the MAML to address the cold-start problem in recommender systems.\n157. Although MetaEmb only optimizes the embeddings of items, we have also applied the same approach to optimize the embeddings of users.\n158. These base models share the common embedding and DNN structure.\n159. The dimensionality of embedding vectors of each input field is fixed to 32 for all our experiments.\n160. The Adam optimizer with a learning rate of 1e-3 is used to optimize the model parameters, and the training is performed for 3 epochs with a batch size of 512.\n161. 7shows the CTR performance for cold-start users and items.\n162. Because there is limited data for cold start users and items, we do not calculate AUC by scenarios, and evaluate the overall performance of cold start users and items.\n163. From the table, we can observe several phenomena.\n164. Firstly, compared to the results shown in Table5, the AUC for cold-start users and items are generally lower than the overall level, which demonstrates that AntM 2 C\'s data can effectively reflect the differences between cold and warm items and users.\n165. Secondly, different cold-start methods show distinguishable results in AntM 2 C, and all of them are significantly better than the DNN model without cold-start optimization.\n166. This indicates that AntM 2 C can effectively compare the effects of different cold-start methods and demonstrate the distinctiveness between methods.\n167. Finally, the lower performance of zero-shot compared to few-shot indicates that zero-shot CTR prediction is more challenging than few-shot.\n168. The two cold start modes provided by AntM 2 C can comprehensively evaluate cold-start CTR prediction.\n169. With the rise of large language models (LLMs), it has become a hot research topic to effectively transfer the knowledge of LLM to CTR prediction.\n170. There have been many works[3,4,9,11]based on multi-modal CTR modeling using features such as item and user text.\n171. In multi-modal evaluation, we adapt the same data processing approach as in multi-scenario evaluation mentioned in Section 4.1.1,and additionally include the text features from Table3: user query entities and item entities.\n172. The text features will be used as inputs to the model together with other ID features.\n173. For the baseline model, we use the language model to process the text features, and then concatenate the text embedding with other ID features and input them into the multi-scenario model described in Section 4.1.2.\n174. For ease of evaluation, we choose MMoE as the backbone and pre-trained Bertbase13[1]as the text embedding extractor.\n175. The output dimension of Bert\'s embeddings is 768.\n176. Then, a DNN with two layers, each layer having [768, 32] units, is used to reduce the dimension of Bert\'s embedding to 32.\n177. This reduced embedding is concatenated with other features and input into the MMOE model.\n178. More powerful language models and the application of text features will continue to be supplemented in future works.\n179. Table8shows the evaluation results of the multimodal CTR.\n180. It can be observed that, after adding the text modality, the CTR performance is better in data-sparse scenarios C, D, and E compared to using only the ID modality in the MMoE.\n181. Since the current baseline for using the text modality is relatively simple, the improvement in performance is not significant.\n182. However, this shows the potential of the text modality provided in AntM 2 C to improve CTR performance.\n\n请严格按照JSON格式返回：{"labels": [0,1,0,...]}，不要包含任何其他文字。'}], 'model': 'qwen-plus', 'response_format': {'type': 'json_object'}}}
2025-10-14 21:46:20,221 - DEBUG - Sending HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
2025-10-14 21:46:20,222 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-10-14 21:46:20,222 - DEBUG - send_request_headers.complete
2025-10-14 21:46:20,222 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-10-14 21:46:20,222 - DEBUG - send_request_body.complete
2025-10-14 21:46:20,222 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-10-14 21:46:34,274 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'vary', b'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding'), (b'x-request-id', b'1f730c89-03cb-4ba7-b761-ecc6e1d1940d'), (b'x-dashscope-call-gateway', b'true'), (b'content-type', b'application/json'), (b'req-cost-time', b'14022'), (b'req-arrive-time', b'1760449571356'), (b'resp-start-time', b'1760449585378'), (b'x-envoy-upstream-service-time', b'13987'), (b'content-encoding', b'gzip'), (b'date', b'Tue, 14 Oct 2025 13:46:25 GMT'), (b'server', b'istio-envoy'), (b'transfer-encoding', b'chunked')])
2025-10-14 21:46:34,275 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-14 21:46:34,275 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-10-14 21:46:34,275 - DEBUG - receive_response_body.complete
2025-10-14 21:46:34,275 - DEBUG - response_closed.started
2025-10-14 21:46:34,275 - DEBUG - response_closed.complete
2025-10-14 21:46:34,275 - DEBUG - HTTP Response: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "200 OK" Headers({'vary': 'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding', 'x-request-id': '1f730c89-03cb-4ba7-b761-ecc6e1d1940d', 'x-dashscope-call-gateway': 'true', 'content-type': 'application/json', 'req-cost-time': '14022', 'req-arrive-time': '1760449571356', 'resp-start-time': '1760449585378', 'x-envoy-upstream-service-time': '13987', 'content-encoding': 'gzip', 'date': 'Tue, 14 Oct 2025 13:46:25 GMT', 'server': 'istio-envoy', 'transfer-encoding': 'chunked'})
2025-10-14 21:46:34,275 - DEBUG - request_id: 1f730c89-03cb-4ba7-b761-ecc6e1d1940d
2025-10-14 21:46:34,275 - DEBUG - API request completed in 14.06 seconds
2025-10-14 21:46:34,275 - DEBUG - Raw model response: {"labels": [0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]}
2025-10-14 21:46:34,275 - INFO - Successfully processed 130 labels
2025-10-14 21:46:34,275 - ERROR - Label count mismatch for AntM 2 C_ A Large Scale Dataset For Multi_Scenario Multi_Modal CTR Prediction
2025-10-14 21:46:34,275 - INFO - Evaluating paper 5/18: AU_AIR_ A Multi_modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance
2025-10-14 21:46:34,275 - INFO - Starting model prediction
2025-10-14 21:46:34,275 - INFO - Attempt 1 of 5
2025-10-14 21:46:34,276 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-82a0f7b2-2833-4f6e-a965-57929091f924', 'json_data': {'messages': [{'role': 'system', 'content': '你是一个专门用于识别学术论文中数据集描述的助手。你需要判断每个输入的句子是否描述了研究中使用的数据集。请确保：\n    1. 只输出JSON格式的结果\n    2. 结果格式必须为 {"labels": [0,1,0,...]}\n    3. 不要输出任何其他文字或解释'}, {'role': 'user', 'content': '你是一位专业的学术论文数据集描述识别专家。请仔细分析下面这篇论文中的每个句子，判断它们是否描述了本研究特定构建的数据集信息。\n\n    论文标题: AU_AIR_ A Multi_modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance\n\n    判断标准:\n    判断为1(数据集描述)的标准：\n    句子必须直接描述本研究特定构建的数据集,包括:\n    1. 该数据集的构建过程和方法\n    2. 该数据集的具体构成和规模\n    3. 该数据的来源和收集方式\n    4. 该数据的预处理步骤\n    5. 该数据集的可获取方式(如发布地址)\n    6. 或者其他直接或者间接描述了该数据集的句子\n\n    所有其他类型的句子均标记为0。\n    \n    请对每个句子进行分析，返回一个JSON格式的标签数组。格式要求：\n    1. 必须是有效的JSON格式\n    2. 只包含labels字段，值为0和1组成的数组\n    3. 数组长度必须与句子数量相同\n    4. 示例格式：{"labels": [0,1,0,1,...]}\n\n    需要判断的句子：\n\n    1. Abstract: Unmanned aerial vehicles (UAVs) with mounted cameras have the advantage of capturing aerial (bird-view) images.\n2. The availability of aerial visual data and the recent advances in object detection algorithms led the computer vision community to focus on object detection tasks on aerial images.\n3. As a result of this, several aerial datasets have been introduced, including visual data with object annotations.\n4. UAVs are used solely as flying-cameras in these datasets, discarding different data types regarding the flight (e.g., time, location, internal sensors).\n5. In this work, we propose a multi-purpose aerial dataset (AU-AIR) that has multi-modal sensor data (i.e., visual, time, location, altitude, IMU, velocity) collected in real-world outdoor environments.\n6. The AU-AIR dataset includes meta-data for extracted frames (i.e., bounding box annotations for trafficrelated object category) from recorded RGB videos.\n7. Moreover, we emphasize the differences between natural and aerial images in the context of object detection task.\n8. For this end, we train and test mobile object detectors (including YOLOv3-Tiny and MobileNetv2-SSDLite) on the AU-AIR dataset, which are applicable for real-time object detection using on-board computers with UAVs.\n9. Since our dataset has diversity in recorded data types, it contributes to filling the gap between computer vision and robotics.\n10. The dataset is available at https://bozcani.github.io/auairdataset.\n11. Unmanned aerial vehicles (UAVs) are extensively used as flying platforms of sensors for different domains such as traffic surveillance[1], managing the urban environment[2], package delivery[3]or aerial cinematography[4].\n12. For these applications, UAVs are equipped with mounted cameras and mainly gather visual data of the environment.\n13. Then, computer vision algorithms are applied to aerial visual data to extract high-level information regarding the environment.\n14. Object detection is one of the most studied problems in computer vision.\n15. The recent advances in deep learning (variants of convolutional neural networks (CNNs) mainly) have led to breakthrough object detection performances with the availability of large datasets and computing power.\n16. Since these methods require a large number of training samples, several datasets (e.g., COCO[5], Pascal VOC[6]) have been introduced for benchmarking for the object detection task.\n17. The samples in these datasets consist of natural images that are mainly captured by handheld cameras.\n18. The significant differences between natural and aerial images (such as object layouts and sizes) cause these object detectors to have trouble to find objects in aerial images.\n19. Therefore, several datasets (e.g.,[7]-[13]) have been introduced in recent years as a benchmark for object detection in aerial images.\n20. Besides visual data gathered by a camera, the data from other sensors might give crucial information about the environment.\n21. The use of UAVs as only flying cameras cut off the potential advance in multi-modal object detection algorithms for aerial applications.\n22. For instance, the recent advances in perception for autonomous driving have brought new datasets such as[14]-[16]including multi-modal data (e.g., RGB images, Global Positioning System (GPS) coordinates, inertial measurement unit (IMU) data).\n23. Although the data fusion for object detection is still open research topic[17], these multi-modal datasets allow a benchmark for further research.\n24. However, to the best of our knowledge, there is no such multi-modal dataset collected in a real-world outdoor environment for UAVs.\n25. In this work, we present a multi-modal UAV dataset (The AU-AIR dataset) in order to push forward the development of computer vision and robotic algorithms targeted at autonomous aerial surveillance.\n26. The AU-AIR dataset meets vision and robotics for UAVs having the multi-modal data from different on-board sensors.\n27. The dataset consists of 8 video streams (over 2 hours in total) for traffic surveillance.\n28. The videos mainly are recorded at Skejby Nordlandsvej and P.O Pedersensvej roads (Aarhus, Denmark).\n29. The dataset includes aerial videos, time, GPS coordinates and the altitude of the UAV, IMU data, and the velocity.\n30. The videos are recorded at different flight altitudes from 5 meters to 30 meters and in different camera angles from 45 degrees to 90 degrees (i.e., complete bird-view images that the camera is perpendicular to the Earth).\n31. Instances belonging to different object categories related to the traffic surveillance context are annotated with bounding boxes in video frames.\n32. Moreover, each extracted video frame is labeled with the flight data (See Fig.1).\n33. The whole dataset includes 32,823 labeled video frames with object annotations and the corresponding flight data.\n34. Eight object categories are annotated including person, car, van, truck, motorbike, bike, bus, trailer.\n35. The total number of annotated instances is 132,034.\n36. The dataset is split into 30,000 training-validation samples and 2,823 test samples.\n37. In this work, we emphasize differences between aerial and natural images in the context of object detection tasks.\n38. To this end, we compare image samples and object instances between the AU-AIR dataset and the COCO dataset[5].\n39. In our experiments, we train and evaluate two mobile object detectors (including YOLOv3-tiny[18]and MobileNetv2-SSD Lite[19]on the AU-AIR dataset.\n40. We form a baseline, including mobile object detectors since we focus on realtime performance and the applicability of object detection task onboard computers mounted on UAV.\n41. In recent years, several drone datasets have been introduced for object detection tasks ([7]-[13]).\n42. Zhu et al.[7]propose a UAV dataset (VisDrone) consisting of visual data and object annotations in images and frames.\n43. In the VisDrone dataset, object instances belonging the certain categories are annotated by bounding boxes and category labels.\n44. Besides object annotations, VisDrone includes some vision-related attributes such as the visibility of a scene, occlusion status.\n45. Du et al.[8]propose a benchmark dataset for object detection and tracking in aerial images.\n46. The dataset also includes meta information regarding the flight altitude.\n47. Hsieh et al.[9]propose a UAV-based counting dataset (CARPK) including object instances that belong to the car category.\n48. Robicquet et al.[10]introduce a UAV dataset (Stanford) that collects images and videos of six types of objects in the Stanford campus area.\n49. In this dataset, some of the object categories dominate the dataset having a high number of samples, whereas the remaining object categories have significantly less number of instances.\n50. Mueller et al.[11]propose synthetic dataset created by a simulator for target tracking with a UAV.\n51. Collins et al.[12]introduce a benchmarking website (VIVID) with an evaluation dataset collected under the DARPA VIVID program.\n52. Krajewski et al.propose an aerial dataset collected from highways, including object bounding boxes and labels of vehicles.\n53. These datasets are annotated by common objects in an environment such as humans and different types of vehicles (e.g., car, bike, van).\n54. However, they only include visual data and bounding box annotations for objects and discard other sensory data.\n55. Among these studies, only UAVDT[8]includes an attribute that gives limited information about the flight altitude (i.e., labels such as "low-level", "mid-level" and "high-level").\n56. Fonder et al.[20]propose a synthetic dataset (Mid-Air) for low altitude drone flights in unstructured environments (e.g., forest, country).\n57. It includes multi-modal data regarding the flight (e.g., visual, GPS, IMU data) without any annotations for visual data.\n58. There are also multi-modal drone datasets in the literature ([20]-[24]).\n59. However, the visual data are not collected for object detection since the main focus of these studies is the UAV navigation.\n60. Therefore, these datasets do not have object annotations.\n61. The comparison of existing datasets is given in TableI.\n62. Looking also at the summary of the existing studies in TableI, the followings are the main contributions of this work: • To the best of our knowledge, the AU-AIR dataset is the first multi-modal UAV dataset for object detection.\n63. The dataset includes flight data (i.e., time, GPS, altitude, IMU data) in addition to visual data and objects annotations.\n64. • Considering the real-time applicability, we form a baseline training and testing mobile object detectors with the AU-AIR dataset.\n65. We emphasize the differences between object detection in aerial images and natural images.\n66. The availability of large amounts of data and processing power enables deep neural networks to achieve state-of-theart results for object detection.\n67. Currently, deep learningbased object detectors are separated into two groups.\n68. The first group consists of region-based CNNs that ascend on image classifiers.\n69. Region-based CNNs propose image regions that are likely to contain an object and classify the region into a predefined object category.\n70. The second group has only one stage converting to the object detection problem into the bounding box prediction for objects, without re-purposing image classifiers.\n71. Faster-R-CNN[25]is one of the wellknown models belonging to the first group, YOLO[26]and SSD[27]are the popular object detectors that belong to the second group.\n72. Deep learning-based object detectors have trained and performed on large datasets such as COCO[5]and PASCAL[6].\n73. These datasets include natural images that contain a single object or multi objects in their natural environments.\n74. Most of the images in these datasets are captured by humans using a handheld camera so that the vast majority of images have side-view.\n75. There are challenges of the object detection in natural images such as occlusion, illumination changes, rotation, low resolution, crowd existence of instances.\n76. Aerial images have different characteristics from natural images due to having a bird\'s-eye view.\n77. First of all, objects in natural images are much larger than their counterparts in aerial images.\n78. For example, an object category such as humans may occupy a large number of pixels in natural images.\n79. However, it may have a few numbers of pixels   in an aerial image that is quite challenging to detect for object detectors (See Fig.2).\n80. Moreover, aerial images can be fed to a network with higher dimensions that increases computational cost in order to prevent the diminishing of pixels belonging to small objects.\n81. Secondly, an occlusion is observed in different conditions for natural and aerial images.\n82. In natural images, an object instance may be occluded by another foreground object instance (e.g., a human in front of a car).\n83. (See Fig.3.\n84. Thirdly, the perspective in aerial images makes appearances of objects short and squat.\n85. This fact diminishes the information regarding an object height (See Fig.4).\n86. Moreover, although aerial images can supply more contextual information about an environment by a broader view angle, the object instances may be amid cluttered.\n87. Lastly, having a drone to capture aerial images, the altitude changes during the flight can cause varieties in object size and appearance in aerial images.\n88. Therefore, a recording of aerial videos at different altitudes may change the levels of challenges mentioned above.\n89. To address the challenges mentioned in Section II, we propose a multi-modal drone dataset (AU-AIR) including videos, object annotations in the extracted frames and sensor data for the corresponding frames.\n90. The data are captured by low-level flight (max.30 meters) and for the scenario of a traffic surveillance.\n91. The AU-AIR dataset consists of video clips, sensor data, and object bounding box annotations for video frames.\n92. We have used a quadrotor (Parrot Bebop 2) to capture the videos and record the flight data.\n93. An on-board camera has recorded the videos with a resolution of 1920 × 1080 pixels at 30 frames per second (fps).\n94. The sensor data have been recorded for every 20 milliseconds.\n95. The AU-AIR dataset consists of 8 video clips (approximately in 2 hours of a total length) with 32,823 extracted frames.\n96. All videos are recorded for a scenario of aerial traffic surveillance at the intersection of Skejby Nordlandsvej and P.\n97. Moreover, the videos cover various lighting conditions due to the time of the day and the weather conditions (e.g., sunny, partly sunny, cloudy).\n98. Capturing an aerial video with a UAV brings different challenges for visual surveillance that are significantly different from natural images.\n99. To add these challenges in our dataset, we have captured the videos in different flight altitudes and camera angles.\n100. The flight altitude changes between 10 meters to 30 meters in the videos and the camera angle is adjusted from 45 degrees to 90 degrees (perpendicular to the Earth).\n101. An increase in the camera angle makes object detection task more challenging since images get differ from natural images.\n102. Although the videos have been recorded with 30 fps, we have extracted five frames for every second in order to prevent the redundant occurrence of frames.\n103. Both of raw videos and extracted frames have a resolution of 1920×1080 pixels.\n104. Considering a traffic surveillance scenario, we have manually annotated specific object categories in the frames.\n105. For annotation, we used a bounding box and object category index for each instance.\n106. The annotated object categories include eight types of objects which highly occur during the traffic surveillance: person, car, bus, van, truck, bike, motorbike, and trailer.\n107. For annotation, we employed workers on Amazons Mechanical Turk (AMT)[28].\n108. In order to increase the labeling quality, three workers annotated the same frame separately.\n109. Then, we combined annotations if they have the same object labels, and whose bounding boxes overlap more than a certain threshold.\n110. We chose a threshold as a value of 0.75 experimentally.\n111. In case this condition is not satisfied, we manually fine-tuned the bounding boxes and class labels.\n112. The category distribution over the dataset can be seen in Fig.5.\n113. In the context of traffic surveillance, cars appear significantly more than other classes, and three vehicle types (car, van, truck) have a major portion of annotated bounding boxes.\n114. The AU-AIR dataset includes frames that are captured in different flight altitudes (See Fig.6).\n115. We recorded the data mainly for 10 meters, 20 meters, and 30 meters with different camera angles from 45 degrees to 90 degrees.\n116. In addition to visual data and object annotations, the AU-AIR dataset includes sensor data that are logged during the video recording.\n117. In the dataset, we have the following attributes for each extracted frame: • la: latitude of the UAV (read from GPS sensor).\n118. • lo: longitude of the UAV (read from GPS sensor) • a: altitude of the UAV (read from altimeter) • φ: UAV roll angle (rotation around the x axis) (read from IMU sensor) • θ: UAV pitch angle (rotation around the y axis) (read from IMU sensor) • ψ: UAV yaw angle (rotation around the z axis) (read from IMU sensor) • V x : speed on the x axis • V y : speed on the y axis • V z : speed on the z axis TableIIshows unit values and ranges for each attribute except the date.\n119. The date (d) has a format of MMDDYYYY-HHMMSS where MM, DD, YYYY, HH, MM, SS indicates the month, day, year, hour, minutes, and second, respectively.\n120. The velocities (V x , V y , V z ) and rotation angles (φ, θ, ψ) are calculated according to the UAV body-frame given in Fig.7.\n121. We train and evaluate mobile object detectors with our dataset.\n122. During the evaluation, we consider real-time performance rather than achieving a state-of-the-art accuracy for the sake of the applicability.\n123. Therefore, we choose two mobile object detectors (YOLOv3-Tiny[18]and MobileNetv2-SSDLite[19]), which have a reasonable trade-off between the detection accuracy and the inference time.\n124. We configure YOLOv3-Tiny[18]and MobileNetv2-SSDLite[19]for the bench-marking using the default parameters (e.g., learning rate, input size) as suggested in the original papers.\n125. We use the models that are trained on the COCO dataset as backbones.\n126. We split the AU-AIR dataset into %60 training, %10 validation and %30 testing samples.\n127. The object detectors are adapted to the total number of classes in the AU-AIR dataset (8 classes in total) by changing their last layers.\n128. To compare detection performances, we use mean average precision (mAP) that is a prominent metric in object detection[5],[6].\n129. It is the mean of the average precision (AP) values which compute the precision score for an object category at discretized recall values over 0 to 1[6].\n130. We consider 11 different recall values as in[6]and the intersection over union (IoU) threshold as 0.5.\n131. For benchmarking, we train YOLOv3-Tiny and MobileNetv2-SSDLite with the AU-AIR Dataset.\n132. We use the batch size of 32 and Adam optimizer with the default parameters (alpha= 0.001, beta1=0.9,beta2=0.999).\n133. The training is stopped when the validation error starts to increase.\n134. Both networks are pre-trained on the COCO dataset.\n135. In order to see the effect of the training with an aerial dataset and a natural image dataset, we also use YOLOv3-Tiny and MobileNetv2-SSDLite trained on the COCO dataset without further training with the AU-AIR dataset.\n136. The results are given in TableIII.\n137. As shown in TableIII, the networks only trained on the COCO dataset have poor results.\n138. This is expected since the characteristics of natural images are significantly different from natural images.\n139. We observe that the AP values of motorbike and bicycle categories are significantly lower than the AP values of other categories.\n140. This fact might happen due to the class imbalance problem and the small object sizes of these categories.\n141. However, the bus category has the highest AP value, although there are fewer bus instances.\n142. This might result from the large size of bus instances in the frames.\n143. Furthermore, although the size of human instances is usually as small as the sizes of motorbike and bicycles, the AP values of the human category are relatively higher than these classes.\n144. This fact might be a consequence of the high number of human instances.\n145. There is no available AP values for the van and trailer categories in Table III since they do not exist in the COCO dataset.\n146. The baselines trained on the AU-AIR dataset are good at finding objects in aerial images that are captured at different altitudes and view angles.\n147. Qualitative results can be seen in Fig.8.\n148. Among the baselines, YOLOv3-Tiny has higher AP values and mAP value compared to MobileNetv2-SSDLite.\n149. There is no significant difference between inference times (17.5 FPS and 17 FPS for YOLOv3-Tiny and MobileNetv2-SSDLite on TX2, respectively).\n150. Since the number of instances of each object category is imbalanced in the AU-AIR dataset (Fig.5), we consider several methods to solve the imbalanced class problem in the next version of the dataset.\n151. As a first step, we will try to collect more data to balance the number of instances.\n152. Besides, we may consider adding synthetic data (i.e., changing the brightness of images, translation, rotation) to increase the number of object categories which has a low number of samples in the current version.\n153. We use AMT to annotate objects in images.\n154. Although three different people annotate one image and the annotations are manually checked by ourselves, there might be still overlooked samples that have weak annotations (e.g., unlabelled instances, loose bounding box drawings).\n155. Therefore, we consider using a three-step workflow proposed by Su et al.[29].\n156. In this workflow, the first worker draws a bounding box around an instance, the second worker verifies whether the bounding box is correctly drawn, and the third worker checks whether all object instances are annotated.\n157. Unlike other UAV object detection datasets, ours includes sensor data corresponding to each frame.\n158. In this work, we give a baseline only for object annotations and visual data.\n159. As future work, more baselines may be added to encourage research using sensor data (e.g., navigation and control of a UAV, object detection using multi-modal data).\n160. Also, we can add more visual sensors, such as multi-spectral cameras.\n161. We have used a ready-to-fly quadrotor (i.e., Parrot Bebop 2) to collect the whole dataset.\n162. We also consider collecting more samples from other platforms (e.g., different types of UAVs) using cameras that have different resolutions and frame rates.\n163. In this dataset, traffic surveillance is the primary context.\n164. In future work, we consider increasing the number of environment contexts to increase diversity in the dataset.\n165. In this work, we propose the AU-AIR dataset that is a multi-modal UAV dataset collected in an outdoor environment.\n166. Our aim is to fill the gap between computer vision and robotics having a diverse range of recorded data types for UAVs.\n167. Including visual data, object annotations, and flight data, it can be used for different research fields focused on data fusion.\n168. We have emphasized the differences between natural images and aerial images affecting the object detection task.\n169. Moreover, since we consider real-time performance and applicability in real-world scenarios, we have created a baseline, including two mobile object detectors in the literature (i.e., YOLOv3-Tiny[18]and MobileNetv2-SSDLite[19]).\n170. In our experiments, we showed that mobile networks trained on natural images have trouble in detecting objects in aerial images.\n\n请严格按照JSON格式返回：{"labels": [0,1,0,...]}，不要包含任何其他文字。'}], 'model': 'qwen-plus', 'response_format': {'type': 'json_object'}}}
2025-10-14 21:46:34,277 - DEBUG - Sending HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
2025-10-14 21:46:34,277 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-10-14 21:46:34,277 - DEBUG - send_request_headers.complete
2025-10-14 21:46:34,277 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-10-14 21:46:34,277 - DEBUG - send_request_body.complete
2025-10-14 21:46:34,277 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-10-14 21:46:47,871 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'vary', b'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding'), (b'x-request-id', b'9a9f2b75-a56d-4100-99e6-1aaaae4fc8e3'), (b'x-dashscope-call-gateway', b'true'), (b'content-type', b'application/json'), (b'req-cost-time', b'13563'), (b'req-arrive-time', b'1760449585411'), (b'resp-start-time', b'1760449598974'), (b'x-envoy-upstream-service-time', b'13529'), (b'content-encoding', b'gzip'), (b'date', b'Tue, 14 Oct 2025 13:46:38 GMT'), (b'server', b'istio-envoy'), (b'transfer-encoding', b'chunked')])
2025-10-14 21:46:47,871 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-14 21:46:47,871 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-10-14 21:46:47,871 - DEBUG - receive_response_body.complete
2025-10-14 21:46:47,871 - DEBUG - response_closed.started
2025-10-14 21:46:47,871 - DEBUG - response_closed.complete
2025-10-14 21:46:47,871 - DEBUG - HTTP Response: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "200 OK" Headers({'vary': 'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding', 'x-request-id': '9a9f2b75-a56d-4100-99e6-1aaaae4fc8e3', 'x-dashscope-call-gateway': 'true', 'content-type': 'application/json', 'req-cost-time': '13563', 'req-arrive-time': '1760449585411', 'resp-start-time': '1760449598974', 'x-envoy-upstream-service-time': '13529', 'content-encoding': 'gzip', 'date': 'Tue, 14 Oct 2025 13:46:38 GMT', 'server': 'istio-envoy', 'transfer-encoding': 'chunked'})
2025-10-14 21:46:47,871 - DEBUG - request_id: 9a9f2b75-a56d-4100-99e6-1aaaae4fc8e3
2025-10-14 21:46:47,871 - DEBUG - API request completed in 13.60 seconds
2025-10-14 21:46:47,871 - DEBUG - Raw model response: {"labels": [1,0,0,0,1,1,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]}
2025-10-14 21:46:47,871 - INFO - Successfully processed 130 labels
2025-10-14 21:46:47,871 - ERROR - Label count mismatch for AU_AIR_ A Multi_modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance
2025-10-14 21:46:47,872 - INFO - Evaluating paper 6/18: AV_NeRF_ Learning Neural Fields for Real_World Audio_Visual Scene Synthesis
2025-10-14 21:46:47,872 - INFO - Starting model prediction
2025-10-14 21:46:47,872 - INFO - Attempt 1 of 5
2025-10-14 21:46:47,872 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-d460f446-935a-4513-ac6f-b1903aabde9e', 'json_data': {'messages': [{'role': 'system', 'content': '你是一个专门用于识别学术论文中数据集描述的助手。你需要判断每个输入的句子是否描述了研究中使用的数据集。请确保：\n    1. 只输出JSON格式的结果\n    2. 结果格式必须为 {"labels": [0,1,0,...]}\n    3. 不要输出任何其他文字或解释'}, {'role': 'user', 'content': '你是一位专业的学术论文数据集描述识别专家。请仔细分析下面这篇论文中的每个句子，判断它们是否描述了本研究特定构建的数据集信息。\n\n    论文标题: AV_NeRF_ Learning Neural Fields for Real_World Audio_Visual Scene Synthesis\n\n    判断标准:\n    判断为1(数据集描述)的标准：\n    句子必须直接描述本研究特定构建的数据集,包括:\n    1. 该数据集的构建过程和方法\n    2. 该数据集的具体构成和规模\n    3. 该数据的来源和收集方式\n    4. 该数据的预处理步骤\n    5. 该数据集的可获取方式(如发布地址)\n    6. 或者其他直接或者间接描述了该数据集的句子\n\n    所有其他类型的句子均标记为0。\n    \n    请对每个句子进行分析，返回一个JSON格式的标签数组。格式要求：\n    1. 必须是有效的JSON格式\n    2. 只包含labels字段，值为0和1组成的数组\n    3. 数组长度必须与句子数量相同\n    4. 示例格式：{"labels": [0,1,0,1,...]}\n\n    需要判断的句子：\n\n    1. Abstract: Can machines recording an audio-visual scene produce realistic, matching audiovisual experiences at novel positions and novel view directions?\n2. We answer it by studying a new task-real-world audio-visual scene synthesis-and a first-of-itskind NeRF-based approach for multimodal learning.\n3. Concretely, given a video recording of an audio-visual scene, the task is to synthesize new videos with spatial audios along arbitrary novel camera trajectories in that scene.\n4. We propose an acoustic-aware audio generation module that integrates prior knowledge of audio propagation into NeRF, in which we implicitly associate audio generation with the 3D geometry and material properties of a visual environment.\n5. Furthermore, we present a coordinate transformation module that expresses a view direction relative to the sound source, enabling the model to learn sound source-centric acoustic fields.\n6. To facilitate the study of this new task, we collect a high-quality Real-World Audio-Visual Scene (RWAVS) dataset.\n7. We demonstrate the advantages of our method on this real-world dataset and the simulation-based SoundSpaces dataset.\n8. We recommend that readers visit our project page for convincing comparisons: https://liangsusan-git.github.io/project/avnerf/.\n9. We study a new task, real-world audio-visual scene synthesis, to generate target videos and audios along novel camera trajectories from source audio-visual recordings of known trajectories.\n10. By learning from real-world source videos with binaural audio, we aim to generate target video frames and spatial audios that exhibit consistency with the given camera trajectory visually and acoustically.\n11. This consistency ensures perceptual realism and immersion, enriching the overall user experience.\n12. As far as we know, attempts in the audio-visual learning literature [1-11] have yet to succeed in solving this challenging task thus far.\n13. Although there are similar works[12][13][14][15], these methods have constraints that limit their ability to solve this new task.\n14. Luo et al. [12]  propose neural acoustic fields to model sound propagation in a room.\n15. Su et al. [13]  introduce representing audio scenes by disentangling the scene\'s geometry features.\n16. These methods are tailored for estimating room impulse response signals in a simulation environment that are difficult to obtain in a real-world scene.\n17. Concurrent to our work, ViGAS proposed by Chen et al. [15]  learns to synthesize new sounds by inferring the audio-visual cues.\n18. However, ViGAS is limited to a few viewpoints for audio generation.\n19. We introduce AV-NeRF, a novel NeRF-based method of synthesizing real-world audio-visual scenes.\n20. AV-NeRF enables the generation of videos and spatial audios, following arbitrary camera trajectories.\n21. It utilizes source videos and camera poses as references.\n22. AV-NeRF consists of two branches: A-NeRF, which learns the acoustic fields of an environment, and V-NeRF, which models color and density fields.\n23. We represent a static audio field as a continuous function using A-NeRF, which takes the listener\'s position and head direction as input.\n24. A-NeRF effectively models the energy decay of sound as the sound travels from the source to the listener by correlating the listener\'s position with the 37th Conference on Neural Information Processing Systems (NeurIPS 2023).\n25. To the best of our knowledge, our method is the first NeRF-based system capable of synthesizing realworld videos with perceptually realistic binaural audios at arbitrary poses.\n26. However, existing datasets do not meet the specific requirements of our experiments, particularly in terms of simultaneously providing camera poses, high-quality binaural audios, and images.\n27. Therefore, we curated a highquality audio-visual scene dataset (real) to address this gap and facilitate further research on this problem.\n28. Additionally, we utilize (synthetic) SoundSpaces dataset[4]to validate our method.\n29. (1) RWAVS Dataset.\n30. We collected the Real-World Audio-Visual Scene (RWAVS) dataset to benchmark our method.\n31. In order to increase the diversity of our dataset, we recorded data across different scenarios.\n32. Fig.5shows the example scenarios we used for data recording, including both indoor and outdoor environments, which we believe represent most daily settings.\n33. RWAVS dataset comprises multimodal data, including camera poses, high-quality binaural audios, and videos.\n34. Unlike Replay-NVAS dataset[15], where the environment and the recording viewpoint are constant, RWAVS dataset contains various viewpoints in diverse environments.\n35. During data recording, we randomly moved around the environment while holding the device, capturing various acoustic and visual signals.\n36. RWAVS dataset encompasses all positions and directions (360 • ) within an environment.\n37. In detail, we employed a 3Dio Free Space XLR binaural microphone for capturing high-quality stereo audio, a TASCAM DR-60DMKII for recording and storing audio, and a GoPro Max for capturing accompanying videos.\n38. Additionally, an LG XBOOM 360 omnidirectional speaker was used as the sound source.\n39. For each environment and sound source combination, we collected data ranging from 10 to 25 minutes, resulting in a total collection of 232 minutes (3.8 hours) of data from diverse environments with varying source positions.\n40. We extract key frames at 1 fps from recorded videos and use COLMAP[46]to estimate the corresponding camera pose.\n41. Each key frame is accompanied by one-second binaural audio and one-second source audio, forming a complete data sample.\n42. For audio clips with noticeable background noise, we perform noise suppression using Adobe Audition[47].\n43. We split 80% data as training samples and the rest as validation samples.\n44. After pre-processing, we obtain 9850 and 2469 samples for training and validation, respectively.\n45. This dataset is challenging because of the diverse environments and various camera poses.\n46. We will release this dataset to the research community.\n47. (2) SoundSpaces Dataset.\n48. While RWAVS offers realistic training samples, its realism restricts its scale because it is time-consuming to record high-quality multimodal data in the real world.\n49. Therefore, we use the synthetic SoundSpaces dataset to augment our experiments.\n50. To evaluate our method on SoundSpaces dataset, we modify AV-NeRF to estimate impulse responses instead of the acoustic mask while keeping all other components intact.\n51. We follow NAF[12]selecting six representative indoor scenes, consisting of two single rooms with rectangular walls, two single rooms with non-rectangular walls, and two multi-room layouts.\n52. In each scene, SoundSpaces dataset provides an extensive collection of impulse response signals for sound source and sound receiver pairs, which are densely sampled from a 2D room grid.\n53. Each pair includes four discrete head orientations (0 • , 90 • , 180 • , and 270 • ), and each orientation is associated with two-channel binaural RIRs.\n54. We render RGB and depth images for each sound receiver pose using Habitat-Sim simulator[48,49].\n55. We maintain the same training/test split as NAF, allocating 90% data for training and 10% data for testing.\n56. Comparison with State-of-the-art.\n57. We compare AV-NeRF with the following baselines: (1) Mono-Mono duplicates the source audio a s twice to generate a fake binaural audio without modifying the source audio; (2) Mono-Energy assumes that the average energy of the target audio a t is known, scales the energy of the input audio to match the target, and duplicates the scaled audio to generate a stereo audio; (3) Stereo-Energy assumes that the energy of the two channels of the target audio a t is known, separately scales the energy of the input audio to match the target, and combines the two scaled channels to generate a stereo audio; (4) IRNAS[13]learns representing audio scenes by disentangling scene\'s geometry features with implicit neural fields, and we adapt INRAS to predict wave masks on RWAVS dataset; (5) NAF[12]designs local feature grids and an implicit decoder to capture the sound propagation in a physical scene, and we modify NAF to predict magnitude masks on RWAVS dataset; (6) ViGAS[15]achieves novel-view acoustic synthesis by analyzing audio-visual cues from source viewpoints.\n58. We select magnitude distance (MAG)[29], which measures the audio quality in the time-frequency domain, and envelope distance (ENV)[30], which measures the audio quality in the time domain, to evaluate various methods.\n59. Please refer to the supplementary material for implementation details.\n60. AV-NeRF outperforms all baselines across different environments, including office, house, apartment, and outdoors, by a significant margin (Table1(2) adding visual information to the input of A-NeRF is the most effective multimodal fusion method compared with concatenation and adding visual information to all layers of A-NeRF (Table2middle); (3) using embeddings represent relative angles outperforms applying positional encoding to either absolute or relative angles (Table2right).\n61. "Absolute Direction" represents applying positional encoding to the absolute angle, "Relative Direction" means transforming the relative angle with the positional encoding, and "Relative Embedding" is the embedding method.\n62. Visualization.\n63. We visualize the synthesized audio-visual scenes in Fig.6to intuitively assess the generation quality of our model.\n64. AV-NeRF can synthesize realistic binaural audios that have the same signal envelope and channel difference as the ground-truth audios.\n65. We compare AV-NeRF with traditional audio coding methods[50,51]and advanced learningbased neural field methods[12,13]using T60, C50, and EDT metrics[13].\n66. Please refer to our supplementary material for implementation details.\n67. Table3shows that AV-NeRF outruns both traditional and advanced methods, achieving 21% relative improvement on T60 metric compared with the previous state-of-the-art method INRAS, 5% on C50, and 16% on EDT.\n\n请严格按照JSON格式返回：{"labels": [0,1,0,...]}，不要包含任何其他文字。'}], 'model': 'qwen-plus', 'response_format': {'type': 'json_object'}}}
2025-10-14 21:46:47,873 - DEBUG - Sending HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
2025-10-14 21:46:47,873 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-10-14 21:46:47,873 - DEBUG - send_request_headers.complete
2025-10-14 21:46:47,873 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-10-14 21:46:47,873 - DEBUG - send_request_body.complete
2025-10-14 21:46:47,873 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-10-14 21:46:51,684 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'vary', b'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding'), (b'x-request-id', b'f9fb3e58-86c9-49ca-b2ba-2d98b87b1ec7'), (b'x-dashscope-call-gateway', b'true'), (b'content-type', b'application/json'), (b'req-cost-time', b'3778'), (b'req-arrive-time', b'1760449599008'), (b'resp-start-time', b'1760449602787'), (b'x-envoy-upstream-service-time', b'3776'), (b'content-encoding', b'gzip'), (b'date', b'Tue, 14 Oct 2025 13:46:42 GMT'), (b'server', b'istio-envoy'), (b'transfer-encoding', b'chunked')])
2025-10-14 21:46:51,685 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-14 21:46:51,685 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-10-14 21:46:51,685 - DEBUG - receive_response_body.complete
2025-10-14 21:46:51,685 - DEBUG - response_closed.started
2025-10-14 21:46:51,686 - DEBUG - response_closed.complete
2025-10-14 21:46:51,686 - DEBUG - HTTP Response: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "200 OK" Headers({'vary': 'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding', 'x-request-id': 'f9fb3e58-86c9-49ca-b2ba-2d98b87b1ec7', 'x-dashscope-call-gateway': 'true', 'content-type': 'application/json', 'req-cost-time': '3778', 'req-arrive-time': '1760449599008', 'resp-start-time': '1760449602787', 'x-envoy-upstream-service-time': '3776', 'content-encoding': 'gzip', 'date': 'Tue, 14 Oct 2025 13:46:42 GMT', 'server': 'istio-envoy', 'transfer-encoding': 'chunked'})
2025-10-14 21:46:51,686 - DEBUG - request_id: f9fb3e58-86c9-49ca-b2ba-2d98b87b1ec7
2025-10-14 21:46:51,687 - DEBUG - API request completed in 3.82 seconds
2025-10-14 21:46:51,687 - DEBUG - Raw model response: {"labels": [0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}
2025-10-14 21:46:51,687 - INFO - Successfully processed 66 labels
2025-10-14 21:46:51,687 - ERROR - Label count mismatch for AV_NeRF_ Learning Neural Fields for Real_World Audio_Visual Scene Synthesis
2025-10-14 21:46:51,687 - INFO - Evaluating paper 7/18: BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment
2025-10-14 21:46:51,688 - INFO - Starting model prediction
2025-10-14 21:46:51,688 - INFO - Attempt 1 of 5
2025-10-14 21:46:51,689 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-fd27ea71-7f47-4375-8d82-cb25188dab25', 'json_data': {'messages': [{'role': 'system', 'content': '你是一个专门用于识别学术论文中数据集描述的助手。你需要判断每个输入的句子是否描述了研究中使用的数据集。请确保：\n    1. 只输出JSON格式的结果\n    2. 结果格式必须为 {"labels": [0,1,0,...]}\n    3. 不要输出任何其他文字或解释'}, {'role': 'user', 'content': '你是一位专业的学术论文数据集描述识别专家。请仔细分析下面这篇论文中的每个句子，判断它们是否描述了本研究特定构建的数据集信息。\n\n    论文标题: BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment\n\n    判断标准:\n    判断为1(数据集描述)的标准：\n    句子必须直接描述本研究特定构建的数据集,包括:\n    1. 该数据集的构建过程和方法\n    2. 该数据集的具体构成和规模\n    3. 该数据的来源和收集方式\n    4. 该数据的预处理步骤\n    5. 该数据集的可获取方式(如发布地址)\n    6. 或者其他直接或者间接描述了该数据集的句子\n\n    所有其他类型的句子均标记为0。\n    \n    请对每个句子进行分析，返回一个JSON格式的标签数组。格式要求：\n    1. 必须是有效的JSON格式\n    2. 只包含labels字段，值为0和1组成的数组\n    3. 数组长度必须与句子数量相同\n    4. 示例格式：{"labels": [0,1,0,1,...]}\n\n    需要判断的句子：\n\n    1. Abstract: Banding, also known as staircase-like contours, frequently occurs in flat areas of images/videos processed by the compression or quantization algorithms.\n2. As undesirable artifacts, banding destroys the original image structure, thus inevitably degrading users\' quality of experience (QoE).\n3. In this paper, we systematically investigate the banding image quality assessment (IQA) problem, aiming to detect the image banding artifacts and evaluate their perceptual visual quality.\n4. Considering that the existing image banding databases only contain limited content sources and banding generation methods, and lack perceptual quality labels (i.e.mean opinion scores), we first build the largest banding IQA database so far, named Banding Artifact Noticeable Database (BAND-2k), which consists of 2,000 banding images generated by 15 compression and quantization schemes.\n5. A total of 23 workers participated in the subjective IQA experiment, yielding over 214,000 patch-level banding class labels and 44,371 reliable image-level quality rating scores.\n6. Subsequently, we develop an effective no-reference (NR) banding evaluator for banding detection and quality assessment by leveraging frequency characteristics of banding artifacts.\n7. To be more specific, a dual convolutional neural network (CNN) is employed to concurrently learn the feature representation from the high-frequency and low-frequency maps, thereby enhancing the ability to discern banding artifacts.\n8. The quality score of a banding image is generated by pooling the banding detection maps masked by the spatial frequency filters.\n9. The experimental results demonstrate that our banding evaluator achieves a remarkably high accuracy in banding detection and also exhibits high SRCC and PLCC results with the perceptual quality labels, even without directly learning a regression model for banding quality evaluation.\n10. These findings unveil the strong correlations between the intensity of banding artifacts and the perceptual visual quality, thus validating the necessity of banding quality assessment.\n11. The BAND-2k database and the proposed banding evaluator will be available at https://github.com/zijianchen98/BAND-2k.\n12. R ECENT years have witnessed a rapid emergence of media streaming services and social platforms.\n13. YouTube, Netflix, and TikTok account for more than half of the world\'s video traffic.\n14. Improving the quality of images under limited encoding, Zijian Chen, Wei Sun, Jun Jia, Zicheng Zhang, Xiongkuo Min, and Guangtao Zhai are with the Institute of Image Communication and Information Processing, Shanghai Jiao Tong University, Shanghai 200240, China (e-mail: {zijian.chen,sunguwei, jiajun0302, zzc1998, minxiongkuo, zhaiguangtao}@sjtu.edu.cn).\n15. Fangfang Lu is with the College of Computer Science and Technology, Shanghai University of Electric Power, Shanghai 200290, China (email:lufangfang@shiep.edu.cn).\n16. Jing Liu is with the School of Electrical and Information Engineering, Tianjin University, Tianjin, China.(e-mail:jliu tju@tju.edu.cn).\n17. transmission bandwidth, and storage condition is a necessary prerequisite for meeting the quality of experience (QoE) of users.\n18. In the stages between image acquisition and display, an image may suffer from various types of degradation, while banding artifacts are a kind of false contour distortion that is quite perceptible to the human eye.\n19. Since the visual quality of image contents greatly affects the QoE of end-users, it is highly desirable to design an effective banding image quality assessment (IQA) method, which aims to automatically detect the traces of such false contours and predict the objective quality of banding images that can be used to develop pre-processing or postprocessing debanding algorithms and optimize the performance of streaming media application.\n20. Normally, banding artifacts take on the appearance of annual rings, radiation circles, halos, or geographical contour lines and especially exist in the background regions (e.g., sky, water, and wall surface), where the color transition is not smooth enough.\n21. Nearly all existing image or video encoders, including H.264/AVC[28], VP9[38], and H.265/HEVC[27]can introduce such artifacts more or less.\n22. Current banding IQA research can be divided into two categories: subjective quality assessment and objective quality assessment.\n23. The existing subjective banding IQA research[39],[41]-[43]mainly investigates the limited banding scenarios with internal-used and undisclosed databases while lacking the quality label and may be insufficiently generalizable to large-scale commercial applications.\n24. Meanwhile, general IQA methods aiming at common distortions are inapplicable for banding exacerbated images due to the essential differences between them.\n25. First, banding artifacts usually hold tiny, staircaselike, and regional structures, which can be regarded as a kind of high-frequency artifact in smooth areas, while general distortions occur obviously in the whole image and are globally uniform.\n26. Second, the perceptual severity of banding is quantified based on its fraction of coverage and intensity in an image, which is widely divergent from the design philosophy of many existing IQA approaches.\n27. As a result, it is challenging to design an effective banding IQA method.\n28. To address these limitations, we first conduct a comprehensive subjective study of banding exacerbated images and create the largest banding IQA database to date with reliable mean opinion scores (MOS) and patch-level banding labels.\n29. We also propose a novel no-reference banding evaluator for banding detection and quality assessment by leveraging the frequency characteristics of banding artifacts.\n30. First, due to the dissimilar peculiarities of banding and smooth regions, the same distortion in different regions, e.g., textual and pictorial regions, may lead to different visual perception of human beings.\n31. Considering that banding manifests as a high-frequency artifact that exists in the lowfrequency smooth region, we propose a dual-branch CNN, which takes the high-frequency map and low-frequency map as inputs simultaneously, to hierarchically incorporate different visual features from the first layer and the last layer of our Resnet-50 backbone, thus making the model learn more effective banding feature representation and achieving more accurate banding region discrimination.\n32. Furthermore, inspired by the previous studies[15],[16], spatial frequency extracts information consistent with the human visual system (HVS), which not only reflects the overall active level in an image but also intuitively quantifies the contrast information.\n33. In other words, the value of spatial frequency is large in smooth areas, while becoming small in areas with harsh contrast changes, i.e., banding areas.\n34. Based on this mechanism, we adopt a spatial frequency masking strategy to refine the detected banding map and then pool the masked banding detection map to obtain the image-level banding quality score.\n35. • We propose a novel no-reference banding evaluator for banding detection and quality assessment based on frequency characteristics of banding artifacts, which utilizes a dual-branch CNN model to extract hierarchical banding-related feature representation from the high-frequency maps and low-frequency maps simultaneously.\n36. • Experimental results show that the proposed banding evaluator achieves the best performance in banding detection and significantly surpasses baselines in terms of SRCC and PLCC in the banding IQA task, which demonstrates the effectiveness of the proposed model.\n37. The remainder of this paper is organized as follows.\n38. Section II provides an overview of related works, including the stateof-the-art banding databases and detection methods.\n39. Section III introduces the construction of the BAND-2k database and the subjective assessment study.\n40. Section IV proposes an effective no-reference banding evaluator for banding detection and quality assessment.\n41. Section V gives the experimental results and analysis.\n42. Section VI concludes this paper.\n43. In this section, we first provide an overview of the state-ofthe-art banding-related databases (TableI) and then review the banding detection and evaluation methods.\n44. The first banding artifact-relevant VQA database was proposed by Wang et al.[39], which consists of 21 stimuli with different quantization grades generated by VP9[38]from 7 clips of 1280 × 720 30fps video.\n45. Authors in[41]investigated the effect of encoding parameters and dithering on the visibility of banding.\n46. Nine 4k-10bit source clips from the existing Netflix catalogue between 1 and 5 seconds were used to generate banding distorted videos.\n47. Each source content was downsampled to appropriate resolutions (1080p, 2k, or 4k) with certain bit-depth and further compressed by libaom (an AV1 codec library) at QPs {12, 20, 32}.\n48. More recently, Kapoor et al.[43]constructed one of the first databases for data-driven image banding assessment models.\n49. This research included about 1,440 images shot from over 600 pristine HD videos with a resolution of 1920 × 1080.\n50. Six levels of bit-depth quantization in luminance and chrominance channels are introduced to obtain different intensities of banding.\n51. Meanwhile, the banding images were semi-automatically segmented and labeled into banded and non-banded to form a patch-level banding dataset, which allows for training machine learning-based and deep learning-based banding classification methods.\n52. However, to the best of our knowledge, thus far there still lacking a benchmarking dataset in the banding detection and the corresponding banding IQA domain.\n53. Researchers either resort to image/video quality datasets that do not aim at banding distortion or build a small, attribute-restricted, in-house dataset by themselves.\n54. This motivates us to construct a large-scale subjective assessment database focus on the perceived bandingaffected image quality.\n55. Early research on banding detection mainly focuses on false contour identification, which aims to find the wrong boundary rather than a "true" region edge in the image.\n56. Authors in[31]-[33]utilized monotonicity or non-monotonicity features of local support regions including the gradient, contrast, variance, and entropy information to measure the loss of low-amplitude detail caused by banding.\n57. However, these works ignored the perceptual characteristics of the human visual system (HVS) and thus did not perform a good correlation with subjective tests.\n58. Another banding detection strategy is conducted at the pixel-level estimation and segmentation.\n59. Bhagavathy et al.[30]proposed to identify banding artifacts by calculating the likelihood of pixel difference.\n60. Baugh et al.[34]measured the severity of banding based on the number of a group of connected pixels with the same color.\n61. Wang et al.[39]first detected uniform segments to find possible banding areas and further incorporated edge features (e.g.length and contrast) to capture false boundaries.\n62. Nevertheless, these kinds of methods are typically sensitive to edge noise and are computationally expensive, causing limited application in realtime scenarios.\n63. Towards addressing these problems, Tu et al.[42]presented a completely no-reference banding detection method, which combines various properties of HVS with a number of preprocessing steps to refine banding edge detection.\n64. Instead of regarding banding detection as a false edge detection problem, Tandon et al.[41]heuristically utilized the effect of contrast sensitivity function (CSF) on banding visibility and its dependence on spatial frequency.\n65. Based on this, Krasula et al.[40]further compared the banding annoyance with more commonly studied compression artifacts and proposed a banding-aware video quality metric.\n66. In recent years, deep learning approaches have prevailed in various VQA tasks.\n67. As the pioneering work, Kapoor et al.[43]developed an automated CNN-based banding detector for the first time, which is a simple two-stage algorithm and gives rise to devising other learning-based techniques.\n68. In this work, we build a large-scale banding database and propose a data-driven banding indicator that can generate pixelwise banding visibility maps with corresponding subjectively  consistent quality scores by combining human visual mechanisms and deep learning techniques.\n69. Subjective banding image quality assessment facilitates the development of automatic objective banding image and video quality models.\n70. We created the largest banding database in existence, denoted as the BAND-2k database, which consists of 2,000 banding distorted images and over 214,000 patch-level banding class labels.\n71. Then, a subjective experiment was conducted to obtain the mean opinion scores (MOS) of the BAND-2k database.\n72. The workflow of the banding database construction is shown in Fig.1.\n73. To build a content-rich and balanced database, we manually collected source videos including computer graphics (CG), usergenerated content (UGC), and professionally-generated content (PGC) from two popular media websites Bilibili.comand Youtube.com.\n74. Then, 885 clips with multiple spatial resolutions (i.e., 4096×2160, 3840×2160, 1920×1080) and frame rates (i.e., 60, 50, 30, 25) are chosen as candidate.\n75. Note that videos on the mentioned websites are firstly annotated by the community with assigned a number of favorites, views, and downloads.\n76. These statistics correlate with the content and quality of a video, which guides our choices to some extent.\n77. All videos selected on the website are released under an appropriate creative commons (CC) license that allows further editing and redistribution.\n78. After content selection, we further unified the format of all video clips, especially the spatial resolution and the pixel format, which avoid the effect of other facts on visual quality.\n79. Concretely, we first converted the frame rate of the original clips to 25fps, which is to reduce the storage pressure while ensuring the graphics quality.\n80. Considering the commonly used aspect ratio of the user interface and displays is 16:9, we cropped the partially unqualified videos rather than shrinking images unevenly.\n81. Then, we downsampled the trimmed spatial resolution 3840×2160 to a lower resolution -1920×1080 for the following subjective study.\n82. Before conducting the following experiments, we manually removed the videos that are either too dark or bright, overly blurry or colorful, which helps to obtain more reliable subjective assessments.\n83. To avoid redundancy and to make sure the diversity of selected contents, we also conducted attribute analysis studies on the selected images.\n84. Four metrics that correlated with human perception, i.e., contrast, brightness, sharpness, and colorfulness, are adopted as content diversity metrics.\n85. All video attributes are calculated on every 10 frames to reduce computational complexity, which are then averaged over frames of each video sequence.\n86. Fig.2shows the distribution of attributes extracted from the selected videos.\n87. • Contrast: The contrast metric is simply defined as the standard deviation of pixel gray-scale intensities[25].\n88. • Colorfulness: The colorfulness metric is measured by the R, G, and B components[25].\n89. We first compute two matrices of differences between channels rg = R -G and yb = 1 2 (R + G) -B.\n90. Then, the colorfulness metric can be calculated as µ 2 rg + µ 2 yb + σ 2 rg + σ 2 yb , where µ and σ are the mean and standard deviation of their corresponding terms, respectively.\n91. • Sharpness: The cumulative probability of blur detection (CPBD) metric[24]is used to measure the image sharpness, which estimates the probability of detecting blur at each edge.\n92. • Brightness: The brightness of an image is obtained directly from the pixel gray intensities in R, G, and B channels.\n93. Finally, the number of source videos are reduced to 873.\n94. Fig.3displays thumbnails for 30 selected representative video clips.\n95. To simulate authentic banding artifacts that exist in real viewing scenarios, we introduced four encoding/transcoding strategies including H.264/AVC[28], H.265/HEVC[27], VP9[38], and bit-depth manipulation[43]with fifteen quantization schemes in total.For H.264/AVC and H.265/HEVC, considering the range of their quantization parameter (QP in ffmpeg), we selected three typical QP values, namely, QP = {18, 33, 43}.\n96. This is because that coded video clips with a QP value smaller than 18 normally provide perceptual lossless quality, while coded video clips with a QP value larger than 43 will not be able to offer adequate quality, which may affect the subjective test of banding artifacts.\n97. Similarly, for the VP9 encoder, the QP values under our close inspection are chosen as {28, 45, 60}.\n98. Moreover, we applied the same quantization strategies as in[43], which introduces banding distortion by scaling bit-depth in luminance and chrominance channels.\n99. Here, the color coding scheme YCbCr4:2:0 is applied for maximum compatibility.\n100. To sum up, we generated fifteen levels of banding with different intensities and shapes to enhance the diversity of the database.\n101. The banding database is then built by extracting frames from the distorted video clips, resulting in 2,000 images with a resolution of 1920×1080.\n102. Fig.4shows the visualization results of banding exacerbated images.\n103. In the process of image patch labeling, we initially intend to extract image patches of size 235×235 from banding images directly and perform annotation operations.\n104. However, considering the theoretical number of patches, it will cost a lot of manpower and time to label patch by patch.\n105. Therefore, we first segmented the banding images roughly and label them into banded and non-banded regions.\n106. Then, labelled image patches are generated from these segmented and labelled images by a sliding window.\n107. Specifically, we followed the same demarcation of the banded and non-banded images in[43]that a patch is labelled as banded if it has more than 30% overlap with banded regions in the image.\n108. Eventually, a banding dataset containing 2,000 distorted images with 1920×1080 resolution and 214,324 labelled image patches of size 235×235 is built.\n109. To the best of our knowledge, it is the largest banding dataset in existence, which enables training various machine/deep learning based banding detection models and facilitates the development of image/video debanding techniques.\n110. TableIIreports the composition of labelled image patch dataset.\n111. It can be observed that the number of banded patches is a bit smaller than non-banded patches due to the fact that banding usually appears in smooth background areas.\n112. As shown in Fig.1, the subjective quality study contains four steps.\n113. In addition to preparing the experimental environment, subjects should pass the qualification test first to participate in the study.\n114. After the subjective rating, all resulting scores need to be analyzed and examined before generating the final mean opinion score (MOS).\n115. 1) Experimental Environment Setting: In this study, a total of 25 inexperienced subjects are gathered in a laboratory environment, where relevant experimental configuration must satisfy the following requirements: • Considering the viewing effect, desktops and laptops are allowed as displays.\n116. • The resolution of displays must be larger than or equal to 1920×1080 to show the images without spatial downsampling.\n117. • The viewing distance and optimal horizontal viewing angle are set as 1.9 times the height of the display and respectively.\n118. 500 recommendation[26].\n119. As a result, we used a 27-inch AOC Q27U2D monitor with a resolution of 2560×1440 for assessment with 25 subjects.\n120. Due to the large number of images to be assessed, we divided the dataset into ten sessions to avoid visual fatigue.\n121. Each session of tests took nearly 2 hours with a 30-minute break for each participant.\n122. 2) Qualification Test: Before starting the main experiment, subjects are required to pass a quiz to get the qualification of conducting follow-up experiments.\n123. Firstly, we manually selected 10 labeled banding exacerbated images beyond the database as training images to familiarize subjects with the operation interface and the goal of this subjective test.\n124. The quiz consists of two parts including banding classification and image-level quality rating.\n125. In banding classification, subjects were told to divide the test image into banded or non-banded.\n126. In image-level quality rating, subjects were instructed to focus on the coverage and intensity of banding areas, as well as the overall quality of images to get the final quality score.\n127. The rating scale is continuous from 0 to 100 while a higher value indicates more severe banding (i.e., more visible or occupying larger portion of the image).\n128. To make the quiz objective and fair, we take the banding images labeled by domain experts as the ground truth, while two existing banding metrics BBAND[42]and DBI[43]are used to determine the normal range of the rating scores.\n129. That is the quality scores rated by subjects should not exceed 20% of the above banding metrics.\n130. As a result, only 23 subjects with an accuracy above 80% in banding classification and subjective scores in the normal range were allowed to pass the quiz.\n131. Note that the steps for taking a quiz are the same as the main experiment, which aims to guarantee the consistency of results.\n132. 3) Formal Study: We adopted the single-stimulus (SS) method in this test.\n133. Ten golden images that have the acknowledged high quality or poor quality (assessed by BBAND[42]and DBI[43]) were added to each session for controlling the scoring deviations.\n134. Besides, 3 repeated images are randomly inserted into each session to ensure consistency of scores before and after subjects scoring.\n135. At last, 23 qualified subjects were asked to provide their opinions on the shuffled image groups.\n136. The resulting scores were collected and packed for further analysis.\n137. 4) Result Analysis: In total, 46,000 scores were collected by 23 qualified subjects in the main study phase.\n138. However, considering the qualification quiz cannot completely disallow those unreliable workers to muddle through to the main study while reliable subjects may also occasionally score odd values, which may be caused by the inter-individual differences in perceiving the quality of the unique characteristics of different contents.\n139. Thus, we further investigated the confidence of rating scores and removed outliers following the Grubbs\' test[23],[45].\n140. Concretely, let s = (s 1 , s 2 , . . ., s N ) denote a set of raw scores collected for one distorted image.\n141. The test statistics is defined as the maximum absolute deviation of a sample standard deviation (SD) unit from the sample mean, which is mathematically expressed as where s and σ s denote the mean and standard deviation of the sample, respectively.\n142. Then, given a significant level α, a sample is detected as an outlier if where t 2 α/(2N ),N -2 represents the upper critical value of the tdistribution with N -2 degrees of freedom and a significance level of α/(2N ).\n143. Empirically, we set the significance level α at 0.05.\n144. Then, a sample is identified as an outlier if its distance to the sample mean is larger than 2.5 times SD and is removed.\n145. Following the aforementioned steps, the total number of scores Fig.6.\n146. The overall architecture of the proposed method.\n147. Given a banding distorted image, it is first divided into patches.\n148. Then, the patch-level high-frequency map (HFM) and low-frequency map (LFM) are generated by Sobel operation and piece-wise smooth algorithm[19], respectively.\n149. After that, a dual-branch CNN (CNN H and CNN L ) is deployed to extract hierarchical features with different visual information and thus classify the patches into banded or non-banded.\n150. Lastly, a spatial frequency masking strategy is introduced to refine the banding map and calculate the image-level banding quality score.\n151. Note that the dual-branch networks do not share parameters.\n152. was reduced to 44,371, and MOS was created by averaging the scores for each image.\n153. Fig.5presents the histogram of MOSs over the entire database, showing a broad MOS distribution of banding images.\n154. In this section, we describe the architecture of the proposed banding evaluator in detail, as shown in Fig.6.\n155. As stated before, banding usually appear as high-frequency information in the smooth background, while humans perceive high-frequency texture regions and low-frequency plateau regions through different neural channels concurrently, and transfer the upper visual features into the cerebral cortex for final processing[17],[44].\n156. Inspired by this, we employ high-frequency maps (HFM) and low-frequency maps (LFM) as the deep learning network inputs, which represent the texture and structural information of the image respectively, to mimic the recognition mechanism of the human brain for better banding identification.\n157. High-frequency Maps.\n158. Since gradient has been widely used to represent edge information and has been confirmed beneficial to acquire high-frequency components with low computational cost[20]-[22], we apply the isotropic Sobel operator to each patch for enhancing the details of banding artifacts.\n159. Given an input patch I, the high-frequency map is calculated by where S x and S y are the horizontal and vertical isotropic Sobel operators, respectively." * " denotes the convolution operation.\n160. Low-frequency Maps.\n161. To maintain the principal content of the image and filter out the influence of high-frequency information, we use the piece-wise smooth algorithm[19]to generate the low-frequency map by minimizing a function for image approximation recovery: where L represents the low-frequency map, Ω and E denotes the image domain and edge set, respectively.\n162. P indicates the pixel and E dσ represents the total edge length.\n163. The coefficients α and β are positive regularization constants.\n164. An example of frequency maps is shown in Fig.7.\n165. To obtain the overall banding score, we first divide the banding image into 235×235 patches and apply CNN-based classification to each patch, resulting in a banding classification label for each patch, i.e., banded or non-banded.\n166. As shown in Fig.6, the proposed network consists of two parallel branches, namely CNN H and CNN L , which take the patch-level high-frequency map and low-frequency map as input, respectively.\n167. For each branch, we propose to use Resnet-50[18]as the backbone.\n168. Specifically, we incorporate the feature maps extracted from the first convolutional layer and the last layer of Resnet-50 as hierarchical visual features, which represent different visual information[46],[47]and can be used as predictive information to enhance the discrimination ability of the network for banding and non-banded regions.\n169. Afterward, the features extracted from two branches are concatenated first and reshaped into 128dimensional vectors through two fully-connected layers, which is further followed with the sigmoid activation function to output the final predicted label, namely banded or non-banded.\n170. Of note is that sharing parameters is extremely unfavorable for extracting low-and high-frequency features simultaneously, we thereby deploy two branches that work independently and do not share parameters.\n171. The loss function adopted here is binary cross entropy.\n172. With the trained CNN H and CNN L , each input patch is predicted to obtain a banding or non-banded label.\n173. To better guide the pre-processing and post-processing debanding algorithms, it is necessary to generate a quality score for the entire banding image.\n174. Since the visibility of edge is also affected by content, we further consider the effect of spatially varying content information on the local quality of human perception.\n175. As a consequence, we introduce the spatial frequency masking strategy to determine the weighting factor for the detected banding regions in each patch adaptively and thus obtain the image-level banding severity score while refining the visibility of banding artifacts.\n176. 1) Spatial Frequency Masking: The spatial frequency is defined as the activity level of an image, which establishes a filterbank based on the visual stimulus and is in accordance with HVS[16].\n177. In this paper, we propose to apply spatial frequency as an effective contrast criterion to banding measurement.\n178. Specifically, given an image of size I W × I H , divided into N × N patches, where I W and I H denote the number of columns and rows respectively.\n179. The column (CF k ) and row (RF k ) frequencies of the image patches are given by where I(x, y) is the pixel value of the image patch.\n180. Then, the resulting spatial frequency of an N × N patch is computed as where k is the number of patches (1 ≤ k ≤ I W I H N 2 ).\n181. Since most banding regions are likely to have large contrast including edges and textures, which should be assigned greater weights than the smooth and blurred areas.\n182. Accordingly, we design a banding visibility transfer function to express spatial frequency masking as a function of the local textural feature.\n183. The final spatial frequency masking weight is calculated at each patch as where γ is the scaling constant factor chosen to tune the shape of the transfer function.\n184. We used γ = 1.5 in our implementation.\n185. 2) Building a Banding Metric: The visibility of banding artifacts depends on the combination of multiple visual mechanisms.\n186. In this paper, we propose a simple but effective product model for attribute integration at each predicted banding patch to obtain the entire banding map (BM): where P k denotes the predicted label of k-th patch and w k is the weight parameter that scales the visibility of measured contours, i.e., gradient magnitude of the high-frequency map, |HFM k (i, j) | at region (i, j).\n187. Furthermore, inspired by previous psychovisual findings that the QoE of observers is dominated by those regions having poor quality[14],[42], we thereby leverage the worst p% percentile visual pooling to calculate an average banding score from the generated BM, where p is set to 80 in this experiment.\n188. As a result, the perceptual score of the overall banding image is defined as where M is the total number of patches in image I.\n189. T p% denotes the index set of the top p% non-zero pixel-wise value contained in k-th patch of the BM.\n190. In this section, we first present the experimental protocol in detail and then evaluate the performance of the proposed method on two tasks, namely patch-level banding classification and banding image quality assessment.\n191. After that, the ablation study and cross-database validation are conducted to prove the robustness and effectiveness of the proposed method.\n192. Finally, we test the computational efficiency of our method.\n193. A. Experimental Protocol 1) Databases and Settings: We choose two databases to train and test the effectiveness of the proposed banding IQA method, which are the database released in[43]and our proposed BAND-2k database.\n194. The detail information of these two datasets can be found in TableI.\n195. The proposed model is implemented by PyTorch[13].\n196. Before training, we randomly split the training, validation, and testing set into 8:1:1 (as shown in TableII).\n197. We use the Adam optimizer with the initial learning rate set as 1e-4 and set the batch size as 32.\n198. The training process is stopped after 25 epochs.\n199. The resolution of each cropped patch is fixed to 235×235.\n200. All experiments on both the[43]database and the BAND-2k database are conducted repeatedly 10 times to obtain the mean performance.\n201. 2) Baseline Algorithms: We include a number of representative IQA algorithms in our evaluation as references to be compared against.\n202. These baseline methods include: • General FR IQA methods: We choose PSNR, SSIM[12], MS-SSIM[11], LPIPS[10]as baselines.\n203. These are the most commonly used FR IQA metrics in practical applications such as video coding, image enhancement, etc. • General NR IQA methods: BRISQUE[37], NIQE[36], NIMA[8], DBCNN[35], HyberIQA[9], and StairIQA[1].\n204. • Banding IQA methods: Considering that there exists few research on banding detection and quality assessment, we barely select the BBAND[42], CAMBI[41], VMAF BA[40]and DBI[43]metrics as comparisons.\n205. 3) Evaluation Criteria: To evaluate the IQA methods comprehensively, a total of seven evaluation indexes in two categories are adopted.\n206. For patch-level banding classification, we follow the common procedures as in[43]and utilize the area under the receiver operating characteristics (AUROC), the area under the precision-recall curve (AUPRC), and accuracy as the classification performance metrics.\n207. For banding image quality assessment, four mainstream metrics are selected as the evaluation criteria: Spearman rank-order correlation coefficient (SRCC) and Kendall rank-order correlation coefficient (KRCC) measure the prediction monotonicity, while Pearson linear correlation coefficient (PLCC) and root mean square error (RMSE) are calculated to assess prediction consistency.\n208. Considering the potential nonlinear mapping characteristics between the objective scores and the subjective scores, we perform score alignment by mapping the predicted value using the five-parameter logistic function before calculating PLCC and RMSE values[7].\n209. Since our goal is to develop an effective banding IQA approach, we regard the identification of banding patches as an important preceding process to achieve accurate banding quality prediction.\n210. However, there exist few methods that are designed for banding classification and nearly all IQA methods produce scalar values only while failing in classifying banding regions directly.\n211. Therefore, we adopt a thresholding step to convert the single quality value into binary classification results as[43]does.\n212. Concretely, a half-interval search algorithm[48]is employed to find the optimal threshold value that can generate the best classification result.\n213. Based on the above premise, TableIIIreports the experimental results on both the database from[43]and the BAND-2k database.\n214. We highlight the best results in boldface.\n215. As compared to other state-of-the-art IQA methods, our proposed method yields the best overall performance in terms of AUROC, AUPRC, and accuracy.\n216. It is shown that most general FR IQA and NR IQA models perform poorly on the patch-level banding classification task while performing fairly well on other IQA tasks[1],[9],[35], indicating that the current approaches are not sensitive to banding distortion.\n217. Benefiting from the powerful feature extraction ability of CNNs, our proposed method and the customized NR IQA models for banding artifacts detection (DBI[43]) reach a significant performance in the discrimination of false contours.\n218. However, the performance of banding IQA method BBAND, CAMBI, and VMAF BA is surprisingly poor compared with other methods, which shows their vulnerability in identifying local banding artifacts from texture regions and are not suitable for patch-level banding identification.\n219. In addition, we investigate the computational complexity in terms of execution time per image patch.\n220. It can be observed that except for those traditional FR IQA models, our method achieves comparable speed in patch-level banding classification, which determines the prediction efficiency of the subsequent image-level quality assessment, making it a favorable choice in time-constrained scenarios.\n221. Considering that there exist no image banding databases with subjective scores attached in the public domain, we merely compare the performance of the proposed method with the baseline approaches on the proposed BAND-2k database.\n222. The results are shown in TableIV, from which we can observe that our proposed method largely surpasses all baselines in terms of SRCC, KRCC, and PLCC except for the RMSE.\n223. Compared to the secondbest model, our method achieves 18.07%SRCC improvements, 24.91% KRCC improvements, and 18.01%PLCC improvements on the BAND-2k database.\n224. We also present scatter plots of predictions versus MOS for better visualization in Fig.8.\n225. Overall,  (a) PSNR, (b) SSIM[12], (c) MS-SSIM[11], (d) LPIPS[10], (e) VMAF BA[40], (f) BRISQUE[37], (g) NIQE[36], (h) NIMA[8], (i) DBCNN[35], (j) HyperIQA[9], (k) StairIQA[1], (l) BBAND[42], (m) CAMBI[41], (n) DBI[43], and (o) Ours.\n226. Fig.9. Visual comparisons of the banding map results.\n227. From top to bottom are banding images and their corresponding banding maps generated by BBAND[42], DBI[43], and our proposed method, respectively.\n228. The first five columns of images from left to right are from BAND-2k, while the rest images are from[43].\n229. the performance of traditional FR IQA models such as PSNR, SSIM, and MS-SSIM is remarkably inferior in banding images assessment and is uncorrelated with the MOS, which is consistent with the numerical results presented in TableIV.\n230. The reason is that PSNR and SSIM-based methods do perform not well on tiny, regional, and content-independent banding distortion while lacking the consideration for the mechanism of HVS.\n231. It is also worth mentioning that the deep CNN architecturebased methods (DBCNN[35], HyperIQA[9], and StairIQA[1]), despite performing well on LIVEC[6], KonIQ-10k[29], and other universal image quality evaluation databases[4],[5], underperformed our proposed model by a notable margin on the BAND-2k database.\n232. (2) the perception of banding artifacts is explicitly intensity-aware.\n233. These are the issues that the CNN-based approaches above do not take into account.\n234. To some extent, banding distortion is more like a kind of local distortion than common global distortion such as Gaussian noise, blur, and dither.\n235. This suggests that it is potentially valuable to integrate some local texture, edge, contrast, or other visibility-related features into quality prediction models when assessing banding images.\n236. Fortunately, with the help of the proposed scoring strategy, our method gains the ability to convert the area range of identified banding regions to indicative annoying scores.\n237. Surprisingly, the banding IQA methods BBAND[42], CAMBI[41], and VMAF BA[40], however, did not perform very well on the BAND-2k dataset.\n238. We infer that this is due to differences in the test environment.\n239. First, the source videos selected in CAMBI and VMAF BA are from the internal Netflix 4K catalogue while the source contents in BAND-2k are collected from the public streaming websites with different resolutions, leading to an uneven quality level.\n240. Second, the methods of artificially introducing banding distortion are different.\n241. 264 compression techniques are used to generate the banding artifacts, limiting the adaptive ability of the algorithm in other scenarios.\n242. For further investigation, we compare our method with other two methods, i.e., BBAND[42], DBI[43], and visually study the generated banding maps to verify their effectiveness in banding detection.\n243. The visualization results are shown in Fig.9, where seven representative banding images are selected for reference.\n244. As shown, the banding maps generated from BBAND are mostly disordered and have a lot of discontinuity while the banding maps generated by DBI are too vague to locate the exact location of banding artifacts, making it difficult to develop pixel-level debanding techniques.\n245. Overall, we can compendiously conclude that: (1) in comparison with BBAND and DBI, banding maps computed by our proposed method could provide an accurate, clear indication for banding artifacts; (2) in comparison with BBAND and DBI, the quality prediction of banding images guided by our method could achieve a high consistency with HVS.\n246. Moreover, to make a statistically meaningful comparison among different IQA methods, we further conduct the widely used F-test[2],[3]to assess the statistical significance of the proposed method.\n247. Based on the assumption that the model\'s prediction residuals follow the Gaussian distribution, the lefttailed F-test with a confidence level of 95% is performed on the residuals of every two IQA models.\n248. The results of significance tests on the BAND-2k database are shown in Fig.10.\n249. A value of \'1\' (colored in green) indicates that the model in the row is significantly better than the model in the column, while a value of \'0\' (colored in red) indicates that the model in the row is not significantly better than the model in the column.\n250. It is shown that our proposed method performs significantly better than other models, which is consistent with the observations from the above comparison experiments.\n251. In this section, we explore the effectiveness of our model\'s design philosophy.\n252. To verify the importance of the dual-branch (DB) architecture, the baseline variants use the same backbone as the proposed method, except that only a single branch (SB) is reserved.\n253. Then, we use the original banding images as the input while removing the frequency map generation module (SB-I).\n254. Besides, the high-frequency maps and low-frequency maps generation modules are retained respectively (SB-HFM and SB-LFM).\n255. It can be observed from Table V that SB-LFM achieved the worst results, since the low-frequency map filters out the highfrequency banding information to a certain extent, which reduces the ability of the model to identify false contours.\n256. SB-I performs significantly better than SB-HFM and SB-LFM, resulting from that it contains richer image information, but it is still inferior to our method, which demonstrates the effectiveness of the dualbranch scheme.\n257. To investigate the effect of the banding feature extraction, we further design two variants with different input combinations of frequency maps.\n258. First, the high-frequency map was taken as the input of both the CN N H and CN N L layers (DB-HFM).\n259. Then, we replace the inputs with the low-frequency maps (DB-LFM).\n260. As shown in TableV, the performance of DB-HFM and DB-LFM is far apart from our approach, which matches our hypothesis that the high-frequency texture information contained in HFM and the low-frequency background information contained in LFM are crucial to enhance the capacity of discernment for banding artifacts.\n261. Therefore, we may conclude that our model is the most suitable model among those compared variants in terms of both banding classification and IQA applications.\n262. Due to the effects of different compression techniques, shooting equipment, scenes, etc., the image content and banding distortions may vary significantly in practical applications.\n263. For the database[43], it only includes limited types of image sources and means of triggering banding distortion.\n264. As a result, we conduct a cross-database validation to verify the generalizability of the proposed model, wherein the database presented by[43]and BAND-2k are included.\n265. That is, we trained the model on one full database and report the test performance on the other.\n266. We mainly compare the proposed method with four learningbased models, i.e., DBCNN[35], HyperIQA[9], StairIQA[1], and DBI[43].\n267. Since MOS information is not provided in the database[43], which is an essential part of methods training, we condensed part of the experiments.\n268. TableVIand VII report the experimental results in terms of patch-level banding classification and image quality assessment.\n269. We can observe that our proposed method generalization between database[43]and BAND-2k was surprisingly good.\n270. Besides, it is worth noting that the performance of these methods trained on the BAND-2k has improved a little compared to the previous versions that were trained on the database[43], which further demonstrates the superiority of the proposed database BAND-2k.\n271. The efficiency of an image quality prediction model is of great importance in practical industrial deployments.\n272. Therefore, we measured the average running time of the compared IQA models, as shown in TableVIII.\n273. The experiments were performed in MATLAB R2021a and Python 3.7 under Windows 10 64bit system on a Lenovo laptop with Intel Core i5-9300HF CPU@2.4GHz,16GB RAM, and NVIDIA GTX 1660Ti 6G GPU.\n274. It can be observed that the proposed method achieves a reasonable running time among the FR, NR, and other bandingspecified IQA algorithms.\n275. Generally, the execution time of classical IQA algorithms is significantly less than learning-based methods.\n276. Simpler NSS-based models such as BRISQUE and NIQE still show competitive efficiency relative to CNN models while exhibiting inferior performance in banding image quality assessment.\n277. For CAMBI and VMAF BA , we use the officially launched software package, which is based on the stand-alone C library libvmaf and therefore surpasses other methods in speed.\n278. Moreover, unlike the general quality evaluation using regression to predict scores, the patch-wise prediction strategy that we adopted may increase the complexity.\n279. Note that although we deployed a more complex network structure, a nearly 10 times speedup has been seen when comparing DBI with our method since the sliding window mechanism[43]is removed.\n280. In this paper, we conduct a comprehensive exploration of banding images from both subjective and objective perspectives.\n281. Specifically, we construct the largest ecologically valid banding IQA database to date named BAND-2k database, which consists of 2,000 banding images generated by fifteen compression and quantization schemes, achieving several times larger in number and diversity than the existing banding dataset.\n282. The construction process of the database, including distortion content preparation, subjective test procedure, and the removal of outlying data, is described in detail in this paper.\n283. Relying on this database, we proposed a novel banding evaluator using the frequency characteristic of banding artifacts, which models the banding as high-frequency artifacts that contained in the low-frequency smoothing region.\n284. A dual-branch CNN is devised to extract hierarchical features to classify the banding regions, upon which we introduce the spatial frequency masking to refine and compute an overall banding score.\n285. Experimental results show that our proposed method outperforms the baseline algorithms significantly in patch-level banding classification and banding IQA tasks.\n286. We believe that our study will benefit further development, calibration, and benchmarking of banding IQA models.\n\n请严格按照JSON格式返回：{"labels": [0,1,0,...]}，不要包含任何其他文字。'}], 'model': 'qwen-plus', 'response_format': {'type': 'json_object'}}}
2025-10-14 21:46:51,693 - DEBUG - Sending HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
2025-10-14 21:46:51,693 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-10-14 21:46:51,693 - DEBUG - send_request_headers.complete
2025-10-14 21:46:51,693 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-10-14 21:46:51,693 - DEBUG - send_request_body.complete
2025-10-14 21:46:51,693 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-10-14 21:51:40,111 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'vary', b'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding'), (b'x-request-id', b'bdd9936f-cfa9-4620-a048-212c88c70df3'), (b'x-dashscope-call-gateway', b'true'), (b'content-type', b'application/json'), (b'req-cost-time', b'288399'), (b'req-arrive-time', b'1760449602827'), (b'resp-start-time', b'1760449891227'), (b'x-envoy-upstream-service-time', b'288334'), (b'content-encoding', b'gzip'), (b'date', b'Tue, 14 Oct 2025 13:51:30 GMT'), (b'server', b'istio-envoy'), (b'transfer-encoding', b'chunked')])
2025-10-14 21:51:40,112 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-14 21:51:40,112 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-10-14 21:51:40,112 - DEBUG - receive_response_body.complete
2025-10-14 21:51:40,113 - DEBUG - response_closed.started
2025-10-14 21:51:40,113 - DEBUG - response_closed.complete
2025-10-14 21:51:40,113 - DEBUG - HTTP Response: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "200 OK" Headers({'vary': 'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding', 'x-request-id': 'bdd9936f-cfa9-4620-a048-212c88c70df3', 'x-dashscope-call-gateway': 'true', 'content-type': 'application/json', 'req-cost-time': '288399', 'req-arrive-time': '1760449602827', 'resp-start-time': '1760449891227', 'x-envoy-upstream-service-time': '288334', 'content-encoding': 'gzip', 'date': 'Tue, 14 Oct 2025 13:51:30 GMT', 'server': 'istio-envoy', 'transfer-encoding': 'chunked'})
2025-10-14 21:51:40,113 - DEBUG - request_id: bdd9936f-cfa9-4620-a048-212c88c70df3
2025-10-14 21:51:40,115 - DEBUG - API request completed in 288.43 seconds
2025-10-14 21:51:40,115 - DEBUG - Raw model response: {"labels": [1,0,0,1,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
2025-10-14 21:51:40,116 - ERROR - JSON parsing error: Expecting value: line 1 column 12107 (char 12106)
2025-10-14 21:51:40,116 - ERROR - Problematic content: {"labels": [1,0,0,1,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
2025-10-14 21:51:40,117 - INFO - Attempt 2 of 5
2025-10-14 21:51:40,119 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-a7211aea-51ec-4e0c-bd3a-a664d6892080', 'json_data': {'messages': [{'role': 'system', 'content': '你是一个专门用于识别学术论文中数据集描述的助手。你需要判断每个输入的句子是否描述了研究中使用的数据集。请确保：\n    1. 只输出JSON格式的结果\n    2. 结果格式必须为 {"labels": [0,1,0,...]}\n    3. 不要输出任何其他文字或解释'}, {'role': 'user', 'content': '你是一位专业的学术论文数据集描述识别专家。请仔细分析下面这篇论文中的每个句子，判断它们是否描述了本研究特定构建的数据集信息。\n\n    论文标题: BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment\n\n    判断标准:\n    判断为1(数据集描述)的标准：\n    句子必须直接描述本研究特定构建的数据集,包括:\n    1. 该数据集的构建过程和方法\n    2. 该数据集的具体构成和规模\n    3. 该数据的来源和收集方式\n    4. 该数据的预处理步骤\n    5. 该数据集的可获取方式(如发布地址)\n    6. 或者其他直接或者间接描述了该数据集的句子\n\n    所有其他类型的句子均标记为0。\n    \n    请对每个句子进行分析，返回一个JSON格式的标签数组。格式要求：\n    1. 必须是有效的JSON格式\n    2. 只包含labels字段，值为0和1组成的数组\n    3. 数组长度必须与句子数量相同\n    4. 示例格式：{"labels": [0,1,0,1,...]}\n\n    需要判断的句子：\n\n    1. Abstract: Banding, also known as staircase-like contours, frequently occurs in flat areas of images/videos processed by the compression or quantization algorithms.\n2. As undesirable artifacts, banding destroys the original image structure, thus inevitably degrading users\' quality of experience (QoE).\n3. In this paper, we systematically investigate the banding image quality assessment (IQA) problem, aiming to detect the image banding artifacts and evaluate their perceptual visual quality.\n4. Considering that the existing image banding databases only contain limited content sources and banding generation methods, and lack perceptual quality labels (i.e.mean opinion scores), we first build the largest banding IQA database so far, named Banding Artifact Noticeable Database (BAND-2k), which consists of 2,000 banding images generated by 15 compression and quantization schemes.\n5. A total of 23 workers participated in the subjective IQA experiment, yielding over 214,000 patch-level banding class labels and 44,371 reliable image-level quality rating scores.\n6. Subsequently, we develop an effective no-reference (NR) banding evaluator for banding detection and quality assessment by leveraging frequency characteristics of banding artifacts.\n7. To be more specific, a dual convolutional neural network (CNN) is employed to concurrently learn the feature representation from the high-frequency and low-frequency maps, thereby enhancing the ability to discern banding artifacts.\n8. The quality score of a banding image is generated by pooling the banding detection maps masked by the spatial frequency filters.\n9. The experimental results demonstrate that our banding evaluator achieves a remarkably high accuracy in banding detection and also exhibits high SRCC and PLCC results with the perceptual quality labels, even without directly learning a regression model for banding quality evaluation.\n10. These findings unveil the strong correlations between the intensity of banding artifacts and the perceptual visual quality, thus validating the necessity of banding quality assessment.\n11. The BAND-2k database and the proposed banding evaluator will be available at https://github.com/zijianchen98/BAND-2k.\n12. R ECENT years have witnessed a rapid emergence of media streaming services and social platforms.\n13. YouTube, Netflix, and TikTok account for more than half of the world\'s video traffic.\n14. Improving the quality of images under limited encoding, Zijian Chen, Wei Sun, Jun Jia, Zicheng Zhang, Xiongkuo Min, and Guangtao Zhai are with the Institute of Image Communication and Information Processing, Shanghai Jiao Tong University, Shanghai 200240, China (e-mail: {zijian.chen,sunguwei, jiajun0302, zzc1998, minxiongkuo, zhaiguangtao}@sjtu.edu.cn).\n15. Fangfang Lu is with the College of Computer Science and Technology, Shanghai University of Electric Power, Shanghai 200290, China (email:lufangfang@shiep.edu.cn).\n16. Jing Liu is with the School of Electrical and Information Engineering, Tianjin University, Tianjin, China.(e-mail:jliu tju@tju.edu.cn).\n17. transmission bandwidth, and storage condition is a necessary prerequisite for meeting the quality of experience (QoE) of users.\n18. In the stages between image acquisition and display, an image may suffer from various types of degradation, while banding artifacts are a kind of false contour distortion that is quite perceptible to the human eye.\n19. Since the visual quality of image contents greatly affects the QoE of end-users, it is highly desirable to design an effective banding image quality assessment (IQA) method, which aims to automatically detect the traces of such false contours and predict the objective quality of banding images that can be used to develop pre-processing or postprocessing debanding algorithms and optimize the performance of streaming media application.\n20. Normally, banding artifacts take on the appearance of annual rings, radiation circles, halos, or geographical contour lines and especially exist in the background regions (e.g., sky, water, and wall surface), where the color transition is not smooth enough.\n21. Nearly all existing image or video encoders, including H.264/AVC[28], VP9[38], and H.265/HEVC[27]can introduce such artifacts more or less.\n22. Current banding IQA research can be divided into two categories: subjective quality assessment and objective quality assessment.\n23. The existing subjective banding IQA research[39],[41]-[43]mainly investigates the limited banding scenarios with internal-used and undisclosed databases while lacking the quality label and may be insufficiently generalizable to large-scale commercial applications.\n24. Meanwhile, general IQA methods aiming at common distortions are inapplicable for banding exacerbated images due to the essential differences between them.\n25. First, banding artifacts usually hold tiny, staircaselike, and regional structures, which can be regarded as a kind of high-frequency artifact in smooth areas, while general distortions occur obviously in the whole image and are globally uniform.\n26. Second, the perceptual severity of banding is quantified based on its fraction of coverage and intensity in an image, which is widely divergent from the design philosophy of many existing IQA approaches.\n27. As a result, it is challenging to design an effective banding IQA method.\n28. To address these limitations, we first conduct a comprehensive subjective study of banding exacerbated images and create the largest banding IQA database to date with reliable mean opinion scores (MOS) and patch-level banding labels.\n29. We also propose a novel no-reference banding evaluator for banding detection and quality assessment by leveraging the frequency characteristics of banding artifacts.\n30. First, due to the dissimilar peculiarities of banding and smooth regions, the same distortion in different regions, e.g., textual and pictorial regions, may lead to different visual perception of human beings.\n31. Considering that banding manifests as a high-frequency artifact that exists in the lowfrequency smooth region, we propose a dual-branch CNN, which takes the high-frequency map and low-frequency map as inputs simultaneously, to hierarchically incorporate different visual features from the first layer and the last layer of our Resnet-50 backbone, thus making the model learn more effective banding feature representation and achieving more accurate banding region discrimination.\n32. Furthermore, inspired by the previous studies[15],[16], spatial frequency extracts information consistent with the human visual system (HVS), which not only reflects the overall active level in an image but also intuitively quantifies the contrast information.\n33. In other words, the value of spatial frequency is large in smooth areas, while becoming small in areas with harsh contrast changes, i.e., banding areas.\n34. Based on this mechanism, we adopt a spatial frequency masking strategy to refine the detected banding map and then pool the masked banding detection map to obtain the image-level banding quality score.\n35. • We propose a novel no-reference banding evaluator for banding detection and quality assessment based on frequency characteristics of banding artifacts, which utilizes a dual-branch CNN model to extract hierarchical banding-related feature representation from the high-frequency maps and low-frequency maps simultaneously.\n36. • Experimental results show that the proposed banding evaluator achieves the best performance in banding detection and significantly surpasses baselines in terms of SRCC and PLCC in the banding IQA task, which demonstrates the effectiveness of the proposed model.\n37. The remainder of this paper is organized as follows.\n38. Section II provides an overview of related works, including the stateof-the-art banding databases and detection methods.\n39. Section III introduces the construction of the BAND-2k database and the subjective assessment study.\n40. Section IV proposes an effective no-reference banding evaluator for banding detection and quality assessment.\n41. Section V gives the experimental results and analysis.\n42. Section VI concludes this paper.\n43. In this section, we first provide an overview of the state-ofthe-art banding-related databases (TableI) and then review the banding detection and evaluation methods.\n44. The first banding artifact-relevant VQA database was proposed by Wang et al.[39], which consists of 21 stimuli with different quantization grades generated by VP9[38]from 7 clips of 1280 × 720 30fps video.\n45. Authors in[41]investigated the effect of encoding parameters and dithering on the visibility of banding.\n46. Nine 4k-10bit source clips from the existing Netflix catalogue between 1 and 5 seconds were used to generate banding distorted videos.\n47. Each source content was downsampled to appropriate resolutions (1080p, 2k, or 4k) with certain bit-depth and further compressed by libaom (an AV1 codec library) at QPs {12, 20, 32}.\n48. More recently, Kapoor et al.[43]constructed one of the first databases for data-driven image banding assessment models.\n49. This research included about 1,440 images shot from over 600 pristine HD videos with a resolution of 1920 × 1080.\n50. Six levels of bit-depth quantization in luminance and chrominance channels are introduced to obtain different intensities of banding.\n51. Meanwhile, the banding images were semi-automatically segmented and labeled into banded and non-banded to form a patch-level banding dataset, which allows for training machine learning-based and deep learning-based banding classification methods.\n52. However, to the best of our knowledge, thus far there still lacking a benchmarking dataset in the banding detection and the corresponding banding IQA domain.\n53. Researchers either resort to image/video quality datasets that do not aim at banding distortion or build a small, attribute-restricted, in-house dataset by themselves.\n54. This motivates us to construct a large-scale subjective assessment database focus on the perceived bandingaffected image quality.\n55. Early research on banding detection mainly focuses on false contour identification, which aims to find the wrong boundary rather than a "true" region edge in the image.\n56. Authors in[31]-[33]utilized monotonicity or non-monotonicity features of local support regions including the gradient, contrast, variance, and entropy information to measure the loss of low-amplitude detail caused by banding.\n57. However, these works ignored the perceptual characteristics of the human visual system (HVS) and thus did not perform a good correlation with subjective tests.\n58. Another banding detection strategy is conducted at the pixel-level estimation and segmentation.\n59. Bhagavathy et al.[30]proposed to identify banding artifacts by calculating the likelihood of pixel difference.\n60. Baugh et al.[34]measured the severity of banding based on the number of a group of connected pixels with the same color.\n61. Wang et al.[39]first detected uniform segments to find possible banding areas and further incorporated edge features (e.g.length and contrast) to capture false boundaries.\n62. Nevertheless, these kinds of methods are typically sensitive to edge noise and are computationally expensive, causing limited application in realtime scenarios.\n63. Towards addressing these problems, Tu et al.[42]presented a completely no-reference banding detection method, which combines various properties of HVS with a number of preprocessing steps to refine banding edge detection.\n64. Instead of regarding banding detection as a false edge detection problem, Tandon et al.[41]heuristically utilized the effect of contrast sensitivity function (CSF) on banding visibility and its dependence on spatial frequency.\n65. Based on this, Krasula et al.[40]further compared the banding annoyance with more commonly studied compression artifacts and proposed a banding-aware video quality metric.\n66. In recent years, deep learning approaches have prevailed in various VQA tasks.\n67. As the pioneering work, Kapoor et al.[43]developed an automated CNN-based banding detector for the first time, which is a simple two-stage algorithm and gives rise to devising other learning-based techniques.\n68. In this work, we build a large-scale banding database and propose a data-driven banding indicator that can generate pixelwise banding visibility maps with corresponding subjectively  consistent quality scores by combining human visual mechanisms and deep learning techniques.\n69. Subjective banding image quality assessment facilitates the development of automatic objective banding image and video quality models.\n70. We created the largest banding database in existence, denoted as the BAND-2k database, which consists of 2,000 banding distorted images and over 214,000 patch-level banding class labels.\n71. Then, a subjective experiment was conducted to obtain the mean opinion scores (MOS) of the BAND-2k database.\n72. The workflow of the banding database construction is shown in Fig.1.\n73. To build a content-rich and balanced database, we manually collected source videos including computer graphics (CG), usergenerated content (UGC), and professionally-generated content (PGC) from two popular media websites Bilibili.comand Youtube.com.\n74. Then, 885 clips with multiple spatial resolutions (i.e., 4096×2160, 3840×2160, 1920×1080) and frame rates (i.e., 60, 50, 30, 25) are chosen as candidate.\n75. Note that videos on the mentioned websites are firstly annotated by the community with assigned a number of favorites, views, and downloads.\n76. These statistics correlate with the content and quality of a video, which guides our choices to some extent.\n77. All videos selected on the website are released under an appropriate creative commons (CC) license that allows further editing and redistribution.\n78. After content selection, we further unified the format of all video clips, especially the spatial resolution and the pixel format, which avoid the effect of other facts on visual quality.\n79. Concretely, we first converted the frame rate of the original clips to 25fps, which is to reduce the storage pressure while ensuring the graphics quality.\n80. Considering the commonly used aspect ratio of the user interface and displays is 16:9, we cropped the partially unqualified videos rather than shrinking images unevenly.\n81. Then, we downsampled the trimmed spatial resolution 3840×2160 to a lower resolution -1920×1080 for the following subjective study.\n82. Before conducting the following experiments, we manually removed the videos that are either too dark or bright, overly blurry or colorful, which helps to obtain more reliable subjective assessments.\n83. To avoid redundancy and to make sure the diversity of selected contents, we also conducted attribute analysis studies on the selected images.\n84. Four metrics that correlated with human perception, i.e., contrast, brightness, sharpness, and colorfulness, are adopted as content diversity metrics.\n85. All video attributes are calculated on every 10 frames to reduce computational complexity, which are then averaged over frames of each video sequence.\n86. Fig.2shows the distribution of attributes extracted from the selected videos.\n87. • Contrast: The contrast metric is simply defined as the standard deviation of pixel gray-scale intensities[25].\n88. • Colorfulness: The colorfulness metric is measured by the R, G, and B components[25].\n89. We first compute two matrices of differences between channels rg = R -G and yb = 1 2 (R + G) -B.\n90. Then, the colorfulness metric can be calculated as µ 2 rg + µ 2 yb + σ 2 rg + σ 2 yb , where µ and σ are the mean and standard deviation of their corresponding terms, respectively.\n91. • Sharpness: The cumulative probability of blur detection (CPBD) metric[24]is used to measure the image sharpness, which estimates the probability of detecting blur at each edge.\n92. • Brightness: The brightness of an image is obtained directly from the pixel gray intensities in R, G, and B channels.\n93. Finally, the number of source videos are reduced to 873.\n94. Fig.3displays thumbnails for 30 selected representative video clips.\n95. To simulate authentic banding artifacts that exist in real viewing scenarios, we introduced four encoding/transcoding strategies including H.264/AVC[28], H.265/HEVC[27], VP9[38], and bit-depth manipulation[43]with fifteen quantization schemes in total.For H.264/AVC and H.265/HEVC, considering the range of their quantization parameter (QP in ffmpeg), we selected three typical QP values, namely, QP = {18, 33, 43}.\n96. This is because that coded video clips with a QP value smaller than 18 normally provide perceptual lossless quality, while coded video clips with a QP value larger than 43 will not be able to offer adequate quality, which may affect the subjective test of banding artifacts.\n97. Similarly, for the VP9 encoder, the QP values under our close inspection are chosen as {28, 45, 60}.\n98. Moreover, we applied the same quantization strategies as in[43], which introduces banding distortion by scaling bit-depth in luminance and chrominance channels.\n99. Here, the color coding scheme YCbCr4:2:0 is applied for maximum compatibility.\n100. To sum up, we generated fifteen levels of banding with different intensities and shapes to enhance the diversity of the database.\n101. The banding database is then built by extracting frames from the distorted video clips, resulting in 2,000 images with a resolution of 1920×1080.\n102. Fig.4shows the visualization results of banding exacerbated images.\n103. In the process of image patch labeling, we initially intend to extract image patches of size 235×235 from banding images directly and perform annotation operations.\n104. However, considering the theoretical number of patches, it will cost a lot of manpower and time to label patch by patch.\n105. Therefore, we first segmented the banding images roughly and label them into banded and non-banded regions.\n106. Then, labelled image patches are generated from these segmented and labelled images by a sliding window.\n107. Specifically, we followed the same demarcation of the banded and non-banded images in[43]that a patch is labelled as banded if it has more than 30% overlap with banded regions in the image.\n108. Eventually, a banding dataset containing 2,000 distorted images with 1920×1080 resolution and 214,324 labelled image patches of size 235×235 is built.\n109. To the best of our knowledge, it is the largest banding dataset in existence, which enables training various machine/deep learning based banding detection models and facilitates the development of image/video debanding techniques.\n110. TableIIreports the composition of labelled image patch dataset.\n111. It can be observed that the number of banded patches is a bit smaller than non-banded patches due to the fact that banding usually appears in smooth background areas.\n112. As shown in Fig.1, the subjective quality study contains four steps.\n113. In addition to preparing the experimental environment, subjects should pass the qualification test first to participate in the study.\n114. After the subjective rating, all resulting scores need to be analyzed and examined before generating the final mean opinion score (MOS).\n115. 1) Experimental Environment Setting: In this study, a total of 25 inexperienced subjects are gathered in a laboratory environment, where relevant experimental configuration must satisfy the following requirements: • Considering the viewing effect, desktops and laptops are allowed as displays.\n116. • The resolution of displays must be larger than or equal to 1920×1080 to show the images without spatial downsampling.\n117. • The viewing distance and optimal horizontal viewing angle are set as 1.9 times the height of the display and respectively.\n118. 500 recommendation[26].\n119. As a result, we used a 27-inch AOC Q27U2D monitor with a resolution of 2560×1440 for assessment with 25 subjects.\n120. Due to the large number of images to be assessed, we divided the dataset into ten sessions to avoid visual fatigue.\n121. Each session of tests took nearly 2 hours with a 30-minute break for each participant.\n122. 2) Qualification Test: Before starting the main experiment, subjects are required to pass a quiz to get the qualification of conducting follow-up experiments.\n123. Firstly, we manually selected 10 labeled banding exacerbated images beyond the database as training images to familiarize subjects with the operation interface and the goal of this subjective test.\n124. The quiz consists of two parts including banding classification and image-level quality rating.\n125. In banding classification, subjects were told to divide the test image into banded or non-banded.\n126. In image-level quality rating, subjects were instructed to focus on the coverage and intensity of banding areas, as well as the overall quality of images to get the final quality score.\n127. The rating scale is continuous from 0 to 100 while a higher value indicates more severe banding (i.e., more visible or occupying larger portion of the image).\n128. To make the quiz objective and fair, we take the banding images labeled by domain experts as the ground truth, while two existing banding metrics BBAND[42]and DBI[43]are used to determine the normal range of the rating scores.\n129. That is the quality scores rated by subjects should not exceed 20% of the above banding metrics.\n130. As a result, only 23 subjects with an accuracy above 80% in banding classification and subjective scores in the normal range were allowed to pass the quiz.\n131. Note that the steps for taking a quiz are the same as the main experiment, which aims to guarantee the consistency of results.\n132. 3) Formal Study: We adopted the single-stimulus (SS) method in this test.\n133. Ten golden images that have the acknowledged high quality or poor quality (assessed by BBAND[42]and DBI[43]) were added to each session for controlling the scoring deviations.\n134. Besides, 3 repeated images are randomly inserted into each session to ensure consistency of scores before and after subjects scoring.\n135. At last, 23 qualified subjects were asked to provide their opinions on the shuffled image groups.\n136. The resulting scores were collected and packed for further analysis.\n137. 4) Result Analysis: In total, 46,000 scores were collected by 23 qualified subjects in the main study phase.\n138. However, considering the qualification quiz cannot completely disallow those unreliable workers to muddle through to the main study while reliable subjects may also occasionally score odd values, which may be caused by the inter-individual differences in perceiving the quality of the unique characteristics of different contents.\n139. Thus, we further investigated the confidence of rating scores and removed outliers following the Grubbs\' test[23],[45].\n140. Concretely, let s = (s 1 , s 2 , . . ., s N ) denote a set of raw scores collected for one distorted image.\n141. The test statistics is defined as the maximum absolute deviation of a sample standard deviation (SD) unit from the sample mean, which is mathematically expressed as where s and σ s denote the mean and standard deviation of the sample, respectively.\n142. Then, given a significant level α, a sample is detected as an outlier if where t 2 α/(2N ),N -2 represents the upper critical value of the tdistribution with N -2 degrees of freedom and a significance level of α/(2N ).\n143. Empirically, we set the significance level α at 0.05.\n144. Then, a sample is identified as an outlier if its distance to the sample mean is larger than 2.5 times SD and is removed.\n145. Following the aforementioned steps, the total number of scores Fig.6.\n146. The overall architecture of the proposed method.\n147. Given a banding distorted image, it is first divided into patches.\n148. Then, the patch-level high-frequency map (HFM) and low-frequency map (LFM) are generated by Sobel operation and piece-wise smooth algorithm[19], respectively.\n149. After that, a dual-branch CNN (CNN H and CNN L ) is deployed to extract hierarchical features with different visual information and thus classify the patches into banded or non-banded.\n150. Lastly, a spatial frequency masking strategy is introduced to refine the banding map and calculate the image-level banding quality score.\n151. Note that the dual-branch networks do not share parameters.\n152. was reduced to 44,371, and MOS was created by averaging the scores for each image.\n153. Fig.5presents the histogram of MOSs over the entire database, showing a broad MOS distribution of banding images.\n154. In this section, we describe the architecture of the proposed banding evaluator in detail, as shown in Fig.6.\n155. As stated before, banding usually appear as high-frequency information in the smooth background, while humans perceive high-frequency texture regions and low-frequency plateau regions through different neural channels concurrently, and transfer the upper visual features into the cerebral cortex for final processing[17],[44].\n156. Inspired by this, we employ high-frequency maps (HFM) and low-frequency maps (LFM) as the deep learning network inputs, which represent the texture and structural information of the image respectively, to mimic the recognition mechanism of the human brain for better banding identification.\n157. High-frequency Maps.\n158. Since gradient has been widely used to represent edge information and has been confirmed beneficial to acquire high-frequency components with low computational cost[20]-[22], we apply the isotropic Sobel operator to each patch for enhancing the details of banding artifacts.\n159. Given an input patch I, the high-frequency map is calculated by where S x and S y are the horizontal and vertical isotropic Sobel operators, respectively." * " denotes the convolution operation.\n160. Low-frequency Maps.\n161. To maintain the principal content of the image and filter out the influence of high-frequency information, we use the piece-wise smooth algorithm[19]to generate the low-frequency map by minimizing a function for image approximation recovery: where L represents the low-frequency map, Ω and E denotes the image domain and edge set, respectively.\n162. P indicates the pixel and E dσ represents the total edge length.\n163. The coefficients α and β are positive regularization constants.\n164. An example of frequency maps is shown in Fig.7.\n165. To obtain the overall banding score, we first divide the banding image into 235×235 patches and apply CNN-based classification to each patch, resulting in a banding classification label for each patch, i.e., banded or non-banded.\n166. As shown in Fig.6, the proposed network consists of two parallel branches, namely CNN H and CNN L , which take the patch-level high-frequency map and low-frequency map as input, respectively.\n167. For each branch, we propose to use Resnet-50[18]as the backbone.\n168. Specifically, we incorporate the feature maps extracted from the first convolutional layer and the last layer of Resnet-50 as hierarchical visual features, which represent different visual information[46],[47]and can be used as predictive information to enhance the discrimination ability of the network for banding and non-banded regions.\n169. Afterward, the features extracted from two branches are concatenated first and reshaped into 128dimensional vectors through two fully-connected layers, which is further followed with the sigmoid activation function to output the final predicted label, namely banded or non-banded.\n170. Of note is that sharing parameters is extremely unfavorable for extracting low-and high-frequency features simultaneously, we thereby deploy two branches that work independently and do not share parameters.\n171. The loss function adopted here is binary cross entropy.\n172. With the trained CNN H and CNN L , each input patch is predicted to obtain a banding or non-banded label.\n173. To better guide the pre-processing and post-processing debanding algorithms, it is necessary to generate a quality score for the entire banding image.\n174. Since the visibility of edge is also affected by content, we further consider the effect of spatially varying content information on the local quality of human perception.\n175. As a consequence, we introduce the spatial frequency masking strategy to determine the weighting factor for the detected banding regions in each patch adaptively and thus obtain the image-level banding severity score while refining the visibility of banding artifacts.\n176. 1) Spatial Frequency Masking: The spatial frequency is defined as the activity level of an image, which establishes a filterbank based on the visual stimulus and is in accordance with HVS[16].\n177. In this paper, we propose to apply spatial frequency as an effective contrast criterion to banding measurement.\n178. Specifically, given an image of size I W × I H , divided into N × N patches, where I W and I H denote the number of columns and rows respectively.\n179. The column (CF k ) and row (RF k ) frequencies of the image patches are given by where I(x, y) is the pixel value of the image patch.\n180. Then, the resulting spatial frequency of an N × N patch is computed as where k is the number of patches (1 ≤ k ≤ I W I H N 2 ).\n181. Since most banding regions are likely to have large contrast including edges and textures, which should be assigned greater weights than the smooth and blurred areas.\n182. Accordingly, we design a banding visibility transfer function to express spatial frequency masking as a function of the local textural feature.\n183. The final spatial frequency masking weight is calculated at each patch as where γ is the scaling constant factor chosen to tune the shape of the transfer function.\n184. We used γ = 1.5 in our implementation.\n185. 2) Building a Banding Metric: The visibility of banding artifacts depends on the combination of multiple visual mechanisms.\n186. In this paper, we propose a simple but effective product model for attribute integration at each predicted banding patch to obtain the entire banding map (BM): where P k denotes the predicted label of k-th patch and w k is the weight parameter that scales the visibility of measured contours, i.e., gradient magnitude of the high-frequency map, |HFM k (i, j) | at region (i, j).\n187. Furthermore, inspired by previous psychovisual findings that the QoE of observers is dominated by those regions having poor quality[14],[42], we thereby leverage the worst p% percentile visual pooling to calculate an average banding score from the generated BM, where p is set to 80 in this experiment.\n188. As a result, the perceptual score of the overall banding image is defined as where M is the total number of patches in image I.\n189. T p% denotes the index set of the top p% non-zero pixel-wise value contained in k-th patch of the BM.\n190. In this section, we first present the experimental protocol in detail and then evaluate the performance of the proposed method on two tasks, namely patch-level banding classification and banding image quality assessment.\n191. After that, the ablation study and cross-database validation are conducted to prove the robustness and effectiveness of the proposed method.\n192. Finally, we test the computational efficiency of our method.\n193. A. Experimental Protocol 1) Databases and Settings: We choose two databases to train and test the effectiveness of the proposed banding IQA method, which are the database released in[43]and our proposed BAND-2k database.\n194. The detail information of these two datasets can be found in TableI.\n195. The proposed model is implemented by PyTorch[13].\n196. Before training, we randomly split the training, validation, and testing set into 8:1:1 (as shown in TableII).\n197. We use the Adam optimizer with the initial learning rate set as 1e-4 and set the batch size as 32.\n198. The training process is stopped after 25 epochs.\n199. The resolution of each cropped patch is fixed to 235×235.\n200. All experiments on both the[43]database and the BAND-2k database are conducted repeatedly 10 times to obtain the mean performance.\n201. 2) Baseline Algorithms: We include a number of representative IQA algorithms in our evaluation as references to be compared against.\n202. These baseline methods include: • General FR IQA methods: We choose PSNR, SSIM[12], MS-SSIM[11], LPIPS[10]as baselines.\n203. These are the most commonly used FR IQA metrics in practical applications such as video coding, image enhancement, etc. • General NR IQA methods: BRISQUE[37], NIQE[36], NIMA[8], DBCNN[35], HyberIQA[9], and StairIQA[1].\n204. • Banding IQA methods: Considering that there exists few research on banding detection and quality assessment, we barely select the BBAND[42], CAMBI[41], VMAF BA[40]and DBI[43]metrics as comparisons.\n205. 3) Evaluation Criteria: To evaluate the IQA methods comprehensively, a total of seven evaluation indexes in two categories are adopted.\n206. For patch-level banding classification, we follow the common procedures as in[43]and utilize the area under the receiver operating characteristics (AUROC), the area under the precision-recall curve (AUPRC), and accuracy as the classification performance metrics.\n207. For banding image quality assessment, four mainstream metrics are selected as the evaluation criteria: Spearman rank-order correlation coefficient (SRCC) and Kendall rank-order correlation coefficient (KRCC) measure the prediction monotonicity, while Pearson linear correlation coefficient (PLCC) and root mean square error (RMSE) are calculated to assess prediction consistency.\n208. Considering the potential nonlinear mapping characteristics between the objective scores and the subjective scores, we perform score alignment by mapping the predicted value using the five-parameter logistic function before calculating PLCC and RMSE values[7].\n209. Since our goal is to develop an effective banding IQA approach, we regard the identification of banding patches as an important preceding process to achieve accurate banding quality prediction.\n210. However, there exist few methods that are designed for banding classification and nearly all IQA methods produce scalar values only while failing in classifying banding regions directly.\n211. Therefore, we adopt a thresholding step to convert the single quality value into binary classification results as[43]does.\n212. Concretely, a half-interval search algorithm[48]is employed to find the optimal threshold value that can generate the best classification result.\n213. Based on the above premise, TableIIIreports the experimental results on both the database from[43]and the BAND-2k database.\n214. We highlight the best results in boldface.\n215. As compared to other state-of-the-art IQA methods, our proposed method yields the best overall performance in terms of AUROC, AUPRC, and accuracy.\n216. It is shown that most general FR IQA and NR IQA models perform poorly on the patch-level banding classification task while performing fairly well on other IQA tasks[1],[9],[35], indicating that the current approaches are not sensitive to banding distortion.\n217. Benefiting from the powerful feature extraction ability of CNNs, our proposed method and the customized NR IQA models for banding artifacts detection (DBI[43]) reach a significant performance in the discrimination of false contours.\n218. However, the performance of banding IQA method BBAND, CAMBI, and VMAF BA is surprisingly poor compared with other methods, which shows their vulnerability in identifying local banding artifacts from texture regions and are not suitable for patch-level banding identification.\n219. In addition, we investigate the computational complexity in terms of execution time per image patch.\n220. It can be observed that except for those traditional FR IQA models, our method achieves comparable speed in patch-level banding classification, which determines the prediction efficiency of the subsequent image-level quality assessment, making it a favorable choice in time-constrained scenarios.\n221. Considering that there exist no image banding databases with subjective scores attached in the public domain, we merely compare the performance of the proposed method with the baseline approaches on the proposed BAND-2k database.\n222. The results are shown in TableIV, from which we can observe that our proposed method largely surpasses all baselines in terms of SRCC, KRCC, and PLCC except for the RMSE.\n223. Compared to the secondbest model, our method achieves 18.07%SRCC improvements, 24.91% KRCC improvements, and 18.01%PLCC improvements on the BAND-2k database.\n224. We also present scatter plots of predictions versus MOS for better visualization in Fig.8.\n225. Overall,  (a) PSNR, (b) SSIM[12], (c) MS-SSIM[11], (d) LPIPS[10], (e) VMAF BA[40], (f) BRISQUE[37], (g) NIQE[36], (h) NIMA[8], (i) DBCNN[35], (j) HyperIQA[9], (k) StairIQA[1], (l) BBAND[42], (m) CAMBI[41], (n) DBI[43], and (o) Ours.\n226. Fig.9. Visual comparisons of the banding map results.\n227. From top to bottom are banding images and their corresponding banding maps generated by BBAND[42], DBI[43], and our proposed method, respectively.\n228. The first five columns of images from left to right are from BAND-2k, while the rest images are from[43].\n229. the performance of traditional FR IQA models such as PSNR, SSIM, and MS-SSIM is remarkably inferior in banding images assessment and is uncorrelated with the MOS, which is consistent with the numerical results presented in TableIV.\n230. The reason is that PSNR and SSIM-based methods do perform not well on tiny, regional, and content-independent banding distortion while lacking the consideration for the mechanism of HVS.\n231. It is also worth mentioning that the deep CNN architecturebased methods (DBCNN[35], HyperIQA[9], and StairIQA[1]), despite performing well on LIVEC[6], KonIQ-10k[29], and other universal image quality evaluation databases[4],[5], underperformed our proposed model by a notable margin on the BAND-2k database.\n232. (2) the perception of banding artifacts is explicitly intensity-aware.\n233. These are the issues that the CNN-based approaches above do not take into account.\n234. To some extent, banding distortion is more like a kind of local distortion than common global distortion such as Gaussian noise, blur, and dither.\n235. This suggests that it is potentially valuable to integrate some local texture, edge, contrast, or other visibility-related features into quality prediction models when assessing banding images.\n236. Fortunately, with the help of the proposed scoring strategy, our method gains the ability to convert the area range of identified banding regions to indicative annoying scores.\n237. Surprisingly, the banding IQA methods BBAND[42], CAMBI[41], and VMAF BA[40], however, did not perform very well on the BAND-2k dataset.\n238. We infer that this is due to differences in the test environment.\n239. First, the source videos selected in CAMBI and VMAF BA are from the internal Netflix 4K catalogue while the source contents in BAND-2k are collected from the public streaming websites with different resolutions, leading to an uneven quality level.\n240. Second, the methods of artificially introducing banding distortion are different.\n241. 264 compression techniques are used to generate the banding artifacts, limiting the adaptive ability of the algorithm in other scenarios.\n242. For further investigation, we compare our method with other two methods, i.e., BBAND[42], DBI[43], and visually study the generated banding maps to verify their effectiveness in banding detection.\n243. The visualization results are shown in Fig.9, where seven representative banding images are selected for reference.\n244. As shown, the banding maps generated from BBAND are mostly disordered and have a lot of discontinuity while the banding maps generated by DBI are too vague to locate the exact location of banding artifacts, making it difficult to develop pixel-level debanding techniques.\n245. Overall, we can compendiously conclude that: (1) in comparison with BBAND and DBI, banding maps computed by our proposed method could provide an accurate, clear indication for banding artifacts; (2) in comparison with BBAND and DBI, the quality prediction of banding images guided by our method could achieve a high consistency with HVS.\n246. Moreover, to make a statistically meaningful comparison among different IQA methods, we further conduct the widely used F-test[2],[3]to assess the statistical significance of the proposed method.\n247. Based on the assumption that the model\'s prediction residuals follow the Gaussian distribution, the lefttailed F-test with a confidence level of 95% is performed on the residuals of every two IQA models.\n248. The results of significance tests on the BAND-2k database are shown in Fig.10.\n249. A value of \'1\' (colored in green) indicates that the model in the row is significantly better than the model in the column, while a value of \'0\' (colored in red) indicates that the model in the row is not significantly better than the model in the column.\n250. It is shown that our proposed method performs significantly better than other models, which is consistent with the observations from the above comparison experiments.\n251. In this section, we explore the effectiveness of our model\'s design philosophy.\n252. To verify the importance of the dual-branch (DB) architecture, the baseline variants use the same backbone as the proposed method, except that only a single branch (SB) is reserved.\n253. Then, we use the original banding images as the input while removing the frequency map generation module (SB-I).\n254. Besides, the high-frequency maps and low-frequency maps generation modules are retained respectively (SB-HFM and SB-LFM).\n255. It can be observed from Table V that SB-LFM achieved the worst results, since the low-frequency map filters out the highfrequency banding information to a certain extent, which reduces the ability of the model to identify false contours.\n256. SB-I performs significantly better than SB-HFM and SB-LFM, resulting from that it contains richer image information, but it is still inferior to our method, which demonstrates the effectiveness of the dualbranch scheme.\n257. To investigate the effect of the banding feature extraction, we further design two variants with different input combinations of frequency maps.\n258. First, the high-frequency map was taken as the input of both the CN N H and CN N L layers (DB-HFM).\n259. Then, we replace the inputs with the low-frequency maps (DB-LFM).\n260. As shown in TableV, the performance of DB-HFM and DB-LFM is far apart from our approach, which matches our hypothesis that the high-frequency texture information contained in HFM and the low-frequency background information contained in LFM are crucial to enhance the capacity of discernment for banding artifacts.\n261. Therefore, we may conclude that our model is the most suitable model among those compared variants in terms of both banding classification and IQA applications.\n262. Due to the effects of different compression techniques, shooting equipment, scenes, etc., the image content and banding distortions may vary significantly in practical applications.\n263. For the database[43], it only includes limited types of image sources and means of triggering banding distortion.\n264. As a result, we conduct a cross-database validation to verify the generalizability of the proposed model, wherein the database presented by[43]and BAND-2k are included.\n265. That is, we trained the model on one full database and report the test performance on the other.\n266. We mainly compare the proposed method with four learningbased models, i.e., DBCNN[35], HyperIQA[9], StairIQA[1], and DBI[43].\n267. Since MOS information is not provided in the database[43], which is an essential part of methods training, we condensed part of the experiments.\n268. TableVIand VII report the experimental results in terms of patch-level banding classification and image quality assessment.\n269. We can observe that our proposed method generalization between database[43]and BAND-2k was surprisingly good.\n270. Besides, it is worth noting that the performance of these methods trained on the BAND-2k has improved a little compared to the previous versions that were trained on the database[43], which further demonstrates the superiority of the proposed database BAND-2k.\n271. The efficiency of an image quality prediction model is of great importance in practical industrial deployments.\n272. Therefore, we measured the average running time of the compared IQA models, as shown in TableVIII.\n273. The experiments were performed in MATLAB R2021a and Python 3.7 under Windows 10 64bit system on a Lenovo laptop with Intel Core i5-9300HF CPU@2.4GHz,16GB RAM, and NVIDIA GTX 1660Ti 6G GPU.\n274. It can be observed that the proposed method achieves a reasonable running time among the FR, NR, and other bandingspecified IQA algorithms.\n275. Generally, the execution time of classical IQA algorithms is significantly less than learning-based methods.\n276. Simpler NSS-based models such as BRISQUE and NIQE still show competitive efficiency relative to CNN models while exhibiting inferior performance in banding image quality assessment.\n277. For CAMBI and VMAF BA , we use the officially launched software package, which is based on the stand-alone C library libvmaf and therefore surpasses other methods in speed.\n278. Moreover, unlike the general quality evaluation using regression to predict scores, the patch-wise prediction strategy that we adopted may increase the complexity.\n279. Note that although we deployed a more complex network structure, a nearly 10 times speedup has been seen when comparing DBI with our method since the sliding window mechanism[43]is removed.\n280. In this paper, we conduct a comprehensive exploration of banding images from both subjective and objective perspectives.\n281. Specifically, we construct the largest ecologically valid banding IQA database to date named BAND-2k database, which consists of 2,000 banding images generated by fifteen compression and quantization schemes, achieving several times larger in number and diversity than the existing banding dataset.\n282. The construction process of the database, including distortion content preparation, subjective test procedure, and the removal of outlying data, is described in detail in this paper.\n283. Relying on this database, we proposed a novel banding evaluator using the frequency characteristic of banding artifacts, which models the banding as high-frequency artifacts that contained in the low-frequency smoothing region.\n284. A dual-branch CNN is devised to extract hierarchical features to classify the banding regions, upon which we introduce the spatial frequency masking to refine and compute an overall banding score.\n285. Experimental results show that our proposed method outperforms the baseline algorithms significantly in patch-level banding classification and banding IQA tasks.\n286. We believe that our study will benefit further development, calibration, and benchmarking of banding IQA models.\n\n请严格按照JSON格式返回：{"labels": [0,1,0,...]}，不要包含任何其他文字。'}], 'model': 'qwen-plus', 'response_format': {'type': 'json_object'}}}
2025-10-14 21:51:40,126 - DEBUG - Sending HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
2025-10-14 21:51:40,126 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-10-14 21:51:40,126 - DEBUG - send_request_headers.complete
2025-10-14 21:51:40,126 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-10-14 21:51:40,126 - DEBUG - send_request_body.complete
2025-10-14 21:51:40,126 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-10-14 21:56:28,443 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'vary', b'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding'), (b'x-request-id', b'f303e02e-b80e-4d35-95e0-25943357d803'), (b'x-dashscope-call-gateway', b'true'), (b'content-type', b'application/json'), (b'req-cost-time', b'288298'), (b'req-arrive-time', b'1760449891273'), (b'resp-start-time', b'1760450179571'), (b'x-envoy-upstream-service-time', b'288231'), (b'content-encoding', b'gzip'), (b'date', b'Tue, 14 Oct 2025 13:56:19 GMT'), (b'server', b'istio-envoy'), (b'transfer-encoding', b'chunked')])
2025-10-14 21:56:28,443 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-14 21:56:28,444 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-10-14 21:56:28,444 - DEBUG - receive_response_body.complete
2025-10-14 21:56:28,444 - DEBUG - response_closed.started
2025-10-14 21:56:28,444 - DEBUG - response_closed.complete
2025-10-14 21:56:28,445 - DEBUG - HTTP Response: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "200 OK" Headers({'vary': 'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding', 'x-request-id': 'f303e02e-b80e-4d35-95e0-25943357d803', 'x-dashscope-call-gateway': 'true', 'content-type': 'application/json', 'req-cost-time': '288298', 'req-arrive-time': '1760449891273', 'resp-start-time': '1760450179571', 'x-envoy-upstream-service-time': '288231', 'content-encoding': 'gzip', 'date': 'Tue, 14 Oct 2025 13:56:19 GMT', 'server': 'istio-envoy', 'transfer-encoding': 'chunked'})
2025-10-14 21:56:28,445 - DEBUG - request_id: f303e02e-b80e-4d35-95e0-25943357d803
2025-10-14 21:56:28,446 - DEBUG - API request completed in 288.33 seconds
2025-10-14 21:56:28,446 - DEBUG - Raw model response: {"labels": [1,0,0,1,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1
2025-10-14 21:56:28,447 - ERROR - JSON parsing error: Expecting ',' delimiter: line 1 column 13482 (char 13481)
2025-10-14 21:56:28,447 - ERROR - Problematic content: {"labels": [1,0,0,1,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1
2025-10-14 21:56:28,449 - INFO - Attempt 3 of 5
2025-10-14 21:56:28,450 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-2f2ff7d9-29a5-453c-9b35-fcda48922303', 'json_data': {'messages': [{'role': 'system', 'content': '你是一个专门用于识别学术论文中数据集描述的助手。你需要判断每个输入的句子是否描述了研究中使用的数据集。请确保：\n    1. 只输出JSON格式的结果\n    2. 结果格式必须为 {"labels": [0,1,0,...]}\n    3. 不要输出任何其他文字或解释'}, {'role': 'user', 'content': '你是一位专业的学术论文数据集描述识别专家。请仔细分析下面这篇论文中的每个句子，判断它们是否描述了本研究特定构建的数据集信息。\n\n    论文标题: BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment\n\n    判断标准:\n    判断为1(数据集描述)的标准：\n    句子必须直接描述本研究特定构建的数据集,包括:\n    1. 该数据集的构建过程和方法\n    2. 该数据集的具体构成和规模\n    3. 该数据的来源和收集方式\n    4. 该数据的预处理步骤\n    5. 该数据集的可获取方式(如发布地址)\n    6. 或者其他直接或者间接描述了该数据集的句子\n\n    所有其他类型的句子均标记为0。\n    \n    请对每个句子进行分析，返回一个JSON格式的标签数组。格式要求：\n    1. 必须是有效的JSON格式\n    2. 只包含labels字段，值为0和1组成的数组\n    3. 数组长度必须与句子数量相同\n    4. 示例格式：{"labels": [0,1,0,1,...]}\n\n    需要判断的句子：\n\n    1. Abstract: Banding, also known as staircase-like contours, frequently occurs in flat areas of images/videos processed by the compression or quantization algorithms.\n2. As undesirable artifacts, banding destroys the original image structure, thus inevitably degrading users\' quality of experience (QoE).\n3. In this paper, we systematically investigate the banding image quality assessment (IQA) problem, aiming to detect the image banding artifacts and evaluate their perceptual visual quality.\n4. Considering that the existing image banding databases only contain limited content sources and banding generation methods, and lack perceptual quality labels (i.e.mean opinion scores), we first build the largest banding IQA database so far, named Banding Artifact Noticeable Database (BAND-2k), which consists of 2,000 banding images generated by 15 compression and quantization schemes.\n5. A total of 23 workers participated in the subjective IQA experiment, yielding over 214,000 patch-level banding class labels and 44,371 reliable image-level quality rating scores.\n6. Subsequently, we develop an effective no-reference (NR) banding evaluator for banding detection and quality assessment by leveraging frequency characteristics of banding artifacts.\n7. To be more specific, a dual convolutional neural network (CNN) is employed to concurrently learn the feature representation from the high-frequency and low-frequency maps, thereby enhancing the ability to discern banding artifacts.\n8. The quality score of a banding image is generated by pooling the banding detection maps masked by the spatial frequency filters.\n9. The experimental results demonstrate that our banding evaluator achieves a remarkably high accuracy in banding detection and also exhibits high SRCC and PLCC results with the perceptual quality labels, even without directly learning a regression model for banding quality evaluation.\n10. These findings unveil the strong correlations between the intensity of banding artifacts and the perceptual visual quality, thus validating the necessity of banding quality assessment.\n11. The BAND-2k database and the proposed banding evaluator will be available at https://github.com/zijianchen98/BAND-2k.\n12. R ECENT years have witnessed a rapid emergence of media streaming services and social platforms.\n13. YouTube, Netflix, and TikTok account for more than half of the world\'s video traffic.\n14. Improving the quality of images under limited encoding, Zijian Chen, Wei Sun, Jun Jia, Zicheng Zhang, Xiongkuo Min, and Guangtao Zhai are with the Institute of Image Communication and Information Processing, Shanghai Jiao Tong University, Shanghai 200240, China (e-mail: {zijian.chen,sunguwei, jiajun0302, zzc1998, minxiongkuo, zhaiguangtao}@sjtu.edu.cn).\n15. Fangfang Lu is with the College of Computer Science and Technology, Shanghai University of Electric Power, Shanghai 200290, China (email:lufangfang@shiep.edu.cn).\n16. Jing Liu is with the School of Electrical and Information Engineering, Tianjin University, Tianjin, China.(e-mail:jliu tju@tju.edu.cn).\n17. transmission bandwidth, and storage condition is a necessary prerequisite for meeting the quality of experience (QoE) of users.\n18. In the stages between image acquisition and display, an image may suffer from various types of degradation, while banding artifacts are a kind of false contour distortion that is quite perceptible to the human eye.\n19. Since the visual quality of image contents greatly affects the QoE of end-users, it is highly desirable to design an effective banding image quality assessment (IQA) method, which aims to automatically detect the traces of such false contours and predict the objective quality of banding images that can be used to develop pre-processing or postprocessing debanding algorithms and optimize the performance of streaming media application.\n20. Normally, banding artifacts take on the appearance of annual rings, radiation circles, halos, or geographical contour lines and especially exist in the background regions (e.g., sky, water, and wall surface), where the color transition is not smooth enough.\n21. Nearly all existing image or video encoders, including H.264/AVC[28], VP9[38], and H.265/HEVC[27]can introduce such artifacts more or less.\n22. Current banding IQA research can be divided into two categories: subjective quality assessment and objective quality assessment.\n23. The existing subjective banding IQA research[39],[41]-[43]mainly investigates the limited banding scenarios with internal-used and undisclosed databases while lacking the quality label and may be insufficiently generalizable to large-scale commercial applications.\n24. Meanwhile, general IQA methods aiming at common distortions are inapplicable for banding exacerbated images due to the essential differences between them.\n25. First, banding artifacts usually hold tiny, staircaselike, and regional structures, which can be regarded as a kind of high-frequency artifact in smooth areas, while general distortions occur obviously in the whole image and are globally uniform.\n26. Second, the perceptual severity of banding is quantified based on its fraction of coverage and intensity in an image, which is widely divergent from the design philosophy of many existing IQA approaches.\n27. As a result, it is challenging to design an effective banding IQA method.\n28. To address these limitations, we first conduct a comprehensive subjective study of banding exacerbated images and create the largest banding IQA database to date with reliable mean opinion scores (MOS) and patch-level banding labels.\n29. We also propose a novel no-reference banding evaluator for banding detection and quality assessment by leveraging the frequency characteristics of banding artifacts.\n30. First, due to the dissimilar peculiarities of banding and smooth regions, the same distortion in different regions, e.g., textual and pictorial regions, may lead to different visual perception of human beings.\n31. Considering that banding manifests as a high-frequency artifact that exists in the lowfrequency smooth region, we propose a dual-branch CNN, which takes the high-frequency map and low-frequency map as inputs simultaneously, to hierarchically incorporate different visual features from the first layer and the last layer of our Resnet-50 backbone, thus making the model learn more effective banding feature representation and achieving more accurate banding region discrimination.\n32. Furthermore, inspired by the previous studies[15],[16], spatial frequency extracts information consistent with the human visual system (HVS), which not only reflects the overall active level in an image but also intuitively quantifies the contrast information.\n33. In other words, the value of spatial frequency is large in smooth areas, while becoming small in areas with harsh contrast changes, i.e., banding areas.\n34. Based on this mechanism, we adopt a spatial frequency masking strategy to refine the detected banding map and then pool the masked banding detection map to obtain the image-level banding quality score.\n35. • We propose a novel no-reference banding evaluator for banding detection and quality assessment based on frequency characteristics of banding artifacts, which utilizes a dual-branch CNN model to extract hierarchical banding-related feature representation from the high-frequency maps and low-frequency maps simultaneously.\n36. • Experimental results show that the proposed banding evaluator achieves the best performance in banding detection and significantly surpasses baselines in terms of SRCC and PLCC in the banding IQA task, which demonstrates the effectiveness of the proposed model.\n37. The remainder of this paper is organized as follows.\n38. Section II provides an overview of related works, including the stateof-the-art banding databases and detection methods.\n39. Section III introduces the construction of the BAND-2k database and the subjective assessment study.\n40. Section IV proposes an effective no-reference banding evaluator for banding detection and quality assessment.\n41. Section V gives the experimental results and analysis.\n42. Section VI concludes this paper.\n43. In this section, we first provide an overview of the state-ofthe-art banding-related databases (TableI) and then review the banding detection and evaluation methods.\n44. The first banding artifact-relevant VQA database was proposed by Wang et al.[39], which consists of 21 stimuli with different quantization grades generated by VP9[38]from 7 clips of 1280 × 720 30fps video.\n45. Authors in[41]investigated the effect of encoding parameters and dithering on the visibility of banding.\n46. Nine 4k-10bit source clips from the existing Netflix catalogue between 1 and 5 seconds were used to generate banding distorted videos.\n47. Each source content was downsampled to appropriate resolutions (1080p, 2k, or 4k) with certain bit-depth and further compressed by libaom (an AV1 codec library) at QPs {12, 20, 32}.\n48. More recently, Kapoor et al.[43]constructed one of the first databases for data-driven image banding assessment models.\n49. This research included about 1,440 images shot from over 600 pristine HD videos with a resolution of 1920 × 1080.\n50. Six levels of bit-depth quantization in luminance and chrominance channels are introduced to obtain different intensities of banding.\n51. Meanwhile, the banding images were semi-automatically segmented and labeled into banded and non-banded to form a patch-level banding dataset, which allows for training machine learning-based and deep learning-based banding classification methods.\n52. However, to the best of our knowledge, thus far there still lacking a benchmarking dataset in the banding detection and the corresponding banding IQA domain.\n53. Researchers either resort to image/video quality datasets that do not aim at banding distortion or build a small, attribute-restricted, in-house dataset by themselves.\n54. This motivates us to construct a large-scale subjective assessment database focus on the perceived bandingaffected image quality.\n55. Early research on banding detection mainly focuses on false contour identification, which aims to find the wrong boundary rather than a "true" region edge in the image.\n56. Authors in[31]-[33]utilized monotonicity or non-monotonicity features of local support regions including the gradient, contrast, variance, and entropy information to measure the loss of low-amplitude detail caused by banding.\n57. However, these works ignored the perceptual characteristics of the human visual system (HVS) and thus did not perform a good correlation with subjective tests.\n58. Another banding detection strategy is conducted at the pixel-level estimation and segmentation.\n59. Bhagavathy et al.[30]proposed to identify banding artifacts by calculating the likelihood of pixel difference.\n60. Baugh et al.[34]measured the severity of banding based on the number of a group of connected pixels with the same color.\n61. Wang et al.[39]first detected uniform segments to find possible banding areas and further incorporated edge features (e.g.length and contrast) to capture false boundaries.\n62. Nevertheless, these kinds of methods are typically sensitive to edge noise and are computationally expensive, causing limited application in realtime scenarios.\n63. Towards addressing these problems, Tu et al.[42]presented a completely no-reference banding detection method, which combines various properties of HVS with a number of preprocessing steps to refine banding edge detection.\n64. Instead of regarding banding detection as a false edge detection problem, Tandon et al.[41]heuristically utilized the effect of contrast sensitivity function (CSF) on banding visibility and its dependence on spatial frequency.\n65. Based on this, Krasula et al.[40]further compared the banding annoyance with more commonly studied compression artifacts and proposed a banding-aware video quality metric.\n66. In recent years, deep learning approaches have prevailed in various VQA tasks.\n67. As the pioneering work, Kapoor et al.[43]developed an automated CNN-based banding detector for the first time, which is a simple two-stage algorithm and gives rise to devising other learning-based techniques.\n68. In this work, we build a large-scale banding database and propose a data-driven banding indicator that can generate pixelwise banding visibility maps with corresponding subjectively  consistent quality scores by combining human visual mechanisms and deep learning techniques.\n69. Subjective banding image quality assessment facilitates the development of automatic objective banding image and video quality models.\n70. We created the largest banding database in existence, denoted as the BAND-2k database, which consists of 2,000 banding distorted images and over 214,000 patch-level banding class labels.\n71. Then, a subjective experiment was conducted to obtain the mean opinion scores (MOS) of the BAND-2k database.\n72. The workflow of the banding database construction is shown in Fig.1.\n73. To build a content-rich and balanced database, we manually collected source videos including computer graphics (CG), usergenerated content (UGC), and professionally-generated content (PGC) from two popular media websites Bilibili.comand Youtube.com.\n74. Then, 885 clips with multiple spatial resolutions (i.e., 4096×2160, 3840×2160, 1920×1080) and frame rates (i.e., 60, 50, 30, 25) are chosen as candidate.\n75. Note that videos on the mentioned websites are firstly annotated by the community with assigned a number of favorites, views, and downloads.\n76. These statistics correlate with the content and quality of a video, which guides our choices to some extent.\n77. All videos selected on the website are released under an appropriate creative commons (CC) license that allows further editing and redistribution.\n78. After content selection, we further unified the format of all video clips, especially the spatial resolution and the pixel format, which avoid the effect of other facts on visual quality.\n79. Concretely, we first converted the frame rate of the original clips to 25fps, which is to reduce the storage pressure while ensuring the graphics quality.\n80. Considering the commonly used aspect ratio of the user interface and displays is 16:9, we cropped the partially unqualified videos rather than shrinking images unevenly.\n81. Then, we downsampled the trimmed spatial resolution 3840×2160 to a lower resolution -1920×1080 for the following subjective study.\n82. Before conducting the following experiments, we manually removed the videos that are either too dark or bright, overly blurry or colorful, which helps to obtain more reliable subjective assessments.\n83. To avoid redundancy and to make sure the diversity of selected contents, we also conducted attribute analysis studies on the selected images.\n84. Four metrics that correlated with human perception, i.e., contrast, brightness, sharpness, and colorfulness, are adopted as content diversity metrics.\n85. All video attributes are calculated on every 10 frames to reduce computational complexity, which are then averaged over frames of each video sequence.\n86. Fig.2shows the distribution of attributes extracted from the selected videos.\n87. • Contrast: The contrast metric is simply defined as the standard deviation of pixel gray-scale intensities[25].\n88. • Colorfulness: The colorfulness metric is measured by the R, G, and B components[25].\n89. We first compute two matrices of differences between channels rg = R -G and yb = 1 2 (R + G) -B.\n90. Then, the colorfulness metric can be calculated as µ 2 rg + µ 2 yb + σ 2 rg + σ 2 yb , where µ and σ are the mean and standard deviation of their corresponding terms, respectively.\n91. • Sharpness: The cumulative probability of blur detection (CPBD) metric[24]is used to measure the image sharpness, which estimates the probability of detecting blur at each edge.\n92. • Brightness: The brightness of an image is obtained directly from the pixel gray intensities in R, G, and B channels.\n93. Finally, the number of source videos are reduced to 873.\n94. Fig.3displays thumbnails for 30 selected representative video clips.\n95. To simulate authentic banding artifacts that exist in real viewing scenarios, we introduced four encoding/transcoding strategies including H.264/AVC[28], H.265/HEVC[27], VP9[38], and bit-depth manipulation[43]with fifteen quantization schemes in total.For H.264/AVC and H.265/HEVC, considering the range of their quantization parameter (QP in ffmpeg), we selected three typical QP values, namely, QP = {18, 33, 43}.\n96. This is because that coded video clips with a QP value smaller than 18 normally provide perceptual lossless quality, while coded video clips with a QP value larger than 43 will not be able to offer adequate quality, which may affect the subjective test of banding artifacts.\n97. Similarly, for the VP9 encoder, the QP values under our close inspection are chosen as {28, 45, 60}.\n98. Moreover, we applied the same quantization strategies as in[43], which introduces banding distortion by scaling bit-depth in luminance and chrominance channels.\n99. Here, the color coding scheme YCbCr4:2:0 is applied for maximum compatibility.\n100. To sum up, we generated fifteen levels of banding with different intensities and shapes to enhance the diversity of the database.\n101. The banding database is then built by extracting frames from the distorted video clips, resulting in 2,000 images with a resolution of 1920×1080.\n102. Fig.4shows the visualization results of banding exacerbated images.\n103. In the process of image patch labeling, we initially intend to extract image patches of size 235×235 from banding images directly and perform annotation operations.\n104. However, considering the theoretical number of patches, it will cost a lot of manpower and time to label patch by patch.\n105. Therefore, we first segmented the banding images roughly and label them into banded and non-banded regions.\n106. Then, labelled image patches are generated from these segmented and labelled images by a sliding window.\n107. Specifically, we followed the same demarcation of the banded and non-banded images in[43]that a patch is labelled as banded if it has more than 30% overlap with banded regions in the image.\n108. Eventually, a banding dataset containing 2,000 distorted images with 1920×1080 resolution and 214,324 labelled image patches of size 235×235 is built.\n109. To the best of our knowledge, it is the largest banding dataset in existence, which enables training various machine/deep learning based banding detection models and facilitates the development of image/video debanding techniques.\n110. TableIIreports the composition of labelled image patch dataset.\n111. It can be observed that the number of banded patches is a bit smaller than non-banded patches due to the fact that banding usually appears in smooth background areas.\n112. As shown in Fig.1, the subjective quality study contains four steps.\n113. In addition to preparing the experimental environment, subjects should pass the qualification test first to participate in the study.\n114. After the subjective rating, all resulting scores need to be analyzed and examined before generating the final mean opinion score (MOS).\n115. 1) Experimental Environment Setting: In this study, a total of 25 inexperienced subjects are gathered in a laboratory environment, where relevant experimental configuration must satisfy the following requirements: • Considering the viewing effect, desktops and laptops are allowed as displays.\n116. • The resolution of displays must be larger than or equal to 1920×1080 to show the images without spatial downsampling.\n117. • The viewing distance and optimal horizontal viewing angle are set as 1.9 times the height of the display and respectively.\n118. 500 recommendation[26].\n119. As a result, we used a 27-inch AOC Q27U2D monitor with a resolution of 2560×1440 for assessment with 25 subjects.\n120. Due to the large number of images to be assessed, we divided the dataset into ten sessions to avoid visual fatigue.\n121. Each session of tests took nearly 2 hours with a 30-minute break for each participant.\n122. 2) Qualification Test: Before starting the main experiment, subjects are required to pass a quiz to get the qualification of conducting follow-up experiments.\n123. Firstly, we manually selected 10 labeled banding exacerbated images beyond the database as training images to familiarize subjects with the operation interface and the goal of this subjective test.\n124. The quiz consists of two parts including banding classification and image-level quality rating.\n125. In banding classification, subjects were told to divide the test image into banded or non-banded.\n126. In image-level quality rating, subjects were instructed to focus on the coverage and intensity of banding areas, as well as the overall quality of images to get the final quality score.\n127. The rating scale is continuous from 0 to 100 while a higher value indicates more severe banding (i.e., more visible or occupying larger portion of the image).\n128. To make the quiz objective and fair, we take the banding images labeled by domain experts as the ground truth, while two existing banding metrics BBAND[42]and DBI[43]are used to determine the normal range of the rating scores.\n129. That is the quality scores rated by subjects should not exceed 20% of the above banding metrics.\n130. As a result, only 23 subjects with an accuracy above 80% in banding classification and subjective scores in the normal range were allowed to pass the quiz.\n131. Note that the steps for taking a quiz are the same as the main experiment, which aims to guarantee the consistency of results.\n132. 3) Formal Study: We adopted the single-stimulus (SS) method in this test.\n133. Ten golden images that have the acknowledged high quality or poor quality (assessed by BBAND[42]and DBI[43]) were added to each session for controlling the scoring deviations.\n134. Besides, 3 repeated images are randomly inserted into each session to ensure consistency of scores before and after subjects scoring.\n135. At last, 23 qualified subjects were asked to provide their opinions on the shuffled image groups.\n136. The resulting scores were collected and packed for further analysis.\n137. 4) Result Analysis: In total, 46,000 scores were collected by 23 qualified subjects in the main study phase.\n138. However, considering the qualification quiz cannot completely disallow those unreliable workers to muddle through to the main study while reliable subjects may also occasionally score odd values, which may be caused by the inter-individual differences in perceiving the quality of the unique characteristics of different contents.\n139. Thus, we further investigated the confidence of rating scores and removed outliers following the Grubbs\' test[23],[45].\n140. Concretely, let s = (s 1 , s 2 , . . ., s N ) denote a set of raw scores collected for one distorted image.\n141. The test statistics is defined as the maximum absolute deviation of a sample standard deviation (SD) unit from the sample mean, which is mathematically expressed as where s and σ s denote the mean and standard deviation of the sample, respectively.\n142. Then, given a significant level α, a sample is detected as an outlier if where t 2 α/(2N ),N -2 represents the upper critical value of the tdistribution with N -2 degrees of freedom and a significance level of α/(2N ).\n143. Empirically, we set the significance level α at 0.05.\n144. Then, a sample is identified as an outlier if its distance to the sample mean is larger than 2.5 times SD and is removed.\n145. Following the aforementioned steps, the total number of scores Fig.6.\n146. The overall architecture of the proposed method.\n147. Given a banding distorted image, it is first divided into patches.\n148. Then, the patch-level high-frequency map (HFM) and low-frequency map (LFM) are generated by Sobel operation and piece-wise smooth algorithm[19], respectively.\n149. After that, a dual-branch CNN (CNN H and CNN L ) is deployed to extract hierarchical features with different visual information and thus classify the patches into banded or non-banded.\n150. Lastly, a spatial frequency masking strategy is introduced to refine the banding map and calculate the image-level banding quality score.\n151. Note that the dual-branch networks do not share parameters.\n152. was reduced to 44,371, and MOS was created by averaging the scores for each image.\n153. Fig.5presents the histogram of MOSs over the entire database, showing a broad MOS distribution of banding images.\n154. In this section, we describe the architecture of the proposed banding evaluator in detail, as shown in Fig.6.\n155. As stated before, banding usually appear as high-frequency information in the smooth background, while humans perceive high-frequency texture regions and low-frequency plateau regions through different neural channels concurrently, and transfer the upper visual features into the cerebral cortex for final processing[17],[44].\n156. Inspired by this, we employ high-frequency maps (HFM) and low-frequency maps (LFM) as the deep learning network inputs, which represent the texture and structural information of the image respectively, to mimic the recognition mechanism of the human brain for better banding identification.\n157. High-frequency Maps.\n158. Since gradient has been widely used to represent edge information and has been confirmed beneficial to acquire high-frequency components with low computational cost[20]-[22], we apply the isotropic Sobel operator to each patch for enhancing the details of banding artifacts.\n159. Given an input patch I, the high-frequency map is calculated by where S x and S y are the horizontal and vertical isotropic Sobel operators, respectively." * " denotes the convolution operation.\n160. Low-frequency Maps.\n161. To maintain the principal content of the image and filter out the influence of high-frequency information, we use the piece-wise smooth algorithm[19]to generate the low-frequency map by minimizing a function for image approximation recovery: where L represents the low-frequency map, Ω and E denotes the image domain and edge set, respectively.\n162. P indicates the pixel and E dσ represents the total edge length.\n163. The coefficients α and β are positive regularization constants.\n164. An example of frequency maps is shown in Fig.7.\n165. To obtain the overall banding score, we first divide the banding image into 235×235 patches and apply CNN-based classification to each patch, resulting in a banding classification label for each patch, i.e., banded or non-banded.\n166. As shown in Fig.6, the proposed network consists of two parallel branches, namely CNN H and CNN L , which take the patch-level high-frequency map and low-frequency map as input, respectively.\n167. For each branch, we propose to use Resnet-50[18]as the backbone.\n168. Specifically, we incorporate the feature maps extracted from the first convolutional layer and the last layer of Resnet-50 as hierarchical visual features, which represent different visual information[46],[47]and can be used as predictive information to enhance the discrimination ability of the network for banding and non-banded regions.\n169. Afterward, the features extracted from two branches are concatenated first and reshaped into 128dimensional vectors through two fully-connected layers, which is further followed with the sigmoid activation function to output the final predicted label, namely banded or non-banded.\n170. Of note is that sharing parameters is extremely unfavorable for extracting low-and high-frequency features simultaneously, we thereby deploy two branches that work independently and do not share parameters.\n171. The loss function adopted here is binary cross entropy.\n172. With the trained CNN H and CNN L , each input patch is predicted to obtain a banding or non-banded label.\n173. To better guide the pre-processing and post-processing debanding algorithms, it is necessary to generate a quality score for the entire banding image.\n174. Since the visibility of edge is also affected by content, we further consider the effect of spatially varying content information on the local quality of human perception.\n175. As a consequence, we introduce the spatial frequency masking strategy to determine the weighting factor for the detected banding regions in each patch adaptively and thus obtain the image-level banding severity score while refining the visibility of banding artifacts.\n176. 1) Spatial Frequency Masking: The spatial frequency is defined as the activity level of an image, which establishes a filterbank based on the visual stimulus and is in accordance with HVS[16].\n177. In this paper, we propose to apply spatial frequency as an effective contrast criterion to banding measurement.\n178. Specifically, given an image of size I W × I H , divided into N × N patches, where I W and I H denote the number of columns and rows respectively.\n179. The column (CF k ) and row (RF k ) frequencies of the image patches are given by where I(x, y) is the pixel value of the image patch.\n180. Then, the resulting spatial frequency of an N × N patch is computed as where k is the number of patches (1 ≤ k ≤ I W I H N 2 ).\n181. Since most banding regions are likely to have large contrast including edges and textures, which should be assigned greater weights than the smooth and blurred areas.\n182. Accordingly, we design a banding visibility transfer function to express spatial frequency masking as a function of the local textural feature.\n183. The final spatial frequency masking weight is calculated at each patch as where γ is the scaling constant factor chosen to tune the shape of the transfer function.\n184. We used γ = 1.5 in our implementation.\n185. 2) Building a Banding Metric: The visibility of banding artifacts depends on the combination of multiple visual mechanisms.\n186. In this paper, we propose a simple but effective product model for attribute integration at each predicted banding patch to obtain the entire banding map (BM): where P k denotes the predicted label of k-th patch and w k is the weight parameter that scales the visibility of measured contours, i.e., gradient magnitude of the high-frequency map, |HFM k (i, j) | at region (i, j).\n187. Furthermore, inspired by previous psychovisual findings that the QoE of observers is dominated by those regions having poor quality[14],[42], we thereby leverage the worst p% percentile visual pooling to calculate an average banding score from the generated BM, where p is set to 80 in this experiment.\n188. As a result, the perceptual score of the overall banding image is defined as where M is the total number of patches in image I.\n189. T p% denotes the index set of the top p% non-zero pixel-wise value contained in k-th patch of the BM.\n190. In this section, we first present the experimental protocol in detail and then evaluate the performance of the proposed method on two tasks, namely patch-level banding classification and banding image quality assessment.\n191. After that, the ablation study and cross-database validation are conducted to prove the robustness and effectiveness of the proposed method.\n192. Finally, we test the computational efficiency of our method.\n193. A. Experimental Protocol 1) Databases and Settings: We choose two databases to train and test the effectiveness of the proposed banding IQA method, which are the database released in[43]and our proposed BAND-2k database.\n194. The detail information of these two datasets can be found in TableI.\n195. The proposed model is implemented by PyTorch[13].\n196. Before training, we randomly split the training, validation, and testing set into 8:1:1 (as shown in TableII).\n197. We use the Adam optimizer with the initial learning rate set as 1e-4 and set the batch size as 32.\n198. The training process is stopped after 25 epochs.\n199. The resolution of each cropped patch is fixed to 235×235.\n200. All experiments on both the[43]database and the BAND-2k database are conducted repeatedly 10 times to obtain the mean performance.\n201. 2) Baseline Algorithms: We include a number of representative IQA algorithms in our evaluation as references to be compared against.\n202. These baseline methods include: • General FR IQA methods: We choose PSNR, SSIM[12], MS-SSIM[11], LPIPS[10]as baselines.\n203. These are the most commonly used FR IQA metrics in practical applications such as video coding, image enhancement, etc. • General NR IQA methods: BRISQUE[37], NIQE[36], NIMA[8], DBCNN[35], HyberIQA[9], and StairIQA[1].\n204. • Banding IQA methods: Considering that there exists few research on banding detection and quality assessment, we barely select the BBAND[42], CAMBI[41], VMAF BA[40]and DBI[43]metrics as comparisons.\n205. 3) Evaluation Criteria: To evaluate the IQA methods comprehensively, a total of seven evaluation indexes in two categories are adopted.\n206. For patch-level banding classification, we follow the common procedures as in[43]and utilize the area under the receiver operating characteristics (AUROC), the area under the precision-recall curve (AUPRC), and accuracy as the classification performance metrics.\n207. For banding image quality assessment, four mainstream metrics are selected as the evaluation criteria: Spearman rank-order correlation coefficient (SRCC) and Kendall rank-order correlation coefficient (KRCC) measure the prediction monotonicity, while Pearson linear correlation coefficient (PLCC) and root mean square error (RMSE) are calculated to assess prediction consistency.\n208. Considering the potential nonlinear mapping characteristics between the objective scores and the subjective scores, we perform score alignment by mapping the predicted value using the five-parameter logistic function before calculating PLCC and RMSE values[7].\n209. Since our goal is to develop an effective banding IQA approach, we regard the identification of banding patches as an important preceding process to achieve accurate banding quality prediction.\n210. However, there exist few methods that are designed for banding classification and nearly all IQA methods produce scalar values only while failing in classifying banding regions directly.\n211. Therefore, we adopt a thresholding step to convert the single quality value into binary classification results as[43]does.\n212. Concretely, a half-interval search algorithm[48]is employed to find the optimal threshold value that can generate the best classification result.\n213. Based on the above premise, TableIIIreports the experimental results on both the database from[43]and the BAND-2k database.\n214. We highlight the best results in boldface.\n215. As compared to other state-of-the-art IQA methods, our proposed method yields the best overall performance in terms of AUROC, AUPRC, and accuracy.\n216. It is shown that most general FR IQA and NR IQA models perform poorly on the patch-level banding classification task while performing fairly well on other IQA tasks[1],[9],[35], indicating that the current approaches are not sensitive to banding distortion.\n217. Benefiting from the powerful feature extraction ability of CNNs, our proposed method and the customized NR IQA models for banding artifacts detection (DBI[43]) reach a significant performance in the discrimination of false contours.\n218. However, the performance of banding IQA method BBAND, CAMBI, and VMAF BA is surprisingly poor compared with other methods, which shows their vulnerability in identifying local banding artifacts from texture regions and are not suitable for patch-level banding identification.\n219. In addition, we investigate the computational complexity in terms of execution time per image patch.\n220. It can be observed that except for those traditional FR IQA models, our method achieves comparable speed in patch-level banding classification, which determines the prediction efficiency of the subsequent image-level quality assessment, making it a favorable choice in time-constrained scenarios.\n221. Considering that there exist no image banding databases with subjective scores attached in the public domain, we merely compare the performance of the proposed method with the baseline approaches on the proposed BAND-2k database.\n222. The results are shown in TableIV, from which we can observe that our proposed method largely surpasses all baselines in terms of SRCC, KRCC, and PLCC except for the RMSE.\n223. Compared to the secondbest model, our method achieves 18.07%SRCC improvements, 24.91% KRCC improvements, and 18.01%PLCC improvements on the BAND-2k database.\n224. We also present scatter plots of predictions versus MOS for better visualization in Fig.8.\n225. Overall,  (a) PSNR, (b) SSIM[12], (c) MS-SSIM[11], (d) LPIPS[10], (e) VMAF BA[40], (f) BRISQUE[37], (g) NIQE[36], (h) NIMA[8], (i) DBCNN[35], (j) HyperIQA[9], (k) StairIQA[1], (l) BBAND[42], (m) CAMBI[41], (n) DBI[43], and (o) Ours.\n226. Fig.9. Visual comparisons of the banding map results.\n227. From top to bottom are banding images and their corresponding banding maps generated by BBAND[42], DBI[43], and our proposed method, respectively.\n228. The first five columns of images from left to right are from BAND-2k, while the rest images are from[43].\n229. the performance of traditional FR IQA models such as PSNR, SSIM, and MS-SSIM is remarkably inferior in banding images assessment and is uncorrelated with the MOS, which is consistent with the numerical results presented in TableIV.\n230. The reason is that PSNR and SSIM-based methods do perform not well on tiny, regional, and content-independent banding distortion while lacking the consideration for the mechanism of HVS.\n231. It is also worth mentioning that the deep CNN architecturebased methods (DBCNN[35], HyperIQA[9], and StairIQA[1]), despite performing well on LIVEC[6], KonIQ-10k[29], and other universal image quality evaluation databases[4],[5], underperformed our proposed model by a notable margin on the BAND-2k database.\n232. (2) the perception of banding artifacts is explicitly intensity-aware.\n233. These are the issues that the CNN-based approaches above do not take into account.\n234. To some extent, banding distortion is more like a kind of local distortion than common global distortion such as Gaussian noise, blur, and dither.\n235. This suggests that it is potentially valuable to integrate some local texture, edge, contrast, or other visibility-related features into quality prediction models when assessing banding images.\n236. Fortunately, with the help of the proposed scoring strategy, our method gains the ability to convert the area range of identified banding regions to indicative annoying scores.\n237. Surprisingly, the banding IQA methods BBAND[42], CAMBI[41], and VMAF BA[40], however, did not perform very well on the BAND-2k dataset.\n238. We infer that this is due to differences in the test environment.\n239. First, the source videos selected in CAMBI and VMAF BA are from the internal Netflix 4K catalogue while the source contents in BAND-2k are collected from the public streaming websites with different resolutions, leading to an uneven quality level.\n240. Second, the methods of artificially introducing banding distortion are different.\n241. 264 compression techniques are used to generate the banding artifacts, limiting the adaptive ability of the algorithm in other scenarios.\n242. For further investigation, we compare our method with other two methods, i.e., BBAND[42], DBI[43], and visually study the generated banding maps to verify their effectiveness in banding detection.\n243. The visualization results are shown in Fig.9, where seven representative banding images are selected for reference.\n244. As shown, the banding maps generated from BBAND are mostly disordered and have a lot of discontinuity while the banding maps generated by DBI are too vague to locate the exact location of banding artifacts, making it difficult to develop pixel-level debanding techniques.\n245. Overall, we can compendiously conclude that: (1) in comparison with BBAND and DBI, banding maps computed by our proposed method could provide an accurate, clear indication for banding artifacts; (2) in comparison with BBAND and DBI, the quality prediction of banding images guided by our method could achieve a high consistency with HVS.\n246. Moreover, to make a statistically meaningful comparison among different IQA methods, we further conduct the widely used F-test[2],[3]to assess the statistical significance of the proposed method.\n247. Based on the assumption that the model\'s prediction residuals follow the Gaussian distribution, the lefttailed F-test with a confidence level of 95% is performed on the residuals of every two IQA models.\n248. The results of significance tests on the BAND-2k database are shown in Fig.10.\n249. A value of \'1\' (colored in green) indicates that the model in the row is significantly better than the model in the column, while a value of \'0\' (colored in red) indicates that the model in the row is not significantly better than the model in the column.\n250. It is shown that our proposed method performs significantly better than other models, which is consistent with the observations from the above comparison experiments.\n251. In this section, we explore the effectiveness of our model\'s design philosophy.\n252. To verify the importance of the dual-branch (DB) architecture, the baseline variants use the same backbone as the proposed method, except that only a single branch (SB) is reserved.\n253. Then, we use the original banding images as the input while removing the frequency map generation module (SB-I).\n254. Besides, the high-frequency maps and low-frequency maps generation modules are retained respectively (SB-HFM and SB-LFM).\n255. It can be observed from Table V that SB-LFM achieved the worst results, since the low-frequency map filters out the highfrequency banding information to a certain extent, which reduces the ability of the model to identify false contours.\n256. SB-I performs significantly better than SB-HFM and SB-LFM, resulting from that it contains richer image information, but it is still inferior to our method, which demonstrates the effectiveness of the dualbranch scheme.\n257. To investigate the effect of the banding feature extraction, we further design two variants with different input combinations of frequency maps.\n258. First, the high-frequency map was taken as the input of both the CN N H and CN N L layers (DB-HFM).\n259. Then, we replace the inputs with the low-frequency maps (DB-LFM).\n260. As shown in TableV, the performance of DB-HFM and DB-LFM is far apart from our approach, which matches our hypothesis that the high-frequency texture information contained in HFM and the low-frequency background information contained in LFM are crucial to enhance the capacity of discernment for banding artifacts.\n261. Therefore, we may conclude that our model is the most suitable model among those compared variants in terms of both banding classification and IQA applications.\n262. Due to the effects of different compression techniques, shooting equipment, scenes, etc., the image content and banding distortions may vary significantly in practical applications.\n263. For the database[43], it only includes limited types of image sources and means of triggering banding distortion.\n264. As a result, we conduct a cross-database validation to verify the generalizability of the proposed model, wherein the database presented by[43]and BAND-2k are included.\n265. That is, we trained the model on one full database and report the test performance on the other.\n266. We mainly compare the proposed method with four learningbased models, i.e., DBCNN[35], HyperIQA[9], StairIQA[1], and DBI[43].\n267. Since MOS information is not provided in the database[43], which is an essential part of methods training, we condensed part of the experiments.\n268. TableVIand VII report the experimental results in terms of patch-level banding classification and image quality assessment.\n269. We can observe that our proposed method generalization between database[43]and BAND-2k was surprisingly good.\n270. Besides, it is worth noting that the performance of these methods trained on the BAND-2k has improved a little compared to the previous versions that were trained on the database[43], which further demonstrates the superiority of the proposed database BAND-2k.\n271. The efficiency of an image quality prediction model is of great importance in practical industrial deployments.\n272. Therefore, we measured the average running time of the compared IQA models, as shown in TableVIII.\n273. The experiments were performed in MATLAB R2021a and Python 3.7 under Windows 10 64bit system on a Lenovo laptop with Intel Core i5-9300HF CPU@2.4GHz,16GB RAM, and NVIDIA GTX 1660Ti 6G GPU.\n274. It can be observed that the proposed method achieves a reasonable running time among the FR, NR, and other bandingspecified IQA algorithms.\n275. Generally, the execution time of classical IQA algorithms is significantly less than learning-based methods.\n276. Simpler NSS-based models such as BRISQUE and NIQE still show competitive efficiency relative to CNN models while exhibiting inferior performance in banding image quality assessment.\n277. For CAMBI and VMAF BA , we use the officially launched software package, which is based on the stand-alone C library libvmaf and therefore surpasses other methods in speed.\n278. Moreover, unlike the general quality evaluation using regression to predict scores, the patch-wise prediction strategy that we adopted may increase the complexity.\n279. Note that although we deployed a more complex network structure, a nearly 10 times speedup has been seen when comparing DBI with our method since the sliding window mechanism[43]is removed.\n280. In this paper, we conduct a comprehensive exploration of banding images from both subjective and objective perspectives.\n281. Specifically, we construct the largest ecologically valid banding IQA database to date named BAND-2k database, which consists of 2,000 banding images generated by fifteen compression and quantization schemes, achieving several times larger in number and diversity than the existing banding dataset.\n282. The construction process of the database, including distortion content preparation, subjective test procedure, and the removal of outlying data, is described in detail in this paper.\n283. Relying on this database, we proposed a novel banding evaluator using the frequency characteristic of banding artifacts, which models the banding as high-frequency artifacts that contained in the low-frequency smoothing region.\n284. A dual-branch CNN is devised to extract hierarchical features to classify the banding regions, upon which we introduce the spatial frequency masking to refine and compute an overall banding score.\n285. Experimental results show that our proposed method outperforms the baseline algorithms significantly in patch-level banding classification and banding IQA tasks.\n286. We believe that our study will benefit further development, calibration, and benchmarking of banding IQA models.\n\n请严格按照JSON格式返回：{"labels": [0,1,0,...]}，不要包含任何其他文字。'}], 'model': 'qwen-plus', 'response_format': {'type': 'json_object'}}}
2025-10-14 21:56:28,461 - DEBUG - Sending HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
2025-10-14 21:56:28,462 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-10-14 21:56:28,462 - DEBUG - send_request_headers.complete
2025-10-14 21:56:28,462 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-10-14 21:56:28,463 - DEBUG - send_request_body.complete
2025-10-14 21:56:28,463 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-10-14 21:56:36,683 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'vary', b'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding'), (b'x-request-id', b'737961f2-d780-4153-bae1-ef44f08eb8e1'), (b'x-dashscope-call-gateway', b'true'), (b'content-type', b'application/json'), (b'req-cost-time', b'8190'), (b'req-arrive-time', b'1760450179622'), (b'resp-start-time', b'1760450187812'), (b'x-envoy-upstream-service-time', b'8125'), (b'content-encoding', b'gzip'), (b'date', b'Tue, 14 Oct 2025 13:56:27 GMT'), (b'server', b'istio-envoy'), (b'transfer-encoding', b'chunked')])
2025-10-14 21:56:36,684 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-14 21:56:36,685 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-10-14 21:56:36,685 - DEBUG - receive_response_body.complete
2025-10-14 21:56:36,685 - DEBUG - response_closed.started
2025-10-14 21:56:36,685 - DEBUG - response_closed.complete
2025-10-14 21:56:36,685 - DEBUG - HTTP Response: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "200 OK" Headers({'vary': 'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding', 'x-request-id': '737961f2-d780-4153-bae1-ef44f08eb8e1', 'x-dashscope-call-gateway': 'true', 'content-type': 'application/json', 'req-cost-time': '8190', 'req-arrive-time': '1760450179622', 'resp-start-time': '1760450187812', 'x-envoy-upstream-service-time': '8125', 'content-encoding': 'gzip', 'date': 'Tue, 14 Oct 2025 13:56:27 GMT', 'server': 'istio-envoy', 'transfer-encoding': 'chunked'})
2025-10-14 21:56:36,686 - DEBUG - request_id: 737961f2-d780-4153-bae1-ef44f08eb8e1
2025-10-14 21:56:36,686 - DEBUG - API request completed in 8.24 seconds
2025-10-14 21:56:36,687 - DEBUG - Raw model response: {"labels": [1,0,0,1,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]}
2025-10-14 21:56:36,687 - INFO - Successfully processed 151 labels
2025-10-14 21:56:36,687 - ERROR - Label count mismatch for BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment
2025-10-14 21:56:36,687 - INFO - Evaluating paper 8/18: Comprehensive Named Entity Recognition on CORD_19 with Distant or Weak Supervision
2025-10-14 21:56:36,687 - INFO - Starting model prediction
2025-10-14 21:56:36,687 - INFO - Attempt 1 of 5
2025-10-14 21:56:36,689 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-6119c15b-59f4-44a8-a10f-d6bc0c672187', 'json_data': {'messages': [{'role': 'system', 'content': '你是一个专门用于识别学术论文中数据集描述的助手。你需要判断每个输入的句子是否描述了研究中使用的数据集。请确保：\n    1. 只输出JSON格式的结果\n    2. 结果格式必须为 {"labels": [0,1,0,...]}\n    3. 不要输出任何其他文字或解释'}, {'role': 'user', 'content': '你是一位专业的学术论文数据集描述识别专家。请仔细分析下面这篇论文中的每个句子，判断它们是否描述了本研究特定构建的数据集信息。\n\n    论文标题: Comprehensive Named Entity Recognition on CORD_19 with Distant or Weak Supervision\n\n    判断标准:\n    判断为1(数据集描述)的标准：\n    句子必须直接描述本研究特定构建的数据集,包括:\n    1. 该数据集的构建过程和方法\n    2. 该数据集的具体构成和规模\n    3. 该数据的来源和收集方式\n    4. 该数据的预处理步骤\n    5. 该数据集的可获取方式(如发布地址)\n    6. 或者其他直接或者间接描述了该数据集的句子\n\n    所有其他类型的句子均标记为0。\n    \n    请对每个句子进行分析，返回一个JSON格式的标签数组。格式要求：\n    1. 必须是有效的JSON格式\n    2. 只包含labels字段，值为0和1组成的数组\n    3. 数组长度必须与句子数量相同\n    4. 示例格式：{"labels": [0,1,0,1,...]}\n\n    需要判断的句子：\n\n    1. Abstract: We created this CORD-NER dataset with comprehensive named entity recognition (NER) on the COVID-19 Open Research  Dataset Challenge (CORD-19) corpus (2020-03-13).\n2. This CORD-NER dataset covers 75 fine-grained entity types: In addition to the common biomedical entity types (e.g., genes, chemicals and diseases), it covers many new entity types related explicitly to the COVID-19 studies (e.g., coronaviruses, viral proteins, evolution, materials, substrates and immune responses), which may benefit research on COVID-19 related virus, spreading mechanisms, and potential vaccines.\n3. CORD-NER annotation is a combination of four sources with different NER methods.\n4. The quality of CORD-NER annotation surpasses SciSpacy (over 10% higher on the F1 score based on a sample set of documents), a fully supervised BioNER tool.\n5. Moreover, CORD-NER supports incrementally adding new documents as well as adding new entity types when needed by adding dozens of seeds as the input examples.\n6. We will constantly update CORD-NER based on the incremental updates of the CORD-19 corpus and the improvement of our system.\n7. Coronavirus disease 2019 (COVID-19) is an infectious disease caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2).\n8. The disease was first identified in 2019 in Wuhan, Central China, and has since spread globally, resulting in the 20192020 coronavirus pandemic.\n9. On March 16th, 2020, researchers and leaders from the Allen Institute for AI, Chan Zuckerberg Initiative (CZI), Georgetown University\'s Center for Security and Emerging Technology (CSET), Microsoft, and the National Library of Medicine (NLM) at the National Institutes of Health released the  Open Research Dataset (CORD-19)1of scholarly literature about COVID-19, SARS-CoV-2, and the coronavirus group.\n10. Named entity recognition (NER) is a fundamental step in text mining system development to facilitate COVID-19 studies.\n11. There is a critical need for NER methods that can quickly adapt to all the COVID-19 related new types without much human effort for training data annotation.\n12. We created this CORD-NER dataset2with comprehensive named entity annotation on theCORD-19 corpus (2020-03-13).\n13. This dataset covers 75 fine-grained named entity types.\n14. CORD-NER is automatically generated by combining the annotation results from four sources.\n15. In the following sections, we introduce the details of CORD-NER dataset construction.\n16. We also show some NER annotation results in this dataset.\n17. The input corpus is generated from the 29,500 documents in theCORD-19 corpus (2020-03-13).\n18. We first merge all the meta-data (all sources metadata 2020-03-13.csv) with their corresponding full-text papers.\n19. Then we create a tokenized corpus (CORD-NER-corpus.json)for further NER annotations.\n20. The input corpus is a combination of the "title", "abstract" and "full-text" from the CORD-19 corpus.\n21. We first conduct automatic phrase mining and tokenization on the input corpus using AutoPhrase(Shang et al., 2018a).\n22. Then we do a second round of tokenization with Spacy3on the phrase-replaced corpus.\n23. We found that keeping the AutoPhrase results will significantly improve the distantly-and weakly-supervised NER performance.\n24. CORD-NER annotation is a combination of four sources with different NER methods: 1.Pre-trained NER on 18 general entity types from Spacy using the model "en core web sm".\n25. 2.Pre-trained NER on 18 biomedical entity types from SciSpacy 4 using the models "en ner bionlp13cg md" and "en ner bc5cdr md".\n\n请严格按照JSON格式返回：{"labels": [0,1,0,...]}，不要包含任何其他文字。'}], 'model': 'qwen-plus', 'response_format': {'type': 'json_object'}}}
2025-10-14 21:56:36,691 - DEBUG - Sending HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
2025-10-14 21:56:36,691 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-10-14 21:56:36,691 - DEBUG - send_request_headers.complete
2025-10-14 21:56:36,691 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-10-14 21:56:36,691 - DEBUG - send_request_body.complete
2025-10-14 21:56:36,691 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-10-14 21:56:39,032 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'vary', b'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding'), (b'x-request-id', b'53013159-e9f6-4c4d-916d-426abf6da8a8'), (b'x-dashscope-call-gateway', b'true'), (b'content-type', b'application/json'), (b'req-cost-time', b'2303'), (b'req-arrive-time', b'1760450187851'), (b'resp-start-time', b'1760450190155'), (b'x-envoy-upstream-service-time', b'2302'), (b'content-encoding', b'gzip'), (b'date', b'Tue, 14 Oct 2025 13:56:29 GMT'), (b'server', b'istio-envoy'), (b'transfer-encoding', b'chunked')])
2025-10-14 21:56:39,032 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-14 21:56:39,033 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-10-14 21:56:39,033 - DEBUG - receive_response_body.complete
2025-10-14 21:56:39,033 - DEBUG - response_closed.started
2025-10-14 21:56:39,034 - DEBUG - response_closed.complete
2025-10-14 21:56:39,034 - DEBUG - HTTP Response: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "200 OK" Headers({'vary': 'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding', 'x-request-id': '53013159-e9f6-4c4d-916d-426abf6da8a8', 'x-dashscope-call-gateway': 'true', 'content-type': 'application/json', 'req-cost-time': '2303', 'req-arrive-time': '1760450187851', 'resp-start-time': '1760450190155', 'x-envoy-upstream-service-time': '2302', 'content-encoding': 'gzip', 'date': 'Tue, 14 Oct 2025 13:56:29 GMT', 'server': 'istio-envoy', 'transfer-encoding': 'chunked'})
2025-10-14 21:56:39,034 - DEBUG - request_id: 53013159-e9f6-4c4d-916d-426abf6da8a8
2025-10-14 21:56:39,035 - DEBUG - API request completed in 2.35 seconds
2025-10-14 21:56:39,035 - DEBUG - Raw model response: {"labels": [1,1,1,0,1,1,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1]}
2025-10-14 21:56:39,035 - INFO - Successfully processed 25 labels
2025-10-14 21:56:39,045 - INFO - Evaluating paper 9/18: DCQA_ DOCUMENT_LEVEL CHART QUESTION ANSWERING TOWARDS COMPLEX REASONING AND COMMON_SENSE UNDERSTANDING
2025-10-14 21:56:39,045 - INFO - Starting model prediction
2025-10-14 21:56:39,045 - INFO - Attempt 1 of 5
2025-10-14 21:56:39,045 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-7e5739e4-5903-4dbb-9c79-45ddc4ffdadf', 'json_data': {'messages': [{'role': 'system', 'content': '你是一个专门用于识别学术论文中数据集描述的助手。你需要判断每个输入的句子是否描述了研究中使用的数据集。请确保：\n    1. 只输出JSON格式的结果\n    2. 结果格式必须为 {"labels": [0,1,0,...]}\n    3. 不要输出任何其他文字或解释'}, {'role': 'user', 'content': '你是一位专业的学术论文数据集描述识别专家。请仔细分析下面这篇论文中的每个句子，判断它们是否描述了本研究特定构建的数据集信息。\n\n    论文标题: DCQA_ DOCUMENT_LEVEL CHART QUESTION ANSWERING TOWARDS COMPLEX REASONING AND COMMON_SENSE UNDERSTANDING\n\n    判断标准:\n    判断为1(数据集描述)的标准：\n    句子必须直接描述本研究特定构建的数据集,包括:\n    1. 该数据集的构建过程和方法\n    2. 该数据集的具体构成和规模\n    3. 该数据的来源和收集方式\n    4. 该数据的预处理步骤\n    5. 该数据集的可获取方式(如发布地址)\n    6. 或者其他直接或者间接描述了该数据集的句子\n\n    所有其他类型的句子均标记为0。\n    \n    请对每个句子进行分析，返回一个JSON格式的标签数组。格式要求：\n    1. 必须是有效的JSON格式\n    2. 只包含labels字段，值为0和1组成的数组\n    3. 数组长度必须与句子数量相同\n    4. 示例格式：{"labels": [0,1,0,1,...]}\n\n    需要判断的句子：\n\n    1. Abstract: Visually-situated languages such as charts and plots are omnipresent in real-world documents.\n2. These graphical depictions are human-readable and are often analyzed in visuallyrich documents to address a variety of questions that necessitate complex reasoning and common-sense responses.\n3. Despite the growing number of datasets that aim to answer questions over charts, most only address this task in isolation, without considering the broader context of document-level question answering.\n4. Moreover, such datasets lack adequate common-sense reasoning information in their questions.\n5. In this work, we introduce a novel task named document-level chart question answering (DCQA).\n6. The goal of this task is to conduct document-level question answering, extracting charts or plots in the document via document layout analysis (DLA) first and subsequently performing chart question answering (CQA).\n7. The newly developed benchmark dataset comprises 50,010 synthetic documents integrating charts in a wide range of styles (6 styles in contrast to 3 for PlotQA and ChartQA) and includes 699,051 questions that demand a high degree of reasoning ability and common-sense understanding.\n8. Besides, we present the development of a potent question-answer generation engine that employs table data, a rich color set, and basic question templates to produce a vast array of reasoning question-answer pairs automatically.\n9. Based on DCQA, we devise an OCR-free transformer for document-level chartoriented understanding, capable of DLA and answering complex reasoning and common-sense questions over charts in an OCR-free manner.\n10. Our DCQA dataset is expected to foster research on understanding visualizations in documents, especially for scenarios that require complex reasoning for charts in the visually-rich document.\n11. We implement and evaluate a set of baselines, and our proposed method achieves comparable results.\n12. The emergence of visual language as a novel communicative tool, characterized by a tightly integrated interplay of visual and textual components, can be attributed to a confluence of factors, notably globalization, the growing intricacy of commerce and technology, and the convergence of lexicons from diverse fields that were once disparate[7].\n13. The prevalence of visually-situated language in various document types, such as academic, business, medical, and others, is markedly high[8][9][10].\n14. Gaining a comprehensive understanding of these graphical representations, such as charts and plots, is essential in extracting valuable and pragmatic insights from data[11].\n15. To conduct data analysis, individuals frequently pose intricate queries that require common-sense and arithmetic or logical operations pertaining to graphical representations.\n16. Answering such inquiries demands a substantial level of cognitive and reasoning exertion, as individuals are required to be aware of common sense and integrate numerous logical operations, including but not limited to retrieving entities, comparing trends, calculating averages, finding extremum, etc.Typically, the chart question answering (CQA) system[12]aims to generate the desired answer by taking a chart-question pair as input, constituting a fundamental function within the domain of intelligent document understanding (IDU)[13].\n17. Despite the CQA task has drawn ever-growing attention from visual question answering communities in recent years, existing datasets has encountered certain obstacles: (i) Notably, while charts constitute crucial components of documents, the majority of current datasets treat the CQA task solely at the question-answering level, without taking into account its significance as a document-level task.\n18. (ii) Questions generally prioritize reasoning or visual features, potentially losing sight of common sense information that individuals typically consider when posing questions, which is a misalignment with the typical questioning habits of individuals.\n19. (iii) The quantity of chart types, as exemplified by PlotQA and ChartQA datasets, is comparatively restricted (only three).\n20. Such a limited representation fails to capture the broad range of chart styles that are present in real-world documents.\n21. Furthermore, in real-world settings, users typically first identify the location of charts in documents before querying them.\n22. However, directly analyzing document layout poses challenges, as charts lack explicit annotations.\n23. Manually annotating datasets for document layout analysis related to charts is remarkably laborious and time-consuming.\n24. This motivates an automated system to generate annotations associated with charts, eliminating the need for costly labeled data collection.\n25. Such a system would locate charts in arbitrary documents without annotations and produce bounding boxes highlighting them, providing chart-specific layout information without human intervention.\n26. Additionally, certain baseline models rely on obtaining high-quality optical character recognition (OCR) outcomes to extract the data table structure from the chart image.\n27. Therefore, current models\' efficacy generally relies upon the accuracy of OCR results.\n28. Nevertheless, incorporating an OCR-dependent approach for CQA system poses significant challenges.\n29. For one thing, commercially available OCR techniques often exhibit limited adaptability in addressing diverse languages or changes in the domain, which are commonly encountered in the context of charts.\n30. Such limitations may impede the generalization abil-ity of these methods.\n31. For another, the occurrence of errors during the OCR process is unavoidable, and such erroneous outcomes have the potential to propagate to the CQA system, thereby adversely affecting subsequent processes[14].\n32. To alleviate above issues, we go beyond the traditional dataset by presenting a large-scale document-level chart question answering dataset (DCQA).\n33. DCQA comprises 50,010 synthetic documents and 699,051 question-answer pairs generated using our customized semantic-rich question-answer generation engine.\n34. The dataset includes questions that focus on vision, complex reasoning, and common-sense knowledge.\n35. Common-sense knowledge reasoning primarily involves evaluating the ability of CQA models to distinguish between legend labels and entity names belonging to specific parent classes, and subsequently performing reasoning operations based on this discriminative ability.\n36. Each document in the DCQA includes a chart, unrelated images and a descriptive caption related to the chart.\n37. The language used in the DCQA dataset is English.\n38. The chart types exhibit a diverse range of styles and can be broadly categorized into six major types, namely Bar chart, Line plot, Pie chart, Scatter plot, Box plot, and Mixed chart, each of which is further divided into subtypes, yielding a total of 30 chart subtypes.\n39. Figure1displays some examples from DCQA.\n40. More examples are provided in Appendix E.\n41. Drawing upon the DCQA dataset, we further devise a transformer-based OCR-free architecture to perform document layout analysis and chart question answering.\n42. Initially, we exploit swin transformer[15]as the vision backbone to extract visual features of the input document.\n43. Next, the extracted features are fed into the detection component to perform document layout analysis[16].\n44. Upon successfully identifying the chart image, we extract the relevant visual content from the chart, which is then utilized as input to the textual decoder for answer prediction.\n45. This novel OCR-free architecture provides a plug-and-play solution for performing chart question answering directly from the document.\n46. In a nutshell, our contributions are as follows: • We present a comprehensive and extensive documentlevel chart question answering dataset, DCQA, which features a wide range of chart styles and includes question-answer pairs that incorporate complex reasoning and common-sense knowledge.\n47. The dataset\'s scale and diversity make it a valuable resource for researchers interested in developing and evaluating chart question answering models.\n48. • We conceptualize chart question answering as a documentlevel task and propose a transformer-based OCR-free model to effectively address this task.\n49. • We perform comprehensive experiments and thorough analyses on DCQA, verifying the efficacy of our model.\n50. To date, only a limited number of datasets have been explicitly designed for chart question answering.\n51. These datasets include FigureQA[1], DVQA[2], LEAF-QA[3], LEAFQA++[4], PlotQA[5]and ChartQA[6].\n52. Despite consisting of a diverse set of synthetic charts, FigureQA suffers from a lack of specificity in terms of chart element labeling, utilizing only generic titles and color names.\n53. Furthermore, the questions are limited to a few template-based formats with binary "yes/no" answers.\n54. DVQA is limited to a single chart type, namely the Bar chart, and suffers from inadequate semantic relations between the textual elements.(e.g., bar and legend labels are randomly selected words) as well as restricted Y-axis value ranges.\n55. Numeric answers are primarily integers in both the train and test sets and share the same values.\n56. As with FigureQA, all bar plots in DVQA are artificially generated, and the questions are based on a small number of templates.\n57. Both LEAF-QA and its extended version, LEAFQA++, are not publicly available.\n58. Besides, they share a significant limitation: the absence of regression question-answering pairs.\n59. This is evident from the question templates described in their reference and the discrete answer set employed.\n60. Although PlotQA is currently the largest publicly available dataset for CQA, it is limited by imbalanced question distribution, as it is heavily weighted towards data-related questions and lacks an appropriate proportion of queries pertaining to the visual characteristics of chart elements, including color and shape.\n61. Regarding ChartQA, it is the pioneer dataset to compile realworld charts with a blend of human-created QA pairs and machine-generated QA pairs.\n62. However, despite its innovative contribution, ChartQA is characterized by a limited size and encompasses only three distinct plot types.\n63. Furthermore, the paucity of question-answer pairs (two) per chart undermines the potential for a comprehensive understanding of the underlying information conveyed by these visualizations.\n64. This work presents a novel and intricate CQA dataset, which diverges from prior datasets in several respects.\n65. Firstly, DCQA is introduced, which reformulates the CQA task by integrating document layout analysis and chart question answering.\n66. Secondly, in addition to visual and complex reasoning questions, DCQA incorporates common sense-aware questions.\n67. Last but not least, DCQA covers a broad range of chart types1.\n68. In this section, we describe the construction of DCQA from the following four aspects:(i) Data collection, (ii) Chart generation, (iii) Automatic QA pair generation engine, and (iv) Document generation.\n69. The general workflow of the DCQA generation process is shown in Figure3.\n70. A detailed generation procedure is provided in Appendix A.\n71. Given the variability of chart styles in real-world scenarios, integrating real-world sources and randomly generated data for producing charts can augment the models robustness and adaptability to various chart formats encountered in practical scenarios.\n72. Drawing upon this observation, charts included in our dataset was derived by two means: utilizing real-world sources and randomly generated data.\n73. The detailed process of data collection is shown in Appendix A.\n74. We exploit Pyecharts1, a Python visualization tool library based on the Echarts[21]charting library, to generate our charts.\n75. Our DCQA contains six different chart styles: bar chart, line chart, scatter plot, box plot, pie chart, and combination chart (line and bar).\n76. These chart styles can be further divided into 30 sub-types (As shown in Figure2).\n77. The color of chart elements is randomly picked up from a color set Johndecember, which covers a wide range of colors (595).\n78. Besides, the chart presents two distinct styles of background, namely dark and light, of which the former was not previously observed in any of the CQA datasets.\n79. Detailed chart information is provided in Appendix B.\n80. Since the generated charts are from disparate data sources and encompass a wide range of topics, engaging a cadre of individuals with diverse backgrounds, experiences, and expertise is necessary to craft questions about the corresponding charts.\n81. We have meticulously curated a corpus of 573 charts spanning six categories, comprising data extracted from real-world and randomly generated sources, which serve as paradigmatic instances from which questions can be formulated.\n82. We commission a cohort of post-graduate students affiliated with our academic institution, and employees from Huawei company , to generate ten distinct questions for each of the selected charts, with an emphasis on reasoning and common sense awareness.\n83. We have obtained 5730 questions.\n84. After an exhaustive process of meticulous meetings and indepth discussions, we have successfully distilled a total of 324 question templates from the original pool of 5730 questions.\n85. Out of these templates, 204 are specifically tailored for visual and numeric reasoning, while 120 templates are dedicated to common sense reasoning.\n86. Code will be publicly available at github .\n87. Table2displays the statistics of the dataset.\n88. Details in Appendix C.2.\n89. Visual and numeric reasoning: These kinds of questions necessitate combing visual elements understanding and numerical reasoning techniques (e.g., sum, multiple, average, etc.).\n90. Integrating visual and numerical reasoning inquiries can facilitate CQA systems\' comprehension of chart content, as it encourages the concurrent utilization of their visual and analytic faculties, thereby enabling them to engage in a more profound exploration of the underlying message conveyed by the data and achieve a more precise interpretation of chart figures.\n91. Examples of this type of question are presented in Figure1(a) and (b).\n92. Common sense reasoning: Questions of this type demand combining common sense knowledge and numerical operation.\n93. Common sense is able to serve as a facilitator for CQA systems to gain a more profound understanding of the real-life background and context reflected by the data, thus accurately inferring the meaning behind the data.\n94. Meanwhile, numerical reasoning skills can allow readers to fathom the underlying interconnections and relationships of the data and infer potential outcomes and trends.\n95. The combination of both proficiencies can profoundly equip CQA models with diverse conceptualizations of chart content and enable them to increase the usefulness of data in comprehensively examining and scrutinizing data, identifying patterns and trends, and making predictions and decisions.\n96. Examples of this type of question are presented in Figure1 (c).\n97. Construction of the hierarchical entity database: Common sense reasoning is a crucial aspect of DCQA, which primarily involves evaluating a CQA model\'s ability to discriminate between legend labels and entity names that belong to a specific parent class and then perform reasoning operations based on this discriminatory capacity.\n98. Therefore, a hierarchical entity database with a tree-structured architecture and a well-defined set of parent-child relationships is necessary to serve as a source for both entity names and legend labels.\n99. The construction of the hierarchical entity database is expounded upon in Appendix C.1.\n100. Categorization of question difficulty levels: Additionally, the entire set of question templates has been classified into five distinct levels delineated by their respective levels of complexity, namely, beginner, elementary, intermediate, ad-  vanced, and expert.\n101. The statistic of question levels is displayed in Table3.\n102. The difficulty levels of the question templates are manually annotated based on the following criteria: (1) Beginner includes questions about the overall nature of a chart image, such as whether it is horizontal or vertical or the number of columns it contains, as well as retrieval for the value of a specific chart element.\n103. (2) Elementary primarily involves questions carrying out some form of operation on all chart elements within a chart, such as determining the maximum, minimum, median, or mean value.\n104. (3) Intermediate refers to questions that involve applying specific operations to chart elements that satisfy predetermined criteria, including but not limited to identifying the maximum, minimum, median, mean, sum, or difference of chart elements based on their color, legend, or numerical value.\n105. (4) Advanced questions demand performing composite operations on chart elements that meet a specific property (building upon the operations mentioned for intermediate questions), such as finding the sum or difference of two maximum values after they have been identified.\n106. (5) Building upon advanced questions, questions that involve common sense will be categorized as expert-level.\n107. In this paper, answers are generated through an automated process based on a customized set of procedures.\n108. Specifically, a solution step is designed for each question template, with each step representing an atomic operation that is implemented using specific functions to achieve its intended functionality.\n109. When solving specific questions to generate answers, the designed solution steps are followed by invoking corresponding functions, resulting in answers for the respec- Upon completing the question-answer pair generation process, extra analysis is conducted on the distribution of every answer type.\n110. It is noted that the highest proportion of the answer type in the dataset is the binary classification yes or no.\n111. However, the ratio between yes and no is severely imbalanced, with a vastly larger number of no responses compared to yes.\n112. As a result, a post-processing adjustment is necessary to address this language bias and prevent the model from exploiting the answer distribution pattern to output the answer without paying attention to the visual content.\n113. The debiasing procedure can be concisely described as: Firstly, filter out all question templates with Yes/No answers.\n114. Secondly, count the number of Yes/No answers for each Yes/No question template in the dataset.\n115. For each Yes/No question template, determine whether the number of Yes answers exceeds the number of No answers or vice versa.\n116. For the question with a larger proportion of answers, adjust the values of the replaceable modules in the question to change the answer to the less frequent one, and iterate this process until the number of Yes and No answers for the template are equal or differ by only 1.\n117. Most of the Yes/No question templates can be balanced by modifying the answer using the above approach.\n118. However, a few questions cannot be balanced this way, and their answers are not significantly  imbalanced.\n119. Therefore, we do not handle such question templates in this paper for now.\n120. Before debiasing, the dataset\'s overall proportion of yes and no answers was 35.16% and 64.84%, respectively.\n121. After debiasing, the overall proportion of yes and no answers in the dataset became 49.26% and 50.74%, respectively.\n122. The effectiveness of the debiasing is compared in Figure4, where the noticeable changes in the answer distribution of "Yes" and "No" before and after debiasing demonstrate that the debiasing method employed in this study effectively addresses the answer imbalance in binary questions.\n123. In accordance with the methodology outlined in[22], we utilize LaTex to generate synthetic documents that include diverse multimedia elements.\n124. In addition to the chart image, the generated document also incorporates other visual content 2  and textual content produced by ChatGLM[23,24].\n125. org of the synthetic document beyond the mere inclusion of the chart image, which is more consistent with real-world documents.\n126. Detailed document information is provided in Appendix D.\n127. In this section, we provide a comprehensive analysis of the experimental results to establish the validity of the recently developed DCQA dataset and verify the excellent efficacy of our proposed TOT-Doctor model through a comparative evaluation against other baselines.\n128. We compare the TOT-Doctor with the classical approaches include LayoutLMv2[25], LayoutLMv3[26], Pix2Struct[9], and MATCHA[27].\n129. • LayoutLMv3[26]is a multi-modal Transformer framework without the vision backbone that leverages reconstructive objectives for cross-modal alignment learning, showcasing notable generality in the context of document vision tasks.\n130. • Pix2Struct[9]is an image-to-text model tailored for visual language understanding, which is pre-trained on visually-rich screenshots of web pages with screenshot parsing objective.\n131. • MATCHA[27]is a Pix2Struct-based model pre-trained for chart underlying structure understanding and mathematical reasoning.\n132. Our study employs accuracy the primary evaluation metric, wherein the assessment of the predicted answers correctness depends on the nature of the answer type.\n133. In the case of textual answers, such as binary responses, entities, and integers, the evaluation criterion mandates that the predicted answer should match the ground truth exactly.\n134. For numerical answers in the form of floating-point values, it is not always feasible to expect that the predicted answer will precisely match the correct answer.\n135. Therefore, we consider the answer correct if it falls within 5% of the expected value.\n136. This section primarily supplements experimental configurations, including parameters such as batch size and learning rate, and outlines the preprocessing steps undertaken to ensure a fair comparison for the extractive model.\n137. Table4shows the detailed experimental setup, in which CE refers to Cross Entropy.\n138. All models were verified every 5000 iterations during training.\n139. In our implementation of LayoutLMv2 and v3, we employ a multi-step learning rate schedule.\n140. More specifically, we gradually decrease the learning rate by a factor of 2 after each epoch of training.\n141. For other models, we use the cosine scheduler to adjust the learning rate, where the number of warm-up steps is set to 1000.\n142. The LayoutLM series employs a model-based extraction approach, which requires the system to select answers from the optical character recognition (OCR) results.\n143. To enable the model to perform tasks such as binary classification (e.g., Yes/No), we have implemented a simple yet effective solution: we add two special characters, "Yes" and "No", to the OCR results.\n144. TOT-Doctor consists of two main components, namely the document layout analysis and the chart question answering.\n145. The model parameters of TOT-Doctor are calculated and found to be as follows: the encoder of the Swin Transformer used in the document layout analysis has 74M parameters, while the detector component has 48M parameters.\n146. The encoder of the Swin Transformer used in the chart question answering phase has 74M parameters, and the BART has 127M parameters.\n147. Based on the fine-grained document element annotations present within the DCQA dataset introduced in this paper, encompassing various elements such as chart image, picture, textual content, list, caption, header, footer, and page number, the document layout analysis model of the proposed ToT-Doctor framework was trained.\n148. The conclusive results of the document layout analysis testing on the test set are presented in Table5.\n149. Notably, the detection accuracy for chart image reached an impressive 99.901%, exhibiting a near-complete precision in accurately identifying their respective spatial position.\n150. This achievement serves as a robust foundational prerequisite for facilitating subsequent stage of chart question answering task.\n151. The experiment results of our proposed TOT-Doctor and other baselines are displayed in Table6.\n152. Due to the incapacity of the baseline to perform document layout analysis, we add our document layout analysis framework to them before conduct chart question answering.\n153. Based on the data listed in Table6, it can be seen that TOT-Doctor consistently surpasses its counterparts concerning the accuracy in both the validation and test sets, corroborating the efficacy of TOT-Doctor.\n154. It is noteworthy that despite LayoutLMv2 and LayoutLMv3 being reliant on OCR for obtaining the answers, their performance continues to lag behind our OCR-free TOT-Doctor.\n155. This observation proves the robustness and OCR error mitigation capabilities of the TOT-Doctor proposed in this study.\n156. Furthermore, in comparison to the latest state-of-the-art (SOTA) pre-trained visual language understanding model Pix2Struct, TOT-Doctor demonstrates superior performance, achieving a significant improvement of approximately 21.171% on the development dataset, and a respectable enhancement of 20.796% on the test dataset, respectively.\n157. The outstanding performance of our TOT-Doctor model underscores the significance of integrating vision and language features in an OCR-free manner to address the questions posed in DCQA effectively.\n158. The DCQA dataset incorporates five distinct question levels.\n159. In order to better discern the effectiveness of our proposed TOT-Doctor and other baselines, we conduct a performance analysis on each question level.\n160. The results of our analysis are presented in Table7for reference.\n161. It is evident from the results that TOT-Doctor consistently outperforms other baselines in all five levels of questions.\n162. Notably, TOT-Doctor exhibits superior performance on intermediatelevel questions, demonstrating its efficacy in directly applying specific operations such as identifying maximum, minimum, median, mean, and sum to chart elements or analyzing the differences of chart elements based on their color, legend, or numerical value.\n163. However, when encountering the subdifficult advanced-level questions involving composite oper-ations and the most challenging expert-level questions that necessitate commonsense understanding, the performance of all baselines, including TOT-Doctor, considerably decreases compared to the other three more tractable question levels.\n164. This implies that the ability for complex reasoning and commonsense understanding still requires further improvement for analyzing complex documents.\n165. Fig.6: Performance comparison between common sense and numerical reasoning questions on DCQA test set.\n166. We conduct additional experiments to evaluate the performance of the TOT-Doctor and baseline models on different question types.\n167. As discussed before, DCQA comprises two primary question types: visual and numeric reasoning and commonsense reasoning.\n168. Figure6presents the results of all baselines for each question type on the test set.\n169. Our proposed TOT-Doctor outperforms other baselines significantly, particularly in numerical reasoning questions.\n170. To top it all off, TOT-Doctor demonstrates more proficiency in numerical reasoning compared to commonsense understanding.\n171. To further investigate our TOT-Doctor model, we assess the ability of the TOT-Doctor to generate different answer types.\n172. From Table8, we discover that except for LayoutLMv2, all other baselines perform better in answering Yes/No questions.\n173. We observe that LayoutLMv2 and LayoutLMv3 exhibit frustrating performance in generating numerical and string answers.\n174. We speculate that this is mainly because LayoutLMv2 and LayoutLMv3 are extractive models, which means that they cannot generate answers that have not appeared in the document.\n175. This precisely explains their poor performance on the DCQA dataset, where the answers are largely obtained through data reasoning and involve numerical values.\n176. It is noted that EasyOCR 3 is utilized as the OCR system for these two baselines, which deviates from the OCR employed in their original versions.\n177. Based on this observation, we posit that the accuracy of the OCR system may have contributed to the subpar performance of the models.\n178. Moreover, TOT-Doctor is well versed in generating answers in terms of numerical or string, which verifies the robustness of TOT-Doctor.\n179. However, overall, all baselines achieve abysmal accuracy in generating numerical and string answers, highlighting the significant challenge posed by document-level chart understanding, which calls for further research efforts.\n\n请严格按照JSON格式返回：{"labels": [0,1,0,...]}，不要包含任何其他文字。'}], 'model': 'qwen-plus', 'response_format': {'type': 'json_object'}}}
2025-10-14 21:56:39,046 - DEBUG - Sending HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
2025-10-14 21:56:39,046 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-10-14 21:56:39,046 - DEBUG - send_request_headers.complete
2025-10-14 21:56:39,046 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-10-14 21:56:39,046 - DEBUG - send_request_body.complete
2025-10-14 21:56:39,046 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-10-14 21:56:50,134 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'vary', b'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding'), (b'x-request-id', b'91869f7c-f7a2-461f-bdd8-a3d75a699107'), (b'x-dashscope-call-gateway', b'true'), (b'content-type', b'application/json'), (b'req-cost-time', b'11057'), (b'req-arrive-time', b'1760450190207'), (b'resp-start-time', b'1760450201264'), (b'x-envoy-upstream-service-time', b'11022'), (b'content-encoding', b'gzip'), (b'date', b'Tue, 14 Oct 2025 13:56:40 GMT'), (b'server', b'istio-envoy'), (b'transfer-encoding', b'chunked')])
2025-10-14 21:56:50,135 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-14 21:56:50,136 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-10-14 21:56:50,136 - DEBUG - receive_response_body.complete
2025-10-14 21:56:50,136 - DEBUG - response_closed.started
2025-10-14 21:56:50,136 - DEBUG - response_closed.complete
2025-10-14 21:56:50,136 - DEBUG - HTTP Response: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "200 OK" Headers({'vary': 'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding', 'x-request-id': '91869f7c-f7a2-461f-bdd8-a3d75a699107', 'x-dashscope-call-gateway': 'true', 'content-type': 'application/json', 'req-cost-time': '11057', 'req-arrive-time': '1760450190207', 'resp-start-time': '1760450201264', 'x-envoy-upstream-service-time': '11022', 'content-encoding': 'gzip', 'date': 'Tue, 14 Oct 2025 13:56:40 GMT', 'server': 'istio-envoy', 'transfer-encoding': 'chunked'})
2025-10-14 21:56:50,137 - DEBUG - request_id: 91869f7c-f7a2-461f-bdd8-a3d75a699107
2025-10-14 21:56:50,138 - DEBUG - API request completed in 11.09 seconds
2025-10-14 21:56:50,138 - DEBUG - Raw model response: {"labels": [0,0,0,0,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,0,0,1,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}
2025-10-14 21:56:50,138 - INFO - Successfully processed 120 labels
2025-10-14 21:56:50,138 - ERROR - Label count mismatch for DCQA_ DOCUMENT_LEVEL CHART QUESTION ANSWERING TOWARDS COMPLEX REASONING AND COMMON_SENSE UNDERSTANDING
2025-10-14 21:56:50,138 - INFO - Evaluating paper 10/18: Debate Helps Supervise Unreliable Experts
2025-10-14 21:56:50,138 - INFO - Starting model prediction
2025-10-14 21:56:50,138 - INFO - Attempt 1 of 5
2025-10-14 21:56:50,140 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-9c2e24ca-45bf-4031-b2be-9fec73252e6a', 'json_data': {'messages': [{'role': 'system', 'content': '你是一个专门用于识别学术论文中数据集描述的助手。你需要判断每个输入的句子是否描述了研究中使用的数据集。请确保：\n    1. 只输出JSON格式的结果\n    2. 结果格式必须为 {"labels": [0,1,0,...]}\n    3. 不要输出任何其他文字或解释'}, {'role': 'user', 'content': '你是一位专业的学术论文数据集描述识别专家。请仔细分析下面这篇论文中的每个句子，判断它们是否描述了本研究特定构建的数据集信息。\n\n    论文标题: Debate Helps Supervise Unreliable Experts\n\n    判断标准:\n    判断为1(数据集描述)的标准：\n    句子必须直接描述本研究特定构建的数据集,包括:\n    1. 该数据集的构建过程和方法\n    2. 该数据集的具体构成和规模\n    3. 该数据的来源和收集方式\n    4. 该数据的预处理步骤\n    5. 该数据集的可获取方式(如发布地址)\n    6. 或者其他直接或者间接描述了该数据集的句子\n\n    所有其他类型的句子均标记为0。\n    \n    请对每个句子进行分析，返回一个JSON格式的标签数组。格式要求：\n    1. 必须是有效的JSON格式\n    2. 只包含labels字段，值为0和1组成的数组\n    3. 数组长度必须与句子数量相同\n    4. 示例格式：{"labels": [0,1,0,1,...]}\n\n    需要判断的句子：\n\n    1. Abstract: As AI systems are used to answer more difficult questions and potentially help create new knowledge, judging the truthfulness of their outputs becomes more difficult and more important.\n2. How can we supervise unreliable experts-which have access to the truth but may not accurately report it-to give answers that are systematically true and don\'t just superficially seem true, when the supervisor can\'t tell the difference between the two on their own?\n3. In this work, we show that debate between two unreliable experts can help a non-expert judge more reliably identify the truth.\n4. We collect a dataset of human-written debates on hard reading comprehension questions where the judge has not read the source passage, only ever seeing expert arguments and short quotes selectively revealed by \'expert\' debaters who have access to the passage.\n5. In our debates, one expert argues for the correct answer, and the other for an incorrect answer.\n6. Comparing debate to a baseline we call consultancy, where a single expert argues for only one answer which is correct half of the time, we find that debate performs significantly better, with 84% judge accuracy compared to consultancy\'s 74%.\n7. Debates are also more efficient, being 68% of the length of consultancies.\n8. By comparing human to AI debaters, we find evidence that with more skilled (in this case, human) debaters, the performance of debate goes up but the performance of consultancy goes down.\n9. Our error analysis also supports this trend, with 46% of errors in human debate attributable to mistakes by the honest debater (which should go away with increased skill); whereas 52% of errors in human consultancy are due to debaters obfuscating the relevant evidence from the judge (which should become worse with increased skill).\n10. Overall, these results show that debate is a promising approach for supervising increasingly capable but potentially unreliable AI systems.\n11. How can we tell if an AI system is telling the truth?\n12. Current language models trained to act as AI assistants, such asGPT-4 (OpenAI, 2023a)and Claude(Anthropic, 2023b,a) can correctly answer a wide variety of questions, construct coherent essays, and perform well on academic and professional exams(Hendrycks et al., 2020;OpenAI, 2023b).\n13. But the truthfulness of their responses is not robust: Such systems are prone to making false claims, giving misleading explanations about their reasoning Figure1: High-level summary of our experimental setup.\n14. We source hard reading comprehension questions from the QuALITY dataset(Pang et al., 2022)and incentivize human judges who can\'t read the passage to answer them correctly.\n15. Experts who have full access to the passage are allowed to reveal snippets of it (highlighted) in addition to free-text prose.\n16. In debate, the experts simultaneously defend their assigned option in their opening statements, and following rounds are sequential.In consultancy, the non-expert judge only interacts with one expert defending one option chosen at random.\n17. In both settings, the judge chooses when to end the session; sessions average at about 1,000 words total.(Turpin et al., 2023), and reinforcing the inferred opinions of their interlocutors(Perez et al., 2022;Bang et al., 2023;Borji, 2023).\n18. Language models have access to a vast array of information from their training data to draw on and synthesize-far beyond the knowledge of any individual human who might be involved in supervising them.\n19. As such, they could hold the potential to help us answer increasingly difficult questions or even create new knowledge that we otherwise couldn\'t.\n20. However, we expect that it will be increasingly hard to verify and supervise the truthfulness of their outputs in these cases.\n21. As language models become more capable and are used in more complex settings, it is likely that subtle mistakes, deceptive arguments, or selective use of evidence will become more difficult to spot.\n22. Making sure the information they provide is reliable requires effective methods for verifying the outputs of systems that know things we don\'t-a task known as scalable oversight(Amodei et al., 2016).\n23. Proposals for scalable oversight often involve leveraging the AI\'s abilities to help evaluators, for example with recursive reward modeling(Leike et al., 2018), model self-critique(Saunders et al., 2022), and debate(Irving et al., 2018).\n24. In debate-the focus of this work-two equally-capable expert debaters (e.g., AI systems) argue with each other over the answer to a question, each aiming to convince a non-expert (human) judge of their side.\n25. With an adversarial expert pointing out flaws in its arguments, neither debater will be able to get away with claims that its opponent can convincingly refute in the eyes of the judge.\n26. Training AI systems to win such debates should incentivize them not to make such claims in the first place.\n27. AsIrving et al. (2018)argue, this means that debate would incentivize an AI to tell the truth, as long as it is harder to lie than to refute a lie-i.e., the most successful strategies for debate lead judges to make good, informed decisions, rather than, for example, tricking them, confusing them, or prolonging the debate indefinitely.\n28. In this paper, we demonstrate for the first time that debate helps judges find truth on a realistic task, using debates on hard reading comprehension questions.\n29. To test this, we compare debate to a baseline we call consultancy, where the judge interacts with a single unreliable expert who has a 50/50 chance of arguing for the correct answer.\n30. By prompting the consultant to argue for the wrong answer half of the time, this baseline explicitly elicits dishonest behavior which may arise implicitly in Reinforcement Learning from Human Feedback (RLHF), as in cases, e.g., of sycophancy(Perez et al., 2022).\n31. To evaluate this with the strongest possible debaters, we collect and analyze a dataset of all-human debates, enlisting competitive debaters from the New York University debate team.\n32. A high-level overview of our setup is illustrated in Figure1.2For each debate, we pose a reading comprehension question from the QuALITY dataset(Pang et al., 2022)together with two answer choices (one correct, one incorrect), and allow the debaters-but not the judge-to read the story the question is about.\n33. The judge then interactively judges a debate on the question, where the debaters can back up their claims by selectively revealing short excerpts drawn from the story.\n34. Judge accuracy in these debates is 84%, compared to with 74% on consultancy (Section 4).\n35. Debate is also more efficient, being 68% of the length and requiring 61% as much ground-truth evidence, suggesting that it will be a more effective method than open-ended dialogue (cf.Bowman et al., 2022)for helping annotators efficiently supervise untrusted models that exceed their expertise.\n36. We also find that our judges are relatively calibrated overall on debates, though they struggle with overconfidence in the high-confidence regime (Figure5).\n37. While there are still cases when the judge of a debate gets the answer wrong, we find that the most common sources of error should be possible to mitigate with further judge training or stronger debaters.\n38. For example, in 33% of mistakes, the judge ended the session prematurely, either after only a single round or immediately after changing their preferred answer, giving the debaters no opportunity to refute the judge\'s final reasoning.\n39. In 46% of mistakes, the debater arguing for the correct answer missed an important argument or piece of evidence that they could have used (Section 5).\n40. We also include experimental results for AI debate, using GPT-4 as a debater (Section 4).\n41. In this setting, we find no difference between debate and consultancy.\n42. However, even if debate does not work better as an oversight method for current models, that may simply be because they have not yet reached human-level capabilities at deception (i.e., as a consultant) and argumentation (as a debater); it is also possible that we do not optimize GPT-4\'s prompt heavily enough to elicit such capabilities.\n43. It seems plausible that AI systems may soon be capable enough of argumentation and persuasion that debate will be important to incorporate into their training; in Section 8 we lay out an agenda for what this may look like, and what challenges will need to be solved to make this work.\n44. As we use AI systems in more difficult and complex settings, we will need stronger mechanisms to verify their arguments-ideally, methods which improve concordantly and at pace with the system\'s capabilities.\n45. Our results with human debaters demonstrate for the first time that debate, where equally-capable experts point out flaws with each other\'s arguments, can allow a non-expert judge to effectively determine the answers to questions they could not answer on their own.\n46. This suggests that debate may soon be important for effectively supervising models to truthfully answer hard questions.\n47. Source Material and Questions We draw the questions to be debated from the Question Answering with Long Input Texts, Yes!\n48. (QuALITY) dataset of reading comprehension questions(Pang et al., 2022).\n49. 4To focus on especially hard questions, we further restrict our results to questions that were marked by the untimed annotators as requiring more than one or two sentences of context to get correct (the idea being to avoid questions which could be easily resolved with a single quote from one of the debaters).\n50. Each question in QuALITY has four answers, one of which is correct; as our debates consider only two answer choices, we use the correct answer and the incorrect option that was labeled as the best distractor most often by the QuALITY dataset\'s untimed validation annotators.\n51. We only use the Project Gutenberg subset of QuALITY-hard, which consists of questions over public-domain science fiction short stories.\n52. Since the stories are entirely fictional, judges can almost never guess the answer on the basis of prior knowledge, and must rely on the information provided by the debaters.5.\n53. On average, the stories used for our debates have 27.7k characters, or 6.3k tokens using the CoreNLP tokenizer(Manning et al., 2014).\n54. For each turn, we use a character limit of ℓ  = 750 and a quote limit of ℓ  = 250, meaning that on average up to 1.8% of the story could be revealed in each round of the debate.\n55. Experimental Conditions While our main experimental results concern human debaters, we also test with AI debaters.\n56. As the AI debater, we use the version of GPT-4 with a 32,000-token context window available through the OpenAI API as gpt-4-32k.\n57. Prompts are provided in Appendix G.\n58. 6This gives us four experimental conditions: human debate, human consultancy, AI debate, and AI consultancy.\n59. We recruit 19 people to serve as both debaters and judges in our experiments.\n60. Our participants, all of whom were New York University employees during data collection, include 12 undergraduates on the NYU debate team, all with at least one year of experience in competitive debate; 6 members of the research team, three of whom have at least one year of experience with competitive debate; and one NYU Master\'s student with 6 years of experience studying Jewish legal reasoning and argumentation.\n61. After running initial pilots in Fall of 2022 to establish the debate protocol (see Appendix B), we collect debates according to the protocol defined in Section 2, with collection running from February to August of 2023.\n62. During collection, participants can log into our data collection platform to read stories or take turns in their debates at any time, but we also set aside specific times each week when we request that the debaters work, to facilitate near-synchronous debates.\n63. To avoid information leakage between debates, each participant is only allowed to judge one question about each story.\n64. After each debate is complete, all participants fill out a feedback survey with quantitative and qualitative observations which we use to help us analyze the results (see Appendix F).\n65. Participants cannot see the identities of the other participants in the debate until after filling out the feedback form.\n66. Data collection was not perfectly controlled between our four experimental conditions, as some components of our experimental design were developed part of the way through data collection: The consultancy baseline was only developed in June of 2023 and the AI debaters were only incorporated 4We ended up drawing 59% of our questions from the QuaLITY training set, which has 3 untimed validators per question, and 41% from the development set, which has 5 untimed validators per question.\n67. 5In our data, judge priors were between 45%-55% in 91% of debates, and between 35%-65% in 97% of debates.\n68. 6Because the QuALITY questions are drawn from public-domain texts available from Project Gutenberg, it is likely that the passages used in our experiments appear in GPT-4\'s training corpus.\n69. This does not pose a data contamination issue when using GPT-4 as a debater, since debaters are meant to be experts and are given full access to the text anyway.\n70. However, this does could pose issues for future work testing AI judging in our setting, since it might be difficult to guarantee that the models do not use prior knowledge of the story in their decisions, instead of relying on the debate.\n71. Quotes and characters per round measure how close debaters came to their character limits; quote totals are calculated only using new quoted material that hasn\'t been used yet in the debate.\n72. Bits/rd is the amount of information conveyed to the judge on average per round, calculated from the information gain between their final and initial judgment log 2 (  * , / * ,0 ),   is the judge\'s score as defined in Equation1, and ECE final is the expected calibration error of the judge\'s final judgments, calculated with a bin size of 10%. into the data collection platform in July.\n73. The set of debaters who participated in the experiment also varied over the course of these months.\n74. These factors were due to us trying to collect as much data as possible subject to practical limits on engineering capacity, annotator availability, and researcher foresight.\n75. We validate our analysis in Section 4 with partial controls in Appendix D.\n\n请严格按照JSON格式返回：{"labels": [0,1,0,...]}，不要包含任何其他文字。'}], 'model': 'qwen-plus', 'response_format': {'type': 'json_object'}}}
2025-10-14 21:56:50,142 - DEBUG - Sending HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
2025-10-14 21:56:50,143 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-10-14 21:56:50,143 - DEBUG - send_request_headers.complete
2025-10-14 21:56:50,143 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-10-14 21:56:50,143 - DEBUG - send_request_body.complete
2025-10-14 21:56:50,143 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-10-14 21:56:55,469 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'vary', b'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding'), (b'x-request-id', b'cb97d515-00f2-4ffc-a540-c8d03cf736ea'), (b'x-dashscope-call-gateway', b'true'), (b'content-type', b'application/json'), (b'req-cost-time', b'5295'), (b'req-arrive-time', b'1760450201303'), (b'resp-start-time', b'1760450206599'), (b'x-envoy-upstream-service-time', b'5263'), (b'content-encoding', b'gzip'), (b'date', b'Tue, 14 Oct 2025 13:56:46 GMT'), (b'server', b'istio-envoy'), (b'transfer-encoding', b'chunked')])
2025-10-14 21:56:55,470 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-14 21:56:55,470 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-10-14 21:56:55,471 - DEBUG - receive_response_body.complete
2025-10-14 21:56:55,471 - DEBUG - response_closed.started
2025-10-14 21:56:55,471 - DEBUG - response_closed.complete
2025-10-14 21:56:55,471 - DEBUG - HTTP Response: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "200 OK" Headers({'vary': 'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding', 'x-request-id': 'cb97d515-00f2-4ffc-a540-c8d03cf736ea', 'x-dashscope-call-gateway': 'true', 'content-type': 'application/json', 'req-cost-time': '5295', 'req-arrive-time': '1760450201303', 'resp-start-time': '1760450206599', 'x-envoy-upstream-service-time': '5263', 'content-encoding': 'gzip', 'date': 'Tue, 14 Oct 2025 13:56:46 GMT', 'server': 'istio-envoy', 'transfer-encoding': 'chunked'})
2025-10-14 21:56:55,471 - DEBUG - request_id: cb97d515-00f2-4ffc-a540-c8d03cf736ea
2025-10-14 21:56:55,472 - DEBUG - API request completed in 5.33 seconds
2025-10-14 21:56:55,472 - DEBUG - Raw model response: {"labels": [0,0,0,1,1,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]}
2025-10-14 21:56:55,472 - INFO - Successfully processed 82 labels
2025-10-14 21:56:55,472 - ERROR - Label count mismatch for Debate Helps Supervise Unreliable Experts
2025-10-14 21:56:55,473 - INFO - Evaluating paper 11/18: Distilling Large Language Models for Matching Patients to Clinical Trials
2025-10-14 21:56:55,473 - INFO - Starting model prediction
2025-10-14 21:56:55,473 - INFO - Attempt 1 of 5
2025-10-14 21:56:55,474 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-4d628d54-42b4-4c56-9b7c-3aec32963a73', 'json_data': {'messages': [{'role': 'system', 'content': '你是一个专门用于识别学术论文中数据集描述的助手。你需要判断每个输入的句子是否描述了研究中使用的数据集。请确保：\n    1. 只输出JSON格式的结果\n    2. 结果格式必须为 {"labels": [0,1,0,...]}\n    3. 不要输出任何其他文字或解释'}, {'role': 'user', 'content': '你是一位专业的学术论文数据集描述识别专家。请仔细分析下面这篇论文中的每个句子，判断它们是否描述了本研究特定构建的数据集信息。\n\n    论文标题: Distilling Large Language Models for Matching Patients to Clinical Trials\n\n    判断标准:\n    判断为1(数据集描述)的标准：\n    句子必须直接描述本研究特定构建的数据集,包括:\n    1. 该数据集的构建过程和方法\n    2. 该数据集的具体构成和规模\n    3. 该数据的来源和收集方式\n    4. 该数据的预处理步骤\n    5. 该数据集的可获取方式(如发布地址)\n    6. 或者其他直接或者间接描述了该数据集的句子\n\n    所有其他类型的句子均标记为0。\n    \n    请对每个句子进行分析，返回一个JSON格式的标签数组。格式要求：\n    1. 必须是有效的JSON格式\n    2. 只包含labels字段，值为0和1组成的数组\n    3. 数组长度必须与句子数量相同\n    4. 示例格式：{"labels": [0,1,0,1,...]}\n\n    需要判断的句子：\n\n    1. Abstract: The recent success of large language models (LLMs) has paved the way for their adoption in the high-stakes domain of healthcare.\n2. Specifically, the application of LLMs in patient-trial matching, which involves assessing patient eligibility against clinical trial\'s nuanced inclusion and exclusion criteria, has shown promise.\n3. Recent research has shown that GPT-3.5, a widely recognized LLM developed by OpenAI, can outperform existing methods with minimal \'variable engineering\' by simply comparing clinical trial information against patient summaries.\n4. However, there are significant challenges associated with using closed-source proprietary LLMs like GPT-3.5 in practical healthcare applications, such as cost, privacy and reproducibility concerns.\n5. To address these issues, this study presents the first systematic examination of the efficacy of both proprietary (GPT-3.5, and GPT-4) and open-source LLMs (LLAMA 7B,13B, and 70B) for the task of patient-trial matching.\n6. Employing a multifaceted evaluation framework, we conducted extensive automated and human-centric assessments coupled with a detailed error analysis for each model.\n7. To enhance the adaptability of open-source LLMs, we have created a specialized synthetic dataset utilizing GPT-4, enabling effective fine-tuning under constrained data conditions.\n8. Our findings reveal that open-source LLMs, when fine-tuned on this limited and synthetic dataset, demonstrate performance parity with their proprietary counterparts.\n9. This presents a massive opportunity for their deployment in real-world healthcare applications.\n10. To foster further research and applications in this field, we release both the annotated evaluation dataset along with the fine-tuned LLM -Trial-LLAMA -for public use.\n11. Clinical trials represent both the most important and the most challenging aspect of medical advancements.\n12. These trials serve a dual function: first, as a conduit for patients to access potentially life-altering treatments, and second, as a mechanism for the iterative process of drug development and approval.However, a significant number of trials are beleaguered by extended timelines.\n13. Empirical data suggests that, on average, clinical trials take approximately twice as long as initially projected[22], with approximately 40% of trial sites failing to meet their enrollment targets[15].\n14. Apart from others, one of the major challenges in recruiting patients is matching them against suitable trials[5,20,6,26,12,28,19,29].\n15. The process of matching a patient to trials is challenging.\n16. It requires both the meticulous analysis of electronic health records (EHRs) and the contextual interpretation of this data against the backdrop of clinical trial criteria.\n17. This is particularly challenging because a majority of this data is stored in unstructured documents written in free text.\n18. Even the structured data is difficult to query due to the increasing complexity of inclusion and exclusion criteria.\n19. Automating this process can accelerate trials save healthcare providers\' time spent on manual chart reviews.\n20. Current approaches primarily rely on data extraction or classification pipelines[36,49,27].\n21. Nonetheless, these methods require extensive variable engineering, which frequently results in constrained contextual comprehension and limited scalability when dealing with intricate trial criteria.\n22. The emergence of Large Language Models (LLMs), such as Med-PaLM[37]and GPT-4[25], marks a paradigm shift in the domain of automated interpretation of patient health records.\n23. These models embody the cutting-edge in natural language processing (NLP), facilitating nuanced and context-aware analysis of complex medical data.\n24. Leveraging their capabilities, recent research has used these models for a variety of clinical information interpretation tasks, including patient matching[18].\n25. However, their deployment in healthcare settings presents challenges.\n26. One primary concern relates to the risk of Protected Health Information (PHI) leakage when using such models.\n27. Most healthcare organisations prefer on-premise infrastructure for tools that handle identified patient data.\n28. However, due to the cost and computational complexity associated with these models, they often remain in centralized cloud provider environments.\n29. These challenges can make LLMs prohibitive for widespread clinical application.\n30. Moreover, despite their effectiveness, advanced models are often characterized by opacity and proprietary restrictions, which further complicate their integration into healthcare systems subject to stringent regulatory constraints.\n31. In light of these considerations, there is a growing need for the development of open-source LLMs that can match the accuracy of their proprietary counterparts but at a significantly reduced cost.\n32. This also enables healthcare organizations to seamlessly integrate these technologies into their existing infrastructures, mitigating the risk of Protected Health Information (PHI) leaks.\n33. To the best of our knowledge, this study is the first comprehensive examination of the efficacy of open-source LLMs in this domain.\n34. • Our work thoroughly compares open-source LLMs and their proprietary counterparts for patienttrial matching.\n35. • We further explore and elucidate the impact of fine-tuning on various open-source LLMs for patient-trial matching.\n36. • We define the error taxonomy and thoroughly analyze the nature of errors made by the models on this task.\n37. • Along with the experimental details, we publicly release the evaluation dataset and the LLM trained based on LLAMA for patient-trial matching.\n38. We tested both proprietary (GPT-3.5, and GPT-4) and open-source LLMs (LLAMA-2 7B,13B, and 70B, referred to LLAMA hereafter) for the task of patient-trial matching.\n39. For GPT-3.5, we leveraged the Azure Open AI API, specifically gpt-35-turbo-16k-0613 as the model version.\n40. We set the temperature parameter to 0, aiming for deterministic outputs that would ensure consistency and repeatability in our experiments.\n41. This was coupled with a top p setting of 0.95, aligning with our goal to eliminate randomness in the model\'s response generation process.\n42. Additionally, we refrained from applying any frequency or repetition penalties, allowing the model\'s natural language generation capabilities to function without these constraints.\n43. For GPT-4, we employed a similar configuration with gpt-4-0613 as the model version.\n44. For LLAMA, we changed the configuration from Meta.\n45. We initially encountered challenges in aligning the standard versions of these models to produce outputs in the required format, particularly in the context of complex clinical trial criteria.\n46. To address this, we opted for specific versions tailored for chat applications, namely Llama-2-7b-chat-hf, Llama-2-13b-chat-hf, and Llama-2-70b-chat-hf.\n47. These versions offered a more flexible and adaptable framework for our needs.\n48. We adjusted the temperature setting to 0.4 for all LLAMA models, a decision informed by preliminary tests which indicated that a slightly higher temperature prevented the model from collapsing on certain trials where inclusion/exclusion criteria were not clearly defined.\n49. Maintaining the output format was particularly challenging when working with the base LLAMA models.\n50. Despite employing various techniques such as context-free grammar (CFG) to constrain the model\'s output, the results remained suboptimal.Consequently, the models were unable to generate structured output for complex clinical trials.\n51. To circumvent this, we adjusted the model\'s temperature to foster more exploratory behavior and executed five output generations iteratively till the output matched our JSON schema.\n52. This allowed us to generated structured output for majority of clinical trials even with base models.\n53. Compute Novelty Index: noveltyIndex ← 1 -score τ 6: if noveltyIndex > 0 then end for 10: end for 11: return C novel Each criterion in C final is then annotated with a gold-standard answer and corresponding evidences.\n54. These evidential references serve as a basis for language models to substantiate their answers.\n55. To gauge the faithfulness of various models in accurately citing these pieces of evidence, we calculate precision, recall, and F1 scores for each model.\n56. Additionally, we conduct a direct comparison of model performance at the criterion level to evaluate their relative effectiveness.\n57. Different from the metrics used in[18], we created two distinct aspects of Criterion-Level Accuracy (CLA), namely Explicit CLA and Implicit CLA, to holistically assess the model\'s performance.\n58. For Explicit CLA, our focus is on evaluating how accurately the model categorizes each criterion into the correct class, provided that the criterion has been previously identified as \'explicit\' in our manual annotation exercise.\n59. This evaluation primarily concerns criteria for which the necessary information for classification is clearly and directly stated in the patient documentation, leaving minimal room for interpretation or inference.\n60. The accuracy here reflects the model\'s ability to comprehend and correctly apply these straightforward, unambiguous data points.\n61. On the other hand, Implicit CLA tackles a more nuanced challenge: it assesses the model\'s performance on criteria deemed \'implicit\' by the annotators.\n62. These criteria involve situations where the required information is not explicitly stated but rather implied or inferred from the available data.\n63. This often requires connecting disparate pieces of information, understanding subtleties and nuances in the patient data, and making educated guesses based on the context.\n64. Calculating the Implicit CLA involves a thorough analysis of how well the model navigates these complexities and accurately classifies criteria based on less direct information.\n65. Both Explicit and Implicit CLAs are pivotal in understanding the model\'s overall capability to process and interpret clinical trial criteria.\n66. While Explicit CLA provides insight into the model\'s proficiency with clear-cut, straightforward tasks, Implicit CLA sheds light on its ability to handle ambiguity and complexity -crucial aspects in the realm of clinical data interpretation.\n67. For our dataset, we adopted the similar datasets as used in[18]that incorporate the SIGIR dataset[21]and both the 2021 and 2022 versions of the TREC CT cohorts[34,33], as shown in Table1.\n68. For each patient within these datasets, we extract 50 clinical trials, categorizing them into three distinct classifications: "eligible", "excluded", and "irrelevant".\n69. The categorization within the SIGIR dataset required a different approach, given its classification system.\n70. (a) "Will not refer to the trial": This class aligns with the \'irrelevant\' category in our study.\n71. (b) "Will refer to the trial": Corresponds to the \'eligible\' category.\n72. (c) "May refer to the trial": This class does not map directly to any of our predefined categories.\n73. Due to this non-conformity, we excluded all trial-patient combinations classified under (c) "May refer to the trial", to maintain consistency.\n74. To facilitate the fine-tuning of our models, we partition the dataset into a training and test set, adhering to an 80:20 ratio.\n75. This division is implemented along the patient axis to ensure no test patient record gets leaked into the training set.\n76. Prior to splitting, all datasets are combined and thoroughly shuffled.\n77. The specifics of the training and test sets are displayed in Table1.\n78. It is noteworthy that despite the large volume of records in the training set, they are not fully utilized for model training.\n79. Instead, the large size of this set provides with an easy mechanism to sample diverse training samples for fine-tuning while also allowing us to save on compute costs associated with evaluation a large number of model checkpoints.\n80. Evidently, as shown in Table1the sampled dataset is more diverse than the training set.\n81. It is known that the performance of a model improves with the volume of data it is exposed to[16].\n82. Nevertheless, the quality of data plays a pivotal role in determining the output\'s caliber.\n83. Multiple research works have demonstrated that while the fine-tuning performance of a model initially improves rapidly, it tends to reach a saturation point beyond a certain threshold of data exposure [?].\n84. This phenomenon is consistent with our findings with the fine-tuning of the LLAMA models of different sizes.\n85. As illustrated in Figure5b, the performance of all three LLAMA variants exhibits a significant initial leap with exposure to a small data subset, followed by a gradual enhancement as they are introduced to an increasing number of examples.\n86. Notably, the largest LLAMA variant swiftly surpasses the performance of GPT-3.5.\n87. For the assessment of model performance, we utilize the metric of overall criteria level accuracy, encompassing both implicit and explicit criteria.\n88. The process of fine-tuning LLMs presents both computational and methodological challenges, primarily due to the difficulty in providing a dense, multi-token signal that these models require for effective learning.\n89. While labeling for classification tasks typically involves single-token signals, enhancing model performance necessitates the provision of multi-token feedback, which is inherently more complex to curate due to its diversity and volume requirements.\n90. Despite these challenges, our experiments demonstrate that distillation techniques that have been used to enhance the dialogue capabilities of different models[46,43]can be used for the task of patient matching as well.\n91. This method significantly reduces the necessity for manually crafted examples, thereby streamlining the fine-tuning process and making it more affordable.\n\n请严格按照JSON格式返回：{"labels": [0,1,0,...]}，不要包含任何其他文字。'}], 'model': 'qwen-plus', 'response_format': {'type': 'json_object'}}}
2025-10-14 21:56:55,476 - DEBUG - Sending HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
2025-10-14 21:56:55,477 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-10-14 21:56:55,477 - DEBUG - send_request_headers.complete
2025-10-14 21:56:55,477 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-10-14 21:56:55,477 - DEBUG - send_request_body.complete
2025-10-14 21:56:55,477 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-10-14 21:57:00,976 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'vary', b'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding'), (b'x-request-id', b'6e2567bb-836c-42b3-ad34-01371a0c7431'), (b'x-dashscope-call-gateway', b'true'), (b'content-type', b'application/json'), (b'req-cost-time', b'5468'), (b'req-arrive-time', b'1760450206638'), (b'resp-start-time', b'1760450212106'), (b'x-envoy-upstream-service-time', b'5432'), (b'content-encoding', b'gzip'), (b'date', b'Tue, 14 Oct 2025 13:56:51 GMT'), (b'server', b'istio-envoy'), (b'transfer-encoding', b'chunked')])
2025-10-14 21:57:00,977 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-14 21:57:00,977 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-10-14 21:57:00,977 - DEBUG - receive_response_body.complete
2025-10-14 21:57:00,978 - DEBUG - response_closed.started
2025-10-14 21:57:00,978 - DEBUG - response_closed.complete
2025-10-14 21:57:00,978 - DEBUG - HTTP Response: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "200 OK" Headers({'vary': 'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding', 'x-request-id': '6e2567bb-836c-42b3-ad34-01371a0c7431', 'x-dashscope-call-gateway': 'true', 'content-type': 'application/json', 'req-cost-time': '5468', 'req-arrive-time': '1760450206638', 'resp-start-time': '1760450212106', 'x-envoy-upstream-service-time': '5432', 'content-encoding': 'gzip', 'date': 'Tue, 14 Oct 2025 13:56:51 GMT', 'server': 'istio-envoy', 'transfer-encoding': 'chunked'})
2025-10-14 21:57:00,978 - DEBUG - request_id: 6e2567bb-836c-42b3-ad34-01371a0c7431
2025-10-14 21:57:00,979 - DEBUG - API request completed in 5.51 seconds
2025-10-14 21:57:00,979 - DEBUG - Raw model response: {"labels": [0,0,0,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}
2025-10-14 21:57:00,979 - INFO - Successfully processed 89 labels
2025-10-14 21:57:00,979 - ERROR - Label count mismatch for Distilling Large Language Models for Matching Patients to Clinical Trials
2025-10-14 21:57:00,979 - INFO - Evaluating paper 12/18: ECMD_ An Event_Centric Multisensory Driving Dataset for SLAM
2025-10-14 21:57:00,980 - INFO - Starting model prediction
2025-10-14 21:57:00,980 - INFO - Attempt 1 of 5
2025-10-14 21:57:00,981 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-98c99df1-86a0-48fd-b41a-22960bdf4bd7', 'json_data': {'messages': [{'role': 'system', 'content': '你是一个专门用于识别学术论文中数据集描述的助手。你需要判断每个输入的句子是否描述了研究中使用的数据集。请确保：\n    1. 只输出JSON格式的结果\n    2. 结果格式必须为 {"labels": [0,1,0,...]}\n    3. 不要输出任何其他文字或解释'}, {'role': 'user', 'content': '你是一位专业的学术论文数据集描述识别专家。请仔细分析下面这篇论文中的每个句子，判断它们是否描述了本研究特定构建的数据集信息。\n\n    论文标题: ECMD_ An Event_Centric Multisensory Driving Dataset for SLAM\n\n    判断标准:\n    判断为1(数据集描述)的标准：\n    句子必须直接描述本研究特定构建的数据集,包括:\n    1. 该数据集的构建过程和方法\n    2. 该数据集的具体构成和规模\n    3. 该数据的来源和收集方式\n    4. 该数据的预处理步骤\n    5. 该数据集的可获取方式(如发布地址)\n    6. 或者其他直接或者间接描述了该数据集的句子\n\n    所有其他类型的句子均标记为0。\n    \n    请对每个句子进行分析，返回一个JSON格式的标签数组。格式要求：\n    1. 必须是有效的JSON格式\n    2. 只包含labels字段，值为0和1组成的数组\n    3. 数组长度必须与句子数量相同\n    4. 示例格式：{"labels": [0,1,0,1,...]}\n\n    需要判断的句子：\n\n    1. Abstract: Leveraging multiple sensors enhances complex environmental perception and increases resilience to varying luminance conditions and high-speed motion patterns, achieving precise localization and mapping.\n2. This paper proposes, ECMD, an event-centric multisensory dataset containing 81 sequences and covering over 200 km of various challenging driving scenarios including high-speed motion, repetitive scenarios, dynamic objects, etc. ECMD provides data from two sets of stereo event cameras with different resolutions (640×480, 346×260), stereo industrial cameras, an infrared camera, a top-installed mechanical LiDAR with two slanted LiDARs, two consumer-level GNSS receivers, and an onboard IMU.\n3. Meanwhile, the ground-truth of the vehicle was obtained using a centimeter-level high-accuracy GNSS-RTK/INS navigation system.\n4. All sensors are well-calibrated and temporally synchronized at the hardware level, with recording data simultaneously.\n5. We additionally evaluate several state-of-the-art SLAM algorithms for benchmarking visual and LiDAR SLAM and identifying their limitations.\n6. The dataset is available at https://arclab-hku.github.io/ecmd/.\n7. V ISUAL and LiDAR simultaneous localization and map- ping (SLAM) achieved notable progress within driving scenarios in recent years.\n8. However, they encounter the challenging task of operating robustly under heterogeneous environments, such as varying lighting conditions, lowtexture scenarios, repetitive structures, diverse motion patterns, dense dynamic objects, etc. Utilizing novel sensors and integrating multiple sensors can provide a comprehensive perception and enhance the robustness of the entire system[1]-[3].\n9. These motivate us to develop a dataset that integrates novel sensors under realistic and complex driving scenarios, thereby promoting SLAM research.\n10. Event cameras have low latency (µs-level) and high dynamic range (140 dB compared to 60 dB with standard cameras) properties, which offers great opportunities for visual (VO) and visual-inertial odometry (VIO) in rough terrain, aggressive motions, and high dynamic range (HDR)[4].\n11. Unlike traditional frame-based cameras that directly capture fixed-rate intensity frames, event cameras are motionactivated sensors that capture pixel-wise intensity differences asynchronously in continuous streams.\n12. However, the widespread commercialization and implementation of event cameras in robotics are still early due to the expensive cost.\n13. In addition, event cameras confront challenges during rapid vibrations and ego-motion, as these conditions generate a substantial quantity of events, leading to intensive computations.\n14. Conversely, in cases where minimal relative motion between the event camera and the scene exists, such as under static states, they only provide limited information or even introduce noise[5].\n15. Therefore, we embark on this research effort to explore the inquiry: Are event cameras ready for autonomous driving?\n16. There exist several stereo event-based driving datasets that are worth mentioning and exploring.\n17. MVSEC[6]was the first stereo event-based driving dataset proposed for evaluating the localization performance.\n18. While MVSEC employs the low resolution of DAVIS346 which limits the feature detection for accurate localization.\n19. DSEC[7]offers stereo event streams with a high resolution of 0.31 Megapixels(MP).\n20. However, this dataset focuses on computer vision tasks segmentation, depth estimation, optical flow estimation, etc., which is not specifically designed for VO/VIO/SLAM domains.\n21. MA-VIED[8]propose a largescale driving dataset under standard urban scenarios and race track-like loops.\n22. The ground-truth trajectory relies on GNSS-RTK, which only ensures high accuracy in open-sky environments and fails to provide high accuracy in GNSSdenied scenarios such as tunnels or densely street areas.\n23. [9]focuses on collecting both stereo event data and stereo intensity images under indoor and urban driving scenes with the ground-truth of GNSS-RTK/INS.\n24. Their sequences do not encompass extremely high-speed or repetitive scenarios that could be challenging to VO/VIO/SLAM algorithms.\n25. To address the above drawbacks, we propose ECMD, a dataset procured from diverse challenging driving scenarios with a comprehensive suite of sensors for benchmarking various VO/VIO/SLAM algorithms.\n26. To the best of our knowledge, this is the first event-based SLAM dataset specifically focused on densely urbanized driving scenarios.\n27. The contributions of our work can be summarized as follows: 1) Our sensor platform consists of various novel sensors shown in Fig.1, including two sets of stereo event cameras with distinct resolutions (640×480, 346×260), an infrared camera, stereo industrial cameras, three mechanical LiDARs (including two slanted LiDARs), a high-quality inertial measurement unit (IMU), and three global navigation satellite system (GNSS) receivers.\n28. 2) ECMD collects 81 sequences covering over 200 kilometers of trajectories in various driving scenarios, including dense streets, urban, tunnels, highways, bridges, and suburbs.\n29. These sequences are recorded under daylight and nighttime, providing challenging situations for Visual and LiDAR SLAM, e.g., dynamic objects, highspeed motion, repetitive scenarios, and HDR scenes.\n30. Meanwhile, we evaluate existing state-of-the-art visual and LiDAR SLAM algorithms with various sensor modalities on our datasets.\n31. Moreover, our dataset and benchmark results are released publicly available on our website.\n32. The remainder of the paper is organized as follows: Section II introduces the related works.\n33. Section III presents the sensor setup and sensor calibration.\n34. Section IV intro-duces the dataset overview.\n35. Section V demonstrates the dataset application.\n36. Section VI introduces known issues.\n37. The conclusion is given in Section VII.II.RELATED WORKS Currently, several event-based datasets combined with various sensors have been released for VO/VIO/SLAM domains, utilizing handheld devices or a variety of robotics platforms.\n38. DAVIS240C[10], TUM-VIE[11], VECtor[12], and HKU-dataset1were collected by handheld / headmounted devices under indoor environments.\n39. M2DGR[13]utilizes ground robots to collect a multi-sensor dataset with an event camera under large-scale scenes, while the event streams exhibit large noises.\n40. FusionPortable[14]proposes multi-sensor campus-scene datasets with stereo event cameras on diverse platforms (handheld, quadruped robot, and UGV).\n41. Moreover, there exist specialized event-based datasets such as UZH-FPV[15]and GRIFFIN[16], which are targeted for flying robots.\n42. Moreover, a number of event-based datasets are published under large-scale driving scenarios for computer vision.\n43. These autopilot datasets offer more realistic and challenging conditions, including high-speed scenarios, repetitive situations, and HDR scenes compared to datasets collected from handheld devices.\n44. The first dataset catering to driving recordings using an event camera is DDD17[17], as well as the follow-up DDD20[18], for studying the end-to-end driving application incorporating diverse vehicle control data.\n45. [22]published their event-based datasets for the computer vision task of object classification, image reconstruction, and vision place recognition in driving scenarios.\n46. MVSEC[6]is a pioneering cross-modal dataset with stereo event and image cameras, as well as LiDAR.\n47. However, a limitation of MVSEC resides in the utilization of low-resolution event cameras (346×260) with a compact baseline of 10 cm, coupled with the imprecision of the ground-truth derived from GNSS or LiDAR-SLAM.\n48. DSEC[7]proposed an event-based dataset whose scenarios are similar to KITTI[23], providing higher resolution stereo event (640×480) and image, LiDAR, and IMU under various illumination conditions.\n49. M3ED[24]encompasses high-resolution event cameras (1280×720) and covers three different robotics platforms: driving, flight, and legged robot.\n50. However, both DSEC and M3ED datasets are primarily utilized for computer vision fields, such as optical flow estimation, segmentation, and disparity estimation, rather than specifically for localization or mapping problems.\n51. Besides, they do not provide sufficient challenges for SLAM, as the majority of these datasets were collected in rural or suburban areas with relatively low-lying structures, light traffic, and less dynamic objects.\n52. ViViD++[25]focuses on diverse vision sensors for handheld and driving platforms, including event, thermal, and standard cameras.\n53. MA-VIED[8]proposes a comprehensive driving dataset that encompasses race track-like loops, maneuvers, and standard driving scenarios.\n54. However, both of these datasets exclusively offer monocular data for each camera type, thereby precluding the possibility of conducting stereo visual SLAM.\n55. [9]introduces a stereo visual localization dataset that exploits both the high-resolution event and standard cameras under indoor and urban scenarios.\n56. TableI. summarizes the differences between our ECMD and other event-based datasets under autonomous driving scenarios.\n57. Compared to other datasets, our ECMD offers several advantages: (i) Capture diverse visual data format (RGB image, event stream, and infrared image) from multiple types of vision sensors in varying luminance conditions and urbanized scenarios; (ii) 1kHz-rate event streams from different resolution event cameras empower in-depth exploration of event-based perception; (iii) Based on our previous work[26], three LiDARs, including two slanted LiDARs, are employed to collect high-rising building structures for LiDAR point cloud maps generation; (iv) We employ a centimeter-level localization system, GNSS-RTK/INS, as ground-truth, enabling a comprehensive evaluation of various SLAM algorithms.\n58. The data collection platform is shown in Fig.1.\n59. Our sensor suite consists of a multi-camera setup (event camera, industrial camera, and infrared camera) equipped with three LiDARs, high-quality IMU, three GNSS receivers, and GNSS-RTK/INS systems.\n60. The specific specifications of each sensor are presented in TableII.\n61. An Intel NUC (i7-1260P, 32GB RAM) and an industrial computer (i7-10610U, 32GB RAM) are used to run sensor drivers, and record data into ROS bags on the Ubuntu system.\n62. 1) Visual Sensors: Two sets of stereo event cameras with different resolutions, DAVIS436 (346×260) and DVXplorer (640×480), are configured at a baseline of 30 cm respectively.\n63. DAVIS346 produces asynchronous events and intensity frames.\n64. In contrast, DVXplorer exclusively generates events, while its resolution surpasses that of DAVIS346, enabling the provision of more intricate scene information.\n65. Each event camera is equipped with additional infrared filters to mitigate interference from LiDAR.\n66. Two FLIR BFLY-U3-23S3C industrial cameras with a resolution of 1920×1200 are used to capture RGB images at 20 Hz in fixed exposure mode.\n67. Forward-facing stereo industrial cameras are installed with a baseline of 30 cm, ensuring fairness by maintaining consistency with the baseline of the stereo event cameras.\n68. Hikrobot MV-CI003-GL-N6 infrared camera collects thermal frames at 20 Hz, encompassing a response band of 8-14µm and equipped with a 6.3mm focal length lens.\n69. 2) Mechanical LiDAR: We configure three mechanical LiDARs including two slanted LiDARs to collect accurate point clouds of surrounding environments.\n70. Velodyne HDL-32E is positioned on the top of the vehicle to capture the surroundings horizontally.\n71. Two slanted LiDARs, Lslidar C16 and Velodyne VLP-16, are mounted on the left and right sides of the sensor kit, respectively.\n72. This configuration facilitates the thorough recording of architectural particulars relevant to high-rising buildings in urbanized areas and all LiDAR data are collected at 10 Hz.\n73. 3) GNSS-RTK/INS Sensor: A tactical-level Xsens-MTI-30 IMU is employed to collect the raw acceleration and angular velocity at 400 Hz.\n74. The accurate ground-truth of localization is furnished by a centimeter-level GNSS-RTK/INS navigation system, further details can be found in Section IV-B1.\n75. We use a Precision Time Protocol (PTP)[29]device to synchronize the clocks of various data collection devices across the sensor network.\n76. The PTP ensures time accuracy within nanoseconds.\n77. The synchronization device acquires the NMEA[30]output and pulse-per-second (PPS) signal from a u-blox M8T GNSS receiver to align the ROS time of the onboard computers with the GPS time.\n78. This enables sensors such as cameras, LiDAR, and IMU to record timestamps based on the synchronized GPS time.\n79. Moreover, to achieve time synchronization between different event cameras, the DAVIS346 on the rightmost side is configured as the master device and transmits trigger signal pulses to the remaining slave event cameras sequentially from left to right via external cables.\n80. To calibrate the IMU, we position it on a level surface for a duration of three hours and record the raw measurements.\n81. Utilizing the Kalibr toolbox, we can accurately calibrate the random walk and Gaussian white noise of the IMU.\n82. 2) Industrial Cameras Calibration: For industrial cameras, we move the sensor platform against the 9×7 checkerboard in the XYZ-axis and collect the sequence of RGB images and IMU.\n83. Then intrinsics calibration of industrial cameras is achieved by Kalibr toolbox[31], where the pinhole and radial-tangential camera models are adopted.\n84. 3) Event Cameras Calibration: For event cameras, DAVIS346 can produce fixed-rated frames, enabling imagebased calibration, while DVXplorer merely produces asynchronous event streams.\n85. Therefore, E2Calib[32][33] is used to achieve image reconstruction from event streams.\n86. 4) Infrared Camera Calibration: Due to infrared cameras solely capturing the temperature rather than the intensity difference, we design a distinct 9×7 checkerboard to make the pattern detectable for infrared cameras.\n87. As shown in Fig.3(a), the checkerboard intervals are affixed with aluminum materials, and then using a heating plate to raise the temperature of the checkerboard.\n88. Since the superior thermal dissipation of aluminum compared to plastic, a temperature contrast emerges between the two materials, enabling infrared cameras to distinctly capture the lattice shape of the checkerboard, as in Fig.3(b).\n89. With the special infrared image of the checkerboard, intrinsic can be calibrated by Kalibr.\n90. After completing intrinsics calibration, we move the sensor suite in front of checkerboards along the XYZ-RPY-axis and collect data simultaneously.\n91. Subsequently, the extrinsics and the temporal offset between all cameras and IMU could be estimated using Kalibr.\n92. For the calibration of mechanical LiDAR, LI-Init[34]is capable of achieving temporal and spatial calibration for LiDAR and IMU without checkerboards or extra devices in Fig.4.\n93. We rotate and move the device around the XYZ-axis to ensure sufficient excitation until the data accumulation is completed, thus we acquire the extrinsic transformation between LiDAR and IMU.\n94. Our dataset encompasses a wide range of driving scenes, including urban streets, urban roads, tunnels, highways, bridges, and suburban roads.\n95. We have specifically focused on scenarios where visual SLAM algorithms encounter difficulties.\n96. These scenarios involve high-speed motion (up to 110 km/h), limited texture, as well as difficult glare conditions in both daytime and nighttime driving.\n97. We also targeted situations where LiDAR SLAM encounters limitations, such as long corridors or areas with sparse geometric structures.\n98. The complete dataset is partitioned into 81 sequences to facilitate researchers in evaluating their algorithms.\n99. Each sequence has an approximate duration of 120 seconds.\n100. Additionally, we have retained a few sequences with long duration, lasting approximately 34 minutes, specifically for the evaluation of loop closure in large-scale environments and loop closure scenarios.\n101. The summary of sequence types can be found in TableIII. A. Scenarios 1) Dense Urban Street: This scenario focuses on lowspeed vehicles, around 30km/h, proceeding on highly urbanized areas and urban canyons in Hong Kong with multiple light conditions.\n102. The streets are narrow at 10m in width and buildings on both sides of the scene are dense.\n103. Meanwhile, the presence of congested traffic and dynamic crowds may produce the degradation of visual or LiDAR localization, such as Dense street day easy b.\n104. To evaluate the loop closure performance of SLAM, we remarkably recorded sequences Dense street difficult circle and Dense street difficult loop where our vehicle was circling in repeated routes.\n105. 2) Urban Road: This type of scenario records the vehicle traveling at an approximate speed of 60km/h on an expressway in Hong Kong with multiple weather conditions.\n106. Compared to the Dense Urban Street scenario, Urban Road sequences travel through Hong Kong city at a higher speed, while the buildings are not as tightly packed on either side and the road is more spacious with four lanes.\n107. Despite the absence of pedestrians on the road, the scene still includes vehicles overtaking, paralleling, and other situations where the relative motion is not consistent with the absolute motion.\n108. The aforementioned discrepancy might pose a challenge for the VIO or LIO system.\n109. Moreover, the sequence comprises the vehicle traveling during nighttime in rainy conditions.\n110. We record trajectories in rainy situations under nighttime like Urban road night difficult rainy a which are commonly faced in practical driving scenarios, whereas they are not present in previous datasets.\n111. 3) Tunnel: Tunnel scenarios commence with a high-speed vehicle on an open-sky highway, entering an enclosed tunnel without satellite reception.\n112. Inside the tunnel, GNSS positioning is unreliable since the satellite signal is completely blocked.\n113. Meanwhile, the scenario represents a typical and challenging scene for VIO and LIO systems due to the repetitive and texture-less environments for vision sensors and LiDAR.\n114. The sequence collections end after the vehicle exits the tunnel and continues to proceed on the highway for twenty seconds.\n115. 4) Highway: The scenario involves vehicles traveling at speeds up to 100km/h on low-texture highways both during the day and night, with sparse buildings alongside the road.\n116. High speeds, rapid changes in vehicle speed, repetitive visual scenes, and low-texture environments present significant challenges for autonomous driving.\n117. Meanwhile, the vibration of the vehicle body at high-speed motion amplifies the random walk and Gaussian white noise of IMU, thereby diminishing its reliability.\n118. 5) Bridge: The motion pattern of vehicles in bridge scenarios resembles that of highways, with vehicles traveling in a straight line at high speed along the bridge.\n119. However, this scene differs as there are no buildings on either side of the bridge, only the sea surrounds it.\n120. Bridges present scenes with limited texture, and the feature information within these scenes tends to be monotonous and repetitive, which further exacerbates the challenge of achieving accurate localization.\n121. 6) Suburban Road: Suburban road scenarios present complex natural environments characterized by winding and rugged roads, steep slopes, and narrow lanes.\n122. The vehicle navigates the serpentine mountain roads at a moderate speed (approximately 50km/h), with significant altitude changes.\n123. The abundant texture information in the mountain road scene facilitates visual algorithms to extract stable features and construct effective constraints.\n124. 1) Ground-truth Poses: We obtained the ground-truth positioning from the NovAtel SPAN-CPT[28], a highperformance GNSS RTK/INS integrated navigation system.\n125. The ground-truth of most existing event-based driving datasets are derived from LiDAR-SLAM[6][24], GPS/GNSS[6], GNSS-RTK[7][25][24].\n126. The ground-truth derived from LiDAR-SLAM relies on the estimation of vehicle trajectories using LiDAR SLAM which only provides relative trajectories.\n127. It is difficult to quantify the accuracy of ground-truth pose, and errors may even exceed ten meters in some cases.\n128. The complex environment or the equipment malfunctions may disrupt the satellite reception of GPS/GNSS, thus relying solely on GPS/GNSS for ground-truth pose may lead to significant drift.\n129. The GNSS-RTK device can only provide centimeter-level accuracy in the open sky[26]In contrast, our SPAN-CPT can provide continuous high accuracy aided by the internal fiber-optic gyroscopes under high-rise buildings, tunnels, and other environments with weak satellite signals.\n130. Furthermore, we post-process the ground-truth positioning from SPAN-CPT using the state-ofthe-art NovAtel Inertial Explorer[28]software to maximize the accuracy of the trajectory.\n131. For the GNSS positioning benchmark, we provide the WGS84 coordinate data for comparison.\n132. For the evaluation of SLAM algorithms, we provide the tools2to transform the ground-truth data from the WGS84 coordinates to the local frame/ENU frame based on the original points.\n133. 2) LiDAR Point Cloud Maps Generation: Utilizing the ground-truth pose for each frame in conjunction with their corresponding LiDAR point clouds, we accumulate these point clouds to construct a highly accurate LiDAR point cloud map to depict the TsingMa Bridge in Fig.6.\n134. The map encompasses rich spatial information, providing a detailed 3D reconstruction of the bridge and its surrounding areas.\n135. As shown in TableIV., we evaluate the performance of VINS-MONO[35], ORB-SLAM3[36], and ESVIO[37]Fig.6.\n136. The vehicle poses ground-truth on Google map with the LiDAR point cloud maps of Tsing Ma bridge.across various scenes and lighting conditions on our dataset.\n137. The accuracy is quantified using mean position error (MPE, %), which aligns the estimated trajectory with ground-truth through 6-DOF transformation (in SE3) computed by the tool[38].\n138. For the VINS-Mono, we evaluate it separately using RGB images and infrared images.\n139. Due to the resolution provided by industrial cameras being higher in contrast to the infrared camera, we achieve superior performance when utilizing RGB images.\n140. The ORB-SLAM3 often fails to robustly track features during high-speed vehicle movements, potentially resulting in the tracking thread restarts.\n141. The ESVIO leverages the complementary advantages of event streams and RGB images, allowing it to handle the lack of texture in RGB images under broad illumination conditions to achieve higher accuracy.\n142. Fig.7compares event, RGB images, and infrared images under different lighting conditions.\n143. The RGB images offer rich texture under regular luminance scenes in contrast to events and infrared images offer comparatively limited information, e.g., the infrared image struggles to accurately discern traffic left-turn symbol on the ground.\n144. Conversely, the RGB image may lose numerous environmental features under the conditions of low light or over-exposure.\n145. The infrared camera can capture infrared radiation beyond the visible spectrum and event cameras can detect pixel-level intensity changes at low latency.\n146. Both the event camera and the infrared camera are more resilient in external varying lighting conditions, providing effective visibility compared to the industrial camera, e.g., in nighttime scenes, event cameras can capture road signs, and the infrared camera can clearly capture the surrounding bushes.\n147. TableV. demonstrates the performance of LIO-SAM[39], LVI-SAM[40], Fast-LIO2[41], Point-LIO[42]across various scenes on our dataset.\n148. We use the same criteria introduced in Section V-A to evaluate the localization accuracy.\n149. Due to the tilt-mounted LiDAR setups (see Fig.1), we are able to acquire point clouds of towering buildings situated on both sides of the street.\n150. This installation approach compensated for the lack of vertical point clouds compared to the horizontally mounted LiDAR.\n151. In Fig.8, red point clouds are generated from a horizontally mounted LiDAR while white and green point clouds are generated from tilt-mounted LiDARs.\n152. We evaluate the performance of LOAM[43]using three different LiDARs (center, left, and right)in Dense street day medium circle a sequences.\n153. The MPE of LOAM using center LiDAR is 1.02%, compared to 8.67% using the left LiDAR and 2.00% using the right LiDAR.\n154. LOAM using the left LiDAR exhibits significant drift since it initially captures minimal point cloud information.\n155. Although the LOAM merely using tilt-mounted LiDAR produces less accurate results compared to the center LiDAR, multi-LiDAR fusion can integrate complementary information, thereby improving localization accuracy and constructing more precise point cloud maps.\n156. Meanwhile, tunnel scenes present challenges for LiDAR SLAM.\n157. We capture three consecutive frames of LiDAR point clouds at two-second intervals in Fig.9.\n158. It is evident that these LiDAR point clouds exhibit high similarity in the tunnel environment, potentially resulting in degradation phenomena and inaccurate state estimation.\n159. Due to space limitations, we positioned LiDAR closer to the event cameras.\n160. As a consequence, the infrared wavelengths emitted by LiDAR directly impinge on the photoreceptor of event cameras, resulting in continuous disturbances and flickering in the captured images and event streams.\n161. To address this issue, we implement infrared filters on event cameras to counteract the effect.\n162. However, this intervention led to a compromise, resulting in a degradation of the quality of the recorded event data.\n163. During the night or low illumination scenarios, we observed that when event cameras were directly toward a glowing light source, such as street lights or store lighting, event streams would exhibit persistent flickering and produce artifacts around the light source.\n164. This could potentially lead to a distorted view of the observed object.\n165. We postulate this phenomenon is related to the inherent principle of event cameras, and presently, there is no known solution to address this issue.\n166. In this paper, we propose an event-centric autonomous driving dataset generated with multiple sensors across various scenarios for developing SLAM algorithms.\n167. All sensors undergo meticulous calibration and are temporally synchronized at the hardware level.\n168. We employ the GNSS-RTK/INS navigation system, which provides centimeter-level accuracy, to acquire precise ground-truth of the vehicle.\n169. Furthermore, we conduct the evaluation of various state-of-the-art visual and LiDAR SLAM algorithms while identifying their constraints.\n170. We hope this dataset could contribute to the development of visual and LiDAR SLAM.\n\n请严格按照JSON格式返回：{"labels": [0,1,0,...]}，不要包含任何其他文字。'}], 'model': 'qwen-plus', 'response_format': {'type': 'json_object'}}}
2025-10-14 21:57:00,985 - DEBUG - Sending HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
2025-10-14 21:57:00,985 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-10-14 21:57:00,985 - DEBUG - send_request_headers.complete
2025-10-14 21:57:00,985 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-10-14 21:57:00,986 - DEBUG - send_request_body.complete
2025-10-14 21:57:00,986 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-10-14 21:57:07,639 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'vary', b'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding'), (b'x-request-id', b'caa24d27-cc4f-4151-b315-94b13c5170ea'), (b'x-dashscope-call-gateway', b'true'), (b'content-type', b'application/json'), (b'req-cost-time', b'6623'), (b'req-arrive-time', b'1760450212147'), (b'resp-start-time', b'1760450218770'), (b'x-envoy-upstream-service-time', b'6589'), (b'content-encoding', b'gzip'), (b'date', b'Tue, 14 Oct 2025 13:56:58 GMT'), (b'server', b'istio-envoy'), (b'transfer-encoding', b'chunked')])
2025-10-14 21:57:07,639 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-14 21:57:07,639 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-10-14 21:57:07,639 - DEBUG - receive_response_body.complete
2025-10-14 21:57:07,639 - DEBUG - response_closed.started
2025-10-14 21:57:07,639 - DEBUG - response_closed.complete
2025-10-14 21:57:07,639 - DEBUG - HTTP Response: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "200 OK" Headers({'vary': 'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding', 'x-request-id': 'caa24d27-cc4f-4151-b315-94b13c5170ea', 'x-dashscope-call-gateway': 'true', 'content-type': 'application/json', 'req-cost-time': '6623', 'req-arrive-time': '1760450212147', 'resp-start-time': '1760450218770', 'x-envoy-upstream-service-time': '6589', 'content-encoding': 'gzip', 'date': 'Tue, 14 Oct 2025 13:56:58 GMT', 'server': 'istio-envoy', 'transfer-encoding': 'chunked'})
2025-10-14 21:57:07,639 - DEBUG - request_id: caa24d27-cc4f-4151-b315-94b13c5170ea
2025-10-14 21:57:07,640 - DEBUG - API request completed in 6.66 seconds
2025-10-14 21:57:07,640 - DEBUG - Raw model response: {"labels": [0,1,1,1,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,1,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]}
2025-10-14 21:57:07,640 - INFO - Successfully processed 122 labels
2025-10-14 21:57:07,640 - ERROR - Label count mismatch for ECMD_ An Event_Centric Multisensory Driving Dataset for SLAM
2025-10-14 21:57:07,640 - INFO - Evaluating paper 13/18: Inline Citation Classification using Peripheral Context and Time_evolving Augmentation
2025-10-14 21:57:07,640 - INFO - Starting model prediction
2025-10-14 21:57:07,640 - INFO - Attempt 1 of 5
2025-10-14 21:57:07,640 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-13f0966a-07fe-4b25-9ce1-ef0785fbf087', 'json_data': {'messages': [{'role': 'system', 'content': '你是一个专门用于识别学术论文中数据集描述的助手。你需要判断每个输入的句子是否描述了研究中使用的数据集。请确保：\n    1. 只输出JSON格式的结果\n    2. 结果格式必须为 {"labels": [0,1,0,...]}\n    3. 不要输出任何其他文字或解释'}, {'role': 'user', 'content': '你是一位专业的学术论文数据集描述识别专家。请仔细分析下面这篇论文中的每个句子，判断它们是否描述了本研究特定构建的数据集信息。\n\n    论文标题: Inline Citation Classification using Peripheral Context and Time_evolving Augmentation\n\n    判断标准:\n    判断为1(数据集描述)的标准：\n    句子必须直接描述本研究特定构建的数据集,包括:\n    1. 该数据集的构建过程和方法\n    2. 该数据集的具体构成和规模\n    3. 该数据的来源和收集方式\n    4. 该数据的预处理步骤\n    5. 该数据集的可获取方式(如发布地址)\n    6. 或者其他直接或者间接描述了该数据集的句子\n\n    所有其他类型的句子均标记为0。\n    \n    请对每个句子进行分析，返回一个JSON格式的标签数组。格式要求：\n    1. 必须是有效的JSON格式\n    2. 只包含labels字段，值为0和1组成的数组\n    3. 数组长度必须与句子数量相同\n    4. 示例格式：{"labels": [0,1,0,1,...]}\n\n    需要判断的句子：\n\n    1. Abstract: Citation plays a pivotal role in determining the associations among research articles.\n2. It portrays essential information in indicative, supportive, or contrastive studies.\n3. The task of inline citation classification aids in extrapolating these relationships; However, existing studies are still immature and demand further scrutiny.\n4. Current datasets and methods used for inline citation classification only use citation-marked sentences constraining the model to turn a blind eye to domain knowledge and neighboring contextual sentences.\n5. In this paper, we propose a new dataset, named 3Cext, which along with the cited sentences, provides discourse information using the vicinal sentences to analyze the contrasting and entailing relationships as well as domain information.\n6. We propose PeriCite, a Transformer-based deep neural network that fuses peripheral sentences and domain knowledge.\n7. Our model achieves the state-of-the-art on the 3Cext dataset by +0.09 F1 against the best baseline.\n8. We conduct extensive ablations to analyze the efficacy of the proposed dataset and model fusion methods.\n9. For the past several decades, there has been an interest in citation analysis for research evaluation.\n10. Researchers have emphasized the necessity for new methodologies that take into account various components of citing sentences.\n11. A wellknown qualitative technique for assessing the scientific influence is to analyze the sentence in which the research article is mentioned to ascertain the purpose behind the citation.\n12. The context of the citation, or the text in which the cited document is mentioned, has proven to be an effective indicator of the citation\'s intent[25].\n13. Measuring the scientific impact of research articles requires a fundamental understanding of citation intent.\n14. A great way to gauge the significance of a scientific publication is to determine why citations are made in one\'s work and how significant they are.\n15. Previous methods for citation context categorization used a range of annotation techniques with low-to-high granularity.\n16. Comparing the earlier systems is extremely difficult due to the absence of standardized methodologies and annotation schemes.\n17. The 3C shared task[12,13]used a piece of the Academic Citation Typing (ACT) dataset to categorize the reference anchor into \'function\' or \'purpose\' by looking at the citing sentence or the text that contains the citation[19].\n18. Only quantitative elements are considered in traditional citation analysis based solely on the citation count.\n19. One of the biggest obstacles to citation context analysis for citation identification is that there is no multidisciplinary dataset and that there isn\'t any medium to fine-grained schemes that adequately represent the function and its influence[8].\n20. To address this challenge, Kunnath et al.[12]provided a unified task, called the 3C Shared Task, to compare several citation classification approaches on the same dataset to address the shortcomings of citation context categorization.\n21. The main distinction in the second iteration of this task[13]was that the subtasks contained full-text datasets.\n22. However, even with the full text, the metadata associated with the citation sentence was not adequate to understand the reasoning for the citation.\n23. To alleviate the above limitations, we propose a new dataset, named 3Cext, and a new model, named PeriCite that combines the advantages of augmentation and peripheral context.\n24. Experiments show that the cited sentences heavily rely on the peripheral context to strengthen an argument by contrasting or entailing information.\n25. Our main contributions are as follows 1.We extend the 3C dataset[13]-3Cext, which, along with the cited sentence, adds more discourse information by providing contrasting and entailing information using the peripheral sentences.\n26. 2. We propose a novel model, PeriCite, which uses spatial fusion and crosstext attention to attend to contextual information for the peripheral sentences and time-evolving augmentation to counter class imbalance during the training time.\n27. 3. We also compare our proposed model against various baselines and show the efficacy of the module along with ablation studies and error analysis.\n28. We also compare our proposed model against various baselines and show the efficacy of the module along with ablation studies and error analysis.\n29. In this section, we discuss our proposed 3Cext dataset in detail.\n30. Kunnath et al.[12]introduced the ACT dataset, with annotations for 11, 233 citations annotated by 883 authors.\n31. The cited label was masked with "#AUTHOR TAG" denoting the position of the cited object.\n32. Additionally, the 3C dataset contained full text and the label denoting the class of a particle citation (c.f.Table2).\n33. In our work, we extend the 3C dataset to house more discourse information to explain better why a citation is present in a sentence.\n34. Our intuition is that the cited sentences mostly either entail or contrast the adjoining sentences.\n35. To capture the peripheral sentences, we extract the full-text files corresponding to the COREIDs (unique paper ID) in our dataset to follow through on this discovery.\n\n请严格按照JSON格式返回：{"labels": [0,1,0,...]}，不要包含任何其他文字。'}], 'model': 'qwen-plus', 'response_format': {'type': 'json_object'}}}
2025-10-14 21:57:07,641 - DEBUG - Sending HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
2025-10-14 21:57:07,641 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-10-14 21:57:07,641 - DEBUG - send_request_headers.complete
2025-10-14 21:57:07,641 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-10-14 21:57:07,641 - DEBUG - send_request_body.complete
2025-10-14 21:57:07,641 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-10-14 21:57:10,436 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'vary', b'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding'), (b'x-request-id', b'2678d524-b1f7-4ee4-af0a-f5c295c7438c'), (b'x-dashscope-call-gateway', b'true'), (b'content-type', b'application/json'), (b'req-cost-time', b'2764'), (b'req-arrive-time', b'1760450218803'), (b'resp-start-time', b'1760450221567'), (b'x-envoy-upstream-service-time', b'2762'), (b'content-encoding', b'gzip'), (b'date', b'Tue, 14 Oct 2025 13:57:01 GMT'), (b'server', b'istio-envoy'), (b'transfer-encoding', b'chunked')])
2025-10-14 21:57:10,437 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-14 21:57:10,437 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-10-14 21:57:10,437 - DEBUG - receive_response_body.complete
2025-10-14 21:57:10,437 - DEBUG - response_closed.started
2025-10-14 21:57:10,437 - DEBUG - response_closed.complete
2025-10-14 21:57:10,438 - DEBUG - HTTP Response: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "200 OK" Headers({'vary': 'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding', 'x-request-id': '2678d524-b1f7-4ee4-af0a-f5c295c7438c', 'x-dashscope-call-gateway': 'true', 'content-type': 'application/json', 'req-cost-time': '2764', 'req-arrive-time': '1760450218803', 'resp-start-time': '1760450221567', 'x-envoy-upstream-service-time': '2762', 'content-encoding': 'gzip', 'date': 'Tue, 14 Oct 2025 13:57:01 GMT', 'server': 'istio-envoy', 'transfer-encoding': 'chunked'})
2025-10-14 21:57:10,438 - DEBUG - request_id: 2678d524-b1f7-4ee4-af0a-f5c295c7438c
2025-10-14 21:57:10,439 - DEBUG - API request completed in 2.80 seconds
2025-10-14 21:57:10,439 - DEBUG - Raw model response: {"labels": [0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,1,0,0,0,1,0,1]}
2025-10-14 21:57:10,439 - INFO - Successfully processed 35 labels
2025-10-14 21:57:10,447 - INFO - Evaluating paper 14/18: llm_japanese_dataset v0_ Construction of Japanese Chat Dataset for Large Language Models and its Methodology
2025-10-14 21:57:10,447 - INFO - Starting model prediction
2025-10-14 21:57:10,447 - INFO - Attempt 1 of 5
2025-10-14 21:57:10,448 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-599cb472-86d1-4dac-9d0e-e1cfcc3958a7', 'json_data': {'messages': [{'role': 'system', 'content': '你是一个专门用于识别学术论文中数据集描述的助手。你需要判断每个输入的句子是否描述了研究中使用的数据集。请确保：\n    1. 只输出JSON格式的结果\n    2. 结果格式必须为 {"labels": [0,1,0,...]}\n    3. 不要输出任何其他文字或解释'}, {'role': 'user', 'content': '你是一位专业的学术论文数据集描述识别专家。请仔细分析下面这篇论文中的每个句子，判断它们是否描述了本研究特定构建的数据集信息。\n\n    论文标题: llm_japanese_dataset v0_ Construction of Japanese Chat Dataset for Large Language Models and its Methodology\n\n    判断标准:\n    判断为1(数据集描述)的标准：\n    句子必须直接描述本研究特定构建的数据集,包括:\n    1. 该数据集的构建过程和方法\n    2. 该数据集的具体构成和规模\n    3. 该数据的来源和收集方式\n    4. 该数据的预处理步骤\n    5. 该数据集的可获取方式(如发布地址)\n    6. 或者其他直接或者间接描述了该数据集的句子\n\n    所有其他类型的句子均标记为0。\n    \n    请对每个句子进行分析，返回一个JSON格式的标签数组。格式要求：\n    1. 必须是有效的JSON格式\n    2. 只包含labels字段，值为0和1组成的数组\n    3. 数组长度必须与句子数量相同\n    4. 示例格式：{"labels": [0,1,0,1,...]}\n\n    需要判断的句子：\n\n    1. Abstract: This study constructed a Japanese chat dataset for tuning large language models (LLMs), which consist of about 8.4 million records.\n2. Recently, LLMs have been developed and gaining popularity.\n3. However, high-performing LLMs are usually mainly for English.\n4. There are two ways to support languages other than English by those LLMs: constructing LLMs from scratch or tuning existing models.\n5. However, in both ways, datasets are necessary parts.\n6. In this study, we focused on supporting Japanese in those LLMs and making a dataset for training or tuning LLMs in Japanese.\n7. The dataset we constructed consisted of various tasks, such as translation and knowledge tasks.\n8. In our experiment, we tuned an existing LLM using our dataset and evaluated the performance qualitatively.\n9. The results suggest that our dataset is possibly beneficial for LLMs.However, we also revealed some difficulties in constructing LLMs in languages other than English.\n10. 2 Dataset Construction: izumi-lab/llm-japanese-dataset v0 In this study, we created a Japanese chat dataset.\n11. The dataset 1 contains 8,393,726 data points.\n12. In the following, we describe the details of datasets and their creation process.\n13. Large language models (LLMs) have recently achieved remarkable progress in performance and generalization.\n14. Specifically, Transformer-based LLMs such as BERT[3]and the GPT series[17,18,1]have demonstrated high-performance thanks to their pre-training.\n15. Furthermore, models that have evolved from these, such as Chat-GPT[14]and GPT4[15], have gained popularity for their remarkable performance.\n16. Other models such asBard [6], LLaMA[24], Dolly[2], Bloom[21], and Vicuna[26]have also emerged.\n17. Masanori HIRANO, Masahiro SUZUKI, and Hiroki SAKAJI The University of Tokyo, 7-3-1 Hongo, Bunkyo, Tokyo 113-8656 Japan, e-mail: research@mhirano.jp,b2019msuzuki@socsim.org,sakaji@sys.t.utokyo.ac.jpSome of those models are already provided to consumers as a web service.\n18. Moreover, via API, those models and services are also now available for sub-parts of web services, and many spin-off services are emerging.\n19. However, despite the prosperity of language models, there are still challenges in handling diverse prompts, including prompts written in languages other than English.\n20. For example, Alpaca[23]dataset has been proposed due to the incompleteness of LLaMA\'s response.\n21. However, the dataset of Alpaca is only available in English, and the incompleteness pointed out by Alpaca has not been filled yet in the other languages.\n22. Moreover, LLaMA has difficultness to respond appropriately to some prompts in languages other than English.\n23. Considering these challenges, it is necessary to enhance models\' performances in languages other than English.\n24. However, it is not a good idea to study a specific model in terms of performance improvements in the other language.\n25. Moreover, model development is still ongoing and very competitive, and the situation is changing dramatically recently.\n26. It is also easy to assume that newer models with better performance will emerge in a few months or even 1-2 months.\n27. Therefore, enhancing datasets that support model training may be more useful than focusing on specific models.\n28. This approach may also lower the barrier to adapting new models to languages other than English.\n29. Therefore, this study constructed a new chat dataset in Japanese for LLM training, which contains approximately 8.4 million data points, and demonstrated the performance of the dataset qualitatively.\n30. The dataset and trained models are opensourced and publicly available.\n31. The details are as follows: • Dataset: https://huggingface.co/datasets/izumi-lab/llm-japanesedataset • Trained Models (LLaMA 1 epoch): https://huggingface.co/izumi-lab/llama-13b-japanese-lora-v0-1ep.\n32. The more details are explained in the following.\n33. Moreover, data expansion and additional model training are planned as future tasks.\n34. The format of the chat data used for model training is shown below.\n35. In the description of the dataset later, we will omit some of the introductory parts and line breaks.\n36. Below is an instruction that describes a task, paired with an input that provides further context.\n37. Write a response that appropriately completes the request.\n38. Note that, in the following examples, the underlined sentences are originally written in Japanese.\n39. We utilized the aforementioned ParaNatCom[25]to create tasks related to our research paper.\n40. The license for the dataset is CC BY 4.0, and the size of the created dataset is 1,732.\n41. ### Instruction: Please make a title from the abstract of the paper.\n42. 1  ### Input: Superthin nanostructures, particularly with atomic-level thicknesses, typically display unique optical properties because of their exceptional light-matter interactions.\n43. ### Instruction: Please rephrase the following Japanese into easy Japanese.\n44. 1 ### Input: Bill has no sense of adventure at all.\n45. 1  ### Response: Bill has no desire to do anything dangerous.\n46. In addition, we incorporated Japanese-translated versions of existing publicly available chat datasets.\n47. The following datasets were included: • Japanese-Alpaca-LoRA17: A translation of the Alpaca[23]dataset into Japanese.\n48. The license is Apache License 2.0.\n49. • databricks-dolly-15k-ja18: A Japanese-translated version of the dataset used for training Dolly[2].\n50. The license is CC BY-SA 3.0.\n51. The dataset size is 15,015.\n52. This study used LoRA[7]as a method to fine-tune LLMs without significant performance degradations.\n53. It is because building LLMs from scratch requires a massive amount of computational resources.\n54. Furthermore, LLMs with a large number of parameters require GPU resources not only for pre-training but also for fine-tuning.\n55. On the other hand, LoRA updates only small parts of LLM parameters.\n56. Therefore, LoRA is a feasible option for us to evaluate the benefits of our dataset.\n57. The main parameters used in the experiment are shown below.\n58. • Base model: LLaMA 13B[24]• Learning rate: 3e-4 We used PEFT[12]and DeepSpeed ZeRO 2[19]for the implementation.\n59. This tuned model is publicly available at https://huggingface.co/izumilab/llama-13b-japanese-lora-v0-1ep.\n60. In order to increase the reproducibility of the evaluation experiment, the temperature parameter for prompt generation was set to 0.0.\n61. Below are some qualitative comparisons we conducted to assess performance.\n62. The phone rings.\n63. When the call is received, the person receiving the call should receive the call.\n64. 1   Response Example(5)### Input: What are the three major festivals in Kyoto?\n65. )### Output(LLaMA): What are the three major festivals in Kyoto?\n66. What are the three major festivals in Kyoto?\n67. What are the three major festivals in Kyoto?\n\n请严格按照JSON格式返回：{"labels": [0,1,0,...]}，不要包含任何其他文字。'}], 'model': 'qwen-plus', 'response_format': {'type': 'json_object'}}}
2025-10-14 21:57:10,448 - DEBUG - Sending HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
2025-10-14 21:57:10,448 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-10-14 21:57:10,448 - DEBUG - send_request_headers.complete
2025-10-14 21:57:10,448 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-10-14 21:57:10,448 - DEBUG - send_request_body.complete
2025-10-14 21:57:10,449 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-10-14 21:57:15,145 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'vary', b'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding'), (b'x-request-id', b'5f2beba2-5f49-4afa-be1c-373df4e51f84'), (b'x-dashscope-call-gateway', b'true'), (b'content-type', b'application/json'), (b'req-cost-time', b'4665'), (b'req-arrive-time', b'1760450221610'), (b'resp-start-time', b'1760450226276'), (b'x-envoy-upstream-service-time', b'4663'), (b'content-encoding', b'gzip'), (b'date', b'Tue, 14 Oct 2025 13:57:05 GMT'), (b'server', b'istio-envoy'), (b'transfer-encoding', b'chunked')])
2025-10-14 21:57:15,145 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-14 21:57:15,146 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-10-14 21:57:15,146 - DEBUG - receive_response_body.complete
2025-10-14 21:57:15,146 - DEBUG - response_closed.started
2025-10-14 21:57:15,146 - DEBUG - response_closed.complete
2025-10-14 21:57:15,146 - DEBUG - HTTP Response: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "200 OK" Headers({'vary': 'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding', 'x-request-id': '5f2beba2-5f49-4afa-be1c-373df4e51f84', 'x-dashscope-call-gateway': 'true', 'content-type': 'application/json', 'req-cost-time': '4665', 'req-arrive-time': '1760450221610', 'resp-start-time': '1760450226276', 'x-envoy-upstream-service-time': '4663', 'content-encoding': 'gzip', 'date': 'Tue, 14 Oct 2025 13:57:05 GMT', 'server': 'istio-envoy', 'transfer-encoding': 'chunked'})
2025-10-14 21:57:15,147 - DEBUG - request_id: 5f2beba2-5f49-4afa-be1c-373df4e51f84
2025-10-14 21:57:15,148 - DEBUG - API request completed in 4.70 seconds
2025-10-14 21:57:15,148 - DEBUG - Raw model response: {"labels": [1,0,0,0,0,1,1,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,1,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0]}
2025-10-14 21:57:15,148 - INFO - Successfully processed 62 labels
2025-10-14 21:57:15,148 - ERROR - Label count mismatch for llm_japanese_dataset v0_ Construction of Japanese Chat Dataset for Large Language Models and its Methodology
2025-10-14 21:57:15,148 - INFO - Evaluating paper 15/18: Re 3 Dial_ Retrieve_ Reorganize and Rescale Conversations for Long_Turn Open_Domain Dialogue Pre_training
2025-10-14 21:57:15,148 - INFO - Starting model prediction
2025-10-14 21:57:15,148 - INFO - Attempt 1 of 5
2025-10-14 21:57:15,150 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-05c1652f-f077-4865-9398-e81cde327fce', 'json_data': {'messages': [{'role': 'system', 'content': '你是一个专门用于识别学术论文中数据集描述的助手。你需要判断每个输入的句子是否描述了研究中使用的数据集。请确保：\n    1. 只输出JSON格式的结果\n    2. 结果格式必须为 {"labels": [0,1,0,...]}\n    3. 不要输出任何其他文字或解释'}, {'role': 'user', 'content': '你是一位专业的学术论文数据集描述识别专家。请仔细分析下面这篇论文中的每个句子，判断它们是否描述了本研究特定构建的数据集信息。\n\n    论文标题: Re 3 Dial_ Retrieve_ Reorganize and Rescale Conversations for Long_Turn Open_Domain Dialogue Pre_training\n\n    判断标准:\n    判断为1(数据集描述)的标准：\n    句子必须直接描述本研究特定构建的数据集,包括:\n    1. 该数据集的构建过程和方法\n    2. 该数据集的具体构成和规模\n    3. 该数据的来源和收集方式\n    4. 该数据的预处理步骤\n    5. 该数据集的可获取方式(如发布地址)\n    6. 或者其他直接或者间接描述了该数据集的句子\n\n    所有其他类型的句子均标记为0。\n    \n    请对每个句子进行分析，返回一个JSON格式的标签数组。格式要求：\n    1. 必须是有效的JSON格式\n    2. 只包含labels字段，值为0和1组成的数组\n    3. 数组长度必须与句子数量相同\n    4. 示例格式：{"labels": [0,1,0,1,...]}\n\n    需要判断的句子：\n\n    1. Abstract: Pre-training on large-scale open-domain dialogue data can substantially improve the performance of dialogue models.\n2. However, the pre-trained dialogue model\'s ability to utilize long-range context is limited due to the scarcity of long-turn dialogue sessions.\n3. Most dialogues in existing pre-training corpora contain fewer than three turns of dialogue.\n4. To alleviate this issue, we propose the Retrieve, Reorganize and Rescale framework (Re 3 Dial), which can automatically construct billion-scale long-turn dialogues by reorganizing existing short-turn ones.\n5. Given a short-turn session, Re 3 Dial first employs a session retriever to retrieve coherent consecutive sessions.\n6. To this end, we train the retriever to capture semantic and discourse relations within multi-turn dialogues through contrastive training.\n7. Next, Re 3 Dial samples a session from retrieved results following a diversity sampling strategy, which is designed to penalize repetitive or generic sessions.\n8. A longer session is then derived by concatenating the original session and the sampled session.\n9. By repeating the above process, Re 3 Dial can yield a coherent long-turn dialogue.\n10. Extensive experiments on multiple multi-turn dialogue benchmarks demonstrate that Re 3 Dial significantly improves the dialogue model\'s ability to utilize long-range context and thus generate more sensible and informative responses.\n11. Finally, we build a toolkit for efficiently rescaling conversations with Re 3 Dial, which enables us to construct a corpus containing 1B Chinese dialogue sessions with 11.3 turns on average (5× longer than the original corpus).\n12. Our retriever model, code, and data is publicly available at https://github.com/thu-coai/Re3Dial.\n13. Building intelligent open-domain dialogue systems that can generate coherent and engaging multi-turn dialogues with humans has been one of the long-Parents really expect too much from their children in this society, and such children are 100% mentally unhealthy even if they achieve success in the future.\n14. There is no need for children to start working so hard from such a young age.standing goals in AI.\n15. Recently, a variety of largescale open-domain pre-trained dialogue models have dramatically promoted this progress(Roller et al., 2020;Zhou et al., 2021;Shuster et al., 2022b).\n16. And a critical ingredient to the success of these models is the pre-training dialogue corpus.\n17. However, while existing dialogue pre-training corpus collects millions to billions of dialogues from public social media, e.g., Reddit for English(Roller et al., 2020)and Weibo for Chinese(Zhou et al., 2021), long-turn dialogues are highly scarce.\n18. More specifically, based on the publicly reported data statistics shown in Figure1(a), most dialogues in existing pre-training corpora only have less than three turns.\n19. The lack of large-scale long-turn di-alogue data restricts dialogue models from deriving more advanced abilities to utilize long-range context for modeling multi-turn dialogues during pre-training(Xu et al., 2021(Xu et al., , 2022b)).\n20. In this paper, we focus on answering the following research question: Can we automatically build a billionscale long-turn dialogue corpus by reorganizing existing short-turn dialogues?\n21. Our basic idea is to construct a long-turn dialogue via recursively retrieving and selecting one consecutive session from the existing dialogue corpus.\n22. Despite the simplicity of this idea, we still face several challenges to make the constructed corpus effective in enhancing long-turn dialogue pre-training.\n23. First, the selected session should be coherent with the query session.\n24. Otherwise, it will introduce noisy utterances without long-range dependency or break the conversation flow(Liu et al., 2021), which may impact the performance of dialogue models.\n25. Second, our in-depth analysis reveals that the retrieved sessions tend to be biased to be relevant but semantically repetitive with the query or overly generic (e.g., "A: Haha, it\'s so cute.B: Haha! LMAO.") due to both the data bias in the dialogue corpus(Zhou et al., 2021;Lee et al., 2021;Li et al., 2015;Liu et al., 2018)and the model bias of the retriever(Thakur et al., 2021).\n26. These biases significantly lower the diversity and informativeness of the reorganized long-turn dialogues.\n27. To tackle the above challenges, we propose the Retrieve, Reorganize and Rescale framework (Re 3 Dial), which employs an Unsupervised Dense Session Retriever (UDSR) to retrieve coherent short-turn dialogues and reorganize them into a long-turn one.\n28. We train UDSR through contrastive learning by taking consecutive dialogue segments from the same dialogue as positive pairs and those from different dialogues as negative pairs.\n29. To avoid overly retrieving semantically repetitive or generic sessions, we propose a diversity sampling strategy, effectively improving the diversity and informativeness of the reorganized long-turn dialogues.\n30. We verify the effectiveness of Re 3 Dial on three Chinese multi-turn open-domain dialogue benchmarks.\n31. Extensive experiments demonstrate that Re 3 Dial consistently and significantly enhances the dialogue model\'s ability to utilize long-range context, leading to more sensible and informative responses in multi-turn dialogue.\n32. Finally, we develop a toolkit for efficiently rescaling conversations with Re 3 Dial, which enables us to construct a corpus containing 1B Chinese dialogue sessions with 11.3 turns on average (5× longer than that of the original EVA corpus).\n33. We will make our retriever model, toolkit, and data public.\n34. We believe our work provides new opportunities in long-turn dialogue pre-training to the research community.\n35. • We propose to train a dense session retriever on massive unlabeled plain dialogue data with contrastive learning to capture the global semantic and discourse relations within multiturn dialogues.\n36. We also propose the diversity sampling strategy to improve the diversity and informativeness of the automatically constructed corpus.\n37. In the past few years, large-scale pre-training has greatly promoted the progress of the NLP community(Brown et al., 2020).\n38. Recently, large-scale pre-training has also become the mainstream approach to building open-domain dialogue models, both in English(Zhang et al., 2019;Roller et al., 2020;Thoppilan et al., 2022)and Chinese(Bao et al., 2020;Zhou et al., 2021;Gu et al., 2022;Wen et al., 2022).\n39. Through pre-training on massive dialogue data crawled from public social media, these models exhibit strong conversational ability, significantly outperforming traditional non-pre-trained dialogue models.\n40. However, the scarcity of longturn dialogues in the pre-training corpus hinders these models from deriving a better ability to utilize long-range context for modeling multi-turn dialogues during pre-training.\n41. To alleviate this issue, we study how to automatically and efficiently build a large-scale long-turn dialogue corpus based on the existing short-turn dialogue corpus.\n42. We train UDSR on a subset of the EVA pretraining corpus(Zhou et al., 2021), which contains 1,000,000/49,000/1,000 examples for the train/validation/test split.\n43. More details of data processing are provided in Appendix A.1.\n44. We adopt BERT-base(Devlin et al., 2018)as the encoder backbone.\n45. The parameters of E q and E c are not shared according to our preliminary experiments.\n46. Settings We consider three general scenarios where Re Benchmarks We conduct evaluations on three widely-adopted Chinese open-domain multi-turn dialogue benchmarks, including KdConv(Zhou et al., 2020), DuLeMon(Xu et al., 2022b), and NaturalConv(Wang et al., 2021), each has 16~20 turns on average.\n47. Data statistics are shown in Table9.\n48. Metrics We adopt the following automatic metrics for evaluation.\n49. PPL zero-shot measures the perplexity on the test set without fine-tuning on the downstream training sets.\n50. PPL measures the perplexity on the test set after fine-tuning.\n51. BLEU-N measures the precision of the n-gram overlap between generated and ground-truth responses(Papineni et al., 2002)after fine-tuning.\n52. ROUGE-L measures the recall of the n-gram overlap between generated and ground-truth responses(Lin, 2004)after fine-tuning.\n53. Distinct-N measures the percentage of the unique n-grams over all the generated n-grams after fine-tuning(Li et al., 2015).\n54. Table2shows the automatic evaluation results.\n55. In the zero-shot setting, Re  46.25 on DuLeMon, compared to the original baseline\'s performance of 48.79.\n56. This indicates a better ability in multi-turn dialogue modeling.\n57. Moreover, beyond benefiting zero-shot performance, Re 3 Dial can also significantly improve the model\'s performance after fine-tuning on sizable crowdsourcing high-quality long-turn datasets.\n58. Specifically, the Re 3 Dial-trained model achieves better perplexity, BLEU, and ROUGE scores, while showing an improved or comparable generation diversity.\n59. In summary, these results demonstrate that Re 3 Dial provides a well-generalized data foundation in the era of large-scale dialogue pre-training.\n60. We conduct a pair-wise human evaluation to study the models\' performance when provided with dialogue contexts of different lengths.\n61. We first randomly sample 100 long-turn contexts (consisting of at least six turns) from DuLeMon as the Longturn test set.\n62. We then extract the last utterances from these contexts to form the Short-turn test set.\n63. We hence obtain 400 generated responses from the two models.\n64. For each pair of responses (one by the Re 3 Dial-trained model and the other by the Original-trained model), three annotators are hired to give a preference in sensibleness and informativeness, respectively.\n65. Sensibleness mea- sures whether the response is relevant and consistent with the context.\n66. Informativeness measures whether the response is informative given the context.\n67. We adopt majority voting to make final decisions among three annotators.\n68. Effect of Retriever We compare different approaches to retrieve dialogue sessions and evaluate the final dialogue model performance.\n69. We try Random sampling, a term-based retriever BM25, and a state-of-the-art dense retriever Contriever.\n70. Table3presents the results.\n71. All baselines bring fewer improvements or even inversely hurt model performance, especially zero-shot performance in the further pre-training setting.\n72. In contrast, using the retriever in Re 3 Dial achieves consistent and significant improvements across different benchmarks and pre-training settings.\n73. To gain a deeper understanding of the effectiveness of different retrievers in capturing global semantic and discourse relations within multi-turn dialogues, we propose to evaluate the retriever using individual tests in different aspects(Ribeiro et al., 2020).\n74. To this end, we first construct positive pairs following the strategy illustrated in Section 3.1 and introduce perturbations to create negative pairs.\n75. We then compute the retriever\'s accuracy in discriminating between positive and negative pairs, expecting it assigns a higher score to positive pairs.\n76. Our evaluation focuses on three aspects: Irrelevance, Local Relevance, and Discourse Incoherence.\n77. For example, to create a locally relevant negative pair, we keep one utterance from the positive session unchanged while replacing the other utterances with a randomly sampled session.\n78. More details can be found in Appendix E.\n79. The results shown in Table4Overall, these results indicate that automatically building long-turn dialogues to enhance pretraining is non-trivial.Simply improving dialogue turns is insufficient.\n80. It is important to retrieve coherent sessions based on both global semantic relevance and discourse coherence within multi-turn dialogues rather than relying solely on word overlap or semantic similarity.\n81. Otherwise, it will introduce unexpected noise or biases and lead to slightly improved or even decreased model performance.\n82. To further investigate the influence of the proposed diversity sampling strategy in Re 3 Dial, we conduct an ablation study.\n83. As shown in Table5, the dialogue-level and corpus-level weights reduce the bias towards repetitive and generic sessions and improve the diversity and the informativeness of the constructed corpus as expected.\n84. Finally, both of them contribute to the pre-trained dialogue model\'s performance.\n85. To manifest the benefits of Re 3 Dial, we visualize the distribution of PPL zero-shot on samples with varying numbers of dialogue context turns.\n86. Specifically, we first (2) Although other retrieval baselines also exhibit a sharper decreasing trend in perplexity compared to the Original-trained model, they generally yield higher perplexity.\n87. This implies that while these baselines enhance the utilization of long-range context, they capture fewer long-range dependencies compared to Re 3 Dial and may even exhibit inferior performance when the local context is more effectively utilized.\n88. While Re 3 Dial aims to construct a long-turn dialogue pre-training corpus to enhance the utilization of long-range context, there is another line of work that focuses on compressing long contexts into short contexts.\n89. We hence additionally conduct experiments on a retrieval-based baseline and a summarization-based baseline for long-term context modeling and compare them with Re 3 Dial.\n90. We introduce an additional summarization model to summarize long-term context into short sentences.\n91. We try two summarization models: (1) Pegasus-523M(Zhang et al., 2020): It is a widely-adopted encoder-decoder model specifically pre-trained and fine-tuned for text summarization.\n92. (2) ChatGLM-66B(Zeng et al., 2022): It is a widely-adopted instruction-tuned large language model.\n93. We report the average PPL zero-shot over three multi-turn dialogue benchmarks.\n94. From the results shown in Table6, we observe that Re 3 Dial significantly outperforms all baselines in long-turn dialogue benchmarks.\n95. Moreover, augmenting the dialogue model with a context summarization model or a retriever shows less improvement or inversely hurts model performance in several cases.\n96. On the one hand, the two-stage framework suffers from error propagation due to the introduced summarization model or the retriever.\n97. For example, both the summarization model and the retriever may lose important information in the original context.\n98. Moreover, the summarization model could also suffer from hallucination problems(Maynez et al., 2020), thereby introducing new noises.\n99. In contrast, Re 3 Dial keeps the original long-turn context unchanged and thus does not lead to information loss or introduce new noises.\n100. On the other hand, we conjecture that augmenting dialogue models with the context summarization model requires further training on summarizationbased dialogue datasets(Xu et al., 2022a).\n101. In contrast, Re 3 Dial does not require collecting additional training datasets and greatly improves the model performance.\n102. As shown in Table7, the Original-trained model mainly focuses on local context and tends to generate more generic responses (e.g., "I think the same" in responding to the preceding utterance, "they thought it was too risky").\n103. In contrast, the Re 3 Dial-trained dialogue model generates words related to the long-range context (e.g., "fashion designer" which has been mentioned nine turns prior), Original: Well, actually I think the same.\n104. Re 3 Dial: I think it would be a good idea to find another experienced fashion designer , which will help you to achieve your dream.\n105. Table7: Generated responses from the model pretrained on Re 3 Dial and Original corpus (translated from Chinese to English).\n106. We highlight the generated spans that are related to long-range context. leading to a more sensible and specific response.\n107. To show the efficiency of constructing large-scale long-turn dialogue data with Re 3 Dial and allow researchers to explore Re 3 Dial easily, we finally release Re 3 Dial-1B, an improved corpus based on the original EVA corpus that contains 1B sessions with 11.3 turns on average (5× longer than that of the original EVA corpus).\n108. The whole pipeline costs about five days with 32 V100 32G GPUs.\n\n请严格按照JSON格式返回：{"labels": [0,1,0,...]}，不要包含任何其他文字。'}], 'model': 'qwen-plus', 'response_format': {'type': 'json_object'}}}
2025-10-14 21:57:15,153 - DEBUG - Sending HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
2025-10-14 21:57:15,153 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-10-14 21:57:15,153 - DEBUG - send_request_headers.complete
2025-10-14 21:57:15,153 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-10-14 21:57:15,153 - DEBUG - send_request_body.complete
2025-10-14 21:57:15,153 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-10-14 21:57:23,700 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'vary', b'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding'), (b'x-request-id', b'0132a780-a87e-4eff-89cc-01b551a947f4'), (b'x-dashscope-call-gateway', b'true'), (b'content-type', b'application/json'), (b'req-cost-time', b'8515'), (b'req-arrive-time', b'1760450226315'), (b'resp-start-time', b'1760450234831'), (b'x-envoy-upstream-service-time', b'8483'), (b'content-encoding', b'gzip'), (b'date', b'Tue, 14 Oct 2025 13:57:14 GMT'), (b'server', b'istio-envoy'), (b'transfer-encoding', b'chunked')])
2025-10-14 21:57:23,700 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-14 21:57:23,701 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-10-14 21:57:23,701 - DEBUG - receive_response_body.complete
2025-10-14 21:57:23,701 - DEBUG - response_closed.started
2025-10-14 21:57:23,701 - DEBUG - response_closed.complete
2025-10-14 21:57:23,702 - DEBUG - HTTP Response: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "200 OK" Headers({'vary': 'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding', 'x-request-id': '0132a780-a87e-4eff-89cc-01b551a947f4', 'x-dashscope-call-gateway': 'true', 'content-type': 'application/json', 'req-cost-time': '8515', 'req-arrive-time': '1760450226315', 'resp-start-time': '1760450234831', 'x-envoy-upstream-service-time': '8483', 'content-encoding': 'gzip', 'date': 'Tue, 14 Oct 2025 13:57:14 GMT', 'server': 'istio-envoy', 'transfer-encoding': 'chunked'})
2025-10-14 21:57:23,702 - DEBUG - request_id: 0132a780-a87e-4eff-89cc-01b551a947f4
2025-10-14 21:57:23,703 - DEBUG - API request completed in 8.55 seconds
2025-10-14 21:57:23,703 - DEBUG - Raw model response: {"labels": [1,0,0,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,1,1,0,0,0,0,0,1,1,1,0,1,1,1,0,1,1,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}
2025-10-14 21:57:23,703 - INFO - Successfully processed 112 labels
2025-10-14 21:57:23,703 - ERROR - Label count mismatch for Re 3 Dial_ Retrieve_ Reorganize and Rescale Conversations for Long_Turn Open_Domain Dialogue Pre_training
2025-10-14 21:57:23,703 - INFO - Evaluating paper 16/18: Syn2Real_ Forgery Classification via Unsupervised Domain Adaptation
2025-10-14 21:57:23,703 - INFO - Starting model prediction
2025-10-14 21:57:23,703 - INFO - Attempt 1 of 5
2025-10-14 21:57:23,705 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-df48b37f-d0a5-4baa-b058-1a900a37726f', 'json_data': {'messages': [{'role': 'system', 'content': '你是一个专门用于识别学术论文中数据集描述的助手。你需要判断每个输入的句子是否描述了研究中使用的数据集。请确保：\n    1. 只输出JSON格式的结果\n    2. 结果格式必须为 {"labels": [0,1,0,...]}\n    3. 不要输出任何其他文字或解释'}, {'role': 'user', 'content': '你是一位专业的学术论文数据集描述识别专家。请仔细分析下面这篇论文中的每个句子，判断它们是否描述了本研究特定构建的数据集信息。\n\n    论文标题: Syn2Real_ Forgery Classification via Unsupervised Domain Adaptation\n\n    判断标准:\n    判断为1(数据集描述)的标准：\n    句子必须直接描述本研究特定构建的数据集,包括:\n    1. 该数据集的构建过程和方法\n    2. 该数据集的具体构成和规模\n    3. 该数据的来源和收集方式\n    4. 该数据的预处理步骤\n    5. 该数据集的可获取方式(如发布地址)\n    6. 或者其他直接或者间接描述了该数据集的句子\n\n    所有其他类型的句子均标记为0。\n    \n    请对每个句子进行分析，返回一个JSON格式的标签数组。格式要求：\n    1. 必须是有效的JSON格式\n    2. 只包含labels字段，值为0和1组成的数组\n    3. 数组长度必须与句子数量相同\n    4. 示例格式：{"labels": [0,1,0,1,...]}\n\n    需要判断的句子：\n\n    1. Abstract: In recent years, image manipulation is becoming increasingly more accessible, yielding more natural-looking images, owing to the modern tools in image processing and computer vision techniques.\n2. The task of the identification of forged images has become very challenging.\n3. Amongst different types of forgeries, the cases of Copy-Move forgery are increasing manifold, due to the difficulties involved to detect this tampering.\n4. To tackle such problems, publicly available datasets are insufficient.\n5. In this paper, we propose to create a synthetic forged dataset using deep semantic image inpainting and copy-move forgery algorithm.\n6. However, models trained on these datasets have a significant drop in performance when tested on more realistic data.\n7. To alleviate this problem, we use unsupervised domain adaptation networks to detect copy-move forgery in new domains by mapping the feature space from our synthetically generated dataset.\n8. Furthermore, we improvised the F1 score on CA-SIA and CoMoFoD dataset to 80.3% and 78.8%, respectively.\n9. Our approach can be helpful in those cases where the classification of data is unavailable.\n10. With the advancement of new image editing technologies, there is a sharp increase in the number of forgery cases.\n11. While sophisticated image editing tools are meant to enhance the quality of images, they are misused to create forged images for nefarious purposes.\n12. These images look so natural that it is difficult to tell with naked eyes whether they have been tampered or are they authentic.\n13. [10]There are diverse ways of forging images, of which Copy-Move, Splicing, Retouching, and Resampling forgeries are the most common ones.\n14. Copy-Move Forgery (CMF) is a type of passive image forgery technique in which a section of an image is copied and pasted within the same image.\n15. Many post-image processing operations such as rescaling, affine transformations, resizing, and blurring are applied to the copied region.\n16. As the source and target image remains the same, the photometric characteristics of the image remain mostly invariable.\n17. Thus, the detection becomes even more difficult.\n18. For instance, in contrast to CMF, splicing forgery is a composition of two images.\n19. A section is cut from an image and pasted on another image.\n20. As a result, there is an edge discrepancy that makes the detection of splicing forgery relatively easier.\n21. Image tampering can have significant effects in various domains.\n22. For instance, in medical imaging, the images are procured with the utmost sensitivity and is a tiresome process.\n23. There can be ulterior economical motives for tampering these confidential and sophisticated images.\n24. Consequently, it could misguide the patients about their illnesses and injuries.\n25. In the field of education, students can tamper their documents with online available software tools.\n26. The significant impact of image tampering can happen in the socio-political area, as manipulated images can affect the perception of a large group of people.\n27. Many magazines and newspaper editors tamper the images in such a way that they can change the semantic meaning of the image.\n28. There have been several traditional approaches for forgery detection that include mostly block-based and keypoint feature extraction[7,16,21]and matching procedures.\n29. Nowadays, deep learning approaches[20,1,17]have been proposed to counterattack the problem of image forgery.\n30. However, most of the approaches are based on supervised learning.\n31. When there are a lot of labeled examples, then it is easy to train the model via supervised learning.\n32. To counter the problems of training data, we generally surrogate the training data by including the dataset from adjacent modality or use synthetic imagery.\n33. When the same model is evaluated on these datasets, it results in a significant drop in the performance.\n34. It happens due to the shift in style, content, or appearance distribution between various datasets.\n35. In these cases, domain adaptation is needed to learn the distribution shift.\n36. In this work, we show that manipulations in images across different domains can be detected via domain adaptation.\n37. We leverage the power of Convolutional Neural Figure1.\n38. The first and second column shows the example of target domain dataset (CASIA and CoMoFoD respectively).\n39. Subsequent column shows the generated synthetic data from COCO dataset using semantic inpainting and copy-move forgery algorithm.\n40. First row is authentic image of each category and second row is forged image.networks (CNNs) to perceive the distinguishable features of authentic and tampered images.\n41. We tackle the problem of performance drop by incorporating the feature space alignment between our synthetic generated datasets and datasets that are publicly available.\n42. We generate the synthetic dataset using Edge-connect semantic inpainting and CMF algorithm.\n43. Contributions Our main contributions in the paper are summarized as follows: 1) The primary task is to classify images as forged or authentic, for which we employ Unsupervised Domain Adaptation (DA), due to the difference in content and style between our source and target dataset, 2) As the publicly available datasets are small, we generate a new dataset comprising of 80,000 images using deep semantic inpainting and copy-move forgery algorithms on COCO[6]dataset, and 3) We explore two Unsupervised DA methods to adapt the features from source dataset to target dataset, such that the variation between the domains is minimized.\n44. The paper is organized as follows: Section II describes the traditional and deep learning solutions that evolved over the years for forgery classification and a review of domain adaptation methods.\n45. Section III describes our methodology in detail that involves dataset generation, Unsupervised DA, and final architectures used for training.\n46. After that, in Section IV, we evaluate the performance of our architecture on CASIA[2]and CoMoFoD[13]dataset.\n47. Section V discusses the conclusion and future directions of our work.\n48. We applied two methods to generate the dataset.\n49. The inclusion of any one of them shows an increase in performance.\n50. Semantic Inpainting helps the model to learn edge discrepancies when the objects are removed.\n51. Copy-Move tampered images improve the focus of the network to recognize similar patches.\n52. We evaluated our architecture on CASIA V2 and CoMo-FoD datasets.\n53. In our case, the source domain constitutes of COCO CMF and semantic inpainted images, and, target domain comprises CASIA V2 and CoMoFoD datasets.\n54. Exhaustive experiments were done using AlexNet[5]and VGG-7[11]for feature extraction.\n55. These datasets are explained briefly in the following sections: It contains 12,614 images in total, of which 7,497 are authentic, and 5,123 are forged images.\n56. The resolution of images ranges from 240 x 160 to 900 x 600.\n57. The tampered images have been applied to post-processing operations and saved in JPEG and TIFF formats.\n58. Out of these 5,123 tampered images, 3,274 images are copy-move, and 1,788 are splicing.\n59. The number of authentic images presents, respectively, for forged images, are 1,701.\n60. Henceforth, our total dataset size comes out to be of 4,975 images.\n61. This dataset contains 400 images, 200 authentic, and 200 forged.\n62. It contains only copy-move forgery cases in PNG format.\n63. The dimension of images in this dataset is 512 x 512.\n64. Various distortions such as translation, rotation, and scaling are applied to tampered images.\n65. We explored diverse color spaces to get a sense of the behavior of CMF images in different color spaces.\n66. Using Alexnet for feature extraction and DANN for domain adaptation, we varied the number of CMF images across RGB and YCrCb color space for the CASIA dataset.\n67. Chrominance component of YCrCb illuminates the identical regions in images with the same luminosity.\n68. It helps the deep networks to visualize copy-pasted regions in images.\n69. In DANN, we used categorical cross-entropy as loss function and Adam optimizer with learning rate 0.001.\n70. The DDC network is trained using Stochastic Gradient Descent optimizer, with a momentum value of 0.9, and learning rate value of 0.0001.\n71. At the time of training, we initially used only CMF images for unsupervised domain adaptation.\n72. Then, we included semantic inpainted images to study the effects of edge discrepancy in recognizing forged images.\n73. There are no labels used at the time of training.\n74. For the target domain, images are passed with a domain label attached to it, and the source domain has a class label also assigned to it.\n75. The source model adapts the weights to classify target images with the same features into a particular category.\n76. During testing, the target images are passed through the source classifier model, whose weights are now adapted to features specific to target data.\n77. 80% of the data used for training, and then, 20% used for testing.\n78. As CoMoFoD contains only 200 images, all the images were used to learn the discriminative features, as well as for evaluation.\n79. We used classification accuracy, precision, recall, and F1-score as performance metrics to evaluate our architectures.\n80. Precision is expressed as the number of true positives divided by the sum of true and false positives.\n81. The recall is defined as the ratio of true positives by true positives and false negatives.\n82. F1-score is the harmonic mean of recall and precision score.\n83. We will now discuss the results summarized in Table1, 2 and 3.\n84. We trained our architecture on source dataset and evaluated it on target dataset.\n85. From Table As the number of images increased, the results improved for domain adaptation.\n86. Due to complex post-processing operations, YCrCb space was unable to localize same tampered regions.\n87. As RGB color space performed better, therefore, for our future training of domain adaptation algorithms, we chose RGB images for source and target domains.\n88. In starting, we only used CMF images for unsupervised DA.\n89. DANN and DDC were able to minimize the distance between the two datasets distributions, but using only CMF images makes the network biased towards objects resembling the same feature characteristics.\n90. To analyze the contribution of the amount of CMF images for domain adaptation, we examined each time with an increment of 10,000 images.\n91. We saw that just by increasing CMF images, there are no noteworthy changes in the accuracy and F1-score on the target domain.\n92. In cases where the copied and background region are the same, e.g., grass, then the model is unable to distinguish the image as authentic or tampered.\n93. To alleviate this problem, we incorporated semantic inpainted images to learn the edge discriminative features.\n94. Itmodel to learn the dissimilarities near the edges of the images are copy-pasted.\n95. As the target domain contains CMF images, increasing the distribution of semantic images beyond 10,000 images leads to drop in performance.\n96. Table2shows the effect of utilizing both semantic inpainted and copy-move tampered images.\n97. In contrast to contemporary networks such as Inception[12]and ResNet[4], we used AlexNet and VGG-7 as our base models, because, these networks have a huge number of parameters and due to limited amount of target domain images, the model doesn\'t generalize well.\n98. COCO → CASIA: In CASIA, there are 3979 images used for training purposes and 996 images for evaluation.\n99. With DDC, we saw a sudden jump by including semantic inpainted images.\n100. Using DANN, we achieved the best score, when the highest number of images were used.\n101. As there is a large number of images available at the time of training, we can see from Table3that DANN+VGG-7 achieves the highest recall and F1-score.\n102. COCO → CoMoFoD: CoMoFoD dataset is very small.\n103. Due to the presence of 200 images only, we trained and evaluated on the whole dataset.\n104. With the increase in the number of images in the source domain, accuracy and F1 score decreased in DDC, and, insignificant increase using DANN.\n105. As the dataset was small, we can see from Table3that DDC+ MMD with Alexnet as base model performed better compared to VGG-7.\n106. VGG-7 has a huge number of parameters that can\'t be optimized; hence, they performed poorly at the test time.\n107. To compare with previous work, we analyzed our results with BusterNet architecture.\n108. They mainly took into account of CASIA CMF and CoMoFoD dataset.\n109. Other works, mainly used all images of CASIA dataset, not explicitly for CMF images.\n110. In BusterNet, they created and trained on 1 lakh images for supervised training, and then evaluated on these datasets.\n111. In CoMoFoD, they used 200 images as ours, but, in CASIA, they took only 1356 CMFD images into account compared to 4975 of ours.\n112. Our approach improves the accuracy by 5-6% in the case of CASIA and 27-28% in the case of CoMoFoD.\n113. Table3shows the performance comparison between ours and BusterNet.\n114. Whereas Buster-Net has used pixel-wise annotations to learn the class of images, we have not used any label at the time of training.\n115. In our case, as the data distribution is too much imbalanced, precision and recall score plays a significant role.\n116. We can see that our precision score is not in the comparable range of recall scores.\n117. It is due to the reason, as we have less num-ber of positive class images in contrast to the negative class.\n118. As we look into the denominator of precision and recall, in the first case, the denominator is the sum of true plus false positives.\n119. Now, we have too many images in a false class.\n120. It attributes to a large number of false positives.\n121. Whereas in the recall, the denominator is the sum of true positives plus false negatives.\n122. The false-negative number is less as the number of images in the correct class is fewer.\n\n请严格按照JSON格式返回：{"labels": [0,1,0,...]}，不要包含任何其他文字。'}], 'model': 'qwen-plus', 'response_format': {'type': 'json_object'}}}
2025-10-14 21:57:23,707 - DEBUG - Sending HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
2025-10-14 21:57:23,707 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-10-14 21:57:23,707 - DEBUG - send_request_headers.complete
2025-10-14 21:57:23,707 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-10-14 21:57:23,707 - DEBUG - send_request_body.complete
2025-10-14 21:57:23,707 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-10-14 21:57:35,758 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'vary', b'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding'), (b'x-request-id', b'2a7c1fb2-4db9-47ad-b351-a57ac168690c'), (b'x-dashscope-call-gateway', b'true'), (b'content-type', b'application/json'), (b'req-cost-time', b'12018'), (b'req-arrive-time', b'1760450234872'), (b'resp-start-time', b'1760450246890'), (b'x-envoy-upstream-service-time', b'11986'), (b'content-encoding', b'gzip'), (b'date', b'Tue, 14 Oct 2025 13:57:26 GMT'), (b'server', b'istio-envoy'), (b'transfer-encoding', b'chunked')])
2025-10-14 21:57:35,758 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-14 21:57:35,758 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-10-14 21:57:35,758 - DEBUG - receive_response_body.complete
2025-10-14 21:57:35,758 - DEBUG - response_closed.started
2025-10-14 21:57:35,758 - DEBUG - response_closed.complete
2025-10-14 21:57:35,758 - DEBUG - HTTP Response: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "200 OK" Headers({'vary': 'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding', 'x-request-id': '2a7c1fb2-4db9-47ad-b351-a57ac168690c', 'x-dashscope-call-gateway': 'true', 'content-type': 'application/json', 'req-cost-time': '12018', 'req-arrive-time': '1760450234872', 'resp-start-time': '1760450246890', 'x-envoy-upstream-service-time': '11986', 'content-encoding': 'gzip', 'date': 'Tue, 14 Oct 2025 13:57:26 GMT', 'server': 'istio-envoy', 'transfer-encoding': 'chunked'})
2025-10-14 21:57:35,758 - DEBUG - request_id: 2a7c1fb2-4db9-47ad-b351-a57ac168690c
2025-10-14 21:57:35,758 - DEBUG - API request completed in 12.05 seconds
2025-10-14 21:57:35,758 - DEBUG - Raw model response: {"labels": [0,0,0,1,1,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,1,0,0,1,1,1,1,1,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}
2025-10-14 21:57:35,758 - INFO - Successfully processed 105 labels
2025-10-14 21:57:35,758 - ERROR - Label count mismatch for Syn2Real_ Forgery Classification via Unsupervised Domain Adaptation
2025-10-14 21:57:35,758 - INFO - Evaluating paper 17/18: Topic Shift Detection in Chinese Dialogues_ Corpus and Benchmark
2025-10-14 21:57:35,759 - INFO - Starting model prediction
2025-10-14 21:57:35,759 - INFO - Attempt 1 of 5
2025-10-14 21:57:35,759 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-b7b6ab0f-4773-4cc2-b0aa-bfaea57827e6', 'json_data': {'messages': [{'role': 'system', 'content': '你是一个专门用于识别学术论文中数据集描述的助手。你需要判断每个输入的句子是否描述了研究中使用的数据集。请确保：\n    1. 只输出JSON格式的结果\n    2. 结果格式必须为 {"labels": [0,1,0,...]}\n    3. 不要输出任何其他文字或解释'}, {'role': 'user', 'content': '你是一位专业的学术论文数据集描述识别专家。请仔细分析下面这篇论文中的每个句子，判断它们是否描述了本研究特定构建的数据集信息。\n\n    论文标题: Topic Shift Detection in Chinese Dialogues_ Corpus and Benchmark\n\n    判断标准:\n    判断为1(数据集描述)的标准：\n    句子必须直接描述本研究特定构建的数据集,包括:\n    1. 该数据集的构建过程和方法\n    2. 该数据集的具体构成和规模\n    3. 该数据的来源和收集方式\n    4. 该数据的预处理步骤\n    5. 该数据集的可获取方式(如发布地址)\n    6. 或者其他直接或者间接描述了该数据集的句子\n\n    所有其他类型的句子均标记为0。\n    \n    请对每个句子进行分析，返回一个JSON格式的标签数组。格式要求：\n    1. 必须是有效的JSON格式\n    2. 只包含labels字段，值为0和1组成的数组\n    3. 数组长度必须与句子数量相同\n    4. 示例格式：{"labels": [0,1,0,1,...]}\n\n    需要判断的句子：\n\n    1. Abstract: Dialogue topic shift detection is to detect whether an ongoing topic has shifted or should shift in a dialogue, which can be divided into two categories, i.e., response-known task and response-unknown task.\n2. Currently, only a few investigated the latter, because it is still a challenge to predict the topic shift without the response information.\n3. In this paper, we first annotate a Chinese Natural Topic Dialogue (CNTD) corpus consisting of 1308 dialogues to fill the gap in the Chinese natural conversation topic corpus.\n4. And then we focus on the response-unknown task and propose a teacher-student framework based on hierarchical contrastive learning to predict the topic shift without the response.\n5. Specifically, the response at high-level teacher-student is introduced to build the contrastive learning between the response and the context, while the label contrastive learning is constructed at low-level student.\n6. The experimental results on our Chinese CNTD and English TIAGE show the effectiveness of our proposed model.\n7. Dialogue topic shift detection is to detect whether a dialogue\'s utterance has shifted in the topic, which can help the dialog system to change the topic and guide the dialogue actively.\n8. Although dialog topic shift detection is a new task, it has become a hotspot due to its remarkable benefit to many downstream tasks, such as response generation[1]and reading comprehension[2,3], and can help those real-time applications produce on-topic or topic-shift responses which perform well in dialogue scenarios[4,5,6].\n9. The task of dialogue topic shift detection can be divided into two lines, i.e., response-known task and response-unknown task, as shown in Fig.1.\n10. The former can gain the response information and obtain a better result, while the latter is the opposite.\n11. Moreover, both of them are not accessible to future information.\n12. This is the biggest difference from the task of text topic segmentation, in which all the basic utterances are visible to each other.\n13. That is, those existing topic segmentation models cannot be applied to dialogue topic shift detection since it depends on the response and its subsequent utterances heavily.\n14. Therefore, it is more difficult to discern differences between utterances in the task of dialogue topic shift detection.\n15. Due to the absence of future utterances, dialogue topic shift detection is still a challenging task.\n16. In this paper, we focus on the response-unknown task of topic shift detection in Chinese dialogues.\n17. There are two issues in the response-unknown task of topic shift detection in Chinese dialogues, i.e., lack of annotated corpus in Chinese and how to predict the response.\n18. Fig.1.\n19. Two lines of dialogue topic shift detection tasks to detect whether it exists topic shift between the utterances ui-1 and ui, where the response-known task (left) can use the response ui, while the response-unknown task (right) can be regarded as topic shift prediction without the response ui.\n20. There are only a few publicly dialogue topic shift corpus available and most of them are provided for the segmentation task, which does not satisfy natural conversation.\n21. Xie et al.[7]provided a detailed definition of the dialogue topic shift detection task, and annotated an English dialogue topics corpus TIAGE.\n22. Although it can fill the gap in the corpus of English conversation topics, its scale is still too small.\n23. In Chinese, Xu et al.[8]annotated a Chinese dialogue topic corpus.\n24. However, due to its small size and poor quality, this is detrimental to the further research and development of Chinese dialogue topic shift tasks.\n25. To fill the gap in the Chinese natural dialogue topic corpus, we first annotated a Chinese Natural Topic Dialogue (CNTD) corpus which consists of 1308 dialogues with high quality.\n26. Xie et al.[7]also established a benchmark for this response-unknown task based on the T5 model[9]and this benchmark only used the context to predict topic shift and performed poorly due to the lack of the response information.\n27. Thus, it is more challenging to predict the topic shift in natural dialogue without useful response information.\n28. The teacher-student framework has been used widely to obtain information that is not available to the model[1].\n29. To solve the issue of the lack of response information, we propose a teacher-student framework to introduce the response information.\n30. The teacher can obtain the response information, and the student can learn the response information from the teacher through knowledge distillation.\n31. To facilitate knowledge transfer, the student mimics the teacher on every layer instead of just the top layer, which alleviates the delayed supervised signal problem using hierarchical semantic information in the teacher[10].\n32. Besides, we construct hierarchical contrastive learning in which we consider the teacher-student as high-level and the student as low-level.\n33. At high-level, we build an information simulation loss between the context and the response to improve the semantic information of the student model with more reliable predictive information.\n34. At low-level, we design a semantic coherence-aware loss to better distinguish the different shift cases and produce more reliable prediction results.\n35. Finally, the experimental results on our Chinese CNTD and the English TIAGE show that our proposed model outperforms the baselines.\n36. The contributions of this paper are as follows.\n37. -We introduce hierarchical contrastive learning to further improve performance.\n38. -The experimental results both on the CNTD and TIAGE datasets show that our model outperforms the baselines.\n39. Previous studies explored the dialogue topic tasks and published the annotated topic dialogue corpus.\n40. For English, Xie et al.[7]annotated the TIAGE consisting of 500 dialogues with 7861 turns based on PersonaChat[11].\n41. Xu et al.[8]built a dataset including 711 dialogues by joining dialogues from existing multi-turn dialogue datasets: MultiWOZ Corpus[12], and Stanford Dialog Dataset[13].\n42. Both corpora are either small or limited to a particular domain, and neither applies to the study of the natural dialogue domain.\n43. For Chinese, Xu et al.[8]annotated a dataset including 505 phone records of customer service on banking consultation.\n44. However, this corpus is likewise restricted to a few specialized domains while natural dialogues are more complicated.\n45. Natural dialogues have a range of topic shift scenarios, unrestricted topics, and more free colloquialisms in the utterances.\n46. The above corpus is insufficient to fill the gap in the Chinese natural dialogue topic corpus.\n47. The existing corpus of Chinese dialogue topic detection[8]is small and does not satisfy natural conversation.\n48. Although the English dialogue topic corpora can be converted into Chinese by machine translation, they lack natural conversation colloquiality and are small in size.\n49. Therefore, we annotate a Chinese dialogue topic detection corpus CNTD based on NaturalConv dataset[20].\n50. In this section, we show our annotation guidelines and outline the reasons for our selection of corpus sources, as well as the manual annotation procedure and data statistics.\n51. We also analyze the topic shift distribution in CNTD.\n52. Each dialogue in our corpus has a piece of news as a base document, which is not available in other corpus and can be used as additional information for further research and expansion.\n53. The news is from six domains, which brings our conversations closer to natural dialogue.\n54. Besides, the speakers in our corpus are not restricted in any way, which also makes it closer to natural dialogues.\n55. In addition, we annotated the fine-grained dialogues topics, refer to Section 3.2.\n56. Fine-grained labels are beneficial to promote further research on dialogue topics.\n57. Compared with the existing Chinese topic corpus annotated by Xu et al.[8], the dialogues in our corpus do not have meaningless and repetitive turns.\n58. Also, the corpus is more than twice the size of the other corpus.\n59. In addition, the news in the corpus can be studied as additional information for the dialogues.\n60. Following the annotation guidelines in TIAGE[7], we distinguish each dialogue turns whether changed the topic compared with the context.\n61. -Commenting on the previous context: The response is a comment on what is said by the speaker previously; -Question answering: The response is an answer to the question that comes from the speaker previously; -Developing the dialogue to sub-topics: The response develops to a sub-topic compared to the context; -Introducing a relevant but different topic: The response introduces a relevant but different topic compared to the context; -Completely changing the topic: The response completely changes the topic compared to the context.\n62. Among them, we uniformly identify the two cases of greeting and farewell specific to CNTD as the topic shift.\n63. We chose the NaturalConv dataset[20]as the source corpus, which contains about 400K utterances and 19.9K dialogues in multiple domains.\n64. It is designed to collect a multi-turn document grounded dialogue dataset with scenario and naturalness properties of dialogue.\n65. We consider NaturalConv as a promising dataset for dialogue topic detection for the following reasons: 1) NaturalConv is much closer to human-like dialogue with the natural property, including a full and natural setting such as scenario assumption, free topic extension, greetings, etc.; 2) NaturalConv contains about 400K utterances and 19.9K dialogues in multiple domains; 3) The average turn number of this corpus is 20, and longer dialogue contexts tend to exhibit a flow with more topics; 4) The corpus has almost no restrictions or assumptions about the speakers, e.g., no explicit goal is proposed[21].\n66. We have three annotators for coarse-grained annotations and two for fine-grained annotations.\n67. Both annotations are divided into three stages as follows.\n68. Co-annotation Stage First, for coarse-grained annotations, we draw a total of 100 dialogues from each domain of the NaturalConv dataset proportionally for a total of 2014 dialogue turns.\n69. In this stage, three annotators are asked to discuss every 20 dialogues they annotated, and each annotator is asked to give a reason for the annotation during the discussion.\n70. Finally, the Kappa value of all annotators for coarse-grained annotations at this stage is 0.7426.\n71. In addition, we annotated the fine-grained information based on the results of the complete coarse-grained annotations.\n72. Two annotators annotated the same 150 dialogues and discussed them several times for consistency.\n73. Finally, the kappa value of all annotators for fine-grained annotations at this stage is 0.9032.\n74. These kappa values confirm that our annotators already have sufficient annotation capabilities for independent annotation, as well as the high quality of our corpus.\n75. Independent-annotation Stage We ensured the quality of each annotator\'s annotation and judging criteria before starting the second phase of annotation.\n76. For both granularity annotations, we randomly assign the dialogues drawn from each domain to each annotator for independent annotation.\n77. At this stage, we annotate 1208 dialogues for coarse-grained annotations and 1158 dialogues for fine-grained annotations.\n78. Semi-automatic Rechecking Stage Finally, we use a semi-automatic rechecking process to ensure that the corpus is still of high quality.\n79. On the one hand, we automatically format the dialogues with annotations to detect formatting problems caused by manual annotation.\n80. On the other hand, we automatically match the related news to each dialogue and check that the topic attributes are consistent with the dialogue to rule out any possible errors.\n81. Due to the limited time, we randomly select 1308 dialogues from the Natural-Conv dataset and annotate them with four annotators.\n82. Finally, we construct a Chinese natural topic dialogues corpus containing 26K dialogue turns.\n83. As shown in Table2, we randomly split them into 1041 train, 134 validation, and 133 test dialogues respectively, according to the percentage of different categories.\n84. In addition, we show the details of CNTD in Table3, which shows that our corpus has enough topics and long turns which is suitable for dialogue topic detection.\n85. Finally, there are the statistics of our fine-grained labels, as shown in Table4.\n86. We count the number of dialogues with different numbers of topics, as shown in Fig.2.\n87. On another side, we count the distribution of topic shift signals in dialogues, shown in Fig.3.\n88. We can see there are a total of 21 turns and three peaks of topic shift signals, which occur in 2 nd , 4 th , and 18 th turns, respectively.\n89. The reason is that the dialogue in our corpus usually starts with a greeting and   ends with a farewell, which leads to more topic shifts at the beginning and end of the dialogues.\n90. In addition, the NaturalConv corpus gives a piece of news as the base document of the dialogue, so there are more frequent transitions from news to derived topics, leading to the third highest peak in 4 th turn.\n91. However, we think this is consistent with a natural dialogue scenario because people often talk about recent news after daily greetings.\n92. Based on the train/validation/test dataset of CNTD we partitioned in Table2and previous work on TIAGE[7], we extract (context, response) pairs from each dialogue as input and the label of response as a target for the responseunknown task.\n93. In our experiments, every utterance except the first utterance of the dialogue can be considered as a response.\n94. As for evaluation, we report Precision (P), Recall (R), and Micro-F1 scores.\n95. We use BERT as an encoder and fine-tune it during training.\n96. For both the TIAGE and CNTD corpus, all pre-trained model parameters are set to default values.\n97. We conduct our experiments on NVIDIA GeForce GTX 1080 Ti and NVIDIA GeForce GTX 3090 with batch sizes of 2 and 6 for both CNTD and TIAGE, with the initial learning rates of 2e-5.\n98. And we set the epochs of training to 20, and the dropout to 0.5.\n99. For the pre-trained models in the experiment, we apply BERT-base-Chinese and MT5-base to obtain the semantic representation of the dialogues in CNTD, and we apply BERT-base-uncased and T5-base to obtain the semantic representation of the dialogues in TIAGE.\n100. Dialogue topic shift detection is a new task and there is no complex model available, besides a simple T5[7]that can be considered as the SOTA model.\n101. Since we employ BERT as our encoder and the T5 model is used in TIAGE, we use the pre-trained models of T5[9]and BERT[27]as baselines.\n102. For BERT, we For T5, we also connect utterances in the context and classify the undecidable predicted results to the \'not a topic shift\' category.\n103. Table5shows the performance comparison between our model and the baselines, in which TS denotes our teacher-student model without the hierarchical comparative learning (HCL) and Ours denotes our final model, i.e., the addition of SCL on the student side based on the addition of ISL on both the teacher and student sides.\n104. It can be found that on CNTD, our model achieves a good improvement and improves both precision and recall in comparison with the baselines.\n105. Although T5 does not perform poorly on recall, its precision is inadequate in comparison with BERT, and it is clear that T5 is not effective in predicting topics.\n106. In contrast, TS improved by 1.0 in Micro-F1 in comparison with BERT, which confirms that the teacher-student framework is effective in introducing response information.\n107. As well, Ours improved by 4.0 in micro-F1 in comparison with TS, and also showed significant improvement in P and R, which fully demonstrates that our HCL can improve the model\'s ability to discriminate between different topic situations.\n108. In particular, our model improves on CNTD by 5.0 in comparison with the best baseline BERT, which shows the effectiveness of our proposed model.\n109. To verify the effectiveness of the components used in our model, we conduct ablation studies on CTND, and the experimental results are shown in Table6.\n110. If we remove ISL on the teacher side (-ISL S ) or the student side (-ISL T ), the performance of the model decreased by 1.5 and 1.3 on the Micro-F1 value, respectively, with the largest decrease after removing the ISL on the student side.\n111. Although -ISL T has the highest precision in predicting topics and lower error probability than Ours and -ISL S .\n112. However, it can be seen that adding ISL at both the teacher and student sides can better improve the correct prediction rate.\n113. Moreover, if we remove ISL both on the teacher and student side (-ISL T S ), it achieves a similar performance on Micro-F1, in comparison with -ISL S and -ISL T .\n114. However, it achieves the highest precision (58.8%).\n115. If we remove SCL (-SCL) or HCL (-HCL) from our model, the Micro-F1 value of the models -SCL and -HCL drop from 53.9 to 52.4 (-1.5) and 49.9 (-4.0), respectively.\n116. These results show that our Semantic Conherent-aware Loss(SCL), and Hierarchical Contrastive Learning(HCL) are effective for this task, especially HCL.\n117. In addition, we explore the performance of the dialogues with different numbers of topics to analyze our model in comparison with BERT, as shown in Table7.\n118. It can be found that our model has a better performance than BERT on dialogues with fewer topics.\n119. Our model gets at least a 6% improvement in topic shift prediction on dialogues with 2 to 5 topics and obtains above-average performance.\n120. And when the number of topics increases to 9, the performance improves because the conversation length is still about 20 and the topics shift more significantly.\n121. In Table8, we also investigate the recall of the topic shift detection for various topic turns.\n122. Our model is improved for varying degrees across topic turns, with the most significant improvements in turns 7-9.\n123. Even in long topic shift cases, our model can obtain an effective boost.\n124. However, the performance of our model inevitably decreases compared to short topic shift cases.\n125. When there are fewer topic turns, the topic shift situation is simpler, so it is easier to determine.\n126. When the length of turns becomes longer and the situation becomes complicated, the topic of long turns has more information so it is easier to identify.\n127. As shown in Table9, it can be found that our model also achieves a good improvement on English TIAGE.\n128. et al. we obtain the best performance on both recall and Micro-F1 values, especially on micro-F1 with a 5.8% improvement over T5.\n129. This proves that our model achieves the best performance both in English and Chinese.\n130. We also conducted a case study.\n131. The prediction made by our model, the BERT model on the instance, and the manual labels are shown in Table10.\n132. "etc., belonging to the questionanswering scenario.\n133. "etc. belonging to the commenting on the previous context scenario, our model or BERT cannot accurately predict the topic shift in this scenario.\n134. This shows that detecting the topic shifts in natural dialogue is still challenging.\n135. We further analyze the errors of the prediction produced in our experiments.\n136. Specifically, we analyzed the example to explore whether the error in the results of this example is prevalent in other dialogues.\n137. From Table10, we can find that the wrong predictions at 14 th and 18 th turn.\n138. "as \'topic shift\'.\n139. We counted the appearance of many errors, and the errors are mainly divided into two categories.\n140. One is for the "Introducing a relevant but different topic" type of utterance.\n141. It was predicted that no topic shift occurred due to the lack of information about the future of the conversation.\n142. The other is the "commenting on the previous context" category.\n143. Since this type of response does not affect the integrity of the previous topic, it is mostly predicted to be a topic shift.\n144. Table10.\n145. The results of BERT, Ours, and Human of different turns where "1" indicates that a topic shift has occurred and "0" indicates the opposite.\n146. We omit the lines with all 0.\n\n请严格按照JSON格式返回：{"labels": [0,1,0,...]}，不要包含任何其他文字。'}], 'model': 'qwen-plus', 'response_format': {'type': 'json_object'}}}
2025-10-14 21:57:35,760 - DEBUG - Sending HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
2025-10-14 21:57:35,760 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-10-14 21:57:35,760 - DEBUG - send_request_headers.complete
2025-10-14 21:57:35,760 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-10-14 21:57:35,760 - DEBUG - send_request_body.complete
2025-10-14 21:57:35,760 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-10-14 21:57:42,384 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'vary', b'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding'), (b'x-request-id', b'fa1fb9db-0cfa-49ae-bbae-094ad4f5212c'), (b'x-dashscope-call-gateway', b'true'), (b'content-type', b'application/json'), (b'req-cost-time', b'6594'), (b'req-arrive-time', b'1760450246922'), (b'resp-start-time', b'1760450253517'), (b'x-envoy-upstream-service-time', b'6561'), (b'content-encoding', b'gzip'), (b'date', b'Tue, 14 Oct 2025 13:57:33 GMT'), (b'server', b'istio-envoy'), (b'transfer-encoding', b'chunked')])
2025-10-14 21:57:42,385 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-14 21:57:42,385 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-10-14 21:57:42,385 - DEBUG - receive_response_body.complete
2025-10-14 21:57:42,385 - DEBUG - response_closed.started
2025-10-14 21:57:42,385 - DEBUG - response_closed.complete
2025-10-14 21:57:42,385 - DEBUG - HTTP Response: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "200 OK" Headers({'vary': 'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding', 'x-request-id': 'fa1fb9db-0cfa-49ae-bbae-094ad4f5212c', 'x-dashscope-call-gateway': 'true', 'content-type': 'application/json', 'req-cost-time': '6594', 'req-arrive-time': '1760450246922', 'resp-start-time': '1760450253517', 'x-envoy-upstream-service-time': '6561', 'content-encoding': 'gzip', 'date': 'Tue, 14 Oct 2025 13:57:33 GMT', 'server': 'istio-envoy', 'transfer-encoding': 'chunked'})
2025-10-14 21:57:42,385 - DEBUG - request_id: fa1fb9db-0cfa-49ae-bbae-094ad4f5212c
2025-10-14 21:57:42,385 - DEBUG - API request completed in 6.63 seconds
2025-10-14 21:57:42,385 - DEBUG - Raw model response: {"labels": [0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]}
2025-10-14 21:57:42,385 - INFO - Successfully processed 109 labels
2025-10-14 21:57:42,385 - ERROR - Label count mismatch for Topic Shift Detection in Chinese Dialogues_ Corpus and Benchmark
2025-10-14 21:57:42,385 - INFO - Evaluating paper 18/18: Towards Equitable Representation in Text_to_Image Synthesis Models with the Cross_Cultural Understanding Benchmark _CCUB_ Dataset
2025-10-14 21:57:42,386 - INFO - Starting model prediction
2025-10-14 21:57:42,386 - INFO - Attempt 1 of 5
2025-10-14 21:57:42,386 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-d7ce0b29-0388-49a8-ac96-3f098e5c86e6', 'json_data': {'messages': [{'role': 'system', 'content': '你是一个专门用于识别学术论文中数据集描述的助手。你需要判断每个输入的句子是否描述了研究中使用的数据集。请确保：\n    1. 只输出JSON格式的结果\n    2. 结果格式必须为 {"labels": [0,1,0,...]}\n    3. 不要输出任何其他文字或解释'}, {'role': 'user', 'content': '你是一位专业的学术论文数据集描述识别专家。请仔细分析下面这篇论文中的每个句子，判断它们是否描述了本研究特定构建的数据集信息。\n\n    论文标题: Towards Equitable Representation in Text_to_Image Synthesis Models with the Cross_Cultural Understanding Benchmark _CCUB_ Dataset\n\n    判断标准:\n    判断为1(数据集描述)的标准：\n    句子必须直接描述本研究特定构建的数据集,包括:\n    1. 该数据集的构建过程和方法\n    2. 该数据集的具体构成和规模\n    3. 该数据的来源和收集方式\n    4. 该数据的预处理步骤\n    5. 该数据集的可获取方式(如发布地址)\n    6. 或者其他直接或者间接描述了该数据集的句子\n\n    所有其他类型的句子均标记为0。\n    \n    请对每个句子进行分析，返回一个JSON格式的标签数组。格式要求：\n    1. 必须是有效的JSON格式\n    2. 只包含labels字段，值为0和1组成的数组\n    3. 数组长度必须与句子数量相同\n    4. 示例格式：{"labels": [0,1,0,1,...]}\n\n    需要判断的句子：\n\n    1. Abstract: It has been shown that accurate representation in media improves the well-being of the people who consume it.\n2. By contrast, inaccurate representations can negatively affect viewers and lead to harmful perceptions of other cultures.\n3. To achieve inclusive representation in generated images, we propose a culturally-aware priming approach for text-to-image synthesis using a small but culturally curated dataset that we collected, known here as Cross-Cultural Understanding Benchmark (CCUB) Dataset, to fight the bias prevalent in giant datasets.\n4. Our proposed approach is comprised of two fine-tuning techniques: (1) Adding visual context via fine-tuning a pre-trained text-to-image synthesis model, Stable Diffusion, on the CCUB text-image pairs, and (2) Adding semantic context via automated prompt engineering using the finetuned large language model, GPT-3, trained on our CCUB culturally-aware text data.\n5. CCUB dataset is curated and our approach is evaluated by people who have a personal relationship with that particular culture.\n6. Our experiments indicate that priming using both text and image is effective in improving the cultural relevance and decreasing the offensiveness of generated images while maintaining quality.\n7. † indicates corresponding authors.\n8. In media, studies repeatedly show that representation affects the well-being of its viewers[Shaw, 2010;Caswell et al., 2017;Elbaba, 2019].\n9. Representation can positively affect viewers by providing them with role models that they identify with, but it can also negatively affect viewers by creating harmful, stereotypical understandings of people and culture[Castañeda, 2018].\n10. When people are accurately represented in media, it allows people to properly understand cultures without harmful stereo- types forming[Dixon and Linz, 2000;Mastro and Greenberg, 2000].\n11. Despite the benefits of representation, many media generating Artificial Intelligence (AI) models show poor representation in their results[Ntoutsi et al., 2020].\n12. Many of these issues stem from their large training datasets which are gathered by crawling the Internet without filtering supervision and contain malign stereotypes and ethnic slurs among other problematic content[Birhane et al., 2021].\n13. As AI models are increasingly used to create and aid in the production of visual content, it is important that the models have a true understanding of culture such that it can give accurate and proper representation leading to well-being rewards for its consumers.\n14. In this paper, we aim to address such a representation issue in image generation and introduce a new task of culturally-aware image synthesis: generating visual content within a cultural context that is both accurate and inoffensive.\n15. Our overarching goal is to improve the well-being of consumers of the AI generated images with particular attention to those consumers from underrepresented groups.\n16. Specifically, we formulate the culturally-aware text-to-image synthesis task to take an additional input of a country name to specify a cultural context in addition to language description.\n17. It was found that large datasets such as the LAION-5B[Schuhmann et al., 2021]used to train many text-toimage synthesis models such as Stable Diffusion[Rombach et al., 2021]are Anglo-centric and Euro-centric[Birhane et al., 2021]as shown in the top row of Figure1.\n18. As a consequence, these powerful models may generate culturally offensive images due to misrepresentation during training.\n19. Our research question is, how can effective existing text-to-image models be improved to become more culturally representative and thus less offensive?\n20. It may be infeasible to vet billions of training examples for accurate cultural content.\n21. We hypothesize that a small dataset that is veritably representative of a culture can be used to prime pre-trained textto-image models to guide the model towards more culturally accurate content creation.\n22. To verify the hypothesis, we collected a dataset of image and caption pairs for 8 cultures.\n23. For each culture, data was collected by a few people who are native of that culture as they are the people who properly understand it and are most affected by its misrepresentations.\n24. We call this the Cross-Cultural Understanding Benchmark (CCUB) dataset which comprises of 100-200 images each with a manually written caption as shown in Figure2.\n25. We propose two techniques for enhancing the text-toimage pipelines using CCUB.\n26. First, we fine-tune a text-toimage synthesis model, Stable Diffusion, on the CCUB textimage pairs to generate images tailored for a given cultural context.\n27. We evaluate our approach\'s two components individually as well as combined against the baseline of simply specifying the culture in the text prompt.\n28. Our evaluation was performed by native people of each country.\n29. Our survey results based on 2,244 image comparisions conducted by 72 participants from 5 countries indicate that our proposed approach is both less offensive and more cultural relevant than simply adding the country name as a suffix to the prompt.\n30. Our contributions are as follows: 1.\n31. Following the definition of culture in[Halpern, 1955]and [Key and Comrie, 2021], nine categories are used to represent cultural elements in our dataset: food & drink, clothing, artwork, dance and music, religion, architecture, people, city and nature.\n32. The categories are further divided into traditional and modern to reflect a characteristic of the culture that culture changes over time.\n33. Our CCUB image are collected based on the nine cultural categories.\n34. For collection, we recruited cultural experts who confidently know this culture well or belong to it.\n35. Cultural experts are asked to collect 10-20 relevant images containing different objects for each cultural category.\n36. The images were collected either from Creative Commons licensed images from Google searches or the collectors own photographs.\n37. Cultural experts were also asked to select images with common or culturally representative items.\n38. Each image in the CCUB dataset is also captioned by cultural experts forming paired image-text data.\n39. Cultural experts were asked to focus on the general and specific items in each cultural image, rather than adding captions to subtle components of the image.\n40. The captions accurately express cultural contents in English as opposed to large datasets such asLAION [Schuhmann et al., 2021]which are scraped from the internet and not vetted for cultural accuracy.\n41. We produced surveys to evaluate the effectiveness of our two proposed techniques for culturally-aware text-to-image synthesis and compare them to a baseline of simply appending the culture to the prompt, e.g., "A family eating dinner , China." and using an existing text-to-image model.\n42. In setting up our study, we consider a comparative structure between images: the baseline image versus another image from our results.\n43. The setup of a single question in our survey was as follows: Given two images, the participant selects which image best fits three given comparative properties.\n44. The properties analyzed were: (1) Text and Image Alignment: Participants are given a text prompt and consider which of the two images is more similar to the prompt; (2) Cultural Alignment: Participants decide which of the two images is a better representation of the country\'s culture; and (3) Offensiveness: Participants consider which of the two images is more offensive to them.\n45. The participants for the study were selected based on whether they had a personal understanding of the culture for which the images in the survey were generated.\n46. Participants were recruited among university students, friends, and family members of the authors.\n47. It was ensured that the participants would not be able to discern the approaches used to generate the compared images by randomizing the order of questions and images in the survey.\n\n请严格按照JSON格式返回：{"labels": [0,1,0,...]}，不要包含任何其他文字。'}], 'model': 'qwen-plus', 'response_format': {'type': 'json_object'}}}
2025-10-14 21:57:42,387 - DEBUG - Sending HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
2025-10-14 21:57:42,387 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-10-14 21:57:42,387 - DEBUG - send_request_headers.complete
2025-10-14 21:57:42,387 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-10-14 21:57:42,387 - DEBUG - send_request_body.complete
2025-10-14 21:57:42,387 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-10-14 21:57:45,182 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'vary', b'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding'), (b'x-request-id', b'411aba11-10b6-42c8-83d2-5b11a378a598'), (b'x-dashscope-call-gateway', b'true'), (b'content-type', b'application/json'), (b'req-cost-time', b'2763'), (b'req-arrive-time', b'1760450253551'), (b'resp-start-time', b'1760450256315'), (b'x-envoy-upstream-service-time', b'2761'), (b'content-encoding', b'gzip'), (b'date', b'Tue, 14 Oct 2025 13:57:35 GMT'), (b'server', b'istio-envoy'), (b'transfer-encoding', b'chunked')])
2025-10-14 21:57:45,183 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-14 21:57:45,183 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-10-14 21:57:45,184 - DEBUG - receive_response_body.complete
2025-10-14 21:57:45,184 - DEBUG - response_closed.started
2025-10-14 21:57:45,184 - DEBUG - response_closed.complete
2025-10-14 21:57:45,184 - DEBUG - HTTP Response: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "200 OK" Headers({'vary': 'Origin,Access-Control-Request-Method,Access-Control-Request-Headers, Accept-Encoding', 'x-request-id': '411aba11-10b6-42c8-83d2-5b11a378a598', 'x-dashscope-call-gateway': 'true', 'content-type': 'application/json', 'req-cost-time': '2763', 'req-arrive-time': '1760450253551', 'resp-start-time': '1760450256315', 'x-envoy-upstream-service-time': '2761', 'content-encoding': 'gzip', 'date': 'Tue, 14 Oct 2025 13:57:35 GMT', 'server': 'istio-envoy', 'transfer-encoding': 'chunked'})
2025-10-14 21:57:45,185 - DEBUG - request_id: 411aba11-10b6-42c8-83d2-5b11a378a598
2025-10-14 21:57:45,186 - DEBUG - API request completed in 2.80 seconds
2025-10-14 21:57:45,186 - DEBUG - Raw model response: {"labels": [0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,0,1,0,0,1,1,1,1,1,1,1,1,1,1,0,0,0,0,1,1,0]}
2025-10-14 21:57:45,186 - INFO - Successfully processed 47 labels
2025-10-14 21:57:45,224 - DEBUG - close.started
2025-10-14 21:57:45,225 - DEBUG - close.complete
2025-10-22 09:42:02,523 - INFO - Initializing ResearchDatasetEvaluator
2025-10-22 09:42:02,548 - INFO - Starting data loading process
2025-10-22 09:42:02,550 - DEBUG - Processing paper: A Multilingual Multi_Target Dataset for Stance Detection
2025-10-22 09:42:02,550 - DEBUG - Processed 139 sentences for paper A Multilingual Multi_Target Dataset for Stance Detection
2025-10-22 09:42:02,550 - DEBUG - Processing paper: Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation
2025-10-22 09:42:02,550 - DEBUG - Processed 256 sentences for paper Advancing Zero_Shot Digital Human Quality Assessment through Text_Prompted Evaluation
2025-10-22 09:42:02,550 - DEBUG - Processing paper: Amazon_M2_ A Multilingual Multi_locale Shopping Session Dataset for Recommendation and Text Generation
2025-10-22 09:42:02,550 - DEBUG - Processed 121 sentences for paper Amazon_M2_ A Multilingual Multi_locale Shopping Session Dataset for Recommendation and Text Generation
2025-10-22 09:42:02,550 - DEBUG - Processing paper: AntM 2 C_ A Large Scale Dataset For Multi_Scenario Multi_Modal CTR Prediction
2025-10-22 09:42:02,550 - DEBUG - Processed 182 sentences for paper AntM 2 C_ A Large Scale Dataset For Multi_Scenario Multi_Modal CTR Prediction
2025-10-22 09:42:02,550 - DEBUG - Processing paper: AU_AIR_ A Multi_modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance
2025-10-22 09:42:02,550 - DEBUG - Processed 170 sentences for paper AU_AIR_ A Multi_modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance
2025-10-22 09:42:02,550 - DEBUG - Processing paper: AV_NeRF_ Learning Neural Fields for Real_World Audio_Visual Scene Synthesis
2025-10-22 09:42:02,550 - DEBUG - Processed 67 sentences for paper AV_NeRF_ Learning Neural Fields for Real_World Audio_Visual Scene Synthesis
2025-10-22 09:42:02,550 - DEBUG - Processing paper: BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment
2025-10-22 09:42:02,550 - DEBUG - Processed 286 sentences for paper BAND_2k_ Banding Artifact Noticeable Database for Banding Detection and Quality Assessment
2025-10-22 09:42:02,550 - DEBUG - Processing paper: Comprehensive Named Entity Recognition on CORD_19 with Distant or Weak Supervision
2025-10-22 09:42:02,550 - DEBUG - Processed 25 sentences for paper Comprehensive Named Entity Recognition on CORD_19 with Distant or Weak Supervision
2025-10-22 09:42:02,550 - DEBUG - Processing paper: DCQA_ DOCUMENT_LEVEL CHART QUESTION ANSWERING TOWARDS COMPLEX REASONING AND COMMON_SENSE UNDERSTANDING
2025-10-22 09:42:02,550 - DEBUG - Processed 179 sentences for paper DCQA_ DOCUMENT_LEVEL CHART QUESTION ANSWERING TOWARDS COMPLEX REASONING AND COMMON_SENSE UNDERSTANDING
2025-10-22 09:42:02,550 - DEBUG - Processing paper: Debate Helps Supervise Unreliable Experts
2025-10-22 09:42:02,550 - DEBUG - Processed 75 sentences for paper Debate Helps Supervise Unreliable Experts
2025-10-22 09:42:02,550 - DEBUG - Processing paper: Distilling Large Language Models for Matching Patients to Clinical Trials
2025-10-22 09:42:02,550 - DEBUG - Processed 91 sentences for paper Distilling Large Language Models for Matching Patients to Clinical Trials
2025-10-22 09:42:02,550 - DEBUG - Processing paper: ECMD_ An Event_Centric Multisensory Driving Dataset for SLAM
2025-10-22 09:42:02,550 - DEBUG - Processed 170 sentences for paper ECMD_ An Event_Centric Multisensory Driving Dataset for SLAM
2025-10-22 09:42:02,550 - DEBUG - Processing paper: Inline Citation Classification using Peripheral Context and Time_evolving Augmentation
2025-10-22 09:42:02,550 - DEBUG - Processed 35 sentences for paper Inline Citation Classification using Peripheral Context and Time_evolving Augmentation
2025-10-22 09:42:02,550 - DEBUG - Processing paper: llm_japanese_dataset v0_ Construction of Japanese Chat Dataset for Large Language Models and its Methodology
2025-10-22 09:42:02,550 - DEBUG - Processed 67 sentences for paper llm_japanese_dataset v0_ Construction of Japanese Chat Dataset for Large Language Models and its Methodology
2025-10-22 09:42:02,550 - DEBUG - Processing paper: Re 3 Dial_ Retrieve_ Reorganize and Rescale Conversations for Long_Turn Open_Domain Dialogue Pre_training
2025-10-22 09:42:02,550 - DEBUG - Processed 108 sentences for paper Re 3 Dial_ Retrieve_ Reorganize and Rescale Conversations for Long_Turn Open_Domain Dialogue Pre_training
2025-10-22 09:42:02,550 - DEBUG - Processing paper: Syn2Real_ Forgery Classification via Unsupervised Domain Adaptation
2025-10-22 09:42:02,550 - DEBUG - Processed 122 sentences for paper Syn2Real_ Forgery Classification via Unsupervised Domain Adaptation
2025-10-22 09:42:02,550 - DEBUG - Processing paper: Topic Shift Detection in Chinese Dialogues_ Corpus and Benchmark
2025-10-22 09:42:02,550 - DEBUG - Processed 146 sentences for paper Topic Shift Detection in Chinese Dialogues_ Corpus and Benchmark
2025-10-22 09:42:02,550 - DEBUG - Processing paper: Towards Equitable Representation in Text_to_Image Synthesis Models with the Cross_Cultural Understanding Benchmark _CCUB_ Dataset
2025-10-22 09:42:02,550 - DEBUG - Processed 47 sentences for paper Towards Equitable Representation in Text_to_Image Synthesis Models with the Cross_Cultural Understanding Benchmark _CCUB_ Dataset
2025-10-22 09:42:02,550 - INFO - Starting evaluation of 18 papers
2025-10-22 09:42:02,550 - INFO - Evaluating paper 1/18: A Multilingual Multi_Target Dataset for Stance Detection
2025-10-22 09:42:02,550 - INFO - Starting model prediction
2025-10-22 09:42:02,550 - INFO - Attempt 1 of 5
2025-10-22 09:42:02,632 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-7437669d-11e7-4cb0-ba10-87682894e7c2', 'json_data': {'messages': [{'role': 'system', 'content': '你是一个专门用于识别学术论文中数据集描述的助手。你需要判断每个输入的句子是否描述了研究中使用的数据集。请确保：\n    1. 只输出JSON格式的结果\n    2. 结果格式必须为 {"labels": [0,1,0,...]}\n    3. 不要输出任何其他文字或解释'}, {'role': 'user', 'content': '你是一位专业的学术论文数据集描述识别专家。请仔细分析下面这篇论文中的每个句子，判断它们是否描述了本研究特定构建的数据集信息。\n\n    论文标题: A Multilingual Multi_Target Dataset for Stance Detection\n\n    判断标准:\n    判断为1(数据集描述)的标准：\n    句子必须直接描述本研究特定构建的数据集,包括:\n    1. 该数据集的构建过程和方法\n    2. 该数据集的具体构成和规模\n    3. 该数据的来源和收集方式\n    4. 该数据的预处理步骤\n    5. 该数据集的可获取方式(如发布地址)\n    6. 或者其他直接或者间接描述了该数据集的句子\n\n    所有其他类型的句子均标记为0。\n    \n    请对每个句子进行分析，返回一个JSON格式的标签数组。格式要求：\n    1. 必须是有效的JSON格式\n    2. 只包含labels字段，值为0和1组成的数组\n    3. 数组长度必须与句子数量相同\n    4. 示例格式：{"labels": [0,1,0,1,...]}\n\n    需要判断的句子：\n\n    1. Abstract: We extract a large-scale stance detection dataset from comments written by candidates of elections in Switzerland.\n2. The dataset consists of German, French and Italian text, allowing for a cross-lingual evaluation of stance detection.\n3. It contains 67 000 comments on more than 150 political issues (targets).\n4. Unlike stance detection models that have specific target issues, we use the dataset to train a single model on all the issues.\n5. To make learning across targets possible, we prepend to each instance a natural question that represents the target (e.g."Do you support X?").\n6. Baseline results from multilingual BERT show that zero-shot crosslingual and cross-target transfer of stance detection is moderately successful with this approach.\n7. In recent years many datasets have been created for the task of automated stance detection, advancing natural language understanding systems for political science, opinion research and other application areas.\n8. Typically, such benchmarks(Mohammad et al., 2016a)are composed of short pieces of text commenting on politicians or public issues and are manually annotated with their stance towards a target entity (e.g.Climate Change, or Trump).\n9. However, they are limited in scope on multiple levels(Küçük and Can, 2020).\n10. First of all, it is questionable how well current stance detection methods perform in a crosslingual setting, as the multilingual datasets avail-able today are relatively small, and specific to a single target(Taulé et al., 2017(Taulé et al., , 2018)).\n11. Furthermore, specific models tend to be developed for each single target or pair of targets(Sobhani et al., 2017).\n12. Concerns have been raised that cross-target performance is often considerably lower than fully supervised performance(Küçük and Can, 2020).\n13. In this paper we propose a much larger dataset that combines multilinguality and a multitude of topics and targets.\n14. X-stance comprises more than 150 questions about Swiss politics and more than 67k answers given by candidates running for political office in Switzerland.\n15. Questions are available in four languages: English, Swiss Standard German, French, and Italian.\n16. The language of a comment depends on the candidate\'s region of origin.\n17. We have extracted the data from the voting advice application Smartvote.\n18. Candidates respond to questions mainly in categorical form (yes / rather yes / rather no / no).\n19. They can also submit a freetext comment to justify or explain their categorical answer.\n20. An example is given in Figure1.\n21. We transform the dataset into a stance detection task by interpreting the question as a naturallanguage representation of the target, and the commentary as the input to be classified.\n22. The dataset is split into a multilingual training set and into several test sets to evaluate zeroshot cross-lingual and cross-target transfer.\n23. To provide a baseline, we fine-tune a multilingual BERT model(Devlin et al., 2019)on X-stance.\n24. We show that the baseline accuracy is comparable to previous stance detection benchmarks while leaving ample room for improvement.\n25. In addition, the model can generalize to a degree both crosslingually and in a cross-target setting.\n26. We have made the dataset and the code for reproducing the baseline models publicly available.\n27. Figure1: Example of a question and two answers in the X-stance dataset.\n28. The answers were submitted by electoral candidates on a voting advice website.\n29. The author of the German comment was in favor of the issue; the author of the French comment against.\n30. Both authors use comments to explain their respective stance.\n31. Provenance We downloaded the questions and answers via the Smartvote API 2 .\n32. The downloaded data cover 175 communal, cantonal and national elections between 2011 and 2020.\n33. All candidates in an election who participate in Smartvote are asked the same set of questions, but 2 https://smartvote.chdepending on the locale they see translated versions of the questions.\n34. They can answer each question with either \'yes\', \'rather yes\', \'rather no\', or \'no\'.\n35. They can supplement each answer with a comment of at most 500 characters.\n36. The questions asked on Smartvote have been edited by a team of political scientists.\n37. They are intended to cover a broad range of political issues relevant at the time of the election.\n38. A detailed documentation of the design of Smartvote and the editing process of the questions is provided byThurman and Gasser (2009).\n39. Preprocessing We merged the two labels on each pole into a single label: \'yes\' and \'rather yes\' were combined into \'favor\'; \'rather no\', or \'no\' into \'against\'.\n40. This improves the consistency of the data and the comparability to previous stance detection datasets.\n41. We did not further preprocess the text of the comments.\n42. Language Identification As the API does not provide the language of comments, we employed a language identifier to automatically annotate this information.\n43. We used the langdetect library(Shuyo, 2010).\n44. For each responder we classified all the comments jointly, assuming that responders did not switch code during the answering of the questionnaire.\n45. We applied the identifier in a two-step approach.\n46. In the first run we allowed the identifier to output all 55 languages that it supports out of the box, plus Romansh, the fourth official language in Switzerland3.\n47. We found that no Romansh comments were detected and that all unexpected outputs were misclassifications of German, French or Italian comments.\n48. We further concluded that little or no Swiss German comments are in the dataset; otherwise, some of them would have manifested themselves via misclassifications (e.g. as Dutch).\n49. In the second run, drawing from these conclusions, we restricted the identifier\'s set of choices to English, French, German and Italian.\n50. Filtering We pre-filtered the questions and answers to improve the quality of the dataset.\n51. In the right column the model encounters unseen answers to unseen questions within an unseen topic.\n52. The two test sets in parentheses are too small for a significant evaluation. questions and corresponding answers pertaining to national elections were included.\n53. In the context of communal and cantonal elections, candidates have answered both local questions and a subset of the national questions.\n54. Of those elections, we only considered answers to the questions that also had been asked in a national election.\n55. They were only used to augment the training set while the validation and test sets were restricted to answers from national elections.\n56. We discarded the fewer than 20 comments classified as English.\n57. Furthermore, we discarded instances that met any of the following conditions: • Question is not a closed question or does not address a clearly defined political issue.\n58. • No comment was submitted by the candidate or the comment is shorter than 50 characters.\n59. • Comment starts with "but" or a similar indicator that the comment is not self-contained.\n60. • Comment contains a URL.\n61. In total, a fifth of the comments were filtered out.\n62. Topics The questions have been organized by the Smartvote editors into categories (such as "Economy").\n63. We further consolidated the predefined categories into 12 broad topics (Table1).\n64. Compliance The dataset is shared under a CC BY-NC 4.0 license.\n65. Copyright remains with www.smartvote.ch.\n66. Given the sensitive nature of the data, we increase the anonymity of the data by hashing the respondents\' IDs.\n67. No personal attributes of the respondents are included in the dataset.\n68. We provide a data statement(Bender and Friedman, 2018)in Appendix B.\n69. We held out the topics "Healthcare" and "Political System" from the training data and created a separate cross-topic test set that contains the questions and answers related to those topics.\n70. Furthermore, in order to test cross-question generalization performance within previously seen topics, we manually selected 16 held-out questions that are distributed over the remaining 10 topics.\n71. We selected the held-out questions manually because we wanted to make sure that they are truly unseen and that no paraphrases of the questions are found in the training set.\n72. We designated Italian as a test-only language, since relatively few comments have been written in Italian.\n73. From the remaining German and French data we randomly selected a percentage of respondents as validation or as test respondents.\n74. As a result we received one training set, one validation set and four test sets.\n75. The sizes of the sets are listed in Table 2.\n76. We did not consider test sets that are cross-lingual and cross-target at the same time, as they would have been too small to yield significant results.\n77. We evaluate four baselines to obtain an impression of the difficulty of the task.\n78. The first pair of baselines uses the most frequent class in the training set for prediction.\n79. Specifically, the global majority class baseline predicts the most frequent class across all training targets while the target-wise majority class baseline predicts the class that is most frequent for a given target question.\n80. The latter can only be applied to the intra-target test sets.\n81. As a second baseline, we train a fastText bag-ofwords linear classifier(Joulin et al., 2017).\n82. For each comment, we select the translation of the question that matches its language, and concatenate it to the comment.\n83. We tokenize the text using the Europarl preprocessing tools(Koehn, 2005).\n84. The \'against\' class was slightly upsampled in the training data so that the classes are balanced when summing over all questions and topics.\n85. We use the standard settings provided by the fastText library.\n86. The word vectors were set to a size of 300.\n87. We do not initialize them with pre-trained multilingual embeddings since preliminary experiments did not show a beneficial effect.\n88. As our main baseline model we fine-tune multilingual BERT (M-BERT) on the task(Devlin et al., 2019)which has been pre-trained jointly in 104 languages 5 and has established itself as a state of the art for various multilingual tasks(Wu and Dredze, 2019;Pires et al., 2019).\n89. Within the field of stance detection, BERT can outperform both feature-based and other neural approaches in a monolingual English setting(Ghosh et al., 2019).\n90. Architecture In the context of BERT we interpret the X-stance task as sequence pair classification inspired by natural language inference tasks(Bowman et al., 2015).\n91. We follow the procedure outlined byDevlin et al. (2019)for such tasks.\n92. We designate the question as segment A and the comment as segment B.\n93. The two segments are separated with the special token [SEP], and the special token [CLS] is prepended to the sequence.\n94. The final hidden state corresponding to [CLS] is then classified by a linear layer.\n95. We fine-tune the full model with a cross-entropy loss, using the AllenNLP library(Gardner et al., 2018)as a basis for our implementation.\n96. Training As above, we balanced out the number of classes in the training set.\n97. We use a batch size of 16 and a maximum sequence length of 512 subwords, and performed a grid search over the following hyperparameters based on the validation accuracy: • Learning rate: 5e-5, 3e-5, 2e-5 No Italian samples were seen during training, making this a case of zero-shot cross-lingual transfer.\n98. The scores are reported as the macro-average of the F1scores for \'favor\' and for \'against\'.\n99. The grid search was repeated independently for every variant that we test in the following subsections.\n100. Furthermore, the standard recommendations for fine-tuning BERT were used: Adam with β 1 = 0.9 and β 2 = 0.999; an L2 weight decay of 0.01; a learning rate warmup over the first 10% of the steps; and a linear decay of the learning rate.\n101. A dropout probability of 0.1 was set on all layers.\n102. Results Table3shows the results for the crosslingual setting.\n103. M-BERT performs consistently better than the previous baselines.\n104. Even the zeroshot performance in Italian, while significantly lower than the supervised scores, is much better than the target-wise majority class baseline.\n105. Results for the cross-target setting are given in Table4.\n106. Similar to the cross-lingual setting, model performance drops in the cross-target setting, but M-BERT remains the strongest baseline and easily surpasses the majority class baselines.\n107. Furthermore, the cross-question score of M-BERT is slightly lower than the cross-topic score.\n108. The default setup preserves horizontal language consistency in that the language of the questions always corresponds to the language of the comments.\n109. For example, the Italian test instances are combined with the Italian version of the questions, even though during training the model has only ever seen the German and French version of them.\n110. An alternative concept is vertical language consistency, whereby the questions are consistently presented in one language, regardless of the comment.\n111. To test whether horizontal or vertical consistency is more helpful, we train and evaluate M-BERT on a dataset variant where all questions are in their English version.\n112. We chose English as a lingua franca because it had the largest share of data during the pre-training of M-BERT.\n113. Cross-topic Results are shown in Table5.\n114. While the effect is negligible in most settings, cross-lingual performance increases when all questions are in English.\n115. In order to rule out that only the questions or only the comments are necessary to optimally solve the task, we conduct some additional experiments: • Only use a single segment containing the comment, removing the questions from the training and test data (missing questions).\n116. • Only use the question and remove the comment (missing comments).\n117. In both cases the performance decreases across all evaluation settings (Table5).\n118. The loss in performance is much higher when comments are missing, indicating that the comments contain the most important information about stance.\n119. As can be expected, the score achieved without comments is only slightly different from the target-wise majority class baseline.\n120. But there is also a loss in performance when the questions are missing, which underlines the importance of pairing both pieces of text.\n121. The effect of missing questions is especially strong in the supervised and cross-lingual settings.\n122. To illustrate this, we provide in TableA8some examples of comments that occur with multiple different targets in the training set.\n123. Those examples can explain why the target can be essential for disambiguating a stance detection problem.\n124. On the other hand, the effect of omitting the questions is less pronounced in the cross-target settings.\n125. The above single-segment experiments tell us that both the comment and the question provide crucial information.\n126. But it is possible that the M-BERT model, even though trained on both segments, mainly looks at a single segment at test time.\n127. To rule this out, we probe the model with randomized data at test time: • Test the model on versions of the test sets where the comments remain in place but the questions are shuffled randomly (random questions).\n128. We make sure that the random questions come from the same test set and language as the original questions.\n129. • Keep the questions in place and randomize the comments (random comments).\n130. Again we shuffle the comments only within test set boundaries.\n131. The results in Table5show that the performance of the model decreases in both cases, confirming that it learns to take into account both segments.\n132. 4.6 How Important are Spelled-Out Targets?\n133. Finally we test whether the target really needs to be represented by natural language (e.g."Do you support X?").\n134. An alternative is to represent the target with a trainable embedding instead.\n135. In order to fit target embeddings smoothly into our architecture, we represent each target type with a different reserved symbol from the M-BERT vocabulary.\n136. Segment A is then set to this symbol instead of a natural language question.\n137. The results for this experiment are listed in the bottom row of Table5.\n138. An M-BERT model that learns target embeddings instead of encoding a question performs clearly worse in the supervised and cross-lingual settings.\n139. From this we conclude that spelled-out natural language questions provide important linguistic detail that can help in stance detection.\n\n请严格按照JSON格式返回：{"labels": [0,1,0,...]}，不要包含任何其他文字。'}], 'model': 'qwen-plus', 'response_format': {'type': 'json_object'}}}
2025-10-22 09:42:02,637 - DEBUG - Sending HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
2025-10-22 09:42:02,637 - DEBUG - connect_tcp.started host='dashscope.aliyuncs.com' port=443 local_address=None timeout=5.0 socket_options=None
2025-10-22 09:42:02,640 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f58ee7040d0>
2025-10-22 09:42:02,640 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x7f5844cab640> server_hostname='dashscope.aliyuncs.com' timeout=5.0
2025-10-22 09:42:02,726 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f58ee704190>
2025-10-22 09:42:02,726 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-10-22 09:42:02,727 - DEBUG - send_request_headers.complete
2025-10-22 09:42:02,727 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-10-22 09:42:02,728 - DEBUG - send_request_body.complete
2025-10-22 09:42:02,728 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-10-22 09:42:06,427 - DEBUG - receive_response_headers.failed exception=KeyboardInterrupt()
2025-10-22 09:42:06,427 - DEBUG - response_closed.started
2025-10-22 09:42:06,427 - DEBUG - response_closed.complete
